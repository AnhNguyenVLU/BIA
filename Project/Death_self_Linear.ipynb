{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z98MBnlEUsCU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load data and convert to PyTorch tensor\n",
        "data = pd.read_excel('Death-self-cl.xlsx')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "fvG8dBGSWpC9",
        "outputId": "15a2ff2f-fd94-447b-ee2f-044713cf6fe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Vietnam</th>\n",
              "      <th>Thailand</th>\n",
              "      <th>Laos</th>\n",
              "      <th>Cambodia</th>\n",
              "      <th>Myanmar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1990</td>\n",
              "      <td>6946</td>\n",
              "      <td>6986</td>\n",
              "      <td>583</td>\n",
              "      <td>689</td>\n",
              "      <td>2987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1991</td>\n",
              "      <td>6993</td>\n",
              "      <td>7411</td>\n",
              "      <td>593</td>\n",
              "      <td>709</td>\n",
              "      <td>2929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1992</td>\n",
              "      <td>7021</td>\n",
              "      <td>7755</td>\n",
              "      <td>603</td>\n",
              "      <td>739</td>\n",
              "      <td>2899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1993</td>\n",
              "      <td>7046</td>\n",
              "      <td>8314</td>\n",
              "      <td>611</td>\n",
              "      <td>774</td>\n",
              "      <td>2892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1994</td>\n",
              "      <td>7054</td>\n",
              "      <td>9073</td>\n",
              "      <td>615</td>\n",
              "      <td>803</td>\n",
              "      <td>2951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1995</td>\n",
              "      <td>7081</td>\n",
              "      <td>9942</td>\n",
              "      <td>617</td>\n",
              "      <td>820</td>\n",
              "      <td>3113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1996</td>\n",
              "      <td>7067</td>\n",
              "      <td>10627</td>\n",
              "      <td>609</td>\n",
              "      <td>825</td>\n",
              "      <td>3079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1997</td>\n",
              "      <td>7043</td>\n",
              "      <td>9819</td>\n",
              "      <td>601</td>\n",
              "      <td>828</td>\n",
              "      <td>3034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1998</td>\n",
              "      <td>7026</td>\n",
              "      <td>9546</td>\n",
              "      <td>593</td>\n",
              "      <td>832</td>\n",
              "      <td>2929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1999</td>\n",
              "      <td>6992</td>\n",
              "      <td>9885</td>\n",
              "      <td>580</td>\n",
              "      <td>829</td>\n",
              "      <td>3162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2000</td>\n",
              "      <td>6834</td>\n",
              "      <td>9498</td>\n",
              "      <td>557</td>\n",
              "      <td>819</td>\n",
              "      <td>2933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2001</td>\n",
              "      <td>6832</td>\n",
              "      <td>8898</td>\n",
              "      <td>551</td>\n",
              "      <td>820</td>\n",
              "      <td>2848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2002</td>\n",
              "      <td>6863</td>\n",
              "      <td>8824</td>\n",
              "      <td>534</td>\n",
              "      <td>818</td>\n",
              "      <td>2831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2003</td>\n",
              "      <td>6909</td>\n",
              "      <td>8544</td>\n",
              "      <td>523</td>\n",
              "      <td>812</td>\n",
              "      <td>2909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2004</td>\n",
              "      <td>6955</td>\n",
              "      <td>7752</td>\n",
              "      <td>513</td>\n",
              "      <td>814</td>\n",
              "      <td>2869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2005</td>\n",
              "      <td>6992</td>\n",
              "      <td>7023</td>\n",
              "      <td>508</td>\n",
              "      <td>811</td>\n",
              "      <td>2811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2006</td>\n",
              "      <td>6976</td>\n",
              "      <td>6765</td>\n",
              "      <td>501</td>\n",
              "      <td>801</td>\n",
              "      <td>2751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2007</td>\n",
              "      <td>7010</td>\n",
              "      <td>6482</td>\n",
              "      <td>494</td>\n",
              "      <td>804</td>\n",
              "      <td>2694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2008</td>\n",
              "      <td>7036</td>\n",
              "      <td>6295</td>\n",
              "      <td>490</td>\n",
              "      <td>800</td>\n",
              "      <td>2748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2009</td>\n",
              "      <td>7092</td>\n",
              "      <td>6198</td>\n",
              "      <td>485</td>\n",
              "      <td>798</td>\n",
              "      <td>2774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2010</td>\n",
              "      <td>7107</td>\n",
              "      <td>6213</td>\n",
              "      <td>475</td>\n",
              "      <td>804</td>\n",
              "      <td>2717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2011</td>\n",
              "      <td>7121</td>\n",
              "      <td>6074</td>\n",
              "      <td>466</td>\n",
              "      <td>802</td>\n",
              "      <td>2699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2012</td>\n",
              "      <td>7189</td>\n",
              "      <td>5961</td>\n",
              "      <td>457</td>\n",
              "      <td>803</td>\n",
              "      <td>2687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2013</td>\n",
              "      <td>7294</td>\n",
              "      <td>5761</td>\n",
              "      <td>458</td>\n",
              "      <td>804</td>\n",
              "      <td>2624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2014</td>\n",
              "      <td>7417</td>\n",
              "      <td>5779</td>\n",
              "      <td>462</td>\n",
              "      <td>808</td>\n",
              "      <td>2717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2015</td>\n",
              "      <td>7486</td>\n",
              "      <td>5877</td>\n",
              "      <td>465</td>\n",
              "      <td>817</td>\n",
              "      <td>2785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2016</td>\n",
              "      <td>7553</td>\n",
              "      <td>6058</td>\n",
              "      <td>469</td>\n",
              "      <td>827</td>\n",
              "      <td>2885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2017</td>\n",
              "      <td>7617</td>\n",
              "      <td>6126</td>\n",
              "      <td>470</td>\n",
              "      <td>831</td>\n",
              "      <td>2949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2018</td>\n",
              "      <td>7631</td>\n",
              "      <td>6195</td>\n",
              "      <td>466</td>\n",
              "      <td>835</td>\n",
              "      <td>2991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2019</td>\n",
              "      <td>7614</td>\n",
              "      <td>6154</td>\n",
              "      <td>458</td>\n",
              "      <td>830</td>\n",
              "      <td>3103</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Year  Vietnam  Thailand  Laos  Cambodia  Myanmar\n",
              "0   1990     6946      6986   583       689     2987\n",
              "1   1991     6993      7411   593       709     2929\n",
              "2   1992     7021      7755   603       739     2899\n",
              "3   1993     7046      8314   611       774     2892\n",
              "4   1994     7054      9073   615       803     2951\n",
              "5   1995     7081      9942   617       820     3113\n",
              "6   1996     7067     10627   609       825     3079\n",
              "7   1997     7043      9819   601       828     3034\n",
              "8   1998     7026      9546   593       832     2929\n",
              "9   1999     6992      9885   580       829     3162\n",
              "10  2000     6834      9498   557       819     2933\n",
              "11  2001     6832      8898   551       820     2848\n",
              "12  2002     6863      8824   534       818     2831\n",
              "13  2003     6909      8544   523       812     2909\n",
              "14  2004     6955      7752   513       814     2869\n",
              "15  2005     6992      7023   508       811     2811\n",
              "16  2006     6976      6765   501       801     2751\n",
              "17  2007     7010      6482   494       804     2694\n",
              "18  2008     7036      6295   490       800     2748\n",
              "19  2009     7092      6198   485       798     2774\n",
              "20  2010     7107      6213   475       804     2717\n",
              "21  2011     7121      6074   466       802     2699\n",
              "22  2012     7189      5961   457       803     2687\n",
              "23  2013     7294      5761   458       804     2624\n",
              "24  2014     7417      5779   462       808     2717\n",
              "25  2015     7486      5877   465       817     2785\n",
              "26  2016     7553      6058   469       827     2885\n",
              "27  2017     7617      6126   470       831     2949\n",
              "28  2018     7631      6195   466       835     2991\n",
              "29  2019     7614      6154   458       830     3103"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sDa6UXB_5J3z"
      },
      "outputs": [],
      "source": [
        "columns = ['Year', 'Vietnam', 'Thailand', 'Laos', 'Cambodia', 'Myanmar']\n",
        "data = data[columns]\n",
        "\n",
        "# Define the input and output columns\n",
        "input_cols = ['Year']\n",
        "output_cols = ['Vietnam', 'Thailand', 'Laos', 'Cambodia', 'Myanmar']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = data[data['Year'] <= 2014]\n",
        "test_data = data[ data['Year'] >= 2015]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f3ZXO7axb6z1",
        "outputId": "e3b8d64b-82e0-4211-e81f-6f8129813a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20000], Training Loss: 1.2276\n",
            "Epoch [2/20000], Training Loss: 1.2661\n",
            "Epoch [3/20000], Training Loss: 1.1643\n",
            "Epoch [4/20000], Training Loss: 1.2211\n",
            "Epoch [5/20000], Training Loss: 1.1801\n",
            "Epoch [6/20000], Training Loss: 1.2292\n",
            "Epoch [7/20000], Training Loss: 1.1995\n",
            "Epoch [8/20000], Training Loss: 1.1555\n",
            "Epoch [9/20000], Training Loss: 1.1393\n",
            "Epoch [10/20000], Training Loss: 1.1781\n",
            "Epoch [11/20000], Training Loss: 1.0801\n",
            "Epoch [12/20000], Training Loss: 1.0971\n",
            "Epoch [13/20000], Training Loss: 1.0874\n",
            "Epoch [14/20000], Training Loss: 1.1619\n",
            "Epoch [15/20000], Training Loss: 1.0415\n",
            "Epoch [16/20000], Training Loss: 1.0397\n",
            "Epoch [17/20000], Training Loss: 1.1206\n",
            "Epoch [18/20000], Training Loss: 1.0835\n",
            "Epoch [19/20000], Training Loss: 1.0679\n",
            "Epoch [20/20000], Training Loss: 1.0666\n",
            "Epoch [21/20000], Training Loss: 1.0591\n",
            "Epoch [22/20000], Training Loss: 1.0127\n",
            "Epoch [23/20000], Training Loss: 0.9560\n",
            "Epoch [24/20000], Training Loss: 0.9990\n",
            "Epoch [25/20000], Training Loss: 1.0011\n",
            "Epoch [26/20000], Training Loss: 1.0200\n",
            "Epoch [27/20000], Training Loss: 0.9719\n",
            "Epoch [28/20000], Training Loss: 0.9710\n",
            "Epoch [29/20000], Training Loss: 0.9667\n",
            "Epoch [30/20000], Training Loss: 0.9730\n",
            "Epoch [31/20000], Training Loss: 0.9176\n",
            "Epoch [32/20000], Training Loss: 0.9912\n",
            "Epoch [33/20000], Training Loss: 0.9284\n",
            "Epoch [34/20000], Training Loss: 0.9155\n",
            "Epoch [35/20000], Training Loss: 0.8955\n",
            "Epoch [36/20000], Training Loss: 0.8958\n",
            "Epoch [37/20000], Training Loss: 0.9657\n",
            "Epoch [38/20000], Training Loss: 0.9091\n",
            "Epoch [39/20000], Training Loss: 0.9016\n",
            "Epoch [40/20000], Training Loss: 0.8459\n",
            "Epoch [41/20000], Training Loss: 0.8951\n",
            "Epoch [42/20000], Training Loss: 0.8719\n",
            "Epoch [43/20000], Training Loss: 0.8773\n",
            "Epoch [44/20000], Training Loss: 0.9018\n",
            "Epoch [45/20000], Training Loss: 0.9258\n",
            "Epoch [46/20000], Training Loss: 0.8757\n",
            "Epoch [47/20000], Training Loss: 0.8096\n",
            "Epoch [48/20000], Training Loss: 0.8618\n",
            "Epoch [49/20000], Training Loss: 0.8606\n",
            "Epoch [50/20000], Training Loss: 0.7976\n",
            "Epoch [51/20000], Training Loss: 0.8057\n",
            "Epoch [52/20000], Training Loss: 0.8552\n",
            "Epoch [53/20000], Training Loss: 0.8491\n",
            "Epoch [54/20000], Training Loss: 0.8438\n",
            "Epoch [55/20000], Training Loss: 0.8619\n",
            "Epoch [56/20000], Training Loss: 0.8205\n",
            "Epoch [57/20000], Training Loss: 0.8172\n",
            "Epoch [58/20000], Training Loss: 0.7782\n",
            "Epoch [59/20000], Training Loss: 0.8207\n",
            "Epoch [60/20000], Training Loss: 0.7656\n",
            "Epoch [61/20000], Training Loss: 0.7634\n",
            "Epoch [62/20000], Training Loss: 0.7942\n",
            "Epoch [63/20000], Training Loss: 0.7592\n",
            "Epoch [64/20000], Training Loss: 0.7677\n",
            "Epoch [65/20000], Training Loss: 0.7704\n",
            "Epoch [66/20000], Training Loss: 0.7585\n",
            "Epoch [67/20000], Training Loss: 0.8138\n",
            "Epoch [68/20000], Training Loss: 0.7345\n",
            "Epoch [69/20000], Training Loss: 0.7255\n",
            "Epoch [70/20000], Training Loss: 0.7450\n",
            "Epoch [71/20000], Training Loss: 0.7484\n",
            "Epoch [72/20000], Training Loss: 0.7432\n",
            "Epoch [73/20000], Training Loss: 0.7335\n",
            "Epoch [74/20000], Training Loss: 0.7423\n",
            "Epoch [75/20000], Training Loss: 0.6930\n",
            "Epoch [76/20000], Training Loss: 0.6776\n",
            "Epoch [77/20000], Training Loss: 0.6989\n",
            "Epoch [78/20000], Training Loss: 0.6898\n",
            "Epoch [79/20000], Training Loss: 0.6980\n",
            "Epoch [80/20000], Training Loss: 0.6666\n",
            "Epoch [81/20000], Training Loss: 0.6654\n",
            "Epoch [82/20000], Training Loss: 0.6578\n",
            "Epoch [83/20000], Training Loss: 0.7160\n",
            "Epoch [84/20000], Training Loss: 0.7050\n",
            "Epoch [85/20000], Training Loss: 0.6597\n",
            "Epoch [86/20000], Training Loss: 0.6936\n",
            "Epoch [87/20000], Training Loss: 0.6516\n",
            "Epoch [88/20000], Training Loss: 0.6844\n",
            "Epoch [89/20000], Training Loss: 0.6414\n",
            "Epoch [90/20000], Training Loss: 0.7024\n",
            "Epoch [91/20000], Training Loss: 0.6451\n",
            "Epoch [92/20000], Training Loss: 0.6800\n",
            "Epoch [93/20000], Training Loss: 0.6670\n",
            "Epoch [94/20000], Training Loss: 0.6552\n",
            "Epoch [95/20000], Training Loss: 0.6894\n",
            "Epoch [96/20000], Training Loss: 0.6702\n",
            "Epoch [97/20000], Training Loss: 0.6228\n",
            "Epoch [98/20000], Training Loss: 0.6167\n",
            "Epoch [99/20000], Training Loss: 0.6597\n",
            "Epoch [100/20000], Training Loss: 0.6363\n",
            "Epoch [101/20000], Training Loss: 0.6262\n",
            "Epoch [102/20000], Training Loss: 0.6104\n",
            "Epoch [103/20000], Training Loss: 0.6427\n",
            "Epoch [104/20000], Training Loss: 0.6375\n",
            "Epoch [105/20000], Training Loss: 0.6318\n",
            "Epoch [106/20000], Training Loss: 0.6214\n",
            "Epoch [107/20000], Training Loss: 0.6316\n",
            "Epoch [108/20000], Training Loss: 0.6175\n",
            "Epoch [109/20000], Training Loss: 0.6461\n",
            "Epoch [110/20000], Training Loss: 0.6194\n",
            "Epoch [111/20000], Training Loss: 0.6130\n",
            "Epoch [112/20000], Training Loss: 0.6380\n",
            "Epoch [113/20000], Training Loss: 0.6050\n",
            "Epoch [114/20000], Training Loss: 0.6167\n",
            "Epoch [115/20000], Training Loss: 0.6242\n",
            "Epoch [116/20000], Training Loss: 0.6082\n",
            "Epoch [117/20000], Training Loss: 0.5937\n",
            "Epoch [118/20000], Training Loss: 0.5978\n",
            "Epoch [119/20000], Training Loss: 0.6309\n",
            "Epoch [120/20000], Training Loss: 0.6106\n",
            "Epoch [121/20000], Training Loss: 0.5998\n",
            "Epoch [122/20000], Training Loss: 0.5907\n",
            "Epoch [123/20000], Training Loss: 0.5810\n",
            "Epoch [124/20000], Training Loss: 0.5895\n",
            "Epoch [125/20000], Training Loss: 0.5842\n",
            "Epoch [126/20000], Training Loss: 0.5894\n",
            "Epoch [127/20000], Training Loss: 0.5984\n",
            "Epoch [128/20000], Training Loss: 0.5854\n",
            "Epoch [129/20000], Training Loss: 0.5613\n",
            "Epoch [130/20000], Training Loss: 0.5725\n",
            "Epoch [131/20000], Training Loss: 0.5823\n",
            "Epoch [132/20000], Training Loss: 0.5872\n",
            "Epoch [133/20000], Training Loss: 0.5586\n",
            "Epoch [134/20000], Training Loss: 0.5664\n",
            "Epoch [135/20000], Training Loss: 0.5841\n",
            "Epoch [136/20000], Training Loss: 0.5756\n",
            "Epoch [137/20000], Training Loss: 0.5865\n",
            "Epoch [138/20000], Training Loss: 0.5801\n",
            "Epoch [139/20000], Training Loss: 0.5515\n",
            "Epoch [140/20000], Training Loss: 0.5569\n",
            "Epoch [141/20000], Training Loss: 0.5998\n",
            "Epoch [142/20000], Training Loss: 0.5603\n",
            "Epoch [143/20000], Training Loss: 0.5758\n",
            "Epoch [144/20000], Training Loss: 0.5451\n",
            "Epoch [145/20000], Training Loss: 0.5282\n",
            "Epoch [146/20000], Training Loss: 0.5935\n",
            "Epoch [147/20000], Training Loss: 0.5667\n",
            "Epoch [148/20000], Training Loss: 0.5974\n",
            "Epoch [149/20000], Training Loss: 0.5553\n",
            "Epoch [150/20000], Training Loss: 0.5778\n",
            "Epoch [151/20000], Training Loss: 0.5668\n",
            "Epoch [152/20000], Training Loss: 0.5684\n",
            "Epoch [153/20000], Training Loss: 0.5654\n",
            "Epoch [154/20000], Training Loss: 0.5701\n",
            "Epoch [155/20000], Training Loss: 0.5487\n",
            "Epoch [156/20000], Training Loss: 0.5699\n",
            "Epoch [157/20000], Training Loss: 0.5233\n",
            "Epoch [158/20000], Training Loss: 0.5229\n",
            "Epoch [159/20000], Training Loss: 0.5673\n",
            "Epoch [160/20000], Training Loss: 0.5225\n",
            "Epoch [161/20000], Training Loss: 0.5495\n",
            "Epoch [162/20000], Training Loss: 0.5198\n",
            "Epoch [163/20000], Training Loss: 0.5952\n",
            "Epoch [164/20000], Training Loss: 0.5650\n",
            "Epoch [165/20000], Training Loss: 0.5321\n",
            "Epoch [166/20000], Training Loss: 0.5611\n",
            "Epoch [167/20000], Training Loss: 0.5671\n",
            "Epoch [168/20000], Training Loss: 0.5310\n",
            "Epoch [169/20000], Training Loss: 0.5852\n",
            "Epoch [170/20000], Training Loss: 0.5560\n",
            "Epoch [171/20000], Training Loss: 0.5214\n",
            "Epoch [172/20000], Training Loss: 0.5264\n",
            "Epoch [173/20000], Training Loss: 0.5482\n",
            "Epoch [174/20000], Training Loss: 0.5434\n",
            "Epoch [175/20000], Training Loss: 0.5505\n",
            "Epoch [176/20000], Training Loss: 0.5244\n",
            "Epoch [177/20000], Training Loss: 0.5499\n",
            "Epoch [178/20000], Training Loss: 0.5538\n",
            "Epoch [179/20000], Training Loss: 0.5746\n",
            "Epoch [180/20000], Training Loss: 0.5270\n",
            "Epoch [181/20000], Training Loss: 0.5220\n",
            "Epoch [182/20000], Training Loss: 0.5429\n",
            "Epoch [183/20000], Training Loss: 0.5319\n",
            "Epoch [184/20000], Training Loss: 0.5210\n",
            "Epoch [185/20000], Training Loss: 0.5286\n",
            "Epoch [186/20000], Training Loss: 0.5322\n",
            "Epoch [187/20000], Training Loss: 0.5047\n",
            "Epoch [188/20000], Training Loss: 0.5457\n",
            "Epoch [189/20000], Training Loss: 0.5127\n",
            "Epoch [190/20000], Training Loss: 0.5576\n",
            "Epoch [191/20000], Training Loss: 0.5159\n",
            "Epoch [192/20000], Training Loss: 0.4990\n",
            "Epoch [193/20000], Training Loss: 0.5456\n",
            "Epoch [194/20000], Training Loss: 0.5242\n",
            "Epoch [195/20000], Training Loss: 0.4862\n",
            "Epoch [196/20000], Training Loss: 0.5114\n",
            "Epoch [197/20000], Training Loss: 0.5462\n",
            "Epoch [198/20000], Training Loss: 0.5269\n",
            "Epoch [199/20000], Training Loss: 0.4913\n",
            "Epoch [200/20000], Training Loss: 0.4853\n",
            "Epoch [201/20000], Training Loss: 0.4873\n",
            "Epoch [202/20000], Training Loss: 0.5060\n",
            "Epoch [203/20000], Training Loss: 0.4940\n",
            "Epoch [204/20000], Training Loss: 0.5093\n",
            "Epoch [205/20000], Training Loss: 0.5327\n",
            "Epoch [206/20000], Training Loss: 0.5052\n",
            "Epoch [207/20000], Training Loss: 0.5196\n",
            "Epoch [208/20000], Training Loss: 0.5191\n",
            "Epoch [209/20000], Training Loss: 0.5382\n",
            "Epoch [210/20000], Training Loss: 0.5101\n",
            "Epoch [211/20000], Training Loss: 0.5186\n",
            "Epoch [212/20000], Training Loss: 0.4805\n",
            "Epoch [213/20000], Training Loss: 0.4829\n",
            "Epoch [214/20000], Training Loss: 0.4815\n",
            "Epoch [215/20000], Training Loss: 0.5172\n",
            "Epoch [216/20000], Training Loss: 0.5178\n",
            "Epoch [217/20000], Training Loss: 0.5167\n",
            "Epoch [218/20000], Training Loss: 0.5191\n",
            "Epoch [219/20000], Training Loss: 0.5273\n",
            "Epoch [220/20000], Training Loss: 0.5268\n",
            "Epoch [221/20000], Training Loss: 0.5270\n",
            "Epoch [222/20000], Training Loss: 0.4738\n",
            "Epoch [223/20000], Training Loss: 0.4954\n",
            "Epoch [224/20000], Training Loss: 0.5066\n",
            "Epoch [225/20000], Training Loss: 0.4855\n",
            "Epoch [226/20000], Training Loss: 0.4942\n",
            "Epoch [227/20000], Training Loss: 0.4992\n",
            "Epoch [228/20000], Training Loss: 0.4944\n",
            "Epoch [229/20000], Training Loss: 0.4979\n",
            "Epoch [230/20000], Training Loss: 0.5344\n",
            "Epoch [231/20000], Training Loss: 0.5113\n",
            "Epoch [232/20000], Training Loss: 0.5068\n",
            "Epoch [233/20000], Training Loss: 0.4770\n",
            "Epoch [234/20000], Training Loss: 0.5253\n",
            "Epoch [235/20000], Training Loss: 0.5038\n",
            "Epoch [236/20000], Training Loss: 0.5021\n",
            "Epoch [237/20000], Training Loss: 0.4982\n",
            "Epoch [238/20000], Training Loss: 0.5164\n",
            "Epoch [239/20000], Training Loss: 0.5095\n",
            "Epoch [240/20000], Training Loss: 0.5056\n",
            "Epoch [241/20000], Training Loss: 0.5019\n",
            "Epoch [242/20000], Training Loss: 0.5328\n",
            "Epoch [243/20000], Training Loss: 0.5019\n",
            "Epoch [244/20000], Training Loss: 0.4886\n",
            "Epoch [245/20000], Training Loss: 0.5302\n",
            "Epoch [246/20000], Training Loss: 0.5263\n",
            "Epoch [247/20000], Training Loss: 0.4885\n",
            "Epoch [248/20000], Training Loss: 0.5127\n",
            "Epoch [249/20000], Training Loss: 0.4904\n",
            "Epoch [250/20000], Training Loss: 0.4750\n",
            "Epoch [251/20000], Training Loss: 0.4757\n",
            "Epoch [252/20000], Training Loss: 0.5035\n",
            "Epoch [253/20000], Training Loss: 0.5445\n",
            "Epoch [254/20000], Training Loss: 0.4751\n",
            "Epoch [255/20000], Training Loss: 0.5239\n",
            "Epoch [256/20000], Training Loss: 0.4989\n",
            "Epoch [257/20000], Training Loss: 0.5173\n",
            "Epoch [258/20000], Training Loss: 0.5115\n",
            "Epoch [259/20000], Training Loss: 0.4916\n",
            "Epoch [260/20000], Training Loss: 0.5096\n",
            "Epoch [261/20000], Training Loss: 0.5352\n",
            "Epoch [262/20000], Training Loss: 0.4954\n",
            "Epoch [263/20000], Training Loss: 0.5248\n",
            "Epoch [264/20000], Training Loss: 0.4968\n",
            "Epoch [265/20000], Training Loss: 0.5369\n",
            "Epoch [266/20000], Training Loss: 0.4800\n",
            "Epoch [267/20000], Training Loss: 0.5399\n",
            "Epoch [268/20000], Training Loss: 0.4886\n",
            "Epoch [269/20000], Training Loss: 0.5098\n",
            "Epoch [270/20000], Training Loss: 0.4738\n",
            "Epoch [271/20000], Training Loss: 0.5210\n",
            "Epoch [272/20000], Training Loss: 0.5281\n",
            "Epoch [273/20000], Training Loss: 0.5197\n",
            "Epoch [274/20000], Training Loss: 0.5472\n",
            "Epoch [275/20000], Training Loss: 0.4817\n",
            "Epoch [276/20000], Training Loss: 0.5109\n",
            "Epoch [277/20000], Training Loss: 0.5382\n",
            "Epoch [278/20000], Training Loss: 0.5428\n",
            "Epoch [279/20000], Training Loss: 0.5179\n",
            "Epoch [280/20000], Training Loss: 0.4746\n",
            "Epoch [281/20000], Training Loss: 0.4923\n",
            "Epoch [282/20000], Training Loss: 0.5122\n",
            "Epoch [283/20000], Training Loss: 0.5182\n",
            "Epoch [284/20000], Training Loss: 0.4697\n",
            "Epoch [285/20000], Training Loss: 0.4828\n",
            "Epoch [286/20000], Training Loss: 0.4972\n",
            "Epoch [287/20000], Training Loss: 0.5094\n",
            "Epoch [288/20000], Training Loss: 0.5560\n",
            "Epoch [289/20000], Training Loss: 0.5141\n",
            "Epoch [290/20000], Training Loss: 0.4579\n",
            "Epoch [291/20000], Training Loss: 0.4781\n",
            "Epoch [292/20000], Training Loss: 0.4796\n",
            "Epoch [293/20000], Training Loss: 0.4832\n",
            "Epoch [294/20000], Training Loss: 0.4968\n",
            "Epoch [295/20000], Training Loss: 0.5269\n",
            "Epoch [296/20000], Training Loss: 0.4902\n",
            "Epoch [297/20000], Training Loss: 0.5184\n",
            "Epoch [298/20000], Training Loss: 0.5052\n",
            "Epoch [299/20000], Training Loss: 0.4912\n",
            "Epoch [300/20000], Training Loss: 0.4879\n",
            "Epoch [301/20000], Training Loss: 0.4810\n",
            "Epoch [302/20000], Training Loss: 0.4919\n",
            "Epoch [303/20000], Training Loss: 0.4841\n",
            "Epoch [304/20000], Training Loss: 0.4502\n",
            "Epoch [305/20000], Training Loss: 0.5059\n",
            "Epoch [306/20000], Training Loss: 0.4798\n",
            "Epoch [307/20000], Training Loss: 0.4928\n",
            "Epoch [308/20000], Training Loss: 0.4869\n",
            "Epoch [309/20000], Training Loss: 0.4885\n",
            "Epoch [310/20000], Training Loss: 0.5084\n",
            "Epoch [311/20000], Training Loss: 0.4820\n",
            "Epoch [312/20000], Training Loss: 0.5077\n",
            "Epoch [313/20000], Training Loss: 0.4546\n",
            "Epoch [314/20000], Training Loss: 0.4948\n",
            "Epoch [315/20000], Training Loss: 0.4786\n",
            "Epoch [316/20000], Training Loss: 0.4927\n",
            "Epoch [317/20000], Training Loss: 0.4867\n",
            "Epoch [318/20000], Training Loss: 0.4766\n",
            "Epoch [319/20000], Training Loss: 0.4785\n",
            "Epoch [320/20000], Training Loss: 0.5148\n",
            "Epoch [321/20000], Training Loss: 0.4606\n",
            "Epoch [322/20000], Training Loss: 0.4802\n",
            "Epoch [323/20000], Training Loss: 0.4898\n",
            "Epoch [324/20000], Training Loss: 0.5138\n",
            "Epoch [325/20000], Training Loss: 0.5287\n",
            "Epoch [326/20000], Training Loss: 0.4866\n",
            "Epoch [327/20000], Training Loss: 0.5110\n",
            "Epoch [328/20000], Training Loss: 0.4393\n",
            "Epoch [329/20000], Training Loss: 0.5142\n",
            "Epoch [330/20000], Training Loss: 0.5195\n",
            "Epoch [331/20000], Training Loss: 0.4922\n",
            "Epoch [332/20000], Training Loss: 0.5080\n",
            "Epoch [333/20000], Training Loss: 0.4581\n",
            "Epoch [334/20000], Training Loss: 0.4640\n",
            "Epoch [335/20000], Training Loss: 0.5116\n",
            "Epoch [336/20000], Training Loss: 0.4808\n",
            "Epoch [337/20000], Training Loss: 0.4799\n",
            "Epoch [338/20000], Training Loss: 0.5268\n",
            "Epoch [339/20000], Training Loss: 0.4761\n",
            "Epoch [340/20000], Training Loss: 0.4916\n",
            "Epoch [341/20000], Training Loss: 0.4629\n",
            "Epoch [342/20000], Training Loss: 0.5012\n",
            "Epoch [343/20000], Training Loss: 0.4724\n",
            "Epoch [344/20000], Training Loss: 0.5207\n",
            "Epoch [345/20000], Training Loss: 0.5361\n",
            "Epoch [346/20000], Training Loss: 0.4739\n",
            "Epoch [347/20000], Training Loss: 0.5488\n",
            "Epoch [348/20000], Training Loss: 0.5262\n",
            "Epoch [349/20000], Training Loss: 0.4508\n",
            "Epoch [350/20000], Training Loss: 0.4619\n",
            "Epoch [351/20000], Training Loss: 0.5028\n",
            "Epoch [352/20000], Training Loss: 0.5187\n",
            "Epoch [353/20000], Training Loss: 0.4971\n",
            "Epoch [354/20000], Training Loss: 0.4930\n",
            "Epoch [355/20000], Training Loss: 0.4435\n",
            "Epoch [356/20000], Training Loss: 0.4918\n",
            "Epoch [357/20000], Training Loss: 0.4778\n",
            "Epoch [358/20000], Training Loss: 0.4733\n",
            "Epoch [359/20000], Training Loss: 0.4956\n",
            "Epoch [360/20000], Training Loss: 0.5124\n",
            "Epoch [361/20000], Training Loss: 0.5146\n",
            "Epoch [362/20000], Training Loss: 0.4809\n",
            "Epoch [363/20000], Training Loss: 0.4854\n",
            "Epoch [364/20000], Training Loss: 0.5106\n",
            "Epoch [365/20000], Training Loss: 0.4707\n",
            "Epoch [366/20000], Training Loss: 0.5239\n",
            "Epoch [367/20000], Training Loss: 0.5170\n",
            "Epoch [368/20000], Training Loss: 0.4772\n",
            "Epoch [369/20000], Training Loss: 0.4523\n",
            "Epoch [370/20000], Training Loss: 0.4782\n",
            "Epoch [371/20000], Training Loss: 0.4991\n",
            "Epoch [372/20000], Training Loss: 0.4779\n",
            "Epoch [373/20000], Training Loss: 0.4805\n",
            "Epoch [374/20000], Training Loss: 0.4953\n",
            "Epoch [375/20000], Training Loss: 0.4852\n",
            "Epoch [376/20000], Training Loss: 0.4941\n",
            "Epoch [377/20000], Training Loss: 0.4732\n",
            "Epoch [378/20000], Training Loss: 0.4945\n",
            "Epoch [379/20000], Training Loss: 0.4768\n",
            "Epoch [380/20000], Training Loss: 0.5139\n",
            "Epoch [381/20000], Training Loss: 0.4569\n",
            "Epoch [382/20000], Training Loss: 0.5406\n",
            "Epoch [383/20000], Training Loss: 0.4669\n",
            "Epoch [384/20000], Training Loss: 0.5118\n",
            "Epoch [385/20000], Training Loss: 0.4993\n",
            "Epoch [386/20000], Training Loss: 0.4727\n",
            "Epoch [387/20000], Training Loss: 0.4810\n",
            "Epoch [388/20000], Training Loss: 0.5158\n",
            "Epoch [389/20000], Training Loss: 0.4918\n",
            "Epoch [390/20000], Training Loss: 0.5009\n",
            "Epoch [391/20000], Training Loss: 0.5055\n",
            "Epoch [392/20000], Training Loss: 0.5121\n",
            "Epoch [393/20000], Training Loss: 0.5241\n",
            "Epoch [394/20000], Training Loss: 0.4686\n",
            "Epoch [395/20000], Training Loss: 0.4892\n",
            "Epoch [396/20000], Training Loss: 0.4718\n",
            "Epoch [397/20000], Training Loss: 0.5241\n",
            "Epoch [398/20000], Training Loss: 0.4905\n",
            "Epoch [399/20000], Training Loss: 0.4934\n",
            "Epoch [400/20000], Training Loss: 0.4764\n",
            "Epoch [401/20000], Training Loss: 0.4435\n",
            "Epoch [402/20000], Training Loss: 0.4731\n",
            "Epoch [403/20000], Training Loss: 0.4937\n",
            "Epoch [404/20000], Training Loss: 0.4579\n",
            "Epoch [405/20000], Training Loss: 0.5216\n",
            "Epoch [406/20000], Training Loss: 0.5061\n",
            "Epoch [407/20000], Training Loss: 0.5274\n",
            "Epoch [408/20000], Training Loss: 0.4606\n",
            "Epoch [409/20000], Training Loss: 0.4902\n",
            "Epoch [410/20000], Training Loss: 0.5315\n",
            "Epoch [411/20000], Training Loss: 0.4554\n",
            "Epoch [412/20000], Training Loss: 0.5128\n",
            "Epoch [413/20000], Training Loss: 0.5071\n",
            "Epoch [414/20000], Training Loss: 0.4653\n",
            "Epoch [415/20000], Training Loss: 0.5115\n",
            "Epoch [416/20000], Training Loss: 0.4974\n",
            "Epoch [417/20000], Training Loss: 0.4679\n",
            "Epoch [418/20000], Training Loss: 0.5167\n",
            "Epoch [419/20000], Training Loss: 0.4684\n",
            "Epoch [420/20000], Training Loss: 0.5325\n",
            "Epoch [421/20000], Training Loss: 0.4922\n",
            "Epoch [422/20000], Training Loss: 0.4798\n",
            "Epoch [423/20000], Training Loss: 0.5273\n",
            "Epoch [424/20000], Training Loss: 0.4910\n",
            "Epoch [425/20000], Training Loss: 0.5002\n",
            "Epoch [426/20000], Training Loss: 0.4560\n",
            "Epoch [427/20000], Training Loss: 0.4848\n",
            "Epoch [428/20000], Training Loss: 0.4796\n",
            "Epoch [429/20000], Training Loss: 0.4938\n",
            "Epoch [430/20000], Training Loss: 0.4889\n",
            "Epoch [431/20000], Training Loss: 0.4737\n",
            "Epoch [432/20000], Training Loss: 0.5285\n",
            "Epoch [433/20000], Training Loss: 0.4448\n",
            "Epoch [434/20000], Training Loss: 0.4660\n",
            "Epoch [435/20000], Training Loss: 0.4958\n",
            "Epoch [436/20000], Training Loss: 0.5475\n",
            "Epoch [437/20000], Training Loss: 0.4847\n",
            "Epoch [438/20000], Training Loss: 0.4472\n",
            "Epoch [439/20000], Training Loss: 0.4724\n",
            "Epoch [440/20000], Training Loss: 0.4913\n",
            "Epoch [441/20000], Training Loss: 0.4603\n",
            "Epoch [442/20000], Training Loss: 0.4561\n",
            "Epoch [443/20000], Training Loss: 0.4680\n",
            "Epoch [444/20000], Training Loss: 0.5206\n",
            "Epoch [445/20000], Training Loss: 0.4837\n",
            "Epoch [446/20000], Training Loss: 0.4990\n",
            "Epoch [447/20000], Training Loss: 0.4780\n",
            "Epoch [448/20000], Training Loss: 0.5175\n",
            "Epoch [449/20000], Training Loss: 0.4664\n",
            "Epoch [450/20000], Training Loss: 0.4963\n",
            "Epoch [451/20000], Training Loss: 0.4946\n",
            "Epoch [452/20000], Training Loss: 0.4727\n",
            "Epoch [453/20000], Training Loss: 0.5129\n",
            "Epoch [454/20000], Training Loss: 0.4794\n",
            "Epoch [455/20000], Training Loss: 0.4938\n",
            "Epoch [456/20000], Training Loss: 0.5386\n",
            "Epoch [457/20000], Training Loss: 0.4826\n",
            "Epoch [458/20000], Training Loss: 0.4953\n",
            "Epoch [459/20000], Training Loss: 0.4827\n",
            "Epoch [460/20000], Training Loss: 0.4953\n",
            "Epoch [461/20000], Training Loss: 0.4805\n",
            "Epoch [462/20000], Training Loss: 0.4783\n",
            "Epoch [463/20000], Training Loss: 0.4712\n",
            "Epoch [464/20000], Training Loss: 0.4385\n",
            "Epoch [465/20000], Training Loss: 0.4599\n",
            "Epoch [466/20000], Training Loss: 0.4804\n",
            "Epoch [467/20000], Training Loss: 0.4317\n",
            "Epoch [468/20000], Training Loss: 0.5032\n",
            "Epoch [469/20000], Training Loss: 0.4580\n",
            "Epoch [470/20000], Training Loss: 0.5267\n",
            "Epoch [471/20000], Training Loss: 0.4513\n",
            "Epoch [472/20000], Training Loss: 0.4656\n",
            "Epoch [473/20000], Training Loss: 0.4880\n",
            "Epoch [474/20000], Training Loss: 0.5061\n",
            "Epoch [475/20000], Training Loss: 0.4849\n",
            "Epoch [476/20000], Training Loss: 0.5428\n",
            "Epoch [477/20000], Training Loss: 0.4668\n",
            "Epoch [478/20000], Training Loss: 0.5102\n",
            "Epoch [479/20000], Training Loss: 0.5258\n",
            "Epoch [480/20000], Training Loss: 0.4951\n",
            "Epoch [481/20000], Training Loss: 0.4298\n",
            "Epoch [482/20000], Training Loss: 0.5088\n",
            "Epoch [483/20000], Training Loss: 0.4938\n",
            "Epoch [484/20000], Training Loss: 0.5123\n",
            "Epoch [485/20000], Training Loss: 0.4880\n",
            "Epoch [486/20000], Training Loss: 0.5145\n",
            "Epoch [487/20000], Training Loss: 0.5041\n",
            "Epoch [488/20000], Training Loss: 0.4325\n",
            "Epoch [489/20000], Training Loss: 0.5442\n",
            "Epoch [490/20000], Training Loss: 0.4685\n",
            "Epoch [491/20000], Training Loss: 0.4479\n",
            "Epoch [492/20000], Training Loss: 0.4859\n",
            "Epoch [493/20000], Training Loss: 0.5016\n",
            "Epoch [494/20000], Training Loss: 0.4643\n",
            "Epoch [495/20000], Training Loss: 0.5318\n",
            "Epoch [496/20000], Training Loss: 0.5140\n",
            "Epoch [497/20000], Training Loss: 0.4409\n",
            "Epoch [498/20000], Training Loss: 0.4977\n",
            "Epoch [499/20000], Training Loss: 0.4769\n",
            "Epoch [500/20000], Training Loss: 0.4394\n",
            "Epoch [501/20000], Training Loss: 0.4826\n",
            "Epoch [502/20000], Training Loss: 0.4798\n",
            "Epoch [503/20000], Training Loss: 0.5000\n",
            "Epoch [504/20000], Training Loss: 0.5221\n",
            "Epoch [505/20000], Training Loss: 0.4914\n",
            "Epoch [506/20000], Training Loss: 0.4788\n",
            "Epoch [507/20000], Training Loss: 0.5049\n",
            "Epoch [508/20000], Training Loss: 0.4942\n",
            "Epoch [509/20000], Training Loss: 0.5165\n",
            "Epoch [510/20000], Training Loss: 0.5173\n",
            "Epoch [511/20000], Training Loss: 0.4831\n",
            "Epoch [512/20000], Training Loss: 0.4634\n",
            "Epoch [513/20000], Training Loss: 0.4594\n",
            "Epoch [514/20000], Training Loss: 0.4517\n",
            "Epoch [515/20000], Training Loss: 0.5218\n",
            "Epoch [516/20000], Training Loss: 0.4635\n",
            "Epoch [517/20000], Training Loss: 0.4615\n",
            "Epoch [518/20000], Training Loss: 0.4970\n",
            "Epoch [519/20000], Training Loss: 0.4869\n",
            "Epoch [520/20000], Training Loss: 0.4695\n",
            "Epoch [521/20000], Training Loss: 0.4697\n",
            "Epoch [522/20000], Training Loss: 0.5311\n",
            "Epoch [523/20000], Training Loss: 0.5430\n",
            "Epoch [524/20000], Training Loss: 0.5010\n",
            "Epoch [525/20000], Training Loss: 0.4799\n",
            "Epoch [526/20000], Training Loss: 0.5051\n",
            "Epoch [527/20000], Training Loss: 0.5584\n",
            "Epoch [528/20000], Training Loss: 0.4955\n",
            "Epoch [529/20000], Training Loss: 0.4721\n",
            "Epoch [530/20000], Training Loss: 0.4612\n",
            "Epoch [531/20000], Training Loss: 0.4765\n",
            "Epoch [532/20000], Training Loss: 0.4992\n",
            "Epoch [533/20000], Training Loss: 0.4600\n",
            "Epoch [534/20000], Training Loss: 0.5051\n",
            "Epoch [535/20000], Training Loss: 0.5234\n",
            "Epoch [536/20000], Training Loss: 0.4732\n",
            "Epoch [537/20000], Training Loss: 0.4928\n",
            "Epoch [538/20000], Training Loss: 0.5392\n",
            "Epoch [539/20000], Training Loss: 0.5064\n",
            "Epoch [540/20000], Training Loss: 0.4922\n",
            "Epoch [541/20000], Training Loss: 0.4991\n",
            "Epoch [542/20000], Training Loss: 0.5015\n",
            "Epoch [543/20000], Training Loss: 0.4908\n",
            "Epoch [544/20000], Training Loss: 0.5102\n",
            "Epoch [545/20000], Training Loss: 0.4399\n",
            "Epoch [546/20000], Training Loss: 0.5068\n",
            "Epoch [547/20000], Training Loss: 0.4671\n",
            "Epoch [548/20000], Training Loss: 0.4809\n",
            "Epoch [549/20000], Training Loss: 0.5092\n",
            "Epoch [550/20000], Training Loss: 0.4879\n",
            "Epoch [551/20000], Training Loss: 0.4509\n",
            "Epoch [552/20000], Training Loss: 0.4746\n",
            "Epoch [553/20000], Training Loss: 0.4474\n",
            "Epoch [554/20000], Training Loss: 0.4871\n",
            "Epoch [555/20000], Training Loss: 0.5067\n",
            "Epoch [556/20000], Training Loss: 0.5433\n",
            "Epoch [557/20000], Training Loss: 0.4918\n",
            "Epoch [558/20000], Training Loss: 0.4936\n",
            "Epoch [559/20000], Training Loss: 0.5099\n",
            "Epoch [560/20000], Training Loss: 0.5333\n",
            "Epoch [561/20000], Training Loss: 0.4984\n",
            "Epoch [562/20000], Training Loss: 0.4755\n",
            "Epoch [563/20000], Training Loss: 0.4817\n",
            "Epoch [564/20000], Training Loss: 0.4617\n",
            "Epoch [565/20000], Training Loss: 0.5002\n",
            "Epoch [566/20000], Training Loss: 0.5071\n",
            "Epoch [567/20000], Training Loss: 0.4811\n",
            "Epoch [568/20000], Training Loss: 0.4919\n",
            "Epoch [569/20000], Training Loss: 0.4592\n",
            "Epoch [570/20000], Training Loss: 0.5322\n",
            "Epoch [571/20000], Training Loss: 0.4789\n",
            "Epoch [572/20000], Training Loss: 0.4871\n",
            "Epoch [573/20000], Training Loss: 0.4974\n",
            "Epoch [574/20000], Training Loss: 0.5179\n",
            "Epoch [575/20000], Training Loss: 0.5030\n",
            "Epoch [576/20000], Training Loss: 0.4621\n",
            "Epoch [577/20000], Training Loss: 0.4597\n",
            "Epoch [578/20000], Training Loss: 0.4997\n",
            "Epoch [579/20000], Training Loss: 0.5086\n",
            "Epoch [580/20000], Training Loss: 0.4718\n",
            "Epoch [581/20000], Training Loss: 0.4891\n",
            "Epoch [582/20000], Training Loss: 0.5260\n",
            "Epoch [583/20000], Training Loss: 0.4799\n",
            "Epoch [584/20000], Training Loss: 0.4939\n",
            "Epoch [585/20000], Training Loss: 0.4743\n",
            "Epoch [586/20000], Training Loss: 0.4734\n",
            "Epoch [587/20000], Training Loss: 0.4536\n",
            "Epoch [588/20000], Training Loss: 0.5263\n",
            "Epoch [589/20000], Training Loss: 0.4783\n",
            "Epoch [590/20000], Training Loss: 0.5337\n",
            "Epoch [591/20000], Training Loss: 0.5488\n",
            "Epoch [592/20000], Training Loss: 0.4825\n",
            "Epoch [593/20000], Training Loss: 0.4691\n",
            "Epoch [594/20000], Training Loss: 0.4822\n",
            "Epoch [595/20000], Training Loss: 0.5037\n",
            "Epoch [596/20000], Training Loss: 0.4619\n",
            "Epoch [597/20000], Training Loss: 0.5036\n",
            "Epoch [598/20000], Training Loss: 0.4838\n",
            "Epoch [599/20000], Training Loss: 0.4957\n",
            "Epoch [600/20000], Training Loss: 0.4891\n",
            "Epoch [601/20000], Training Loss: 0.5361\n",
            "Epoch [602/20000], Training Loss: 0.4960\n",
            "Epoch [603/20000], Training Loss: 0.4982\n",
            "Epoch [604/20000], Training Loss: 0.5122\n",
            "Epoch [605/20000], Training Loss: 0.4492\n",
            "Epoch [606/20000], Training Loss: 0.4679\n",
            "Epoch [607/20000], Training Loss: 0.5003\n",
            "Epoch [608/20000], Training Loss: 0.5115\n",
            "Epoch [609/20000], Training Loss: 0.4836\n",
            "Epoch [610/20000], Training Loss: 0.4749\n",
            "Epoch [611/20000], Training Loss: 0.5064\n",
            "Epoch [612/20000], Training Loss: 0.4962\n",
            "Epoch [613/20000], Training Loss: 0.4495\n",
            "Epoch [614/20000], Training Loss: 0.4795\n",
            "Epoch [615/20000], Training Loss: 0.4657\n",
            "Epoch [616/20000], Training Loss: 0.4776\n",
            "Epoch [617/20000], Training Loss: 0.4586\n",
            "Epoch [618/20000], Training Loss: 0.5210\n",
            "Epoch [619/20000], Training Loss: 0.4706\n",
            "Epoch [620/20000], Training Loss: 0.4733\n",
            "Epoch [621/20000], Training Loss: 0.4405\n",
            "Epoch [622/20000], Training Loss: 0.4610\n",
            "Epoch [623/20000], Training Loss: 0.4813\n",
            "Epoch [624/20000], Training Loss: 0.4797\n",
            "Epoch [625/20000], Training Loss: 0.5433\n",
            "Epoch [626/20000], Training Loss: 0.5380\n",
            "Epoch [627/20000], Training Loss: 0.4902\n",
            "Epoch [628/20000], Training Loss: 0.4556\n",
            "Epoch [629/20000], Training Loss: 0.4692\n",
            "Epoch [630/20000], Training Loss: 0.4546\n",
            "Epoch [631/20000], Training Loss: 0.4871\n",
            "Epoch [632/20000], Training Loss: 0.5279\n",
            "Epoch [633/20000], Training Loss: 0.4602\n",
            "Epoch [634/20000], Training Loss: 0.4960\n",
            "Epoch [635/20000], Training Loss: 0.4470\n",
            "Epoch [636/20000], Training Loss: 0.4777\n",
            "Epoch [637/20000], Training Loss: 0.4810\n",
            "Epoch [638/20000], Training Loss: 0.4781\n",
            "Epoch [639/20000], Training Loss: 0.4557\n",
            "Epoch [640/20000], Training Loss: 0.4803\n",
            "Epoch [641/20000], Training Loss: 0.4854\n",
            "Epoch [642/20000], Training Loss: 0.5287\n",
            "Epoch [643/20000], Training Loss: 0.5204\n",
            "Epoch [644/20000], Training Loss: 0.4868\n",
            "Epoch [645/20000], Training Loss: 0.5143\n",
            "Epoch [646/20000], Training Loss: 0.4877\n",
            "Epoch [647/20000], Training Loss: 0.4517\n",
            "Epoch [648/20000], Training Loss: 0.4520\n",
            "Epoch [649/20000], Training Loss: 0.4739\n",
            "Epoch [650/20000], Training Loss: 0.4995\n",
            "Epoch [651/20000], Training Loss: 0.4473\n",
            "Epoch [652/20000], Training Loss: 0.4584\n",
            "Epoch [653/20000], Training Loss: 0.4741\n",
            "Epoch [654/20000], Training Loss: 0.5271\n",
            "Epoch [655/20000], Training Loss: 0.4670\n",
            "Epoch [656/20000], Training Loss: 0.5150\n",
            "Epoch [657/20000], Training Loss: 0.5047\n",
            "Epoch [658/20000], Training Loss: 0.5144\n",
            "Epoch [659/20000], Training Loss: 0.5149\n",
            "Epoch [660/20000], Training Loss: 0.4719\n",
            "Epoch [661/20000], Training Loss: 0.4912\n",
            "Epoch [662/20000], Training Loss: 0.5019\n",
            "Epoch [663/20000], Training Loss: 0.5379\n",
            "Epoch [664/20000], Training Loss: 0.5165\n",
            "Epoch [665/20000], Training Loss: 0.4478\n",
            "Epoch [666/20000], Training Loss: 0.4975\n",
            "Epoch [667/20000], Training Loss: 0.5206\n",
            "Epoch [668/20000], Training Loss: 0.4891\n",
            "Epoch [669/20000], Training Loss: 0.4650\n",
            "Epoch [670/20000], Training Loss: 0.4794\n",
            "Epoch [671/20000], Training Loss: 0.5047\n",
            "Epoch [672/20000], Training Loss: 0.5039\n",
            "Epoch [673/20000], Training Loss: 0.4726\n",
            "Epoch [674/20000], Training Loss: 0.4996\n",
            "Epoch [675/20000], Training Loss: 0.4843\n",
            "Epoch [676/20000], Training Loss: 0.4279\n",
            "Epoch [677/20000], Training Loss: 0.5126\n",
            "Epoch [678/20000], Training Loss: 0.5318\n",
            "Epoch [679/20000], Training Loss: 0.5019\n",
            "Epoch [680/20000], Training Loss: 0.5033\n",
            "Epoch [681/20000], Training Loss: 0.4930\n",
            "Epoch [682/20000], Training Loss: 0.5272\n",
            "Epoch [683/20000], Training Loss: 0.5104\n",
            "Epoch [684/20000], Training Loss: 0.4669\n",
            "Epoch [685/20000], Training Loss: 0.5229\n",
            "Epoch [686/20000], Training Loss: 0.4502\n",
            "Epoch [687/20000], Training Loss: 0.5079\n",
            "Epoch [688/20000], Training Loss: 0.4535\n",
            "Epoch [689/20000], Training Loss: 0.4891\n",
            "Epoch [690/20000], Training Loss: 0.4628\n",
            "Epoch [691/20000], Training Loss: 0.4823\n",
            "Epoch [692/20000], Training Loss: 0.5205\n",
            "Epoch [693/20000], Training Loss: 0.4915\n",
            "Epoch [694/20000], Training Loss: 0.5035\n",
            "Epoch [695/20000], Training Loss: 0.4807\n",
            "Epoch [696/20000], Training Loss: 0.5195\n",
            "Epoch [697/20000], Training Loss: 0.5185\n",
            "Epoch [698/20000], Training Loss: 0.4477\n",
            "Epoch [699/20000], Training Loss: 0.5500\n",
            "Epoch [700/20000], Training Loss: 0.4827\n",
            "Epoch [701/20000], Training Loss: 0.4649\n",
            "Epoch [702/20000], Training Loss: 0.5089\n",
            "Epoch [703/20000], Training Loss: 0.4749\n",
            "Epoch [704/20000], Training Loss: 0.4500\n",
            "Epoch [705/20000], Training Loss: 0.5049\n",
            "Epoch [706/20000], Training Loss: 0.4875\n",
            "Epoch [707/20000], Training Loss: 0.4713\n",
            "Epoch [708/20000], Training Loss: 0.4828\n",
            "Epoch [709/20000], Training Loss: 0.4780\n",
            "Epoch [710/20000], Training Loss: 0.4839\n",
            "Epoch [711/20000], Training Loss: 0.4672\n",
            "Epoch [712/20000], Training Loss: 0.5467\n",
            "Epoch [713/20000], Training Loss: 0.4873\n",
            "Epoch [714/20000], Training Loss: 0.5080\n",
            "Epoch [715/20000], Training Loss: 0.4829\n",
            "Epoch [716/20000], Training Loss: 0.5157\n",
            "Epoch [717/20000], Training Loss: 0.4914\n",
            "Epoch [718/20000], Training Loss: 0.5384\n",
            "Epoch [719/20000], Training Loss: 0.4877\n",
            "Epoch [720/20000], Training Loss: 0.4790\n",
            "Epoch [721/20000], Training Loss: 0.4688\n",
            "Epoch [722/20000], Training Loss: 0.4670\n",
            "Epoch [723/20000], Training Loss: 0.4913\n",
            "Epoch [724/20000], Training Loss: 0.5048\n",
            "Epoch [725/20000], Training Loss: 0.5150\n",
            "Epoch [726/20000], Training Loss: 0.4536\n",
            "Epoch [727/20000], Training Loss: 0.5015\n",
            "Epoch [728/20000], Training Loss: 0.4736\n",
            "Epoch [729/20000], Training Loss: 0.4996\n",
            "Epoch [730/20000], Training Loss: 0.4797\n",
            "Epoch [731/20000], Training Loss: 0.4545\n",
            "Epoch [732/20000], Training Loss: 0.4896\n",
            "Epoch [733/20000], Training Loss: 0.4401\n",
            "Epoch [734/20000], Training Loss: 0.4667\n",
            "Epoch [735/20000], Training Loss: 0.5170\n",
            "Epoch [736/20000], Training Loss: 0.5519\n",
            "Epoch [737/20000], Training Loss: 0.4983\n",
            "Epoch [738/20000], Training Loss: 0.4610\n",
            "Epoch [739/20000], Training Loss: 0.5413\n",
            "Epoch [740/20000], Training Loss: 0.4642\n",
            "Epoch [741/20000], Training Loss: 0.5305\n",
            "Epoch [742/20000], Training Loss: 0.4909\n",
            "Epoch [743/20000], Training Loss: 0.4522\n",
            "Epoch [744/20000], Training Loss: 0.5238\n",
            "Epoch [745/20000], Training Loss: 0.4592\n",
            "Epoch [746/20000], Training Loss: 0.4961\n",
            "Epoch [747/20000], Training Loss: 0.4554\n",
            "Epoch [748/20000], Training Loss: 0.5120\n",
            "Epoch [749/20000], Training Loss: 0.5198\n",
            "Epoch [750/20000], Training Loss: 0.5310\n",
            "Epoch [751/20000], Training Loss: 0.5070\n",
            "Epoch [752/20000], Training Loss: 0.5056\n",
            "Epoch [753/20000], Training Loss: 0.4698\n",
            "Epoch [754/20000], Training Loss: 0.4799\n",
            "Epoch [755/20000], Training Loss: 0.4996\n",
            "Epoch [756/20000], Training Loss: 0.4541\n",
            "Epoch [757/20000], Training Loss: 0.4742\n",
            "Epoch [758/20000], Training Loss: 0.4689\n",
            "Epoch [759/20000], Training Loss: 0.4848\n",
            "Epoch [760/20000], Training Loss: 0.4543\n",
            "Epoch [761/20000], Training Loss: 0.4740\n",
            "Epoch [762/20000], Training Loss: 0.5008\n",
            "Epoch [763/20000], Training Loss: 0.4932\n",
            "Epoch [764/20000], Training Loss: 0.4808\n",
            "Epoch [765/20000], Training Loss: 0.4692\n",
            "Epoch [766/20000], Training Loss: 0.4744\n",
            "Epoch [767/20000], Training Loss: 0.4726\n",
            "Epoch [768/20000], Training Loss: 0.5021\n",
            "Epoch [769/20000], Training Loss: 0.4848\n",
            "Epoch [770/20000], Training Loss: 0.4735\n",
            "Epoch [771/20000], Training Loss: 0.4382\n",
            "Epoch [772/20000], Training Loss: 0.5143\n",
            "Epoch [773/20000], Training Loss: 0.4589\n",
            "Epoch [774/20000], Training Loss: 0.5094\n",
            "Epoch [775/20000], Training Loss: 0.5094\n",
            "Epoch [776/20000], Training Loss: 0.4725\n",
            "Epoch [777/20000], Training Loss: 0.4803\n",
            "Epoch [778/20000], Training Loss: 0.5024\n",
            "Epoch [779/20000], Training Loss: 0.4623\n",
            "Epoch [780/20000], Training Loss: 0.4917\n",
            "Epoch [781/20000], Training Loss: 0.4874\n",
            "Epoch [782/20000], Training Loss: 0.5163\n",
            "Epoch [783/20000], Training Loss: 0.4737\n",
            "Epoch [784/20000], Training Loss: 0.4704\n",
            "Epoch [785/20000], Training Loss: 0.4657\n",
            "Epoch [786/20000], Training Loss: 0.4732\n",
            "Epoch [787/20000], Training Loss: 0.5321\n",
            "Epoch [788/20000], Training Loss: 0.5430\n",
            "Epoch [789/20000], Training Loss: 0.5180\n",
            "Epoch [790/20000], Training Loss: 0.5335\n",
            "Epoch [791/20000], Training Loss: 0.5206\n",
            "Epoch [792/20000], Training Loss: 0.5120\n",
            "Epoch [793/20000], Training Loss: 0.5015\n",
            "Epoch [794/20000], Training Loss: 0.4336\n",
            "Epoch [795/20000], Training Loss: 0.5316\n",
            "Epoch [796/20000], Training Loss: 0.4932\n",
            "Epoch [797/20000], Training Loss: 0.4736\n",
            "Epoch [798/20000], Training Loss: 0.4759\n",
            "Epoch [799/20000], Training Loss: 0.4890\n",
            "Epoch [800/20000], Training Loss: 0.4981\n",
            "Epoch [801/20000], Training Loss: 0.4868\n",
            "Epoch [802/20000], Training Loss: 0.4798\n",
            "Epoch [803/20000], Training Loss: 0.4795\n",
            "Epoch [804/20000], Training Loss: 0.4847\n",
            "Epoch [805/20000], Training Loss: 0.4499\n",
            "Epoch [806/20000], Training Loss: 0.5308\n",
            "Epoch [807/20000], Training Loss: 0.4850\n",
            "Epoch [808/20000], Training Loss: 0.5184\n",
            "Epoch [809/20000], Training Loss: 0.4783\n",
            "Epoch [810/20000], Training Loss: 0.4654\n",
            "Epoch [811/20000], Training Loss: 0.4803\n",
            "Epoch [812/20000], Training Loss: 0.4646\n",
            "Epoch [813/20000], Training Loss: 0.5096\n",
            "Epoch [814/20000], Training Loss: 0.4854\n",
            "Epoch [815/20000], Training Loss: 0.4839\n",
            "Epoch [816/20000], Training Loss: 0.5333\n",
            "Epoch [817/20000], Training Loss: 0.5164\n",
            "Epoch [818/20000], Training Loss: 0.4354\n",
            "Epoch [819/20000], Training Loss: 0.4399\n",
            "Epoch [820/20000], Training Loss: 0.4690\n",
            "Epoch [821/20000], Training Loss: 0.4927\n",
            "Epoch [822/20000], Training Loss: 0.5056\n",
            "Epoch [823/20000], Training Loss: 0.5210\n",
            "Epoch [824/20000], Training Loss: 0.4814\n",
            "Epoch [825/20000], Training Loss: 0.4875\n",
            "Epoch [826/20000], Training Loss: 0.4708\n",
            "Epoch [827/20000], Training Loss: 0.4471\n",
            "Epoch [828/20000], Training Loss: 0.5033\n",
            "Epoch [829/20000], Training Loss: 0.4941\n",
            "Epoch [830/20000], Training Loss: 0.5160\n",
            "Epoch [831/20000], Training Loss: 0.4765\n",
            "Epoch [832/20000], Training Loss: 0.4875\n",
            "Epoch [833/20000], Training Loss: 0.4819\n",
            "Epoch [834/20000], Training Loss: 0.5079\n",
            "Epoch [835/20000], Training Loss: 0.4505\n",
            "Epoch [836/20000], Training Loss: 0.4990\n",
            "Epoch [837/20000], Training Loss: 0.4707\n",
            "Epoch [838/20000], Training Loss: 0.4963\n",
            "Epoch [839/20000], Training Loss: 0.5106\n",
            "Epoch [840/20000], Training Loss: 0.4713\n",
            "Epoch [841/20000], Training Loss: 0.5389\n",
            "Epoch [842/20000], Training Loss: 0.5074\n",
            "Epoch [843/20000], Training Loss: 0.4612\n",
            "Epoch [844/20000], Training Loss: 0.4403\n",
            "Epoch [845/20000], Training Loss: 0.5150\n",
            "Epoch [846/20000], Training Loss: 0.4653\n",
            "Epoch [847/20000], Training Loss: 0.4990\n",
            "Epoch [848/20000], Training Loss: 0.5002\n",
            "Epoch [849/20000], Training Loss: 0.4995\n",
            "Epoch [850/20000], Training Loss: 0.4925\n",
            "Epoch [851/20000], Training Loss: 0.5158\n",
            "Epoch [852/20000], Training Loss: 0.4840\n",
            "Epoch [853/20000], Training Loss: 0.4938\n",
            "Epoch [854/20000], Training Loss: 0.5317\n",
            "Epoch [855/20000], Training Loss: 0.5028\n",
            "Epoch [856/20000], Training Loss: 0.5153\n",
            "Epoch [857/20000], Training Loss: 0.4663\n",
            "Epoch [858/20000], Training Loss: 0.4765\n",
            "Epoch [859/20000], Training Loss: 0.4650\n",
            "Epoch [860/20000], Training Loss: 0.4644\n",
            "Epoch [861/20000], Training Loss: 0.5148\n",
            "Epoch [862/20000], Training Loss: 0.4479\n",
            "Epoch [863/20000], Training Loss: 0.4743\n",
            "Epoch [864/20000], Training Loss: 0.4671\n",
            "Epoch [865/20000], Training Loss: 0.4934\n",
            "Epoch [866/20000], Training Loss: 0.4612\n",
            "Epoch [867/20000], Training Loss: 0.5104\n",
            "Epoch [868/20000], Training Loss: 0.4584\n",
            "Epoch [869/20000], Training Loss: 0.5205\n",
            "Epoch [870/20000], Training Loss: 0.5183\n",
            "Epoch [871/20000], Training Loss: 0.4746\n",
            "Epoch [872/20000], Training Loss: 0.4650\n",
            "Epoch [873/20000], Training Loss: 0.4988\n",
            "Epoch [874/20000], Training Loss: 0.4920\n",
            "Epoch [875/20000], Training Loss: 0.4968\n",
            "Epoch [876/20000], Training Loss: 0.4594\n",
            "Epoch [877/20000], Training Loss: 0.4434\n",
            "Epoch [878/20000], Training Loss: 0.4999\n",
            "Epoch [879/20000], Training Loss: 0.4873\n",
            "Epoch [880/20000], Training Loss: 0.4861\n",
            "Epoch [881/20000], Training Loss: 0.5123\n",
            "Epoch [882/20000], Training Loss: 0.4589\n",
            "Epoch [883/20000], Training Loss: 0.4963\n",
            "Epoch [884/20000], Training Loss: 0.5405\n",
            "Epoch [885/20000], Training Loss: 0.5335\n",
            "Epoch [886/20000], Training Loss: 0.4357\n",
            "Epoch [887/20000], Training Loss: 0.4935\n",
            "Epoch [888/20000], Training Loss: 0.5003\n",
            "Epoch [889/20000], Training Loss: 0.5313\n",
            "Epoch [890/20000], Training Loss: 0.4936\n",
            "Epoch [891/20000], Training Loss: 0.5048\n",
            "Epoch [892/20000], Training Loss: 0.4864\n",
            "Epoch [893/20000], Training Loss: 0.4637\n",
            "Epoch [894/20000], Training Loss: 0.4824\n",
            "Epoch [895/20000], Training Loss: 0.4666\n",
            "Epoch [896/20000], Training Loss: 0.4812\n",
            "Epoch [897/20000], Training Loss: 0.4755\n",
            "Epoch [898/20000], Training Loss: 0.4560\n",
            "Epoch [899/20000], Training Loss: 0.5052\n",
            "Epoch [900/20000], Training Loss: 0.5009\n",
            "Epoch [901/20000], Training Loss: 0.4605\n",
            "Epoch [902/20000], Training Loss: 0.4885\n",
            "Epoch [903/20000], Training Loss: 0.4724\n",
            "Epoch [904/20000], Training Loss: 0.4876\n",
            "Epoch [905/20000], Training Loss: 0.4752\n",
            "Epoch [906/20000], Training Loss: 0.4941\n",
            "Epoch [907/20000], Training Loss: 0.5169\n",
            "Epoch [908/20000], Training Loss: 0.4532\n",
            "Epoch [909/20000], Training Loss: 0.4726\n",
            "Epoch [910/20000], Training Loss: 0.4635\n",
            "Epoch [911/20000], Training Loss: 0.4568\n",
            "Epoch [912/20000], Training Loss: 0.5027\n",
            "Epoch [913/20000], Training Loss: 0.4704\n",
            "Epoch [914/20000], Training Loss: 0.5017\n",
            "Epoch [915/20000], Training Loss: 0.5227\n",
            "Epoch [916/20000], Training Loss: 0.4530\n",
            "Epoch [917/20000], Training Loss: 0.5048\n",
            "Epoch [918/20000], Training Loss: 0.4645\n",
            "Epoch [919/20000], Training Loss: 0.4497\n",
            "Epoch [920/20000], Training Loss: 0.4402\n",
            "Epoch [921/20000], Training Loss: 0.5369\n",
            "Epoch [922/20000], Training Loss: 0.5081\n",
            "Epoch [923/20000], Training Loss: 0.4670\n",
            "Epoch [924/20000], Training Loss: 0.5159\n",
            "Epoch [925/20000], Training Loss: 0.5411\n",
            "Epoch [926/20000], Training Loss: 0.4921\n",
            "Epoch [927/20000], Training Loss: 0.5077\n",
            "Epoch [928/20000], Training Loss: 0.4741\n",
            "Epoch [929/20000], Training Loss: 0.4621\n",
            "Epoch [930/20000], Training Loss: 0.5386\n",
            "Epoch [931/20000], Training Loss: 0.4804\n",
            "Epoch [932/20000], Training Loss: 0.4756\n",
            "Epoch [933/20000], Training Loss: 0.4894\n",
            "Epoch [934/20000], Training Loss: 0.5111\n",
            "Epoch [935/20000], Training Loss: 0.4611\n",
            "Epoch [936/20000], Training Loss: 0.5258\n",
            "Epoch [937/20000], Training Loss: 0.4895\n",
            "Epoch [938/20000], Training Loss: 0.4425\n",
            "Epoch [939/20000], Training Loss: 0.4827\n",
            "Epoch [940/20000], Training Loss: 0.4984\n",
            "Epoch [941/20000], Training Loss: 0.5341\n",
            "Epoch [942/20000], Training Loss: 0.4750\n",
            "Epoch [943/20000], Training Loss: 0.4481\n",
            "Epoch [944/20000], Training Loss: 0.5028\n",
            "Epoch [945/20000], Training Loss: 0.4644\n",
            "Epoch [946/20000], Training Loss: 0.4576\n",
            "Epoch [947/20000], Training Loss: 0.4969\n",
            "Epoch [948/20000], Training Loss: 0.5086\n",
            "Epoch [949/20000], Training Loss: 0.4458\n",
            "Epoch [950/20000], Training Loss: 0.5279\n",
            "Epoch [951/20000], Training Loss: 0.4963\n",
            "Epoch [952/20000], Training Loss: 0.4688\n",
            "Epoch [953/20000], Training Loss: 0.4989\n",
            "Epoch [954/20000], Training Loss: 0.4568\n",
            "Epoch [955/20000], Training Loss: 0.4817\n",
            "Epoch [956/20000], Training Loss: 0.5082\n",
            "Epoch [957/20000], Training Loss: 0.4814\n",
            "Epoch [958/20000], Training Loss: 0.5185\n",
            "Epoch [959/20000], Training Loss: 0.4639\n",
            "Epoch [960/20000], Training Loss: 0.5035\n",
            "Epoch [961/20000], Training Loss: 0.5178\n",
            "Epoch [962/20000], Training Loss: 0.4578\n",
            "Epoch [963/20000], Training Loss: 0.4617\n",
            "Epoch [964/20000], Training Loss: 0.4762\n",
            "Epoch [965/20000], Training Loss: 0.4858\n",
            "Epoch [966/20000], Training Loss: 0.5079\n",
            "Epoch [967/20000], Training Loss: 0.4875\n",
            "Epoch [968/20000], Training Loss: 0.4744\n",
            "Epoch [969/20000], Training Loss: 0.4744\n",
            "Epoch [970/20000], Training Loss: 0.4862\n",
            "Epoch [971/20000], Training Loss: 0.4847\n",
            "Epoch [972/20000], Training Loss: 0.4983\n",
            "Epoch [973/20000], Training Loss: 0.4712\n",
            "Epoch [974/20000], Training Loss: 0.5341\n",
            "Epoch [975/20000], Training Loss: 0.5051\n",
            "Epoch [976/20000], Training Loss: 0.4938\n",
            "Epoch [977/20000], Training Loss: 0.4678\n",
            "Epoch [978/20000], Training Loss: 0.4992\n",
            "Epoch [979/20000], Training Loss: 0.4702\n",
            "Epoch [980/20000], Training Loss: 0.4845\n",
            "Epoch [981/20000], Training Loss: 0.5228\n",
            "Epoch [982/20000], Training Loss: 0.5125\n",
            "Epoch [983/20000], Training Loss: 0.4660\n",
            "Epoch [984/20000], Training Loss: 0.4515\n",
            "Epoch [985/20000], Training Loss: 0.5266\n",
            "Epoch [986/20000], Training Loss: 0.5037\n",
            "Epoch [987/20000], Training Loss: 0.5073\n",
            "Epoch [988/20000], Training Loss: 0.4540\n",
            "Epoch [989/20000], Training Loss: 0.4617\n",
            "Epoch [990/20000], Training Loss: 0.4921\n",
            "Epoch [991/20000], Training Loss: 0.4502\n",
            "Epoch [992/20000], Training Loss: 0.5105\n",
            "Epoch [993/20000], Training Loss: 0.4420\n",
            "Epoch [994/20000], Training Loss: 0.5371\n",
            "Epoch [995/20000], Training Loss: 0.4743\n",
            "Epoch [996/20000], Training Loss: 0.4470\n",
            "Epoch [997/20000], Training Loss: 0.4459\n",
            "Epoch [998/20000], Training Loss: 0.5200\n",
            "Epoch [999/20000], Training Loss: 0.4718\n",
            "Epoch [1000/20000], Training Loss: 0.4933\n",
            "Epoch [1001/20000], Training Loss: 0.4715\n",
            "Epoch [1002/20000], Training Loss: 0.5190\n",
            "Epoch [1003/20000], Training Loss: 0.4898\n",
            "Epoch [1004/20000], Training Loss: 0.4705\n",
            "Epoch [1005/20000], Training Loss: 0.4583\n",
            "Epoch [1006/20000], Training Loss: 0.4655\n",
            "Epoch [1007/20000], Training Loss: 0.4477\n",
            "Epoch [1008/20000], Training Loss: 0.4824\n",
            "Epoch [1009/20000], Training Loss: 0.5300\n",
            "Epoch [1010/20000], Training Loss: 0.5005\n",
            "Epoch [1011/20000], Training Loss: 0.5106\n",
            "Epoch [1012/20000], Training Loss: 0.4956\n",
            "Epoch [1013/20000], Training Loss: 0.4775\n",
            "Epoch [1014/20000], Training Loss: 0.4580\n",
            "Epoch [1015/20000], Training Loss: 0.5079\n",
            "Epoch [1016/20000], Training Loss: 0.5441\n",
            "Epoch [1017/20000], Training Loss: 0.4899\n",
            "Epoch [1018/20000], Training Loss: 0.5332\n",
            "Epoch [1019/20000], Training Loss: 0.5165\n",
            "Epoch [1020/20000], Training Loss: 0.4412\n",
            "Epoch [1021/20000], Training Loss: 0.4919\n",
            "Epoch [1022/20000], Training Loss: 0.4918\n",
            "Epoch [1023/20000], Training Loss: 0.4945\n",
            "Epoch [1024/20000], Training Loss: 0.4832\n",
            "Epoch [1025/20000], Training Loss: 0.4812\n",
            "Epoch [1026/20000], Training Loss: 0.4712\n",
            "Epoch [1027/20000], Training Loss: 0.4737\n",
            "Epoch [1028/20000], Training Loss: 0.5101\n",
            "Epoch [1029/20000], Training Loss: 0.4736\n",
            "Epoch [1030/20000], Training Loss: 0.4638\n",
            "Epoch [1031/20000], Training Loss: 0.4675\n",
            "Epoch [1032/20000], Training Loss: 0.4795\n",
            "Epoch [1033/20000], Training Loss: 0.5345\n",
            "Epoch [1034/20000], Training Loss: 0.4715\n",
            "Epoch [1035/20000], Training Loss: 0.5176\n",
            "Epoch [1036/20000], Training Loss: 0.4593\n",
            "Epoch [1037/20000], Training Loss: 0.4877\n",
            "Epoch [1038/20000], Training Loss: 0.5164\n",
            "Epoch [1039/20000], Training Loss: 0.4992\n",
            "Epoch [1040/20000], Training Loss: 0.5066\n",
            "Epoch [1041/20000], Training Loss: 0.5047\n",
            "Epoch [1042/20000], Training Loss: 0.4808\n",
            "Epoch [1043/20000], Training Loss: 0.4537\n",
            "Epoch [1044/20000], Training Loss: 0.4984\n",
            "Epoch [1045/20000], Training Loss: 0.4891\n",
            "Epoch [1046/20000], Training Loss: 0.4890\n",
            "Epoch [1047/20000], Training Loss: 0.4672\n",
            "Epoch [1048/20000], Training Loss: 0.5087\n",
            "Epoch [1049/20000], Training Loss: 0.5348\n",
            "Epoch [1050/20000], Training Loss: 0.5110\n",
            "Epoch [1051/20000], Training Loss: 0.4998\n",
            "Epoch [1052/20000], Training Loss: 0.4895\n",
            "Epoch [1053/20000], Training Loss: 0.5263\n",
            "Epoch [1054/20000], Training Loss: 0.4902\n",
            "Epoch [1055/20000], Training Loss: 0.5053\n",
            "Epoch [1056/20000], Training Loss: 0.5191\n",
            "Epoch [1057/20000], Training Loss: 0.4758\n",
            "Epoch [1058/20000], Training Loss: 0.4948\n",
            "Epoch [1059/20000], Training Loss: 0.4774\n",
            "Epoch [1060/20000], Training Loss: 0.5024\n",
            "Epoch [1061/20000], Training Loss: 0.5146\n",
            "Epoch [1062/20000], Training Loss: 0.4452\n",
            "Epoch [1063/20000], Training Loss: 0.4555\n",
            "Epoch [1064/20000], Training Loss: 0.4893\n",
            "Epoch [1065/20000], Training Loss: 0.4465\n",
            "Epoch [1066/20000], Training Loss: 0.4465\n",
            "Epoch [1067/20000], Training Loss: 0.4749\n",
            "Epoch [1068/20000], Training Loss: 0.4471\n",
            "Epoch [1069/20000], Training Loss: 0.5112\n",
            "Epoch [1070/20000], Training Loss: 0.4712\n",
            "Epoch [1071/20000], Training Loss: 0.4556\n",
            "Epoch [1072/20000], Training Loss: 0.4830\n",
            "Epoch [1073/20000], Training Loss: 0.5008\n",
            "Epoch [1074/20000], Training Loss: 0.4971\n",
            "Epoch [1075/20000], Training Loss: 0.4701\n",
            "Epoch [1076/20000], Training Loss: 0.4863\n",
            "Epoch [1077/20000], Training Loss: 0.4401\n",
            "Epoch [1078/20000], Training Loss: 0.4755\n",
            "Epoch [1079/20000], Training Loss: 0.4757\n",
            "Epoch [1080/20000], Training Loss: 0.5195\n",
            "Epoch [1081/20000], Training Loss: 0.5364\n",
            "Epoch [1082/20000], Training Loss: 0.4659\n",
            "Epoch [1083/20000], Training Loss: 0.4613\n",
            "Epoch [1084/20000], Training Loss: 0.4641\n",
            "Epoch [1085/20000], Training Loss: 0.4802\n",
            "Epoch [1086/20000], Training Loss: 0.5231\n",
            "Epoch [1087/20000], Training Loss: 0.4753\n",
            "Epoch [1088/20000], Training Loss: 0.5093\n",
            "Epoch [1089/20000], Training Loss: 0.4642\n",
            "Epoch [1090/20000], Training Loss: 0.4749\n",
            "Epoch [1091/20000], Training Loss: 0.4764\n",
            "Epoch [1092/20000], Training Loss: 0.4682\n",
            "Epoch [1093/20000], Training Loss: 0.4640\n",
            "Epoch [1094/20000], Training Loss: 0.5162\n",
            "Epoch [1095/20000], Training Loss: 0.5293\n",
            "Epoch [1096/20000], Training Loss: 0.5145\n",
            "Epoch [1097/20000], Training Loss: 0.5236\n",
            "Epoch [1098/20000], Training Loss: 0.4726\n",
            "Epoch [1099/20000], Training Loss: 0.4953\n",
            "Epoch [1100/20000], Training Loss: 0.4936\n",
            "Epoch [1101/20000], Training Loss: 0.5015\n",
            "Epoch [1102/20000], Training Loss: 0.5123\n",
            "Epoch [1103/20000], Training Loss: 0.5191\n",
            "Epoch [1104/20000], Training Loss: 0.4871\n",
            "Epoch [1105/20000], Training Loss: 0.4540\n",
            "Epoch [1106/20000], Training Loss: 0.5218\n",
            "Epoch [1107/20000], Training Loss: 0.5175\n",
            "Epoch [1108/20000], Training Loss: 0.5038\n",
            "Epoch [1109/20000], Training Loss: 0.4729\n",
            "Epoch [1110/20000], Training Loss: 0.4854\n",
            "Epoch [1111/20000], Training Loss: 0.4457\n",
            "Epoch [1112/20000], Training Loss: 0.4635\n",
            "Epoch [1113/20000], Training Loss: 0.5152\n",
            "Epoch [1114/20000], Training Loss: 0.4709\n",
            "Epoch [1115/20000], Training Loss: 0.5006\n",
            "Epoch [1116/20000], Training Loss: 0.4712\n",
            "Epoch [1117/20000], Training Loss: 0.4866\n",
            "Epoch [1118/20000], Training Loss: 0.4707\n",
            "Epoch [1119/20000], Training Loss: 0.4923\n",
            "Epoch [1120/20000], Training Loss: 0.5015\n",
            "Epoch [1121/20000], Training Loss: 0.4921\n",
            "Epoch [1122/20000], Training Loss: 0.4641\n",
            "Epoch [1123/20000], Training Loss: 0.4800\n",
            "Epoch [1124/20000], Training Loss: 0.4928\n",
            "Epoch [1125/20000], Training Loss: 0.4553\n",
            "Epoch [1126/20000], Training Loss: 0.4607\n",
            "Epoch [1127/20000], Training Loss: 0.4558\n",
            "Epoch [1128/20000], Training Loss: 0.5175\n",
            "Epoch [1129/20000], Training Loss: 0.4397\n",
            "Epoch [1130/20000], Training Loss: 0.5188\n",
            "Epoch [1131/20000], Training Loss: 0.4878\n",
            "Epoch [1132/20000], Training Loss: 0.4903\n",
            "Epoch [1133/20000], Training Loss: 0.5200\n",
            "Epoch [1134/20000], Training Loss: 0.5041\n",
            "Epoch [1135/20000], Training Loss: 0.5022\n",
            "Epoch [1136/20000], Training Loss: 0.5009\n",
            "Epoch [1137/20000], Training Loss: 0.4922\n",
            "Epoch [1138/20000], Training Loss: 0.4269\n",
            "Epoch [1139/20000], Training Loss: 0.5054\n",
            "Epoch [1140/20000], Training Loss: 0.4962\n",
            "Epoch [1141/20000], Training Loss: 0.4713\n",
            "Epoch [1142/20000], Training Loss: 0.4493\n",
            "Epoch [1143/20000], Training Loss: 0.4743\n",
            "Epoch [1144/20000], Training Loss: 0.4617\n",
            "Epoch [1145/20000], Training Loss: 0.5116\n",
            "Epoch [1146/20000], Training Loss: 0.4696\n",
            "Epoch [1147/20000], Training Loss: 0.4957\n",
            "Epoch [1148/20000], Training Loss: 0.4803\n",
            "Epoch [1149/20000], Training Loss: 0.4720\n",
            "Epoch [1150/20000], Training Loss: 0.4765\n",
            "Epoch [1151/20000], Training Loss: 0.4722\n",
            "Epoch [1152/20000], Training Loss: 0.4365\n",
            "Epoch [1153/20000], Training Loss: 0.4824\n",
            "Epoch [1154/20000], Training Loss: 0.4982\n",
            "Epoch [1155/20000], Training Loss: 0.4777\n",
            "Epoch [1156/20000], Training Loss: 0.4751\n",
            "Epoch [1157/20000], Training Loss: 0.4762\n",
            "Epoch [1158/20000], Training Loss: 0.4604\n",
            "Epoch [1159/20000], Training Loss: 0.4724\n",
            "Epoch [1160/20000], Training Loss: 0.4848\n",
            "Epoch [1161/20000], Training Loss: 0.5622\n",
            "Epoch [1162/20000], Training Loss: 0.4509\n",
            "Epoch [1163/20000], Training Loss: 0.4900\n",
            "Epoch [1164/20000], Training Loss: 0.4841\n",
            "Epoch [1165/20000], Training Loss: 0.4883\n",
            "Epoch [1166/20000], Training Loss: 0.5117\n",
            "Epoch [1167/20000], Training Loss: 0.4944\n",
            "Epoch [1168/20000], Training Loss: 0.4793\n",
            "Epoch [1169/20000], Training Loss: 0.4581\n",
            "Epoch [1170/20000], Training Loss: 0.4617\n",
            "Epoch [1171/20000], Training Loss: 0.4912\n",
            "Epoch [1172/20000], Training Loss: 0.5000\n",
            "Epoch [1173/20000], Training Loss: 0.5307\n",
            "Epoch [1174/20000], Training Loss: 0.4467\n",
            "Epoch [1175/20000], Training Loss: 0.5041\n",
            "Epoch [1176/20000], Training Loss: 0.4549\n",
            "Epoch [1177/20000], Training Loss: 0.5086\n",
            "Epoch [1178/20000], Training Loss: 0.4924\n",
            "Epoch [1179/20000], Training Loss: 0.4666\n",
            "Epoch [1180/20000], Training Loss: 0.4961\n",
            "Epoch [1181/20000], Training Loss: 0.4514\n",
            "Epoch [1182/20000], Training Loss: 0.5401\n",
            "Epoch [1183/20000], Training Loss: 0.4874\n",
            "Epoch [1184/20000], Training Loss: 0.5013\n",
            "Epoch [1185/20000], Training Loss: 0.4742\n",
            "Epoch [1186/20000], Training Loss: 0.5298\n",
            "Epoch [1187/20000], Training Loss: 0.4506\n",
            "Epoch [1188/20000], Training Loss: 0.4603\n",
            "Epoch [1189/20000], Training Loss: 0.4766\n",
            "Epoch [1190/20000], Training Loss: 0.5459\n",
            "Epoch [1191/20000], Training Loss: 0.4660\n",
            "Epoch [1192/20000], Training Loss: 0.4814\n",
            "Epoch [1193/20000], Training Loss: 0.4846\n",
            "Epoch [1194/20000], Training Loss: 0.5245\n",
            "Epoch [1195/20000], Training Loss: 0.5304\n",
            "Epoch [1196/20000], Training Loss: 0.5186\n",
            "Epoch [1197/20000], Training Loss: 0.4742\n",
            "Epoch [1198/20000], Training Loss: 0.5170\n",
            "Epoch [1199/20000], Training Loss: 0.4771\n",
            "Epoch [1200/20000], Training Loss: 0.5256\n",
            "Epoch [1201/20000], Training Loss: 0.4671\n",
            "Epoch [1202/20000], Training Loss: 0.5052\n",
            "Epoch [1203/20000], Training Loss: 0.5574\n",
            "Epoch [1204/20000], Training Loss: 0.5210\n",
            "Epoch [1205/20000], Training Loss: 0.5330\n",
            "Epoch [1206/20000], Training Loss: 0.4856\n",
            "Epoch [1207/20000], Training Loss: 0.4761\n",
            "Epoch [1208/20000], Training Loss: 0.5141\n",
            "Epoch [1209/20000], Training Loss: 0.4509\n",
            "Epoch [1210/20000], Training Loss: 0.5063\n",
            "Epoch [1211/20000], Training Loss: 0.4818\n",
            "Epoch [1212/20000], Training Loss: 0.4511\n",
            "Epoch [1213/20000], Training Loss: 0.4974\n",
            "Epoch [1214/20000], Training Loss: 0.4350\n",
            "Epoch [1215/20000], Training Loss: 0.5037\n",
            "Epoch [1216/20000], Training Loss: 0.4921\n",
            "Epoch [1217/20000], Training Loss: 0.4800\n",
            "Epoch [1218/20000], Training Loss: 0.4677\n",
            "Epoch [1219/20000], Training Loss: 0.4908\n",
            "Epoch [1220/20000], Training Loss: 0.4776\n",
            "Epoch [1221/20000], Training Loss: 0.4691\n",
            "Epoch [1222/20000], Training Loss: 0.4550\n",
            "Epoch [1223/20000], Training Loss: 0.4827\n",
            "Epoch [1224/20000], Training Loss: 0.5069\n",
            "Epoch [1225/20000], Training Loss: 0.4731\n",
            "Epoch [1226/20000], Training Loss: 0.4893\n",
            "Epoch [1227/20000], Training Loss: 0.5456\n",
            "Epoch [1228/20000], Training Loss: 0.4632\n",
            "Epoch [1229/20000], Training Loss: 0.4703\n",
            "Epoch [1230/20000], Training Loss: 0.4964\n",
            "Epoch [1231/20000], Training Loss: 0.4292\n",
            "Epoch [1232/20000], Training Loss: 0.5159\n",
            "Epoch [1233/20000], Training Loss: 0.4515\n",
            "Epoch [1234/20000], Training Loss: 0.4579\n",
            "Epoch [1235/20000], Training Loss: 0.5182\n",
            "Epoch [1236/20000], Training Loss: 0.5315\n",
            "Epoch [1237/20000], Training Loss: 0.4894\n",
            "Epoch [1238/20000], Training Loss: 0.4595\n",
            "Epoch [1239/20000], Training Loss: 0.4798\n",
            "Epoch [1240/20000], Training Loss: 0.4941\n",
            "Epoch [1241/20000], Training Loss: 0.4879\n",
            "Epoch [1242/20000], Training Loss: 0.4941\n",
            "Epoch [1243/20000], Training Loss: 0.4878\n",
            "Epoch [1244/20000], Training Loss: 0.5355\n",
            "Epoch [1245/20000], Training Loss: 0.4663\n",
            "Epoch [1246/20000], Training Loss: 0.4824\n",
            "Epoch [1247/20000], Training Loss: 0.4790\n",
            "Epoch [1248/20000], Training Loss: 0.4993\n",
            "Epoch [1249/20000], Training Loss: 0.4861\n",
            "Epoch [1250/20000], Training Loss: 0.4476\n",
            "Epoch [1251/20000], Training Loss: 0.5076\n",
            "Epoch [1252/20000], Training Loss: 0.4775\n",
            "Epoch [1253/20000], Training Loss: 0.4900\n",
            "Epoch [1254/20000], Training Loss: 0.4900\n",
            "Epoch [1255/20000], Training Loss: 0.5052\n",
            "Epoch [1256/20000], Training Loss: 0.5439\n",
            "Epoch [1257/20000], Training Loss: 0.5061\n",
            "Epoch [1258/20000], Training Loss: 0.5026\n",
            "Epoch [1259/20000], Training Loss: 0.4814\n",
            "Epoch [1260/20000], Training Loss: 0.4686\n",
            "Epoch [1261/20000], Training Loss: 0.5181\n",
            "Epoch [1262/20000], Training Loss: 0.4798\n",
            "Epoch [1263/20000], Training Loss: 0.4879\n",
            "Epoch [1264/20000], Training Loss: 0.5245\n",
            "Epoch [1265/20000], Training Loss: 0.4639\n",
            "Epoch [1266/20000], Training Loss: 0.5611\n",
            "Epoch [1267/20000], Training Loss: 0.5339\n",
            "Epoch [1268/20000], Training Loss: 0.4788\n",
            "Epoch [1269/20000], Training Loss: 0.4551\n",
            "Epoch [1270/20000], Training Loss: 0.5507\n",
            "Epoch [1271/20000], Training Loss: 0.5164\n",
            "Epoch [1272/20000], Training Loss: 0.5083\n",
            "Epoch [1273/20000], Training Loss: 0.4386\n",
            "Epoch [1274/20000], Training Loss: 0.4398\n",
            "Epoch [1275/20000], Training Loss: 0.4847\n",
            "Epoch [1276/20000], Training Loss: 0.4639\n",
            "Epoch [1277/20000], Training Loss: 0.4864\n",
            "Epoch [1278/20000], Training Loss: 0.4673\n",
            "Epoch [1279/20000], Training Loss: 0.5364\n",
            "Epoch [1280/20000], Training Loss: 0.5253\n",
            "Epoch [1281/20000], Training Loss: 0.4988\n",
            "Epoch [1282/20000], Training Loss: 0.4942\n",
            "Epoch [1283/20000], Training Loss: 0.4997\n",
            "Epoch [1284/20000], Training Loss: 0.4881\n",
            "Epoch [1285/20000], Training Loss: 0.5044\n",
            "Epoch [1286/20000], Training Loss: 0.4397\n",
            "Epoch [1287/20000], Training Loss: 0.4298\n",
            "Epoch [1288/20000], Training Loss: 0.4518\n",
            "Epoch [1289/20000], Training Loss: 0.5164\n",
            "Epoch [1290/20000], Training Loss: 0.4510\n",
            "Epoch [1291/20000], Training Loss: 0.5151\n",
            "Epoch [1292/20000], Training Loss: 0.5130\n",
            "Epoch [1293/20000], Training Loss: 0.5153\n",
            "Epoch [1294/20000], Training Loss: 0.5136\n",
            "Epoch [1295/20000], Training Loss: 0.5112\n",
            "Epoch [1296/20000], Training Loss: 0.4695\n",
            "Epoch [1297/20000], Training Loss: 0.4798\n",
            "Epoch [1298/20000], Training Loss: 0.5240\n",
            "Epoch [1299/20000], Training Loss: 0.4778\n",
            "Epoch [1300/20000], Training Loss: 0.4735\n",
            "Epoch [1301/20000], Training Loss: 0.4762\n",
            "Epoch [1302/20000], Training Loss: 0.4816\n",
            "Epoch [1303/20000], Training Loss: 0.4657\n",
            "Epoch [1304/20000], Training Loss: 0.5333\n",
            "Epoch [1305/20000], Training Loss: 0.4898\n",
            "Epoch [1306/20000], Training Loss: 0.5261\n",
            "Epoch [1307/20000], Training Loss: 0.4797\n",
            "Epoch [1308/20000], Training Loss: 0.4943\n",
            "Epoch [1309/20000], Training Loss: 0.4597\n",
            "Epoch [1310/20000], Training Loss: 0.4862\n",
            "Epoch [1311/20000], Training Loss: 0.5124\n",
            "Epoch [1312/20000], Training Loss: 0.4650\n",
            "Epoch [1313/20000], Training Loss: 0.4917\n",
            "Epoch [1314/20000], Training Loss: 0.4671\n",
            "Epoch [1315/20000], Training Loss: 0.5233\n",
            "Epoch [1316/20000], Training Loss: 0.5257\n",
            "Epoch [1317/20000], Training Loss: 0.5010\n",
            "Epoch [1318/20000], Training Loss: 0.5111\n",
            "Epoch [1319/20000], Training Loss: 0.4892\n",
            "Epoch [1320/20000], Training Loss: 0.5145\n",
            "Epoch [1321/20000], Training Loss: 0.4790\n",
            "Epoch [1322/20000], Training Loss: 0.4918\n",
            "Epoch [1323/20000], Training Loss: 0.4991\n",
            "Epoch [1324/20000], Training Loss: 0.4910\n",
            "Epoch [1325/20000], Training Loss: 0.5205\n",
            "Epoch [1326/20000], Training Loss: 0.4815\n",
            "Epoch [1327/20000], Training Loss: 0.4615\n",
            "Epoch [1328/20000], Training Loss: 0.5050\n",
            "Epoch [1329/20000], Training Loss: 0.5173\n",
            "Epoch [1330/20000], Training Loss: 0.4846\n",
            "Epoch [1331/20000], Training Loss: 0.4983\n",
            "Epoch [1332/20000], Training Loss: 0.4923\n",
            "Epoch [1333/20000], Training Loss: 0.4845\n",
            "Epoch [1334/20000], Training Loss: 0.5625\n",
            "Epoch [1335/20000], Training Loss: 0.4511\n",
            "Epoch [1336/20000], Training Loss: 0.4839\n",
            "Epoch [1337/20000], Training Loss: 0.5258\n",
            "Epoch [1338/20000], Training Loss: 0.4999\n",
            "Epoch [1339/20000], Training Loss: 0.4780\n",
            "Epoch [1340/20000], Training Loss: 0.4854\n",
            "Epoch [1341/20000], Training Loss: 0.4754\n",
            "Epoch [1342/20000], Training Loss: 0.4973\n",
            "Epoch [1343/20000], Training Loss: 0.4934\n",
            "Epoch [1344/20000], Training Loss: 0.4952\n",
            "Epoch [1345/20000], Training Loss: 0.4575\n",
            "Epoch [1346/20000], Training Loss: 0.4782\n",
            "Epoch [1347/20000], Training Loss: 0.4708\n",
            "Epoch [1348/20000], Training Loss: 0.4852\n",
            "Epoch [1349/20000], Training Loss: 0.4969\n",
            "Epoch [1350/20000], Training Loss: 0.5053\n",
            "Epoch [1351/20000], Training Loss: 0.4745\n",
            "Epoch [1352/20000], Training Loss: 0.5050\n",
            "Epoch [1353/20000], Training Loss: 0.5309\n",
            "Epoch [1354/20000], Training Loss: 0.4415\n",
            "Epoch [1355/20000], Training Loss: 0.4781\n",
            "Epoch [1356/20000], Training Loss: 0.4436\n",
            "Epoch [1357/20000], Training Loss: 0.5095\n",
            "Epoch [1358/20000], Training Loss: 0.4758\n",
            "Epoch [1359/20000], Training Loss: 0.5080\n",
            "Epoch [1360/20000], Training Loss: 0.4777\n",
            "Epoch [1361/20000], Training Loss: 0.4933\n",
            "Epoch [1362/20000], Training Loss: 0.4347\n",
            "Epoch [1363/20000], Training Loss: 0.4781\n",
            "Epoch [1364/20000], Training Loss: 0.5324\n",
            "Epoch [1365/20000], Training Loss: 0.5064\n",
            "Epoch [1366/20000], Training Loss: 0.5155\n",
            "Epoch [1367/20000], Training Loss: 0.4639\n",
            "Epoch [1368/20000], Training Loss: 0.4898\n",
            "Epoch [1369/20000], Training Loss: 0.4906\n",
            "Epoch [1370/20000], Training Loss: 0.4621\n",
            "Epoch [1371/20000], Training Loss: 0.5337\n",
            "Epoch [1372/20000], Training Loss: 0.4959\n",
            "Epoch [1373/20000], Training Loss: 0.4794\n",
            "Epoch [1374/20000], Training Loss: 0.4598\n",
            "Epoch [1375/20000], Training Loss: 0.4960\n",
            "Epoch [1376/20000], Training Loss: 0.5059\n",
            "Epoch [1377/20000], Training Loss: 0.4775\n",
            "Epoch [1378/20000], Training Loss: 0.4583\n",
            "Epoch [1379/20000], Training Loss: 0.5371\n",
            "Epoch [1380/20000], Training Loss: 0.4834\n",
            "Epoch [1381/20000], Training Loss: 0.4572\n",
            "Epoch [1382/20000], Training Loss: 0.5353\n",
            "Epoch [1383/20000], Training Loss: 0.5173\n",
            "Epoch [1384/20000], Training Loss: 0.5234\n",
            "Epoch [1385/20000], Training Loss: 0.4936\n",
            "Epoch [1386/20000], Training Loss: 0.4552\n",
            "Epoch [1387/20000], Training Loss: 0.4611\n",
            "Epoch [1388/20000], Training Loss: 0.5066\n",
            "Epoch [1389/20000], Training Loss: 0.4919\n",
            "Epoch [1390/20000], Training Loss: 0.4723\n",
            "Epoch [1391/20000], Training Loss: 0.5047\n",
            "Epoch [1392/20000], Training Loss: 0.5369\n",
            "Epoch [1393/20000], Training Loss: 0.4981\n",
            "Epoch [1394/20000], Training Loss: 0.5265\n",
            "Epoch [1395/20000], Training Loss: 0.4804\n",
            "Epoch [1396/20000], Training Loss: 0.5523\n",
            "Epoch [1397/20000], Training Loss: 0.5330\n",
            "Epoch [1398/20000], Training Loss: 0.4862\n",
            "Epoch [1399/20000], Training Loss: 0.4690\n",
            "Epoch [1400/20000], Training Loss: 0.5060\n",
            "Epoch [1401/20000], Training Loss: 0.4782\n",
            "Epoch [1402/20000], Training Loss: 0.4587\n",
            "Epoch [1403/20000], Training Loss: 0.4620\n",
            "Epoch [1404/20000], Training Loss: 0.5419\n",
            "Epoch [1405/20000], Training Loss: 0.5164\n",
            "Epoch [1406/20000], Training Loss: 0.5327\n",
            "Epoch [1407/20000], Training Loss: 0.5325\n",
            "Epoch [1408/20000], Training Loss: 0.4983\n",
            "Epoch [1409/20000], Training Loss: 0.4522\n",
            "Epoch [1410/20000], Training Loss: 0.4674\n",
            "Epoch [1411/20000], Training Loss: 0.4398\n",
            "Epoch [1412/20000], Training Loss: 0.5212\n",
            "Epoch [1413/20000], Training Loss: 0.4590\n",
            "Epoch [1414/20000], Training Loss: 0.4848\n",
            "Epoch [1415/20000], Training Loss: 0.4768\n",
            "Epoch [1416/20000], Training Loss: 0.4754\n",
            "Epoch [1417/20000], Training Loss: 0.5130\n",
            "Epoch [1418/20000], Training Loss: 0.4951\n",
            "Epoch [1419/20000], Training Loss: 0.5117\n",
            "Epoch [1420/20000], Training Loss: 0.5111\n",
            "Epoch [1421/20000], Training Loss: 0.5286\n",
            "Epoch [1422/20000], Training Loss: 0.5167\n",
            "Epoch [1423/20000], Training Loss: 0.4777\n",
            "Epoch [1424/20000], Training Loss: 0.5282\n",
            "Epoch [1425/20000], Training Loss: 0.4940\n",
            "Epoch [1426/20000], Training Loss: 0.5379\n",
            "Epoch [1427/20000], Training Loss: 0.4782\n",
            "Epoch [1428/20000], Training Loss: 0.4826\n",
            "Epoch [1429/20000], Training Loss: 0.5087\n",
            "Epoch [1430/20000], Training Loss: 0.5389\n",
            "Epoch [1431/20000], Training Loss: 0.4845\n",
            "Epoch [1432/20000], Training Loss: 0.5222\n",
            "Epoch [1433/20000], Training Loss: 0.4315\n",
            "Epoch [1434/20000], Training Loss: 0.4866\n",
            "Epoch [1435/20000], Training Loss: 0.5321\n",
            "Epoch [1436/20000], Training Loss: 0.4819\n",
            "Epoch [1437/20000], Training Loss: 0.4648\n",
            "Epoch [1438/20000], Training Loss: 0.4414\n",
            "Epoch [1439/20000], Training Loss: 0.5268\n",
            "Epoch [1440/20000], Training Loss: 0.4517\n",
            "Epoch [1441/20000], Training Loss: 0.4634\n",
            "Epoch [1442/20000], Training Loss: 0.5102\n",
            "Epoch [1443/20000], Training Loss: 0.4933\n",
            "Epoch [1444/20000], Training Loss: 0.5253\n",
            "Epoch [1445/20000], Training Loss: 0.5233\n",
            "Epoch [1446/20000], Training Loss: 0.4881\n",
            "Epoch [1447/20000], Training Loss: 0.4908\n",
            "Epoch [1448/20000], Training Loss: 0.4816\n",
            "Epoch [1449/20000], Training Loss: 0.5163\n",
            "Epoch [1450/20000], Training Loss: 0.4728\n",
            "Epoch [1451/20000], Training Loss: 0.4956\n",
            "Epoch [1452/20000], Training Loss: 0.4459\n",
            "Epoch [1453/20000], Training Loss: 0.4641\n",
            "Epoch [1454/20000], Training Loss: 0.4945\n",
            "Epoch [1455/20000], Training Loss: 0.4791\n",
            "Epoch [1456/20000], Training Loss: 0.4788\n",
            "Epoch [1457/20000], Training Loss: 0.5023\n",
            "Epoch [1458/20000], Training Loss: 0.4983\n",
            "Epoch [1459/20000], Training Loss: 0.4484\n",
            "Epoch [1460/20000], Training Loss: 0.5220\n",
            "Epoch [1461/20000], Training Loss: 0.4562\n",
            "Epoch [1462/20000], Training Loss: 0.5055\n",
            "Epoch [1463/20000], Training Loss: 0.5071\n",
            "Epoch [1464/20000], Training Loss: 0.4656\n",
            "Epoch [1465/20000], Training Loss: 0.4568\n",
            "Epoch [1466/20000], Training Loss: 0.4764\n",
            "Epoch [1467/20000], Training Loss: 0.4653\n",
            "Epoch [1468/20000], Training Loss: 0.4776\n",
            "Epoch [1469/20000], Training Loss: 0.4624\n",
            "Epoch [1470/20000], Training Loss: 0.4786\n",
            "Epoch [1471/20000], Training Loss: 0.5228\n",
            "Epoch [1472/20000], Training Loss: 0.5036\n",
            "Epoch [1473/20000], Training Loss: 0.4990\n",
            "Epoch [1474/20000], Training Loss: 0.5100\n",
            "Epoch [1475/20000], Training Loss: 0.4585\n",
            "Epoch [1476/20000], Training Loss: 0.5021\n",
            "Epoch [1477/20000], Training Loss: 0.4603\n",
            "Epoch [1478/20000], Training Loss: 0.4570\n",
            "Epoch [1479/20000], Training Loss: 0.4991\n",
            "Epoch [1480/20000], Training Loss: 0.5032\n",
            "Epoch [1481/20000], Training Loss: 0.4841\n",
            "Epoch [1482/20000], Training Loss: 0.5010\n",
            "Epoch [1483/20000], Training Loss: 0.4841\n",
            "Epoch [1484/20000], Training Loss: 0.4716\n",
            "Epoch [1485/20000], Training Loss: 0.4814\n",
            "Epoch [1486/20000], Training Loss: 0.5199\n",
            "Epoch [1487/20000], Training Loss: 0.5056\n",
            "Epoch [1488/20000], Training Loss: 0.4335\n",
            "Epoch [1489/20000], Training Loss: 0.4797\n",
            "Epoch [1490/20000], Training Loss: 0.4799\n",
            "Epoch [1491/20000], Training Loss: 0.4508\n",
            "Epoch [1492/20000], Training Loss: 0.4628\n",
            "Epoch [1493/20000], Training Loss: 0.5004\n",
            "Epoch [1494/20000], Training Loss: 0.4499\n",
            "Epoch [1495/20000], Training Loss: 0.4939\n",
            "Epoch [1496/20000], Training Loss: 0.5011\n",
            "Epoch [1497/20000], Training Loss: 0.4744\n",
            "Epoch [1498/20000], Training Loss: 0.4596\n",
            "Epoch [1499/20000], Training Loss: 0.4856\n",
            "Epoch [1500/20000], Training Loss: 0.4628\n",
            "Epoch [1501/20000], Training Loss: 0.5130\n",
            "Epoch [1502/20000], Training Loss: 0.4973\n",
            "Epoch [1503/20000], Training Loss: 0.4670\n",
            "Epoch [1504/20000], Training Loss: 0.4807\n",
            "Epoch [1505/20000], Training Loss: 0.5304\n",
            "Epoch [1506/20000], Training Loss: 0.4653\n",
            "Epoch [1507/20000], Training Loss: 0.5396\n",
            "Epoch [1508/20000], Training Loss: 0.5086\n",
            "Epoch [1509/20000], Training Loss: 0.4845\n",
            "Epoch [1510/20000], Training Loss: 0.4518\n",
            "Epoch [1511/20000], Training Loss: 0.4673\n",
            "Epoch [1512/20000], Training Loss: 0.5318\n",
            "Epoch [1513/20000], Training Loss: 0.4859\n",
            "Epoch [1514/20000], Training Loss: 0.5324\n",
            "Epoch [1515/20000], Training Loss: 0.5243\n",
            "Epoch [1516/20000], Training Loss: 0.4573\n",
            "Epoch [1517/20000], Training Loss: 0.4644\n",
            "Epoch [1518/20000], Training Loss: 0.4548\n",
            "Epoch [1519/20000], Training Loss: 0.5024\n",
            "Epoch [1520/20000], Training Loss: 0.4799\n",
            "Epoch [1521/20000], Training Loss: 0.4779\n",
            "Epoch [1522/20000], Training Loss: 0.4749\n",
            "Epoch [1523/20000], Training Loss: 0.4845\n",
            "Epoch [1524/20000], Training Loss: 0.4676\n",
            "Epoch [1525/20000], Training Loss: 0.4646\n",
            "Epoch [1526/20000], Training Loss: 0.4628\n",
            "Epoch [1527/20000], Training Loss: 0.5149\n",
            "Epoch [1528/20000], Training Loss: 0.5093\n",
            "Epoch [1529/20000], Training Loss: 0.4825\n",
            "Epoch [1530/20000], Training Loss: 0.4639\n",
            "Epoch [1531/20000], Training Loss: 0.4891\n",
            "Epoch [1532/20000], Training Loss: 0.4395\n",
            "Epoch [1533/20000], Training Loss: 0.4784\n",
            "Epoch [1534/20000], Training Loss: 0.4758\n",
            "Epoch [1535/20000], Training Loss: 0.4809\n",
            "Epoch [1536/20000], Training Loss: 0.4670\n",
            "Epoch [1537/20000], Training Loss: 0.5135\n",
            "Epoch [1538/20000], Training Loss: 0.4414\n",
            "Epoch [1539/20000], Training Loss: 0.5096\n",
            "Epoch [1540/20000], Training Loss: 0.4685\n",
            "Epoch [1541/20000], Training Loss: 0.4460\n",
            "Epoch [1542/20000], Training Loss: 0.5014\n",
            "Epoch [1543/20000], Training Loss: 0.4719\n",
            "Epoch [1544/20000], Training Loss: 0.4760\n",
            "Epoch [1545/20000], Training Loss: 0.5480\n",
            "Epoch [1546/20000], Training Loss: 0.5125\n",
            "Epoch [1547/20000], Training Loss: 0.5037\n",
            "Epoch [1548/20000], Training Loss: 0.4999\n",
            "Epoch [1549/20000], Training Loss: 0.4924\n",
            "Epoch [1550/20000], Training Loss: 0.4597\n",
            "Epoch [1551/20000], Training Loss: 0.4814\n",
            "Epoch [1552/20000], Training Loss: 0.4523\n",
            "Epoch [1553/20000], Training Loss: 0.4896\n",
            "Epoch [1554/20000], Training Loss: 0.4638\n",
            "Epoch [1555/20000], Training Loss: 0.4886\n",
            "Epoch [1556/20000], Training Loss: 0.4672\n",
            "Epoch [1557/20000], Training Loss: 0.4871\n",
            "Epoch [1558/20000], Training Loss: 0.5122\n",
            "Epoch [1559/20000], Training Loss: 0.5259\n",
            "Epoch [1560/20000], Training Loss: 0.4987\n",
            "Epoch [1561/20000], Training Loss: 0.4711\n",
            "Epoch [1562/20000], Training Loss: 0.5357\n",
            "Epoch [1563/20000], Training Loss: 0.4981\n",
            "Epoch [1564/20000], Training Loss: 0.4574\n",
            "Epoch [1565/20000], Training Loss: 0.5158\n",
            "Epoch [1566/20000], Training Loss: 0.4606\n",
            "Epoch [1567/20000], Training Loss: 0.4520\n",
            "Epoch [1568/20000], Training Loss: 0.5283\n",
            "Epoch [1569/20000], Training Loss: 0.4949\n",
            "Epoch [1570/20000], Training Loss: 0.4659\n",
            "Epoch [1571/20000], Training Loss: 0.4920\n",
            "Epoch [1572/20000], Training Loss: 0.5291\n",
            "Epoch [1573/20000], Training Loss: 0.5682\n",
            "Epoch [1574/20000], Training Loss: 0.4692\n",
            "Epoch [1575/20000], Training Loss: 0.5273\n",
            "Epoch [1576/20000], Training Loss: 0.5342\n",
            "Epoch [1577/20000], Training Loss: 0.5036\n",
            "Epoch [1578/20000], Training Loss: 0.5065\n",
            "Epoch [1579/20000], Training Loss: 0.4848\n",
            "Epoch [1580/20000], Training Loss: 0.5105\n",
            "Epoch [1581/20000], Training Loss: 0.4689\n",
            "Epoch [1582/20000], Training Loss: 0.4932\n",
            "Epoch [1583/20000], Training Loss: 0.4903\n",
            "Epoch [1584/20000], Training Loss: 0.4738\n",
            "Epoch [1585/20000], Training Loss: 0.4362\n",
            "Epoch [1586/20000], Training Loss: 0.4619\n",
            "Epoch [1587/20000], Training Loss: 0.4880\n",
            "Epoch [1588/20000], Training Loss: 0.5209\n",
            "Epoch [1589/20000], Training Loss: 0.4627\n",
            "Epoch [1590/20000], Training Loss: 0.4690\n",
            "Epoch [1591/20000], Training Loss: 0.5255\n",
            "Epoch [1592/20000], Training Loss: 0.5059\n",
            "Epoch [1593/20000], Training Loss: 0.4999\n",
            "Epoch [1594/20000], Training Loss: 0.4438\n",
            "Epoch [1595/20000], Training Loss: 0.5384\n",
            "Epoch [1596/20000], Training Loss: 0.4632\n",
            "Epoch [1597/20000], Training Loss: 0.4625\n",
            "Epoch [1598/20000], Training Loss: 0.5111\n",
            "Epoch [1599/20000], Training Loss: 0.5153\n",
            "Epoch [1600/20000], Training Loss: 0.4772\n",
            "Epoch [1601/20000], Training Loss: 0.5242\n",
            "Epoch [1602/20000], Training Loss: 0.5182\n",
            "Epoch [1603/20000], Training Loss: 0.5064\n",
            "Epoch [1604/20000], Training Loss: 0.5217\n",
            "Epoch [1605/20000], Training Loss: 0.4919\n",
            "Epoch [1606/20000], Training Loss: 0.4670\n",
            "Epoch [1607/20000], Training Loss: 0.4844\n",
            "Epoch [1608/20000], Training Loss: 0.5117\n",
            "Epoch [1609/20000], Training Loss: 0.4534\n",
            "Epoch [1610/20000], Training Loss: 0.5504\n",
            "Epoch [1611/20000], Training Loss: 0.4745\n",
            "Epoch [1612/20000], Training Loss: 0.4926\n",
            "Epoch [1613/20000], Training Loss: 0.4542\n",
            "Epoch [1614/20000], Training Loss: 0.4918\n",
            "Epoch [1615/20000], Training Loss: 0.4771\n",
            "Epoch [1616/20000], Training Loss: 0.4522\n",
            "Epoch [1617/20000], Training Loss: 0.5101\n",
            "Epoch [1618/20000], Training Loss: 0.5169\n",
            "Epoch [1619/20000], Training Loss: 0.5377\n",
            "Epoch [1620/20000], Training Loss: 0.4405\n",
            "Epoch [1621/20000], Training Loss: 0.4744\n",
            "Epoch [1622/20000], Training Loss: 0.4886\n",
            "Epoch [1623/20000], Training Loss: 0.5247\n",
            "Epoch [1624/20000], Training Loss: 0.5267\n",
            "Epoch [1625/20000], Training Loss: 0.4983\n",
            "Epoch [1626/20000], Training Loss: 0.4742\n",
            "Epoch [1627/20000], Training Loss: 0.4949\n",
            "Epoch [1628/20000], Training Loss: 0.5378\n",
            "Epoch [1629/20000], Training Loss: 0.4826\n",
            "Epoch [1630/20000], Training Loss: 0.4840\n",
            "Epoch [1631/20000], Training Loss: 0.4892\n",
            "Epoch [1632/20000], Training Loss: 0.4640\n",
            "Epoch [1633/20000], Training Loss: 0.5177\n",
            "Epoch [1634/20000], Training Loss: 0.5064\n",
            "Epoch [1635/20000], Training Loss: 0.4721\n",
            "Epoch [1636/20000], Training Loss: 0.5108\n",
            "Epoch [1637/20000], Training Loss: 0.4716\n",
            "Epoch [1638/20000], Training Loss: 0.4962\n",
            "Epoch [1639/20000], Training Loss: 0.4668\n",
            "Epoch [1640/20000], Training Loss: 0.5289\n",
            "Epoch [1641/20000], Training Loss: 0.5034\n",
            "Epoch [1642/20000], Training Loss: 0.5272\n",
            "Epoch [1643/20000], Training Loss: 0.4806\n",
            "Epoch [1644/20000], Training Loss: 0.5387\n",
            "Epoch [1645/20000], Training Loss: 0.5170\n",
            "Epoch [1646/20000], Training Loss: 0.5130\n",
            "Epoch [1647/20000], Training Loss: 0.4572\n",
            "Epoch [1648/20000], Training Loss: 0.4901\n",
            "Epoch [1649/20000], Training Loss: 0.4978\n",
            "Epoch [1650/20000], Training Loss: 0.4615\n",
            "Epoch [1651/20000], Training Loss: 0.4719\n",
            "Epoch [1652/20000], Training Loss: 0.4598\n",
            "Epoch [1653/20000], Training Loss: 0.4926\n",
            "Epoch [1654/20000], Training Loss: 0.5380\n",
            "Epoch [1655/20000], Training Loss: 0.5101\n",
            "Epoch [1656/20000], Training Loss: 0.4519\n",
            "Epoch [1657/20000], Training Loss: 0.4753\n",
            "Epoch [1658/20000], Training Loss: 0.4624\n",
            "Epoch [1659/20000], Training Loss: 0.4801\n",
            "Epoch [1660/20000], Training Loss: 0.4421\n",
            "Epoch [1661/20000], Training Loss: 0.4543\n",
            "Epoch [1662/20000], Training Loss: 0.4552\n",
            "Epoch [1663/20000], Training Loss: 0.4774\n",
            "Epoch [1664/20000], Training Loss: 0.4731\n",
            "Epoch [1665/20000], Training Loss: 0.4878\n",
            "Epoch [1666/20000], Training Loss: 0.5101\n",
            "Epoch [1667/20000], Training Loss: 0.4558\n",
            "Epoch [1668/20000], Training Loss: 0.5146\n",
            "Epoch [1669/20000], Training Loss: 0.4390\n",
            "Epoch [1670/20000], Training Loss: 0.4548\n",
            "Epoch [1671/20000], Training Loss: 0.4738\n",
            "Epoch [1672/20000], Training Loss: 0.5060\n",
            "Epoch [1673/20000], Training Loss: 0.4695\n",
            "Epoch [1674/20000], Training Loss: 0.5126\n",
            "Epoch [1675/20000], Training Loss: 0.5092\n",
            "Epoch [1676/20000], Training Loss: 0.5462\n",
            "Epoch [1677/20000], Training Loss: 0.4991\n",
            "Epoch [1678/20000], Training Loss: 0.4939\n",
            "Epoch [1679/20000], Training Loss: 0.4900\n",
            "Epoch [1680/20000], Training Loss: 0.4790\n",
            "Epoch [1681/20000], Training Loss: 0.4678\n",
            "Epoch [1682/20000], Training Loss: 0.5093\n",
            "Epoch [1683/20000], Training Loss: 0.5247\n",
            "Epoch [1684/20000], Training Loss: 0.4846\n",
            "Epoch [1685/20000], Training Loss: 0.4662\n",
            "Epoch [1686/20000], Training Loss: 0.5271\n",
            "Epoch [1687/20000], Training Loss: 0.5176\n",
            "Epoch [1688/20000], Training Loss: 0.4785\n",
            "Epoch [1689/20000], Training Loss: 0.5332\n",
            "Epoch [1690/20000], Training Loss: 0.4516\n",
            "Epoch [1691/20000], Training Loss: 0.5475\n",
            "Epoch [1692/20000], Training Loss: 0.4831\n",
            "Epoch [1693/20000], Training Loss: 0.5038\n",
            "Epoch [1694/20000], Training Loss: 0.4700\n",
            "Epoch [1695/20000], Training Loss: 0.5115\n",
            "Epoch [1696/20000], Training Loss: 0.5373\n",
            "Epoch [1697/20000], Training Loss: 0.4946\n",
            "Epoch [1698/20000], Training Loss: 0.4758\n",
            "Epoch [1699/20000], Training Loss: 0.4902\n",
            "Epoch [1700/20000], Training Loss: 0.4691\n",
            "Epoch [1701/20000], Training Loss: 0.4652\n",
            "Epoch [1702/20000], Training Loss: 0.4586\n",
            "Epoch [1703/20000], Training Loss: 0.5150\n",
            "Epoch [1704/20000], Training Loss: 0.4701\n",
            "Epoch [1705/20000], Training Loss: 0.4928\n",
            "Epoch [1706/20000], Training Loss: 0.4872\n",
            "Epoch [1707/20000], Training Loss: 0.4642\n",
            "Epoch [1708/20000], Training Loss: 0.4893\n",
            "Epoch [1709/20000], Training Loss: 0.5110\n",
            "Epoch [1710/20000], Training Loss: 0.4970\n",
            "Epoch [1711/20000], Training Loss: 0.4839\n",
            "Epoch [1712/20000], Training Loss: 0.4641\n",
            "Epoch [1713/20000], Training Loss: 0.4923\n",
            "Epoch [1714/20000], Training Loss: 0.4620\n",
            "Epoch [1715/20000], Training Loss: 0.4499\n",
            "Epoch [1716/20000], Training Loss: 0.4928\n",
            "Epoch [1717/20000], Training Loss: 0.4822\n",
            "Epoch [1718/20000], Training Loss: 0.5072\n",
            "Epoch [1719/20000], Training Loss: 0.5030\n",
            "Epoch [1720/20000], Training Loss: 0.4796\n",
            "Epoch [1721/20000], Training Loss: 0.4901\n",
            "Epoch [1722/20000], Training Loss: 0.5298\n",
            "Epoch [1723/20000], Training Loss: 0.4751\n",
            "Epoch [1724/20000], Training Loss: 0.4811\n",
            "Epoch [1725/20000], Training Loss: 0.4737\n",
            "Epoch [1726/20000], Training Loss: 0.4457\n",
            "Epoch [1727/20000], Training Loss: 0.4663\n",
            "Epoch [1728/20000], Training Loss: 0.4429\n",
            "Epoch [1729/20000], Training Loss: 0.4628\n",
            "Epoch [1730/20000], Training Loss: 0.4932\n",
            "Epoch [1731/20000], Training Loss: 0.4941\n",
            "Epoch [1732/20000], Training Loss: 0.4404\n",
            "Epoch [1733/20000], Training Loss: 0.4427\n",
            "Epoch [1734/20000], Training Loss: 0.4777\n",
            "Epoch [1735/20000], Training Loss: 0.5008\n",
            "Epoch [1736/20000], Training Loss: 0.4511\n",
            "Epoch [1737/20000], Training Loss: 0.4733\n",
            "Epoch [1738/20000], Training Loss: 0.4820\n",
            "Epoch [1739/20000], Training Loss: 0.4911\n",
            "Epoch [1740/20000], Training Loss: 0.4398\n",
            "Epoch [1741/20000], Training Loss: 0.4865\n",
            "Epoch [1742/20000], Training Loss: 0.5017\n",
            "Epoch [1743/20000], Training Loss: 0.4817\n",
            "Epoch [1744/20000], Training Loss: 0.4747\n",
            "Epoch [1745/20000], Training Loss: 0.4724\n",
            "Epoch [1746/20000], Training Loss: 0.5177\n",
            "Epoch [1747/20000], Training Loss: 0.5039\n",
            "Epoch [1748/20000], Training Loss: 0.5307\n",
            "Epoch [1749/20000], Training Loss: 0.4668\n",
            "Epoch [1750/20000], Training Loss: 0.4848\n",
            "Epoch [1751/20000], Training Loss: 0.4734\n",
            "Epoch [1752/20000], Training Loss: 0.4753\n",
            "Epoch [1753/20000], Training Loss: 0.5220\n",
            "Epoch [1754/20000], Training Loss: 0.4551\n",
            "Epoch [1755/20000], Training Loss: 0.5107\n",
            "Epoch [1756/20000], Training Loss: 0.4660\n",
            "Epoch [1757/20000], Training Loss: 0.4345\n",
            "Epoch [1758/20000], Training Loss: 0.4699\n",
            "Epoch [1759/20000], Training Loss: 0.4965\n",
            "Epoch [1760/20000], Training Loss: 0.5151\n",
            "Epoch [1761/20000], Training Loss: 0.5112\n",
            "Epoch [1762/20000], Training Loss: 0.4819\n",
            "Epoch [1763/20000], Training Loss: 0.4505\n",
            "Epoch [1764/20000], Training Loss: 0.4626\n",
            "Epoch [1765/20000], Training Loss: 0.4703\n",
            "Epoch [1766/20000], Training Loss: 0.4705\n",
            "Epoch [1767/20000], Training Loss: 0.5180\n",
            "Epoch [1768/20000], Training Loss: 0.4994\n",
            "Epoch [1769/20000], Training Loss: 0.4400\n",
            "Epoch [1770/20000], Training Loss: 0.4603\n",
            "Epoch [1771/20000], Training Loss: 0.5065\n",
            "Epoch [1772/20000], Training Loss: 0.4753\n",
            "Epoch [1773/20000], Training Loss: 0.5036\n",
            "Epoch [1774/20000], Training Loss: 0.5178\n",
            "Epoch [1775/20000], Training Loss: 0.5150\n",
            "Epoch [1776/20000], Training Loss: 0.5263\n",
            "Epoch [1777/20000], Training Loss: 0.5013\n",
            "Epoch [1778/20000], Training Loss: 0.4942\n",
            "Epoch [1779/20000], Training Loss: 0.5103\n",
            "Epoch [1780/20000], Training Loss: 0.4778\n",
            "Epoch [1781/20000], Training Loss: 0.5070\n",
            "Epoch [1782/20000], Training Loss: 0.4727\n",
            "Epoch [1783/20000], Training Loss: 0.4982\n",
            "Epoch [1784/20000], Training Loss: 0.5344\n",
            "Epoch [1785/20000], Training Loss: 0.4875\n",
            "Epoch [1786/20000], Training Loss: 0.4626\n",
            "Epoch [1787/20000], Training Loss: 0.4827\n",
            "Epoch [1788/20000], Training Loss: 0.4516\n",
            "Epoch [1789/20000], Training Loss: 0.4729\n",
            "Epoch [1790/20000], Training Loss: 0.4970\n",
            "Epoch [1791/20000], Training Loss: 0.5065\n",
            "Epoch [1792/20000], Training Loss: 0.4774\n",
            "Epoch [1793/20000], Training Loss: 0.4714\n",
            "Epoch [1794/20000], Training Loss: 0.4989\n",
            "Epoch [1795/20000], Training Loss: 0.4870\n",
            "Epoch [1796/20000], Training Loss: 0.5154\n",
            "Epoch [1797/20000], Training Loss: 0.4992\n",
            "Epoch [1798/20000], Training Loss: 0.5174\n",
            "Epoch [1799/20000], Training Loss: 0.4853\n",
            "Epoch [1800/20000], Training Loss: 0.5401\n",
            "Epoch [1801/20000], Training Loss: 0.4950\n",
            "Epoch [1802/20000], Training Loss: 0.4717\n",
            "Epoch [1803/20000], Training Loss: 0.4758\n",
            "Epoch [1804/20000], Training Loss: 0.5132\n",
            "Epoch [1805/20000], Training Loss: 0.5013\n",
            "Epoch [1806/20000], Training Loss: 0.4650\n",
            "Epoch [1807/20000], Training Loss: 0.4996\n",
            "Epoch [1808/20000], Training Loss: 0.4911\n",
            "Epoch [1809/20000], Training Loss: 0.5082\n",
            "Epoch [1810/20000], Training Loss: 0.5190\n",
            "Epoch [1811/20000], Training Loss: 0.4786\n",
            "Epoch [1812/20000], Training Loss: 0.5262\n",
            "Epoch [1813/20000], Training Loss: 0.4711\n",
            "Epoch [1814/20000], Training Loss: 0.4582\n",
            "Epoch [1815/20000], Training Loss: 0.4489\n",
            "Epoch [1816/20000], Training Loss: 0.4838\n",
            "Epoch [1817/20000], Training Loss: 0.4925\n",
            "Epoch [1818/20000], Training Loss: 0.4601\n",
            "Epoch [1819/20000], Training Loss: 0.5201\n",
            "Epoch [1820/20000], Training Loss: 0.5108\n",
            "Epoch [1821/20000], Training Loss: 0.5119\n",
            "Epoch [1822/20000], Training Loss: 0.4932\n",
            "Epoch [1823/20000], Training Loss: 0.4893\n",
            "Epoch [1824/20000], Training Loss: 0.5039\n",
            "Epoch [1825/20000], Training Loss: 0.5071\n",
            "Epoch [1826/20000], Training Loss: 0.4702\n",
            "Epoch [1827/20000], Training Loss: 0.5354\n",
            "Epoch [1828/20000], Training Loss: 0.5057\n",
            "Epoch [1829/20000], Training Loss: 0.4909\n",
            "Epoch [1830/20000], Training Loss: 0.4859\n",
            "Epoch [1831/20000], Training Loss: 0.5073\n",
            "Epoch [1832/20000], Training Loss: 0.4711\n",
            "Epoch [1833/20000], Training Loss: 0.5242\n",
            "Epoch [1834/20000], Training Loss: 0.4755\n",
            "Epoch [1835/20000], Training Loss: 0.4760\n",
            "Epoch [1836/20000], Training Loss: 0.5200\n",
            "Epoch [1837/20000], Training Loss: 0.4796\n",
            "Epoch [1838/20000], Training Loss: 0.5080\n",
            "Epoch [1839/20000], Training Loss: 0.4767\n",
            "Epoch [1840/20000], Training Loss: 0.5011\n",
            "Epoch [1841/20000], Training Loss: 0.5140\n",
            "Epoch [1842/20000], Training Loss: 0.4689\n",
            "Epoch [1843/20000], Training Loss: 0.4872\n",
            "Epoch [1844/20000], Training Loss: 0.5265\n",
            "Epoch [1845/20000], Training Loss: 0.4864\n",
            "Epoch [1846/20000], Training Loss: 0.4998\n",
            "Epoch [1847/20000], Training Loss: 0.5169\n",
            "Epoch [1848/20000], Training Loss: 0.4901\n",
            "Epoch [1849/20000], Training Loss: 0.5268\n",
            "Epoch [1850/20000], Training Loss: 0.5069\n",
            "Epoch [1851/20000], Training Loss: 0.5217\n",
            "Epoch [1852/20000], Training Loss: 0.5145\n",
            "Epoch [1853/20000], Training Loss: 0.4520\n",
            "Epoch [1854/20000], Training Loss: 0.4541\n",
            "Epoch [1855/20000], Training Loss: 0.4640\n",
            "Epoch [1856/20000], Training Loss: 0.4779\n",
            "Epoch [1857/20000], Training Loss: 0.5370\n",
            "Epoch [1858/20000], Training Loss: 0.4645\n",
            "Epoch [1859/20000], Training Loss: 0.4964\n",
            "Epoch [1860/20000], Training Loss: 0.4996\n",
            "Epoch [1861/20000], Training Loss: 0.4645\n",
            "Epoch [1862/20000], Training Loss: 0.5336\n",
            "Epoch [1863/20000], Training Loss: 0.4449\n",
            "Epoch [1864/20000], Training Loss: 0.4843\n",
            "Epoch [1865/20000], Training Loss: 0.4611\n",
            "Epoch [1866/20000], Training Loss: 0.4699\n",
            "Epoch [1867/20000], Training Loss: 0.4740\n",
            "Epoch [1868/20000], Training Loss: 0.4656\n",
            "Epoch [1869/20000], Training Loss: 0.4922\n",
            "Epoch [1870/20000], Training Loss: 0.4942\n",
            "Epoch [1871/20000], Training Loss: 0.4746\n",
            "Epoch [1872/20000], Training Loss: 0.4690\n",
            "Epoch [1873/20000], Training Loss: 0.4489\n",
            "Epoch [1874/20000], Training Loss: 0.4490\n",
            "Epoch [1875/20000], Training Loss: 0.4799\n",
            "Epoch [1876/20000], Training Loss: 0.4681\n",
            "Epoch [1877/20000], Training Loss: 0.4918\n",
            "Epoch [1878/20000], Training Loss: 0.4686\n",
            "Epoch [1879/20000], Training Loss: 0.4563\n",
            "Epoch [1880/20000], Training Loss: 0.4981\n",
            "Epoch [1881/20000], Training Loss: 0.5150\n",
            "Epoch [1882/20000], Training Loss: 0.4787\n",
            "Epoch [1883/20000], Training Loss: 0.4573\n",
            "Epoch [1884/20000], Training Loss: 0.5034\n",
            "Epoch [1885/20000], Training Loss: 0.5303\n",
            "Epoch [1886/20000], Training Loss: 0.4673\n",
            "Epoch [1887/20000], Training Loss: 0.4994\n",
            "Epoch [1888/20000], Training Loss: 0.4660\n",
            "Epoch [1889/20000], Training Loss: 0.5029\n",
            "Epoch [1890/20000], Training Loss: 0.4670\n",
            "Epoch [1891/20000], Training Loss: 0.4683\n",
            "Epoch [1892/20000], Training Loss: 0.4963\n",
            "Epoch [1893/20000], Training Loss: 0.4437\n",
            "Epoch [1894/20000], Training Loss: 0.4890\n",
            "Epoch [1895/20000], Training Loss: 0.4715\n",
            "Epoch [1896/20000], Training Loss: 0.4728\n",
            "Epoch [1897/20000], Training Loss: 0.4840\n",
            "Epoch [1898/20000], Training Loss: 0.4504\n",
            "Epoch [1899/20000], Training Loss: 0.4521\n",
            "Epoch [1900/20000], Training Loss: 0.4376\n",
            "Epoch [1901/20000], Training Loss: 0.4656\n",
            "Epoch [1902/20000], Training Loss: 0.4851\n",
            "Epoch [1903/20000], Training Loss: 0.5089\n",
            "Epoch [1904/20000], Training Loss: 0.5017\n",
            "Epoch [1905/20000], Training Loss: 0.5093\n",
            "Epoch [1906/20000], Training Loss: 0.5232\n",
            "Epoch [1907/20000], Training Loss: 0.4882\n",
            "Epoch [1908/20000], Training Loss: 0.4839\n",
            "Epoch [1909/20000], Training Loss: 0.4928\n",
            "Epoch [1910/20000], Training Loss: 0.4547\n",
            "Epoch [1911/20000], Training Loss: 0.4917\n",
            "Epoch [1912/20000], Training Loss: 0.5138\n",
            "Epoch [1913/20000], Training Loss: 0.5189\n",
            "Epoch [1914/20000], Training Loss: 0.5144\n",
            "Epoch [1915/20000], Training Loss: 0.4593\n",
            "Epoch [1916/20000], Training Loss: 0.5192\n",
            "Epoch [1917/20000], Training Loss: 0.4566\n",
            "Epoch [1918/20000], Training Loss: 0.5483\n",
            "Epoch [1919/20000], Training Loss: 0.4642\n",
            "Epoch [1920/20000], Training Loss: 0.5110\n",
            "Epoch [1921/20000], Training Loss: 0.5386\n",
            "Epoch [1922/20000], Training Loss: 0.4414\n",
            "Epoch [1923/20000], Training Loss: 0.4952\n",
            "Epoch [1924/20000], Training Loss: 0.4654\n",
            "Epoch [1925/20000], Training Loss: 0.5352\n",
            "Epoch [1926/20000], Training Loss: 0.4831\n",
            "Epoch [1927/20000], Training Loss: 0.5130\n",
            "Epoch [1928/20000], Training Loss: 0.4828\n",
            "Epoch [1929/20000], Training Loss: 0.4889\n",
            "Epoch [1930/20000], Training Loss: 0.4640\n",
            "Epoch [1931/20000], Training Loss: 0.4669\n",
            "Epoch [1932/20000], Training Loss: 0.5024\n",
            "Epoch [1933/20000], Training Loss: 0.5066\n",
            "Epoch [1934/20000], Training Loss: 0.5038\n",
            "Epoch [1935/20000], Training Loss: 0.4870\n",
            "Epoch [1936/20000], Training Loss: 0.5341\n",
            "Epoch [1937/20000], Training Loss: 0.4198\n",
            "Epoch [1938/20000], Training Loss: 0.4534\n",
            "Epoch [1939/20000], Training Loss: 0.5425\n",
            "Epoch [1940/20000], Training Loss: 0.4688\n",
            "Epoch [1941/20000], Training Loss: 0.4704\n",
            "Epoch [1942/20000], Training Loss: 0.5431\n",
            "Epoch [1943/20000], Training Loss: 0.4905\n",
            "Epoch [1944/20000], Training Loss: 0.5110\n",
            "Epoch [1945/20000], Training Loss: 0.4818\n",
            "Epoch [1946/20000], Training Loss: 0.4308\n",
            "Epoch [1947/20000], Training Loss: 0.5315\n",
            "Epoch [1948/20000], Training Loss: 0.4916\n",
            "Epoch [1949/20000], Training Loss: 0.4813\n",
            "Epoch [1950/20000], Training Loss: 0.4968\n",
            "Epoch [1951/20000], Training Loss: 0.4907\n",
            "Epoch [1952/20000], Training Loss: 0.4954\n",
            "Epoch [1953/20000], Training Loss: 0.4315\n",
            "Epoch [1954/20000], Training Loss: 0.4905\n",
            "Epoch [1955/20000], Training Loss: 0.5179\n",
            "Epoch [1956/20000], Training Loss: 0.4788\n",
            "Epoch [1957/20000], Training Loss: 0.5101\n",
            "Epoch [1958/20000], Training Loss: 0.4869\n",
            "Epoch [1959/20000], Training Loss: 0.4629\n",
            "Epoch [1960/20000], Training Loss: 0.4450\n",
            "Epoch [1961/20000], Training Loss: 0.4424\n",
            "Epoch [1962/20000], Training Loss: 0.4798\n",
            "Epoch [1963/20000], Training Loss: 0.4605\n",
            "Epoch [1964/20000], Training Loss: 0.4943\n",
            "Epoch [1965/20000], Training Loss: 0.5178\n",
            "Epoch [1966/20000], Training Loss: 0.4761\n",
            "Epoch [1967/20000], Training Loss: 0.5100\n",
            "Epoch [1968/20000], Training Loss: 0.4846\n",
            "Epoch [1969/20000], Training Loss: 0.4959\n",
            "Epoch [1970/20000], Training Loss: 0.4894\n",
            "Epoch [1971/20000], Training Loss: 0.4617\n",
            "Epoch [1972/20000], Training Loss: 0.5138\n",
            "Epoch [1973/20000], Training Loss: 0.4602\n",
            "Epoch [1974/20000], Training Loss: 0.4656\n",
            "Epoch [1975/20000], Training Loss: 0.4879\n",
            "Epoch [1976/20000], Training Loss: 0.5213\n",
            "Epoch [1977/20000], Training Loss: 0.4991\n",
            "Epoch [1978/20000], Training Loss: 0.4998\n",
            "Epoch [1979/20000], Training Loss: 0.5207\n",
            "Epoch [1980/20000], Training Loss: 0.5000\n",
            "Epoch [1981/20000], Training Loss: 0.5059\n",
            "Epoch [1982/20000], Training Loss: 0.4906\n",
            "Epoch [1983/20000], Training Loss: 0.4969\n",
            "Epoch [1984/20000], Training Loss: 0.4888\n",
            "Epoch [1985/20000], Training Loss: 0.4951\n",
            "Epoch [1986/20000], Training Loss: 0.4628\n",
            "Epoch [1987/20000], Training Loss: 0.4864\n",
            "Epoch [1988/20000], Training Loss: 0.4520\n",
            "Epoch [1989/20000], Training Loss: 0.4444\n",
            "Epoch [1990/20000], Training Loss: 0.4551\n",
            "Epoch [1991/20000], Training Loss: 0.4533\n",
            "Epoch [1992/20000], Training Loss: 0.4627\n",
            "Epoch [1993/20000], Training Loss: 0.5189\n",
            "Epoch [1994/20000], Training Loss: 0.5158\n",
            "Epoch [1995/20000], Training Loss: 0.4579\n",
            "Epoch [1996/20000], Training Loss: 0.5194\n",
            "Epoch [1997/20000], Training Loss: 0.4772\n",
            "Epoch [1998/20000], Training Loss: 0.5026\n",
            "Epoch [1999/20000], Training Loss: 0.4766\n",
            "Epoch [2000/20000], Training Loss: 0.4896\n",
            "Epoch [2001/20000], Training Loss: 0.4673\n",
            "Epoch [2002/20000], Training Loss: 0.4565\n",
            "Epoch [2003/20000], Training Loss: 0.5037\n",
            "Epoch [2004/20000], Training Loss: 0.4733\n",
            "Epoch [2005/20000], Training Loss: 0.4513\n",
            "Epoch [2006/20000], Training Loss: 0.5126\n",
            "Epoch [2007/20000], Training Loss: 0.4906\n",
            "Epoch [2008/20000], Training Loss: 0.4762\n",
            "Epoch [2009/20000], Training Loss: 0.4663\n",
            "Epoch [2010/20000], Training Loss: 0.4662\n",
            "Epoch [2011/20000], Training Loss: 0.4582\n",
            "Epoch [2012/20000], Training Loss: 0.4968\n",
            "Epoch [2013/20000], Training Loss: 0.5147\n",
            "Epoch [2014/20000], Training Loss: 0.4908\n",
            "Epoch [2015/20000], Training Loss: 0.5098\n",
            "Epoch [2016/20000], Training Loss: 0.4971\n",
            "Epoch [2017/20000], Training Loss: 0.4780\n",
            "Epoch [2018/20000], Training Loss: 0.4942\n",
            "Epoch [2019/20000], Training Loss: 0.5190\n",
            "Epoch [2020/20000], Training Loss: 0.4790\n",
            "Epoch [2021/20000], Training Loss: 0.4604\n",
            "Epoch [2022/20000], Training Loss: 0.5054\n",
            "Epoch [2023/20000], Training Loss: 0.4589\n",
            "Epoch [2024/20000], Training Loss: 0.4638\n",
            "Epoch [2025/20000], Training Loss: 0.5104\n",
            "Epoch [2026/20000], Training Loss: 0.5000\n",
            "Epoch [2027/20000], Training Loss: 0.5090\n",
            "Epoch [2028/20000], Training Loss: 0.4844\n",
            "Epoch [2029/20000], Training Loss: 0.4817\n",
            "Epoch [2030/20000], Training Loss: 0.4624\n",
            "Epoch [2031/20000], Training Loss: 0.4865\n",
            "Epoch [2032/20000], Training Loss: 0.4930\n",
            "Epoch [2033/20000], Training Loss: 0.5358\n",
            "Epoch [2034/20000], Training Loss: 0.5335\n",
            "Epoch [2035/20000], Training Loss: 0.5073\n",
            "Epoch [2036/20000], Training Loss: 0.4905\n",
            "Epoch [2037/20000], Training Loss: 0.4595\n",
            "Epoch [2038/20000], Training Loss: 0.5275\n",
            "Epoch [2039/20000], Training Loss: 0.4531\n",
            "Epoch [2040/20000], Training Loss: 0.5060\n",
            "Epoch [2041/20000], Training Loss: 0.5012\n",
            "Epoch [2042/20000], Training Loss: 0.4692\n",
            "Epoch [2043/20000], Training Loss: 0.4871\n",
            "Epoch [2044/20000], Training Loss: 0.4820\n",
            "Epoch [2045/20000], Training Loss: 0.4990\n",
            "Epoch [2046/20000], Training Loss: 0.4993\n",
            "Epoch [2047/20000], Training Loss: 0.4748\n",
            "Epoch [2048/20000], Training Loss: 0.4976\n",
            "Epoch [2049/20000], Training Loss: 0.4726\n",
            "Epoch [2050/20000], Training Loss: 0.4920\n",
            "Epoch [2051/20000], Training Loss: 0.5051\n",
            "Epoch [2052/20000], Training Loss: 0.4621\n",
            "Epoch [2053/20000], Training Loss: 0.4884\n",
            "Epoch [2054/20000], Training Loss: 0.4771\n",
            "Epoch [2055/20000], Training Loss: 0.4747\n",
            "Epoch [2056/20000], Training Loss: 0.5183\n",
            "Epoch [2057/20000], Training Loss: 0.4783\n",
            "Epoch [2058/20000], Training Loss: 0.4746\n",
            "Epoch [2059/20000], Training Loss: 0.4907\n",
            "Epoch [2060/20000], Training Loss: 0.5116\n",
            "Epoch [2061/20000], Training Loss: 0.4719\n",
            "Epoch [2062/20000], Training Loss: 0.4617\n",
            "Epoch [2063/20000], Training Loss: 0.5465\n",
            "Epoch [2064/20000], Training Loss: 0.4569\n",
            "Epoch [2065/20000], Training Loss: 0.4454\n",
            "Epoch [2066/20000], Training Loss: 0.4651\n",
            "Epoch [2067/20000], Training Loss: 0.5239\n",
            "Epoch [2068/20000], Training Loss: 0.4784\n",
            "Epoch [2069/20000], Training Loss: 0.4735\n",
            "Epoch [2070/20000], Training Loss: 0.4813\n",
            "Epoch [2071/20000], Training Loss: 0.4799\n",
            "Epoch [2072/20000], Training Loss: 0.4758\n",
            "Epoch [2073/20000], Training Loss: 0.4504\n",
            "Epoch [2074/20000], Training Loss: 0.4578\n",
            "Epoch [2075/20000], Training Loss: 0.4707\n",
            "Epoch [2076/20000], Training Loss: 0.4685\n",
            "Epoch [2077/20000], Training Loss: 0.4511\n",
            "Epoch [2078/20000], Training Loss: 0.4628\n",
            "Epoch [2079/20000], Training Loss: 0.5046\n",
            "Epoch [2080/20000], Training Loss: 0.4877\n",
            "Epoch [2081/20000], Training Loss: 0.4554\n",
            "Epoch [2082/20000], Training Loss: 0.5009\n",
            "Epoch [2083/20000], Training Loss: 0.5049\n",
            "Epoch [2084/20000], Training Loss: 0.4987\n",
            "Epoch [2085/20000], Training Loss: 0.4803\n",
            "Epoch [2086/20000], Training Loss: 0.5111\n",
            "Epoch [2087/20000], Training Loss: 0.4869\n",
            "Epoch [2088/20000], Training Loss: 0.5189\n",
            "Epoch [2089/20000], Training Loss: 0.4692\n",
            "Epoch [2090/20000], Training Loss: 0.4743\n",
            "Epoch [2091/20000], Training Loss: 0.5107\n",
            "Epoch [2092/20000], Training Loss: 0.4987\n",
            "Epoch [2093/20000], Training Loss: 0.4773\n",
            "Epoch [2094/20000], Training Loss: 0.4621\n",
            "Epoch [2095/20000], Training Loss: 0.5127\n",
            "Epoch [2096/20000], Training Loss: 0.4419\n",
            "Epoch [2097/20000], Training Loss: 0.4870\n",
            "Epoch [2098/20000], Training Loss: 0.4333\n",
            "Epoch [2099/20000], Training Loss: 0.4933\n",
            "Epoch [2100/20000], Training Loss: 0.4393\n",
            "Epoch [2101/20000], Training Loss: 0.5161\n",
            "Epoch [2102/20000], Training Loss: 0.5069\n",
            "Epoch [2103/20000], Training Loss: 0.4597\n",
            "Epoch [2104/20000], Training Loss: 0.4820\n",
            "Epoch [2105/20000], Training Loss: 0.4807\n",
            "Epoch [2106/20000], Training Loss: 0.4825\n",
            "Epoch [2107/20000], Training Loss: 0.4807\n",
            "Epoch [2108/20000], Training Loss: 0.5213\n",
            "Epoch [2109/20000], Training Loss: 0.5191\n",
            "Epoch [2110/20000], Training Loss: 0.4588\n",
            "Epoch [2111/20000], Training Loss: 0.4602\n",
            "Epoch [2112/20000], Training Loss: 0.4908\n",
            "Epoch [2113/20000], Training Loss: 0.4465\n",
            "Epoch [2114/20000], Training Loss: 0.4907\n",
            "Epoch [2115/20000], Training Loss: 0.4297\n",
            "Epoch [2116/20000], Training Loss: 0.4704\n",
            "Epoch [2117/20000], Training Loss: 0.4953\n",
            "Epoch [2118/20000], Training Loss: 0.4673\n",
            "Epoch [2119/20000], Training Loss: 0.5053\n",
            "Epoch [2120/20000], Training Loss: 0.4956\n",
            "Epoch [2121/20000], Training Loss: 0.4647\n",
            "Epoch [2122/20000], Training Loss: 0.5671\n",
            "Epoch [2123/20000], Training Loss: 0.4475\n",
            "Epoch [2124/20000], Training Loss: 0.4994\n",
            "Epoch [2125/20000], Training Loss: 0.4912\n",
            "Epoch [2126/20000], Training Loss: 0.4866\n",
            "Epoch [2127/20000], Training Loss: 0.5225\n",
            "Epoch [2128/20000], Training Loss: 0.5065\n",
            "Epoch [2129/20000], Training Loss: 0.4639\n",
            "Epoch [2130/20000], Training Loss: 0.4841\n",
            "Epoch [2131/20000], Training Loss: 0.4933\n",
            "Epoch [2132/20000], Training Loss: 0.4873\n",
            "Epoch [2133/20000], Training Loss: 0.4881\n",
            "Epoch [2134/20000], Training Loss: 0.4560\n",
            "Epoch [2135/20000], Training Loss: 0.4604\n",
            "Epoch [2136/20000], Training Loss: 0.5020\n",
            "Epoch [2137/20000], Training Loss: 0.4908\n",
            "Epoch [2138/20000], Training Loss: 0.5086\n",
            "Epoch [2139/20000], Training Loss: 0.4614\n",
            "Epoch [2140/20000], Training Loss: 0.4785\n",
            "Epoch [2141/20000], Training Loss: 0.4999\n",
            "Epoch [2142/20000], Training Loss: 0.4905\n",
            "Epoch [2143/20000], Training Loss: 0.4815\n",
            "Epoch [2144/20000], Training Loss: 0.4557\n",
            "Epoch [2145/20000], Training Loss: 0.5374\n",
            "Epoch [2146/20000], Training Loss: 0.4920\n",
            "Epoch [2147/20000], Training Loss: 0.4411\n",
            "Epoch [2148/20000], Training Loss: 0.4623\n",
            "Epoch [2149/20000], Training Loss: 0.5018\n",
            "Epoch [2150/20000], Training Loss: 0.4626\n",
            "Epoch [2151/20000], Training Loss: 0.4559\n",
            "Epoch [2152/20000], Training Loss: 0.5417\n",
            "Epoch [2153/20000], Training Loss: 0.4862\n",
            "Epoch [2154/20000], Training Loss: 0.4843\n",
            "Epoch [2155/20000], Training Loss: 0.5104\n",
            "Epoch [2156/20000], Training Loss: 0.4878\n",
            "Epoch [2157/20000], Training Loss: 0.4957\n",
            "Epoch [2158/20000], Training Loss: 0.4695\n",
            "Epoch [2159/20000], Training Loss: 0.4674\n",
            "Epoch [2160/20000], Training Loss: 0.5109\n",
            "Epoch [2161/20000], Training Loss: 0.5024\n",
            "Epoch [2162/20000], Training Loss: 0.5148\n",
            "Epoch [2163/20000], Training Loss: 0.4974\n",
            "Epoch [2164/20000], Training Loss: 0.5262\n",
            "Epoch [2165/20000], Training Loss: 0.5184\n",
            "Epoch [2166/20000], Training Loss: 0.4672\n",
            "Epoch [2167/20000], Training Loss: 0.4703\n",
            "Epoch [2168/20000], Training Loss: 0.4603\n",
            "Epoch [2169/20000], Training Loss: 0.4659\n",
            "Epoch [2170/20000], Training Loss: 0.4587\n",
            "Epoch [2171/20000], Training Loss: 0.4814\n",
            "Epoch [2172/20000], Training Loss: 0.4604\n",
            "Epoch [2173/20000], Training Loss: 0.5191\n",
            "Epoch [2174/20000], Training Loss: 0.4210\n",
            "Epoch [2175/20000], Training Loss: 0.4695\n",
            "Epoch [2176/20000], Training Loss: 0.5291\n",
            "Epoch [2177/20000], Training Loss: 0.5476\n",
            "Epoch [2178/20000], Training Loss: 0.4654\n",
            "Epoch [2179/20000], Training Loss: 0.5357\n",
            "Epoch [2180/20000], Training Loss: 0.4710\n",
            "Epoch [2181/20000], Training Loss: 0.4663\n",
            "Epoch [2182/20000], Training Loss: 0.5244\n",
            "Epoch [2183/20000], Training Loss: 0.4881\n",
            "Epoch [2184/20000], Training Loss: 0.4572\n",
            "Epoch [2185/20000], Training Loss: 0.5439\n",
            "Epoch [2186/20000], Training Loss: 0.4591\n",
            "Epoch [2187/20000], Training Loss: 0.4576\n",
            "Epoch [2188/20000], Training Loss: 0.5097\n",
            "Epoch [2189/20000], Training Loss: 0.4290\n",
            "Epoch [2190/20000], Training Loss: 0.5188\n",
            "Epoch [2191/20000], Training Loss: 0.5488\n",
            "Epoch [2192/20000], Training Loss: 0.4756\n",
            "Epoch [2193/20000], Training Loss: 0.4883\n",
            "Epoch [2194/20000], Training Loss: 0.4361\n",
            "Epoch [2195/20000], Training Loss: 0.5145\n",
            "Epoch [2196/20000], Training Loss: 0.4880\n",
            "Epoch [2197/20000], Training Loss: 0.4954\n",
            "Epoch [2198/20000], Training Loss: 0.4746\n",
            "Epoch [2199/20000], Training Loss: 0.5192\n",
            "Epoch [2200/20000], Training Loss: 0.5148\n",
            "Epoch [2201/20000], Training Loss: 0.5006\n",
            "Epoch [2202/20000], Training Loss: 0.5060\n",
            "Epoch [2203/20000], Training Loss: 0.4973\n",
            "Epoch [2204/20000], Training Loss: 0.4711\n",
            "Epoch [2205/20000], Training Loss: 0.5386\n",
            "Epoch [2206/20000], Training Loss: 0.5059\n",
            "Epoch [2207/20000], Training Loss: 0.4984\n",
            "Epoch [2208/20000], Training Loss: 0.4723\n",
            "Epoch [2209/20000], Training Loss: 0.5147\n",
            "Epoch [2210/20000], Training Loss: 0.4706\n",
            "Epoch [2211/20000], Training Loss: 0.4752\n",
            "Epoch [2212/20000], Training Loss: 0.4509\n",
            "Epoch [2213/20000], Training Loss: 0.5116\n",
            "Epoch [2214/20000], Training Loss: 0.5549\n",
            "Epoch [2215/20000], Training Loss: 0.4658\n",
            "Epoch [2216/20000], Training Loss: 0.4954\n",
            "Epoch [2217/20000], Training Loss: 0.4676\n",
            "Epoch [2218/20000], Training Loss: 0.5207\n",
            "Epoch [2219/20000], Training Loss: 0.5296\n",
            "Epoch [2220/20000], Training Loss: 0.4733\n",
            "Epoch [2221/20000], Training Loss: 0.4990\n",
            "Epoch [2222/20000], Training Loss: 0.5249\n",
            "Epoch [2223/20000], Training Loss: 0.4899\n",
            "Epoch [2224/20000], Training Loss: 0.4880\n",
            "Epoch [2225/20000], Training Loss: 0.5301\n",
            "Epoch [2226/20000], Training Loss: 0.4954\n",
            "Epoch [2227/20000], Training Loss: 0.5029\n",
            "Epoch [2228/20000], Training Loss: 0.4921\n",
            "Epoch [2229/20000], Training Loss: 0.4609\n",
            "Epoch [2230/20000], Training Loss: 0.5234\n",
            "Epoch [2231/20000], Training Loss: 0.5059\n",
            "Epoch [2232/20000], Training Loss: 0.4977\n",
            "Epoch [2233/20000], Training Loss: 0.4598\n",
            "Epoch [2234/20000], Training Loss: 0.4791\n",
            "Epoch [2235/20000], Training Loss: 0.4679\n",
            "Epoch [2236/20000], Training Loss: 0.5348\n",
            "Epoch [2237/20000], Training Loss: 0.5418\n",
            "Epoch [2238/20000], Training Loss: 0.4758\n",
            "Epoch [2239/20000], Training Loss: 0.5512\n",
            "Epoch [2240/20000], Training Loss: 0.5426\n",
            "Epoch [2241/20000], Training Loss: 0.4529\n",
            "Epoch [2242/20000], Training Loss: 0.4673\n",
            "Epoch [2243/20000], Training Loss: 0.5357\n",
            "Epoch [2244/20000], Training Loss: 0.4530\n",
            "Epoch [2245/20000], Training Loss: 0.5295\n",
            "Epoch [2246/20000], Training Loss: 0.5418\n",
            "Epoch [2247/20000], Training Loss: 0.5260\n",
            "Epoch [2248/20000], Training Loss: 0.5257\n",
            "Epoch [2249/20000], Training Loss: 0.4661\n",
            "Epoch [2250/20000], Training Loss: 0.4627\n",
            "Epoch [2251/20000], Training Loss: 0.5074\n",
            "Epoch [2252/20000], Training Loss: 0.4630\n",
            "Epoch [2253/20000], Training Loss: 0.5127\n",
            "Epoch [2254/20000], Training Loss: 0.4690\n",
            "Epoch [2255/20000], Training Loss: 0.4640\n",
            "Epoch [2256/20000], Training Loss: 0.4405\n",
            "Epoch [2257/20000], Training Loss: 0.4932\n",
            "Epoch [2258/20000], Training Loss: 0.4801\n",
            "Epoch [2259/20000], Training Loss: 0.4496\n",
            "Epoch [2260/20000], Training Loss: 0.4781\n",
            "Epoch [2261/20000], Training Loss: 0.5054\n",
            "Epoch [2262/20000], Training Loss: 0.4840\n",
            "Epoch [2263/20000], Training Loss: 0.5246\n",
            "Epoch [2264/20000], Training Loss: 0.4408\n",
            "Epoch [2265/20000], Training Loss: 0.4885\n",
            "Epoch [2266/20000], Training Loss: 0.5318\n",
            "Epoch [2267/20000], Training Loss: 0.4805\n",
            "Epoch [2268/20000], Training Loss: 0.4974\n",
            "Epoch [2269/20000], Training Loss: 0.5071\n",
            "Epoch [2270/20000], Training Loss: 0.5114\n",
            "Epoch [2271/20000], Training Loss: 0.4975\n",
            "Epoch [2272/20000], Training Loss: 0.4439\n",
            "Epoch [2273/20000], Training Loss: 0.4938\n",
            "Epoch [2274/20000], Training Loss: 0.4609\n",
            "Epoch [2275/20000], Training Loss: 0.4396\n",
            "Epoch [2276/20000], Training Loss: 0.4481\n",
            "Epoch [2277/20000], Training Loss: 0.5201\n",
            "Epoch [2278/20000], Training Loss: 0.4848\n",
            "Epoch [2279/20000], Training Loss: 0.4998\n",
            "Epoch [2280/20000], Training Loss: 0.5152\n",
            "Epoch [2281/20000], Training Loss: 0.5160\n",
            "Epoch [2282/20000], Training Loss: 0.4652\n",
            "Epoch [2283/20000], Training Loss: 0.4655\n",
            "Epoch [2284/20000], Training Loss: 0.4496\n",
            "Epoch [2285/20000], Training Loss: 0.4637\n",
            "Epoch [2286/20000], Training Loss: 0.4668\n",
            "Epoch [2287/20000], Training Loss: 0.4278\n",
            "Epoch [2288/20000], Training Loss: 0.4780\n",
            "Epoch [2289/20000], Training Loss: 0.4837\n",
            "Epoch [2290/20000], Training Loss: 0.4632\n",
            "Epoch [2291/20000], Training Loss: 0.4338\n",
            "Epoch [2292/20000], Training Loss: 0.5132\n",
            "Epoch [2293/20000], Training Loss: 0.5120\n",
            "Epoch [2294/20000], Training Loss: 0.4303\n",
            "Epoch [2295/20000], Training Loss: 0.4748\n",
            "Epoch [2296/20000], Training Loss: 0.4358\n",
            "Epoch [2297/20000], Training Loss: 0.4705\n",
            "Epoch [2298/20000], Training Loss: 0.4961\n",
            "Epoch [2299/20000], Training Loss: 0.4691\n",
            "Epoch [2300/20000], Training Loss: 0.4735\n",
            "Epoch [2301/20000], Training Loss: 0.5041\n",
            "Epoch [2302/20000], Training Loss: 0.5241\n",
            "Epoch [2303/20000], Training Loss: 0.4833\n",
            "Epoch [2304/20000], Training Loss: 0.5072\n",
            "Epoch [2305/20000], Training Loss: 0.5226\n",
            "Epoch [2306/20000], Training Loss: 0.5316\n",
            "Epoch [2307/20000], Training Loss: 0.5020\n",
            "Epoch [2308/20000], Training Loss: 0.4970\n",
            "Epoch [2309/20000], Training Loss: 0.4774\n",
            "Epoch [2310/20000], Training Loss: 0.4740\n",
            "Epoch [2311/20000], Training Loss: 0.4621\n",
            "Epoch [2312/20000], Training Loss: 0.4865\n",
            "Epoch [2313/20000], Training Loss: 0.4851\n",
            "Epoch [2314/20000], Training Loss: 0.5000\n",
            "Epoch [2315/20000], Training Loss: 0.5224\n",
            "Epoch [2316/20000], Training Loss: 0.5205\n",
            "Epoch [2317/20000], Training Loss: 0.4815\n",
            "Epoch [2318/20000], Training Loss: 0.4713\n",
            "Epoch [2319/20000], Training Loss: 0.4765\n",
            "Epoch [2320/20000], Training Loss: 0.5382\n",
            "Epoch [2321/20000], Training Loss: 0.5432\n",
            "Epoch [2322/20000], Training Loss: 0.4756\n",
            "Epoch [2323/20000], Training Loss: 0.5021\n",
            "Epoch [2324/20000], Training Loss: 0.4825\n",
            "Epoch [2325/20000], Training Loss: 0.4785\n",
            "Epoch [2326/20000], Training Loss: 0.4631\n",
            "Epoch [2327/20000], Training Loss: 0.4659\n",
            "Epoch [2328/20000], Training Loss: 0.5118\n",
            "Epoch [2329/20000], Training Loss: 0.4625\n",
            "Epoch [2330/20000], Training Loss: 0.4431\n",
            "Epoch [2331/20000], Training Loss: 0.5566\n",
            "Epoch [2332/20000], Training Loss: 0.5520\n",
            "Epoch [2333/20000], Training Loss: 0.5126\n",
            "Epoch [2334/20000], Training Loss: 0.5572\n",
            "Epoch [2335/20000], Training Loss: 0.4451\n",
            "Epoch [2336/20000], Training Loss: 0.4853\n",
            "Epoch [2337/20000], Training Loss: 0.5002\n",
            "Epoch [2338/20000], Training Loss: 0.5231\n",
            "Epoch [2339/20000], Training Loss: 0.4485\n",
            "Epoch [2340/20000], Training Loss: 0.4785\n",
            "Epoch [2341/20000], Training Loss: 0.4821\n",
            "Epoch [2342/20000], Training Loss: 0.4502\n",
            "Epoch [2343/20000], Training Loss: 0.5378\n",
            "Epoch [2344/20000], Training Loss: 0.4674\n",
            "Epoch [2345/20000], Training Loss: 0.5300\n",
            "Epoch [2346/20000], Training Loss: 0.4586\n",
            "Epoch [2347/20000], Training Loss: 0.4382\n",
            "Epoch [2348/20000], Training Loss: 0.4627\n",
            "Epoch [2349/20000], Training Loss: 0.4767\n",
            "Epoch [2350/20000], Training Loss: 0.4648\n",
            "Epoch [2351/20000], Training Loss: 0.4790\n",
            "Epoch [2352/20000], Training Loss: 0.4811\n",
            "Epoch [2353/20000], Training Loss: 0.4786\n",
            "Epoch [2354/20000], Training Loss: 0.4822\n",
            "Epoch [2355/20000], Training Loss: 0.4937\n",
            "Epoch [2356/20000], Training Loss: 0.5181\n",
            "Epoch [2357/20000], Training Loss: 0.5066\n",
            "Epoch [2358/20000], Training Loss: 0.4443\n",
            "Epoch [2359/20000], Training Loss: 0.5221\n",
            "Epoch [2360/20000], Training Loss: 0.4987\n",
            "Epoch [2361/20000], Training Loss: 0.5023\n",
            "Epoch [2362/20000], Training Loss: 0.5054\n",
            "Epoch [2363/20000], Training Loss: 0.5339\n",
            "Epoch [2364/20000], Training Loss: 0.4790\n",
            "Epoch [2365/20000], Training Loss: 0.4919\n",
            "Epoch [2366/20000], Training Loss: 0.4859\n",
            "Epoch [2367/20000], Training Loss: 0.4982\n",
            "Epoch [2368/20000], Training Loss: 0.4957\n",
            "Epoch [2369/20000], Training Loss: 0.4965\n",
            "Epoch [2370/20000], Training Loss: 0.4912\n",
            "Epoch [2371/20000], Training Loss: 0.4649\n",
            "Epoch [2372/20000], Training Loss: 0.5032\n",
            "Epoch [2373/20000], Training Loss: 0.5204\n",
            "Epoch [2374/20000], Training Loss: 0.5312\n",
            "Epoch [2375/20000], Training Loss: 0.4467\n",
            "Epoch [2376/20000], Training Loss: 0.5120\n",
            "Epoch [2377/20000], Training Loss: 0.4673\n",
            "Epoch [2378/20000], Training Loss: 0.5025\n",
            "Epoch [2379/20000], Training Loss: 0.4702\n",
            "Epoch [2380/20000], Training Loss: 0.4821\n",
            "Epoch [2381/20000], Training Loss: 0.4871\n",
            "Epoch [2382/20000], Training Loss: 0.5055\n",
            "Epoch [2383/20000], Training Loss: 0.5057\n",
            "Epoch [2384/20000], Training Loss: 0.4477\n",
            "Epoch [2385/20000], Training Loss: 0.4663\n",
            "Epoch [2386/20000], Training Loss: 0.4807\n",
            "Epoch [2387/20000], Training Loss: 0.4529\n",
            "Epoch [2388/20000], Training Loss: 0.4788\n",
            "Epoch [2389/20000], Training Loss: 0.4814\n",
            "Epoch [2390/20000], Training Loss: 0.4846\n",
            "Epoch [2391/20000], Training Loss: 0.4805\n",
            "Epoch [2392/20000], Training Loss: 0.5232\n",
            "Epoch [2393/20000], Training Loss: 0.4623\n",
            "Epoch [2394/20000], Training Loss: 0.4552\n",
            "Epoch [2395/20000], Training Loss: 0.4439\n",
            "Epoch [2396/20000], Training Loss: 0.5300\n",
            "Epoch [2397/20000], Training Loss: 0.4792\n",
            "Epoch [2398/20000], Training Loss: 0.5044\n",
            "Epoch [2399/20000], Training Loss: 0.5071\n",
            "Epoch [2400/20000], Training Loss: 0.4775\n",
            "Epoch [2401/20000], Training Loss: 0.4939\n",
            "Epoch [2402/20000], Training Loss: 0.5046\n",
            "Epoch [2403/20000], Training Loss: 0.4764\n",
            "Epoch [2404/20000], Training Loss: 0.4664\n",
            "Epoch [2405/20000], Training Loss: 0.5175\n",
            "Epoch [2406/20000], Training Loss: 0.4565\n",
            "Epoch [2407/20000], Training Loss: 0.5077\n",
            "Epoch [2408/20000], Training Loss: 0.4462\n",
            "Epoch [2409/20000], Training Loss: 0.4921\n",
            "Epoch [2410/20000], Training Loss: 0.4881\n",
            "Epoch [2411/20000], Training Loss: 0.4946\n",
            "Epoch [2412/20000], Training Loss: 0.4872\n",
            "Epoch [2413/20000], Training Loss: 0.4631\n",
            "Epoch [2414/20000], Training Loss: 0.4911\n",
            "Epoch [2415/20000], Training Loss: 0.4813\n",
            "Epoch [2416/20000], Training Loss: 0.4992\n",
            "Epoch [2417/20000], Training Loss: 0.4298\n",
            "Epoch [2418/20000], Training Loss: 0.4798\n",
            "Epoch [2419/20000], Training Loss: 0.4666\n",
            "Epoch [2420/20000], Training Loss: 0.5040\n",
            "Epoch [2421/20000], Training Loss: 0.4886\n",
            "Epoch [2422/20000], Training Loss: 0.5059\n",
            "Epoch [2423/20000], Training Loss: 0.4667\n",
            "Epoch [2424/20000], Training Loss: 0.5193\n",
            "Epoch [2425/20000], Training Loss: 0.4905\n",
            "Epoch [2426/20000], Training Loss: 0.5289\n",
            "Epoch [2427/20000], Training Loss: 0.4667\n",
            "Epoch [2428/20000], Training Loss: 0.4440\n",
            "Epoch [2429/20000], Training Loss: 0.4983\n",
            "Epoch [2430/20000], Training Loss: 0.4870\n",
            "Epoch [2431/20000], Training Loss: 0.5034\n",
            "Epoch [2432/20000], Training Loss: 0.5037\n",
            "Epoch [2433/20000], Training Loss: 0.4715\n",
            "Epoch [2434/20000], Training Loss: 0.4500\n",
            "Epoch [2435/20000], Training Loss: 0.4462\n",
            "Epoch [2436/20000], Training Loss: 0.4737\n",
            "Epoch [2437/20000], Training Loss: 0.5043\n",
            "Epoch [2438/20000], Training Loss: 0.4968\n",
            "Epoch [2439/20000], Training Loss: 0.4553\n",
            "Epoch [2440/20000], Training Loss: 0.4819\n",
            "Epoch [2441/20000], Training Loss: 0.4789\n",
            "Epoch [2442/20000], Training Loss: 0.4805\n",
            "Epoch [2443/20000], Training Loss: 0.4911\n",
            "Epoch [2444/20000], Training Loss: 0.4923\n",
            "Epoch [2445/20000], Training Loss: 0.4942\n",
            "Epoch [2446/20000], Training Loss: 0.4907\n",
            "Epoch [2447/20000], Training Loss: 0.4450\n",
            "Epoch [2448/20000], Training Loss: 0.4754\n",
            "Epoch [2449/20000], Training Loss: 0.4871\n",
            "Epoch [2450/20000], Training Loss: 0.4642\n",
            "Epoch [2451/20000], Training Loss: 0.4754\n",
            "Epoch [2452/20000], Training Loss: 0.4807\n",
            "Epoch [2453/20000], Training Loss: 0.5077\n",
            "Epoch [2454/20000], Training Loss: 0.4741\n",
            "Epoch [2455/20000], Training Loss: 0.4620\n",
            "Epoch [2456/20000], Training Loss: 0.4600\n",
            "Epoch [2457/20000], Training Loss: 0.4887\n",
            "Epoch [2458/20000], Training Loss: 0.4468\n",
            "Epoch [2459/20000], Training Loss: 0.4482\n",
            "Epoch [2460/20000], Training Loss: 0.5098\n",
            "Epoch [2461/20000], Training Loss: 0.4961\n",
            "Epoch [2462/20000], Training Loss: 0.4604\n",
            "Epoch [2463/20000], Training Loss: 0.5066\n",
            "Epoch [2464/20000], Training Loss: 0.4491\n",
            "Epoch [2465/20000], Training Loss: 0.4880\n",
            "Epoch [2466/20000], Training Loss: 0.4957\n",
            "Epoch [2467/20000], Training Loss: 0.4807\n",
            "Epoch [2468/20000], Training Loss: 0.5195\n",
            "Epoch [2469/20000], Training Loss: 0.4386\n",
            "Epoch [2470/20000], Training Loss: 0.4841\n",
            "Epoch [2471/20000], Training Loss: 0.4852\n",
            "Epoch [2472/20000], Training Loss: 0.5357\n",
            "Epoch [2473/20000], Training Loss: 0.4914\n",
            "Epoch [2474/20000], Training Loss: 0.4900\n",
            "Epoch [2475/20000], Training Loss: 0.4900\n",
            "Epoch [2476/20000], Training Loss: 0.5020\n",
            "Epoch [2477/20000], Training Loss: 0.4758\n",
            "Epoch [2478/20000], Training Loss: 0.4981\n",
            "Epoch [2479/20000], Training Loss: 0.4701\n",
            "Epoch [2480/20000], Training Loss: 0.4651\n",
            "Epoch [2481/20000], Training Loss: 0.5321\n",
            "Epoch [2482/20000], Training Loss: 0.4839\n",
            "Epoch [2483/20000], Training Loss: 0.4795\n",
            "Epoch [2484/20000], Training Loss: 0.5113\n",
            "Epoch [2485/20000], Training Loss: 0.4592\n",
            "Epoch [2486/20000], Training Loss: 0.4727\n",
            "Epoch [2487/20000], Training Loss: 0.4904\n",
            "Epoch [2488/20000], Training Loss: 0.4957\n",
            "Epoch [2489/20000], Training Loss: 0.4975\n",
            "Epoch [2490/20000], Training Loss: 0.4825\n",
            "Epoch [2491/20000], Training Loss: 0.4474\n",
            "Epoch [2492/20000], Training Loss: 0.4399\n",
            "Epoch [2493/20000], Training Loss: 0.5224\n",
            "Epoch [2494/20000], Training Loss: 0.5080\n",
            "Epoch [2495/20000], Training Loss: 0.4660\n",
            "Epoch [2496/20000], Training Loss: 0.5193\n",
            "Epoch [2497/20000], Training Loss: 0.5194\n",
            "Epoch [2498/20000], Training Loss: 0.5146\n",
            "Epoch [2499/20000], Training Loss: 0.4451\n",
            "Epoch [2500/20000], Training Loss: 0.4656\n",
            "Epoch [2501/20000], Training Loss: 0.4794\n",
            "Epoch [2502/20000], Training Loss: 0.5089\n",
            "Epoch [2503/20000], Training Loss: 0.5098\n",
            "Epoch [2504/20000], Training Loss: 0.4335\n",
            "Epoch [2505/20000], Training Loss: 0.4535\n",
            "Epoch [2506/20000], Training Loss: 0.4824\n",
            "Epoch [2507/20000], Training Loss: 0.4826\n",
            "Epoch [2508/20000], Training Loss: 0.4615\n",
            "Epoch [2509/20000], Training Loss: 0.4807\n",
            "Epoch [2510/20000], Training Loss: 0.4914\n",
            "Epoch [2511/20000], Training Loss: 0.4427\n",
            "Epoch [2512/20000], Training Loss: 0.5407\n",
            "Epoch [2513/20000], Training Loss: 0.4868\n",
            "Epoch [2514/20000], Training Loss: 0.5277\n",
            "Epoch [2515/20000], Training Loss: 0.5149\n",
            "Epoch [2516/20000], Training Loss: 0.5083\n",
            "Epoch [2517/20000], Training Loss: 0.4641\n",
            "Epoch [2518/20000], Training Loss: 0.4623\n",
            "Epoch [2519/20000], Training Loss: 0.4893\n",
            "Epoch [2520/20000], Training Loss: 0.4442\n",
            "Epoch [2521/20000], Training Loss: 0.4545\n",
            "Epoch [2522/20000], Training Loss: 0.4927\n",
            "Epoch [2523/20000], Training Loss: 0.5097\n",
            "Epoch [2524/20000], Training Loss: 0.4804\n",
            "Epoch [2525/20000], Training Loss: 0.4922\n",
            "Epoch [2526/20000], Training Loss: 0.4942\n",
            "Epoch [2527/20000], Training Loss: 0.4782\n",
            "Epoch [2528/20000], Training Loss: 0.4496\n",
            "Epoch [2529/20000], Training Loss: 0.4759\n",
            "Epoch [2530/20000], Training Loss: 0.4799\n",
            "Epoch [2531/20000], Training Loss: 0.5015\n",
            "Epoch [2532/20000], Training Loss: 0.5046\n",
            "Epoch [2533/20000], Training Loss: 0.4909\n",
            "Epoch [2534/20000], Training Loss: 0.4935\n",
            "Epoch [2535/20000], Training Loss: 0.5291\n",
            "Epoch [2536/20000], Training Loss: 0.4406\n",
            "Epoch [2537/20000], Training Loss: 0.5076\n",
            "Epoch [2538/20000], Training Loss: 0.5268\n",
            "Epoch [2539/20000], Training Loss: 0.4637\n",
            "Epoch [2540/20000], Training Loss: 0.4700\n",
            "Epoch [2541/20000], Training Loss: 0.4773\n",
            "Epoch [2542/20000], Training Loss: 0.5256\n",
            "Epoch [2543/20000], Training Loss: 0.4794\n",
            "Epoch [2544/20000], Training Loss: 0.4616\n",
            "Epoch [2545/20000], Training Loss: 0.4704\n",
            "Epoch [2546/20000], Training Loss: 0.4976\n",
            "Epoch [2547/20000], Training Loss: 0.5293\n",
            "Epoch [2548/20000], Training Loss: 0.4886\n",
            "Epoch [2549/20000], Training Loss: 0.4800\n",
            "Epoch [2550/20000], Training Loss: 0.4728\n",
            "Epoch [2551/20000], Training Loss: 0.4898\n",
            "Epoch [2552/20000], Training Loss: 0.5176\n",
            "Epoch [2553/20000], Training Loss: 0.5009\n",
            "Epoch [2554/20000], Training Loss: 0.5046\n",
            "Epoch [2555/20000], Training Loss: 0.5056\n",
            "Epoch [2556/20000], Training Loss: 0.5328\n",
            "Epoch [2557/20000], Training Loss: 0.4588\n",
            "Epoch [2558/20000], Training Loss: 0.4391\n",
            "Epoch [2559/20000], Training Loss: 0.4732\n",
            "Epoch [2560/20000], Training Loss: 0.4500\n",
            "Epoch [2561/20000], Training Loss: 0.5237\n",
            "Epoch [2562/20000], Training Loss: 0.4440\n",
            "Epoch [2563/20000], Training Loss: 0.5233\n",
            "Epoch [2564/20000], Training Loss: 0.5019\n",
            "Epoch [2565/20000], Training Loss: 0.4779\n",
            "Epoch [2566/20000], Training Loss: 0.5100\n",
            "Epoch [2567/20000], Training Loss: 0.4869\n",
            "Epoch [2568/20000], Training Loss: 0.5103\n",
            "Epoch [2569/20000], Training Loss: 0.5172\n",
            "Epoch [2570/20000], Training Loss: 0.4491\n",
            "Epoch [2571/20000], Training Loss: 0.4411\n",
            "Epoch [2572/20000], Training Loss: 0.5090\n",
            "Epoch [2573/20000], Training Loss: 0.4748\n",
            "Epoch [2574/20000], Training Loss: 0.4884\n",
            "Epoch [2575/20000], Training Loss: 0.5347\n",
            "Epoch [2576/20000], Training Loss: 0.4912\n",
            "Epoch [2577/20000], Training Loss: 0.5101\n",
            "Epoch [2578/20000], Training Loss: 0.5226\n",
            "Epoch [2579/20000], Training Loss: 0.5223\n",
            "Epoch [2580/20000], Training Loss: 0.4803\n",
            "Epoch [2581/20000], Training Loss: 0.4930\n",
            "Epoch [2582/20000], Training Loss: 0.5260\n",
            "Epoch [2583/20000], Training Loss: 0.4930\n",
            "Epoch [2584/20000], Training Loss: 0.4648\n",
            "Epoch [2585/20000], Training Loss: 0.4731\n",
            "Epoch [2586/20000], Training Loss: 0.4520\n",
            "Epoch [2587/20000], Training Loss: 0.4911\n",
            "Epoch [2588/20000], Training Loss: 0.4797\n",
            "Epoch [2589/20000], Training Loss: 0.5366\n",
            "Epoch [2590/20000], Training Loss: 0.4704\n",
            "Epoch [2591/20000], Training Loss: 0.4718\n",
            "Epoch [2592/20000], Training Loss: 0.5337\n",
            "Epoch [2593/20000], Training Loss: 0.5099\n",
            "Epoch [2594/20000], Training Loss: 0.4609\n",
            "Epoch [2595/20000], Training Loss: 0.4749\n",
            "Epoch [2596/20000], Training Loss: 0.4517\n",
            "Epoch [2597/20000], Training Loss: 0.4678\n",
            "Epoch [2598/20000], Training Loss: 0.5198\n",
            "Epoch [2599/20000], Training Loss: 0.5109\n",
            "Epoch [2600/20000], Training Loss: 0.4703\n",
            "Epoch [2601/20000], Training Loss: 0.4819\n",
            "Epoch [2602/20000], Training Loss: 0.4945\n",
            "Epoch [2603/20000], Training Loss: 0.5028\n",
            "Epoch [2604/20000], Training Loss: 0.4626\n",
            "Epoch [2605/20000], Training Loss: 0.4669\n",
            "Epoch [2606/20000], Training Loss: 0.5018\n",
            "Epoch [2607/20000], Training Loss: 0.5470\n",
            "Epoch [2608/20000], Training Loss: 0.4679\n",
            "Epoch [2609/20000], Training Loss: 0.4680\n",
            "Epoch [2610/20000], Training Loss: 0.5379\n",
            "Epoch [2611/20000], Training Loss: 0.4649\n",
            "Epoch [2612/20000], Training Loss: 0.5035\n",
            "Epoch [2613/20000], Training Loss: 0.4864\n",
            "Epoch [2614/20000], Training Loss: 0.5260\n",
            "Epoch [2615/20000], Training Loss: 0.4755\n",
            "Epoch [2616/20000], Training Loss: 0.5189\n",
            "Epoch [2617/20000], Training Loss: 0.4291\n",
            "Epoch [2618/20000], Training Loss: 0.4565\n",
            "Epoch [2619/20000], Training Loss: 0.4645\n",
            "Epoch [2620/20000], Training Loss: 0.4787\n",
            "Epoch [2621/20000], Training Loss: 0.4990\n",
            "Epoch [2622/20000], Training Loss: 0.4954\n",
            "Epoch [2623/20000], Training Loss: 0.5036\n",
            "Epoch [2624/20000], Training Loss: 0.5222\n",
            "Epoch [2625/20000], Training Loss: 0.4605\n",
            "Epoch [2626/20000], Training Loss: 0.5375\n",
            "Epoch [2627/20000], Training Loss: 0.4889\n",
            "Epoch [2628/20000], Training Loss: 0.4428\n",
            "Epoch [2629/20000], Training Loss: 0.5216\n",
            "Epoch [2630/20000], Training Loss: 0.4452\n",
            "Epoch [2631/20000], Training Loss: 0.5271\n",
            "Epoch [2632/20000], Training Loss: 0.4840\n",
            "Epoch [2633/20000], Training Loss: 0.4990\n",
            "Epoch [2634/20000], Training Loss: 0.5167\n",
            "Epoch [2635/20000], Training Loss: 0.5224\n",
            "Epoch [2636/20000], Training Loss: 0.4944\n",
            "Epoch [2637/20000], Training Loss: 0.4820\n",
            "Epoch [2638/20000], Training Loss: 0.4994\n",
            "Epoch [2639/20000], Training Loss: 0.5002\n",
            "Epoch [2640/20000], Training Loss: 0.4717\n",
            "Epoch [2641/20000], Training Loss: 0.4966\n",
            "Epoch [2642/20000], Training Loss: 0.5021\n",
            "Epoch [2643/20000], Training Loss: 0.4950\n",
            "Epoch [2644/20000], Training Loss: 0.4345\n",
            "Epoch [2645/20000], Training Loss: 0.4548\n",
            "Epoch [2646/20000], Training Loss: 0.4755\n",
            "Epoch [2647/20000], Training Loss: 0.4854\n",
            "Epoch [2648/20000], Training Loss: 0.4730\n",
            "Epoch [2649/20000], Training Loss: 0.5338\n",
            "Epoch [2650/20000], Training Loss: 0.5080\n",
            "Epoch [2651/20000], Training Loss: 0.4980\n",
            "Epoch [2652/20000], Training Loss: 0.4944\n",
            "Epoch [2653/20000], Training Loss: 0.4901\n",
            "Epoch [2654/20000], Training Loss: 0.4886\n",
            "Epoch [2655/20000], Training Loss: 0.4509\n",
            "Epoch [2656/20000], Training Loss: 0.4952\n",
            "Epoch [2657/20000], Training Loss: 0.4953\n",
            "Epoch [2658/20000], Training Loss: 0.5115\n",
            "Epoch [2659/20000], Training Loss: 0.4693\n",
            "Epoch [2660/20000], Training Loss: 0.5022\n",
            "Epoch [2661/20000], Training Loss: 0.4606\n",
            "Epoch [2662/20000], Training Loss: 0.5095\n",
            "Epoch [2663/20000], Training Loss: 0.4852\n",
            "Epoch [2664/20000], Training Loss: 0.5087\n",
            "Epoch [2665/20000], Training Loss: 0.4898\n",
            "Epoch [2666/20000], Training Loss: 0.5416\n",
            "Epoch [2667/20000], Training Loss: 0.4553\n",
            "Epoch [2668/20000], Training Loss: 0.4984\n",
            "Epoch [2669/20000], Training Loss: 0.5280\n",
            "Epoch [2670/20000], Training Loss: 0.5187\n",
            "Epoch [2671/20000], Training Loss: 0.4637\n",
            "Epoch [2672/20000], Training Loss: 0.4873\n",
            "Epoch [2673/20000], Training Loss: 0.4808\n",
            "Epoch [2674/20000], Training Loss: 0.4967\n",
            "Epoch [2675/20000], Training Loss: 0.4603\n",
            "Epoch [2676/20000], Training Loss: 0.4823\n",
            "Epoch [2677/20000], Training Loss: 0.4677\n",
            "Epoch [2678/20000], Training Loss: 0.5188\n",
            "Epoch [2679/20000], Training Loss: 0.4656\n",
            "Epoch [2680/20000], Training Loss: 0.5154\n",
            "Epoch [2681/20000], Training Loss: 0.4867\n",
            "Epoch [2682/20000], Training Loss: 0.5190\n",
            "Epoch [2683/20000], Training Loss: 0.5092\n",
            "Epoch [2684/20000], Training Loss: 0.4805\n",
            "Epoch [2685/20000], Training Loss: 0.4657\n",
            "Epoch [2686/20000], Training Loss: 0.4720\n",
            "Epoch [2687/20000], Training Loss: 0.4748\n",
            "Epoch [2688/20000], Training Loss: 0.4568\n",
            "Epoch [2689/20000], Training Loss: 0.4922\n",
            "Epoch [2690/20000], Training Loss: 0.5215\n",
            "Epoch [2691/20000], Training Loss: 0.4637\n",
            "Epoch [2692/20000], Training Loss: 0.4611\n",
            "Epoch [2693/20000], Training Loss: 0.5581\n",
            "Epoch [2694/20000], Training Loss: 0.4710\n",
            "Epoch [2695/20000], Training Loss: 0.4443\n",
            "Epoch [2696/20000], Training Loss: 0.5213\n",
            "Epoch [2697/20000], Training Loss: 0.4505\n",
            "Epoch [2698/20000], Training Loss: 0.4970\n",
            "Epoch [2699/20000], Training Loss: 0.5085\n",
            "Epoch [2700/20000], Training Loss: 0.4944\n",
            "Epoch [2701/20000], Training Loss: 0.5013\n",
            "Epoch [2702/20000], Training Loss: 0.5177\n",
            "Epoch [2703/20000], Training Loss: 0.4619\n",
            "Epoch [2704/20000], Training Loss: 0.5020\n",
            "Epoch [2705/20000], Training Loss: 0.4803\n",
            "Epoch [2706/20000], Training Loss: 0.5106\n",
            "Epoch [2707/20000], Training Loss: 0.4951\n",
            "Epoch [2708/20000], Training Loss: 0.5039\n",
            "Epoch [2709/20000], Training Loss: 0.4851\n",
            "Epoch [2710/20000], Training Loss: 0.4955\n",
            "Epoch [2711/20000], Training Loss: 0.5077\n",
            "Epoch [2712/20000], Training Loss: 0.4784\n",
            "Epoch [2713/20000], Training Loss: 0.4981\n",
            "Epoch [2714/20000], Training Loss: 0.4678\n",
            "Epoch [2715/20000], Training Loss: 0.4516\n",
            "Epoch [2716/20000], Training Loss: 0.5137\n",
            "Epoch [2717/20000], Training Loss: 0.4850\n",
            "Epoch [2718/20000], Training Loss: 0.4989\n",
            "Epoch [2719/20000], Training Loss: 0.5117\n",
            "Epoch [2720/20000], Training Loss: 0.4673\n",
            "Epoch [2721/20000], Training Loss: 0.4868\n",
            "Epoch [2722/20000], Training Loss: 0.5308\n",
            "Epoch [2723/20000], Training Loss: 0.4845\n",
            "Epoch [2724/20000], Training Loss: 0.5296\n",
            "Epoch [2725/20000], Training Loss: 0.4914\n",
            "Epoch [2726/20000], Training Loss: 0.5164\n",
            "Epoch [2727/20000], Training Loss: 0.4470\n",
            "Epoch [2728/20000], Training Loss: 0.4393\n",
            "Epoch [2729/20000], Training Loss: 0.4969\n",
            "Epoch [2730/20000], Training Loss: 0.4621\n",
            "Epoch [2731/20000], Training Loss: 0.4678\n",
            "Epoch [2732/20000], Training Loss: 0.5070\n",
            "Epoch [2733/20000], Training Loss: 0.5213\n",
            "Epoch [2734/20000], Training Loss: 0.5261\n",
            "Epoch [2735/20000], Training Loss: 0.4792\n",
            "Epoch [2736/20000], Training Loss: 0.4878\n",
            "Epoch [2737/20000], Training Loss: 0.4685\n",
            "Epoch [2738/20000], Training Loss: 0.5487\n",
            "Epoch [2739/20000], Training Loss: 0.4595\n",
            "Epoch [2740/20000], Training Loss: 0.5018\n",
            "Epoch [2741/20000], Training Loss: 0.4935\n",
            "Epoch [2742/20000], Training Loss: 0.4473\n",
            "Epoch [2743/20000], Training Loss: 0.4687\n",
            "Epoch [2744/20000], Training Loss: 0.4833\n",
            "Epoch [2745/20000], Training Loss: 0.5045\n",
            "Epoch [2746/20000], Training Loss: 0.5158\n",
            "Epoch [2747/20000], Training Loss: 0.4783\n",
            "Epoch [2748/20000], Training Loss: 0.4857\n",
            "Epoch [2749/20000], Training Loss: 0.4829\n",
            "Epoch [2750/20000], Training Loss: 0.4686\n",
            "Epoch [2751/20000], Training Loss: 0.4620\n",
            "Epoch [2752/20000], Training Loss: 0.4768\n",
            "Epoch [2753/20000], Training Loss: 0.4903\n",
            "Epoch [2754/20000], Training Loss: 0.4328\n",
            "Epoch [2755/20000], Training Loss: 0.4865\n",
            "Epoch [2756/20000], Training Loss: 0.5117\n",
            "Epoch [2757/20000], Training Loss: 0.4543\n",
            "Epoch [2758/20000], Training Loss: 0.4985\n",
            "Epoch [2759/20000], Training Loss: 0.5053\n",
            "Epoch [2760/20000], Training Loss: 0.4849\n",
            "Epoch [2761/20000], Training Loss: 0.5408\n",
            "Epoch [2762/20000], Training Loss: 0.5047\n",
            "Epoch [2763/20000], Training Loss: 0.4814\n",
            "Epoch [2764/20000], Training Loss: 0.4990\n",
            "Epoch [2765/20000], Training Loss: 0.4937\n",
            "Epoch [2766/20000], Training Loss: 0.5031\n",
            "Epoch [2767/20000], Training Loss: 0.4584\n",
            "Epoch [2768/20000], Training Loss: 0.4953\n",
            "Epoch [2769/20000], Training Loss: 0.5034\n",
            "Epoch [2770/20000], Training Loss: 0.4936\n",
            "Epoch [2771/20000], Training Loss: 0.5149\n",
            "Epoch [2772/20000], Training Loss: 0.4912\n",
            "Epoch [2773/20000], Training Loss: 0.4518\n",
            "Epoch [2774/20000], Training Loss: 0.4828\n",
            "Epoch [2775/20000], Training Loss: 0.5039\n",
            "Epoch [2776/20000], Training Loss: 0.4793\n",
            "Epoch [2777/20000], Training Loss: 0.4759\n",
            "Epoch [2778/20000], Training Loss: 0.4630\n",
            "Epoch [2779/20000], Training Loss: 0.5043\n",
            "Epoch [2780/20000], Training Loss: 0.4767\n",
            "Epoch [2781/20000], Training Loss: 0.5284\n",
            "Epoch [2782/20000], Training Loss: 0.4891\n",
            "Epoch [2783/20000], Training Loss: 0.4956\n",
            "Epoch [2784/20000], Training Loss: 0.4798\n",
            "Epoch [2785/20000], Training Loss: 0.5111\n",
            "Epoch [2786/20000], Training Loss: 0.4897\n",
            "Epoch [2787/20000], Training Loss: 0.4812\n",
            "Epoch [2788/20000], Training Loss: 0.5218\n",
            "Epoch [2789/20000], Training Loss: 0.5283\n",
            "Epoch [2790/20000], Training Loss: 0.4723\n",
            "Epoch [2791/20000], Training Loss: 0.4386\n",
            "Epoch [2792/20000], Training Loss: 0.4416\n",
            "Epoch [2793/20000], Training Loss: 0.4720\n",
            "Epoch [2794/20000], Training Loss: 0.4637\n",
            "Epoch [2795/20000], Training Loss: 0.4777\n",
            "Epoch [2796/20000], Training Loss: 0.4645\n",
            "Epoch [2797/20000], Training Loss: 0.5284\n",
            "Epoch [2798/20000], Training Loss: 0.4731\n",
            "Epoch [2799/20000], Training Loss: 0.5034\n",
            "Epoch [2800/20000], Training Loss: 0.5483\n",
            "Epoch [2801/20000], Training Loss: 0.5280\n",
            "Epoch [2802/20000], Training Loss: 0.5087\n",
            "Epoch [2803/20000], Training Loss: 0.5314\n",
            "Epoch [2804/20000], Training Loss: 0.4849\n",
            "Epoch [2805/20000], Training Loss: 0.5159\n",
            "Epoch [2806/20000], Training Loss: 0.4842\n",
            "Epoch [2807/20000], Training Loss: 0.4862\n",
            "Epoch [2808/20000], Training Loss: 0.4889\n",
            "Epoch [2809/20000], Training Loss: 0.4993\n",
            "Epoch [2810/20000], Training Loss: 0.4865\n",
            "Epoch [2811/20000], Training Loss: 0.4707\n",
            "Epoch [2812/20000], Training Loss: 0.4980\n",
            "Epoch [2813/20000], Training Loss: 0.4811\n",
            "Epoch [2814/20000], Training Loss: 0.4990\n",
            "Epoch [2815/20000], Training Loss: 0.5097\n",
            "Epoch [2816/20000], Training Loss: 0.4526\n",
            "Epoch [2817/20000], Training Loss: 0.4666\n",
            "Epoch [2818/20000], Training Loss: 0.4526\n",
            "Epoch [2819/20000], Training Loss: 0.4805\n",
            "Epoch [2820/20000], Training Loss: 0.4879\n",
            "Epoch [2821/20000], Training Loss: 0.5357\n",
            "Epoch [2822/20000], Training Loss: 0.4981\n",
            "Epoch [2823/20000], Training Loss: 0.4922\n",
            "Epoch [2824/20000], Training Loss: 0.4918\n",
            "Epoch [2825/20000], Training Loss: 0.4979\n",
            "Epoch [2826/20000], Training Loss: 0.4855\n",
            "Epoch [2827/20000], Training Loss: 0.4717\n",
            "Epoch [2828/20000], Training Loss: 0.4675\n",
            "Epoch [2829/20000], Training Loss: 0.4538\n",
            "Epoch [2830/20000], Training Loss: 0.5017\n",
            "Epoch [2831/20000], Training Loss: 0.4360\n",
            "Epoch [2832/20000], Training Loss: 0.5179\n",
            "Epoch [2833/20000], Training Loss: 0.4760\n",
            "Epoch [2834/20000], Training Loss: 0.5294\n",
            "Epoch [2835/20000], Training Loss: 0.4725\n",
            "Epoch [2836/20000], Training Loss: 0.4449\n",
            "Epoch [2837/20000], Training Loss: 0.4468\n",
            "Epoch [2838/20000], Training Loss: 0.4996\n",
            "Epoch [2839/20000], Training Loss: 0.5065\n",
            "Epoch [2840/20000], Training Loss: 0.4672\n",
            "Epoch [2841/20000], Training Loss: 0.4460\n",
            "Epoch [2842/20000], Training Loss: 0.5228\n",
            "Epoch [2843/20000], Training Loss: 0.4821\n",
            "Epoch [2844/20000], Training Loss: 0.5354\n",
            "Epoch [2845/20000], Training Loss: 0.5232\n",
            "Epoch [2846/20000], Training Loss: 0.5247\n",
            "Epoch [2847/20000], Training Loss: 0.5087\n",
            "Epoch [2848/20000], Training Loss: 0.4732\n",
            "Epoch [2849/20000], Training Loss: 0.4696\n",
            "Epoch [2850/20000], Training Loss: 0.4938\n",
            "Epoch [2851/20000], Training Loss: 0.5539\n",
            "Epoch [2852/20000], Training Loss: 0.5011\n",
            "Epoch [2853/20000], Training Loss: 0.4733\n",
            "Epoch [2854/20000], Training Loss: 0.4953\n",
            "Epoch [2855/20000], Training Loss: 0.4647\n",
            "Epoch [2856/20000], Training Loss: 0.4582\n",
            "Epoch [2857/20000], Training Loss: 0.5027\n",
            "Epoch [2858/20000], Training Loss: 0.5025\n",
            "Epoch [2859/20000], Training Loss: 0.4954\n",
            "Epoch [2860/20000], Training Loss: 0.4635\n",
            "Epoch [2861/20000], Training Loss: 0.5341\n",
            "Epoch [2862/20000], Training Loss: 0.4942\n",
            "Epoch [2863/20000], Training Loss: 0.4617\n",
            "Epoch [2864/20000], Training Loss: 0.4458\n",
            "Epoch [2865/20000], Training Loss: 0.4572\n",
            "Epoch [2866/20000], Training Loss: 0.5334\n",
            "Epoch [2867/20000], Training Loss: 0.5028\n",
            "Epoch [2868/20000], Training Loss: 0.5061\n",
            "Epoch [2869/20000], Training Loss: 0.4969\n",
            "Epoch [2870/20000], Training Loss: 0.5270\n",
            "Epoch [2871/20000], Training Loss: 0.5242\n",
            "Epoch [2872/20000], Training Loss: 0.4924\n",
            "Epoch [2873/20000], Training Loss: 0.4980\n",
            "Epoch [2874/20000], Training Loss: 0.5044\n",
            "Epoch [2875/20000], Training Loss: 0.5144\n",
            "Epoch [2876/20000], Training Loss: 0.4821\n",
            "Epoch [2877/20000], Training Loss: 0.4661\n",
            "Epoch [2878/20000], Training Loss: 0.4783\n",
            "Epoch [2879/20000], Training Loss: 0.4615\n",
            "Epoch [2880/20000], Training Loss: 0.4644\n",
            "Epoch [2881/20000], Training Loss: 0.4424\n",
            "Epoch [2882/20000], Training Loss: 0.4619\n",
            "Epoch [2883/20000], Training Loss: 0.5288\n",
            "Epoch [2884/20000], Training Loss: 0.4693\n",
            "Epoch [2885/20000], Training Loss: 0.4820\n",
            "Epoch [2886/20000], Training Loss: 0.4731\n",
            "Epoch [2887/20000], Training Loss: 0.5338\n",
            "Epoch [2888/20000], Training Loss: 0.4599\n",
            "Epoch [2889/20000], Training Loss: 0.4684\n",
            "Epoch [2890/20000], Training Loss: 0.4758\n",
            "Epoch [2891/20000], Training Loss: 0.4691\n",
            "Epoch [2892/20000], Training Loss: 0.4528\n",
            "Epoch [2893/20000], Training Loss: 0.4859\n",
            "Epoch [2894/20000], Training Loss: 0.5057\n",
            "Epoch [2895/20000], Training Loss: 0.4850\n",
            "Epoch [2896/20000], Training Loss: 0.4911\n",
            "Epoch [2897/20000], Training Loss: 0.4578\n",
            "Epoch [2898/20000], Training Loss: 0.5059\n",
            "Epoch [2899/20000], Training Loss: 0.5147\n",
            "Epoch [2900/20000], Training Loss: 0.5071\n",
            "Epoch [2901/20000], Training Loss: 0.4767\n",
            "Epoch [2902/20000], Training Loss: 0.4876\n",
            "Epoch [2903/20000], Training Loss: 0.4721\n",
            "Epoch [2904/20000], Training Loss: 0.4882\n",
            "Epoch [2905/20000], Training Loss: 0.5217\n",
            "Epoch [2906/20000], Training Loss: 0.5150\n",
            "Epoch [2907/20000], Training Loss: 0.4690\n",
            "Epoch [2908/20000], Training Loss: 0.5198\n",
            "Epoch [2909/20000], Training Loss: 0.4889\n",
            "Epoch [2910/20000], Training Loss: 0.5386\n",
            "Epoch [2911/20000], Training Loss: 0.4834\n",
            "Epoch [2912/20000], Training Loss: 0.4777\n",
            "Epoch [2913/20000], Training Loss: 0.4606\n",
            "Epoch [2914/20000], Training Loss: 0.4813\n",
            "Epoch [2915/20000], Training Loss: 0.4459\n",
            "Epoch [2916/20000], Training Loss: 0.4594\n",
            "Epoch [2917/20000], Training Loss: 0.5022\n",
            "Epoch [2918/20000], Training Loss: 0.5304\n",
            "Epoch [2919/20000], Training Loss: 0.4875\n",
            "Epoch [2920/20000], Training Loss: 0.4402\n",
            "Epoch [2921/20000], Training Loss: 0.4476\n",
            "Epoch [2922/20000], Training Loss: 0.4879\n",
            "Epoch [2923/20000], Training Loss: 0.4300\n",
            "Epoch [2924/20000], Training Loss: 0.4380\n",
            "Epoch [2925/20000], Training Loss: 0.4810\n",
            "Epoch [2926/20000], Training Loss: 0.4607\n",
            "Epoch [2927/20000], Training Loss: 0.5111\n",
            "Epoch [2928/20000], Training Loss: 0.4916\n",
            "Epoch [2929/20000], Training Loss: 0.4551\n",
            "Epoch [2930/20000], Training Loss: 0.5421\n",
            "Epoch [2931/20000], Training Loss: 0.4784\n",
            "Epoch [2932/20000], Training Loss: 0.4803\n",
            "Epoch [2933/20000], Training Loss: 0.5143\n",
            "Epoch [2934/20000], Training Loss: 0.4514\n",
            "Epoch [2935/20000], Training Loss: 0.4914\n",
            "Epoch [2936/20000], Training Loss: 0.4796\n",
            "Epoch [2937/20000], Training Loss: 0.4843\n",
            "Epoch [2938/20000], Training Loss: 0.4633\n",
            "Epoch [2939/20000], Training Loss: 0.5014\n",
            "Epoch [2940/20000], Training Loss: 0.5225\n",
            "Epoch [2941/20000], Training Loss: 0.4800\n",
            "Epoch [2942/20000], Training Loss: 0.5235\n",
            "Epoch [2943/20000], Training Loss: 0.4966\n",
            "Epoch [2944/20000], Training Loss: 0.4179\n",
            "Epoch [2945/20000], Training Loss: 0.4877\n",
            "Epoch [2946/20000], Training Loss: 0.5284\n",
            "Epoch [2947/20000], Training Loss: 0.4624\n",
            "Epoch [2948/20000], Training Loss: 0.5002\n",
            "Epoch [2949/20000], Training Loss: 0.4757\n",
            "Epoch [2950/20000], Training Loss: 0.4450\n",
            "Epoch [2951/20000], Training Loss: 0.4632\n",
            "Epoch [2952/20000], Training Loss: 0.5482\n",
            "Epoch [2953/20000], Training Loss: 0.4705\n",
            "Epoch [2954/20000], Training Loss: 0.4816\n",
            "Epoch [2955/20000], Training Loss: 0.5137\n",
            "Epoch [2956/20000], Training Loss: 0.4424\n",
            "Epoch [2957/20000], Training Loss: 0.5318\n",
            "Epoch [2958/20000], Training Loss: 0.5108\n",
            "Epoch [2959/20000], Training Loss: 0.5060\n",
            "Epoch [2960/20000], Training Loss: 0.4890\n",
            "Epoch [2961/20000], Training Loss: 0.4943\n",
            "Epoch [2962/20000], Training Loss: 0.5482\n",
            "Epoch [2963/20000], Training Loss: 0.4720\n",
            "Epoch [2964/20000], Training Loss: 0.4594\n",
            "Epoch [2965/20000], Training Loss: 0.4587\n",
            "Epoch [2966/20000], Training Loss: 0.4690\n",
            "Epoch [2967/20000], Training Loss: 0.5146\n",
            "Epoch [2968/20000], Training Loss: 0.4470\n",
            "Epoch [2969/20000], Training Loss: 0.4834\n",
            "Epoch [2970/20000], Training Loss: 0.4661\n",
            "Epoch [2971/20000], Training Loss: 0.5002\n",
            "Epoch [2972/20000], Training Loss: 0.4401\n",
            "Epoch [2973/20000], Training Loss: 0.4670\n",
            "Epoch [2974/20000], Training Loss: 0.5210\n",
            "Epoch [2975/20000], Training Loss: 0.5089\n",
            "Epoch [2976/20000], Training Loss: 0.5114\n",
            "Epoch [2977/20000], Training Loss: 0.4833\n",
            "Epoch [2978/20000], Training Loss: 0.4837\n",
            "Epoch [2979/20000], Training Loss: 0.4721\n",
            "Epoch [2980/20000], Training Loss: 0.4961\n",
            "Epoch [2981/20000], Training Loss: 0.5079\n",
            "Epoch [2982/20000], Training Loss: 0.4895\n",
            "Epoch [2983/20000], Training Loss: 0.4605\n",
            "Epoch [2984/20000], Training Loss: 0.4733\n",
            "Epoch [2985/20000], Training Loss: 0.5188\n",
            "Epoch [2986/20000], Training Loss: 0.4950\n",
            "Epoch [2987/20000], Training Loss: 0.4846\n",
            "Epoch [2988/20000], Training Loss: 0.4725\n",
            "Epoch [2989/20000], Training Loss: 0.4526\n",
            "Epoch [2990/20000], Training Loss: 0.4841\n",
            "Epoch [2991/20000], Training Loss: 0.4935\n",
            "Epoch [2992/20000], Training Loss: 0.5243\n",
            "Epoch [2993/20000], Training Loss: 0.4856\n",
            "Epoch [2994/20000], Training Loss: 0.4876\n",
            "Epoch [2995/20000], Training Loss: 0.4553\n",
            "Epoch [2996/20000], Training Loss: 0.4940\n",
            "Epoch [2997/20000], Training Loss: 0.4570\n",
            "Epoch [2998/20000], Training Loss: 0.5556\n",
            "Epoch [2999/20000], Training Loss: 0.5196\n",
            "Epoch [3000/20000], Training Loss: 0.4793\n",
            "Epoch [3001/20000], Training Loss: 0.4722\n",
            "Epoch [3002/20000], Training Loss: 0.4436\n",
            "Epoch [3003/20000], Training Loss: 0.4564\n",
            "Epoch [3004/20000], Training Loss: 0.4803\n",
            "Epoch [3005/20000], Training Loss: 0.5208\n",
            "Epoch [3006/20000], Training Loss: 0.4747\n",
            "Epoch [3007/20000], Training Loss: 0.5291\n",
            "Epoch [3008/20000], Training Loss: 0.4702\n",
            "Epoch [3009/20000], Training Loss: 0.4918\n",
            "Epoch [3010/20000], Training Loss: 0.5315\n",
            "Epoch [3011/20000], Training Loss: 0.4972\n",
            "Epoch [3012/20000], Training Loss: 0.4758\n",
            "Epoch [3013/20000], Training Loss: 0.4932\n",
            "Epoch [3014/20000], Training Loss: 0.4899\n",
            "Epoch [3015/20000], Training Loss: 0.5161\n",
            "Epoch [3016/20000], Training Loss: 0.4778\n",
            "Epoch [3017/20000], Training Loss: 0.4910\n",
            "Epoch [3018/20000], Training Loss: 0.4627\n",
            "Epoch [3019/20000], Training Loss: 0.4691\n",
            "Epoch [3020/20000], Training Loss: 0.4318\n",
            "Epoch [3021/20000], Training Loss: 0.4558\n",
            "Epoch [3022/20000], Training Loss: 0.5118\n",
            "Epoch [3023/20000], Training Loss: 0.4856\n",
            "Epoch [3024/20000], Training Loss: 0.5050\n",
            "Epoch [3025/20000], Training Loss: 0.4810\n",
            "Epoch [3026/20000], Training Loss: 0.5139\n",
            "Epoch [3027/20000], Training Loss: 0.5045\n",
            "Epoch [3028/20000], Training Loss: 0.4657\n",
            "Epoch [3029/20000], Training Loss: 0.4769\n",
            "Epoch [3030/20000], Training Loss: 0.4688\n",
            "Epoch [3031/20000], Training Loss: 0.4827\n",
            "Epoch [3032/20000], Training Loss: 0.5269\n",
            "Epoch [3033/20000], Training Loss: 0.5541\n",
            "Epoch [3034/20000], Training Loss: 0.4852\n",
            "Epoch [3035/20000], Training Loss: 0.4832\n",
            "Epoch [3036/20000], Training Loss: 0.5005\n",
            "Epoch [3037/20000], Training Loss: 0.4986\n",
            "Epoch [3038/20000], Training Loss: 0.5270\n",
            "Epoch [3039/20000], Training Loss: 0.4937\n",
            "Epoch [3040/20000], Training Loss: 0.5200\n",
            "Epoch [3041/20000], Training Loss: 0.5219\n",
            "Epoch [3042/20000], Training Loss: 0.4887\n",
            "Epoch [3043/20000], Training Loss: 0.5202\n",
            "Epoch [3044/20000], Training Loss: 0.4372\n",
            "Epoch [3045/20000], Training Loss: 0.4845\n",
            "Epoch [3046/20000], Training Loss: 0.5114\n",
            "Epoch [3047/20000], Training Loss: 0.5082\n",
            "Epoch [3048/20000], Training Loss: 0.4799\n",
            "Epoch [3049/20000], Training Loss: 0.5267\n",
            "Epoch [3050/20000], Training Loss: 0.5095\n",
            "Epoch [3051/20000], Training Loss: 0.4903\n",
            "Epoch [3052/20000], Training Loss: 0.5449\n",
            "Epoch [3053/20000], Training Loss: 0.5050\n",
            "Epoch [3054/20000], Training Loss: 0.5026\n",
            "Epoch [3055/20000], Training Loss: 0.4542\n",
            "Epoch [3056/20000], Training Loss: 0.5289\n",
            "Epoch [3057/20000], Training Loss: 0.4753\n",
            "Epoch [3058/20000], Training Loss: 0.4970\n",
            "Epoch [3059/20000], Training Loss: 0.4854\n",
            "Epoch [3060/20000], Training Loss: 0.4758\n",
            "Epoch [3061/20000], Training Loss: 0.5257\n",
            "Epoch [3062/20000], Training Loss: 0.4971\n",
            "Epoch [3063/20000], Training Loss: 0.4628\n",
            "Epoch [3064/20000], Training Loss: 0.4506\n",
            "Epoch [3065/20000], Training Loss: 0.5005\n",
            "Epoch [3066/20000], Training Loss: 0.4539\n",
            "Epoch [3067/20000], Training Loss: 0.4615\n",
            "Epoch [3068/20000], Training Loss: 0.4991\n",
            "Epoch [3069/20000], Training Loss: 0.4744\n",
            "Epoch [3070/20000], Training Loss: 0.4814\n",
            "Epoch [3071/20000], Training Loss: 0.5031\n",
            "Epoch [3072/20000], Training Loss: 0.4649\n",
            "Epoch [3073/20000], Training Loss: 0.4816\n",
            "Epoch [3074/20000], Training Loss: 0.4727\n",
            "Epoch [3075/20000], Training Loss: 0.5201\n",
            "Epoch [3076/20000], Training Loss: 0.5246\n",
            "Epoch [3077/20000], Training Loss: 0.4922\n",
            "Epoch [3078/20000], Training Loss: 0.4845\n",
            "Epoch [3079/20000], Training Loss: 0.5010\n",
            "Epoch [3080/20000], Training Loss: 0.4929\n",
            "Epoch [3081/20000], Training Loss: 0.4899\n",
            "Epoch [3082/20000], Training Loss: 0.4841\n",
            "Epoch [3083/20000], Training Loss: 0.4798\n",
            "Epoch [3084/20000], Training Loss: 0.5002\n",
            "Epoch [3085/20000], Training Loss: 0.4458\n",
            "Epoch [3086/20000], Training Loss: 0.4589\n",
            "Epoch [3087/20000], Training Loss: 0.4892\n",
            "Epoch [3088/20000], Training Loss: 0.4932\n",
            "Epoch [3089/20000], Training Loss: 0.4564\n",
            "Epoch [3090/20000], Training Loss: 0.5050\n",
            "Epoch [3091/20000], Training Loss: 0.4734\n",
            "Epoch [3092/20000], Training Loss: 0.4641\n",
            "Epoch [3093/20000], Training Loss: 0.4856\n",
            "Epoch [3094/20000], Training Loss: 0.4988\n",
            "Epoch [3095/20000], Training Loss: 0.4806\n",
            "Epoch [3096/20000], Training Loss: 0.4862\n",
            "Epoch [3097/20000], Training Loss: 0.4533\n",
            "Epoch [3098/20000], Training Loss: 0.5116\n",
            "Epoch [3099/20000], Training Loss: 0.4921\n",
            "Epoch [3100/20000], Training Loss: 0.4826\n",
            "Epoch [3101/20000], Training Loss: 0.5074\n",
            "Epoch [3102/20000], Training Loss: 0.5009\n",
            "Epoch [3103/20000], Training Loss: 0.4707\n",
            "Epoch [3104/20000], Training Loss: 0.4568\n",
            "Epoch [3105/20000], Training Loss: 0.5093\n",
            "Epoch [3106/20000], Training Loss: 0.4953\n",
            "Epoch [3107/20000], Training Loss: 0.4393\n",
            "Epoch [3108/20000], Training Loss: 0.4988\n",
            "Epoch [3109/20000], Training Loss: 0.4722\n",
            "Epoch [3110/20000], Training Loss: 0.4504\n",
            "Epoch [3111/20000], Training Loss: 0.4611\n",
            "Epoch [3112/20000], Training Loss: 0.4977\n",
            "Epoch [3113/20000], Training Loss: 0.4751\n",
            "Epoch [3114/20000], Training Loss: 0.4935\n",
            "Epoch [3115/20000], Training Loss: 0.4885\n",
            "Epoch [3116/20000], Training Loss: 0.4873\n",
            "Epoch [3117/20000], Training Loss: 0.4934\n",
            "Epoch [3118/20000], Training Loss: 0.4477\n",
            "Epoch [3119/20000], Training Loss: 0.4823\n",
            "Epoch [3120/20000], Training Loss: 0.4742\n",
            "Epoch [3121/20000], Training Loss: 0.4472\n",
            "Epoch [3122/20000], Training Loss: 0.4585\n",
            "Epoch [3123/20000], Training Loss: 0.4686\n",
            "Epoch [3124/20000], Training Loss: 0.5001\n",
            "Epoch [3125/20000], Training Loss: 0.5087\n",
            "Epoch [3126/20000], Training Loss: 0.5045\n",
            "Epoch [3127/20000], Training Loss: 0.5027\n",
            "Epoch [3128/20000], Training Loss: 0.4949\n",
            "Epoch [3129/20000], Training Loss: 0.5029\n",
            "Epoch [3130/20000], Training Loss: 0.4421\n",
            "Epoch [3131/20000], Training Loss: 0.4815\n",
            "Epoch [3132/20000], Training Loss: 0.5039\n",
            "Epoch [3133/20000], Training Loss: 0.4830\n",
            "Epoch [3134/20000], Training Loss: 0.4539\n",
            "Epoch [3135/20000], Training Loss: 0.4661\n",
            "Epoch [3136/20000], Training Loss: 0.4735\n",
            "Epoch [3137/20000], Training Loss: 0.5100\n",
            "Epoch [3138/20000], Training Loss: 0.5267\n",
            "Epoch [3139/20000], Training Loss: 0.5007\n",
            "Epoch [3140/20000], Training Loss: 0.4734\n",
            "Epoch [3141/20000], Training Loss: 0.4664\n",
            "Epoch [3142/20000], Training Loss: 0.4684\n",
            "Epoch [3143/20000], Training Loss: 0.4931\n",
            "Epoch [3144/20000], Training Loss: 0.4605\n",
            "Epoch [3145/20000], Training Loss: 0.5134\n",
            "Epoch [3146/20000], Training Loss: 0.4598\n",
            "Epoch [3147/20000], Training Loss: 0.4984\n",
            "Epoch [3148/20000], Training Loss: 0.4399\n",
            "Epoch [3149/20000], Training Loss: 0.4901\n",
            "Epoch [3150/20000], Training Loss: 0.4593\n",
            "Epoch [3151/20000], Training Loss: 0.4850\n",
            "Epoch [3152/20000], Training Loss: 0.5000\n",
            "Epoch [3153/20000], Training Loss: 0.4823\n",
            "Epoch [3154/20000], Training Loss: 0.4908\n",
            "Epoch [3155/20000], Training Loss: 0.5006\n",
            "Epoch [3156/20000], Training Loss: 0.4754\n",
            "Epoch [3157/20000], Training Loss: 0.4712\n",
            "Epoch [3158/20000], Training Loss: 0.5106\n",
            "Epoch [3159/20000], Training Loss: 0.4876\n",
            "Epoch [3160/20000], Training Loss: 0.4768\n",
            "Epoch [3161/20000], Training Loss: 0.4789\n",
            "Epoch [3162/20000], Training Loss: 0.4718\n",
            "Epoch [3163/20000], Training Loss: 0.4663\n",
            "Epoch [3164/20000], Training Loss: 0.5006\n",
            "Epoch [3165/20000], Training Loss: 0.5086\n",
            "Epoch [3166/20000], Training Loss: 0.4642\n",
            "Epoch [3167/20000], Training Loss: 0.4547\n",
            "Epoch [3168/20000], Training Loss: 0.4954\n",
            "Epoch [3169/20000], Training Loss: 0.5184\n",
            "Epoch [3170/20000], Training Loss: 0.5182\n",
            "Epoch [3171/20000], Training Loss: 0.5461\n",
            "Epoch [3172/20000], Training Loss: 0.4678\n",
            "Epoch [3173/20000], Training Loss: 0.5290\n",
            "Epoch [3174/20000], Training Loss: 0.4830\n",
            "Epoch [3175/20000], Training Loss: 0.4405\n",
            "Epoch [3176/20000], Training Loss: 0.4807\n",
            "Epoch [3177/20000], Training Loss: 0.4861\n",
            "Epoch [3178/20000], Training Loss: 0.4949\n",
            "Epoch [3179/20000], Training Loss: 0.4862\n",
            "Epoch [3180/20000], Training Loss: 0.5094\n",
            "Epoch [3181/20000], Training Loss: 0.5101\n",
            "Epoch [3182/20000], Training Loss: 0.5000\n",
            "Epoch [3183/20000], Training Loss: 0.4981\n",
            "Epoch [3184/20000], Training Loss: 0.5245\n",
            "Epoch [3185/20000], Training Loss: 0.4604\n",
            "Epoch [3186/20000], Training Loss: 0.5044\n",
            "Epoch [3187/20000], Training Loss: 0.4958\n",
            "Epoch [3188/20000], Training Loss: 0.4830\n",
            "Epoch [3189/20000], Training Loss: 0.5031\n",
            "Epoch [3190/20000], Training Loss: 0.4921\n",
            "Epoch [3191/20000], Training Loss: 0.5272\n",
            "Epoch [3192/20000], Training Loss: 0.5080\n",
            "Epoch [3193/20000], Training Loss: 0.5090\n",
            "Epoch [3194/20000], Training Loss: 0.4633\n",
            "Epoch [3195/20000], Training Loss: 0.4904\n",
            "Epoch [3196/20000], Training Loss: 0.4696\n",
            "Epoch [3197/20000], Training Loss: 0.5117\n",
            "Epoch [3198/20000], Training Loss: 0.4959\n",
            "Epoch [3199/20000], Training Loss: 0.4743\n",
            "Epoch [3200/20000], Training Loss: 0.4460\n",
            "Epoch [3201/20000], Training Loss: 0.5049\n",
            "Epoch [3202/20000], Training Loss: 0.4595\n",
            "Epoch [3203/20000], Training Loss: 0.4770\n",
            "Epoch [3204/20000], Training Loss: 0.5072\n",
            "Epoch [3205/20000], Training Loss: 0.4941\n",
            "Epoch [3206/20000], Training Loss: 0.4633\n",
            "Epoch [3207/20000], Training Loss: 0.5388\n",
            "Epoch [3208/20000], Training Loss: 0.4787\n",
            "Epoch [3209/20000], Training Loss: 0.4301\n",
            "Epoch [3210/20000], Training Loss: 0.5135\n",
            "Epoch [3211/20000], Training Loss: 0.4794\n",
            "Epoch [3212/20000], Training Loss: 0.4964\n",
            "Epoch [3213/20000], Training Loss: 0.4738\n",
            "Epoch [3214/20000], Training Loss: 0.4684\n",
            "Epoch [3215/20000], Training Loss: 0.5495\n",
            "Epoch [3216/20000], Training Loss: 0.5281\n",
            "Epoch [3217/20000], Training Loss: 0.5047\n",
            "Epoch [3218/20000], Training Loss: 0.4838\n",
            "Epoch [3219/20000], Training Loss: 0.4392\n",
            "Epoch [3220/20000], Training Loss: 0.4913\n",
            "Epoch [3221/20000], Training Loss: 0.4998\n",
            "Epoch [3222/20000], Training Loss: 0.4854\n",
            "Epoch [3223/20000], Training Loss: 0.4739\n",
            "Epoch [3224/20000], Training Loss: 0.5077\n",
            "Epoch [3225/20000], Training Loss: 0.5045\n",
            "Epoch [3226/20000], Training Loss: 0.4318\n",
            "Epoch [3227/20000], Training Loss: 0.5381\n",
            "Epoch [3228/20000], Training Loss: 0.5106\n",
            "Epoch [3229/20000], Training Loss: 0.4783\n",
            "Epoch [3230/20000], Training Loss: 0.5009\n",
            "Epoch [3231/20000], Training Loss: 0.4800\n",
            "Epoch [3232/20000], Training Loss: 0.4674\n",
            "Epoch [3233/20000], Training Loss: 0.5092\n",
            "Epoch [3234/20000], Training Loss: 0.5061\n",
            "Epoch [3235/20000], Training Loss: 0.4989\n",
            "Epoch [3236/20000], Training Loss: 0.5136\n",
            "Epoch [3237/20000], Training Loss: 0.4793\n",
            "Epoch [3238/20000], Training Loss: 0.4610\n",
            "Epoch [3239/20000], Training Loss: 0.4540\n",
            "Epoch [3240/20000], Training Loss: 0.5252\n",
            "Epoch [3241/20000], Training Loss: 0.5216\n",
            "Epoch [3242/20000], Training Loss: 0.5027\n",
            "Epoch [3243/20000], Training Loss: 0.4845\n",
            "Epoch [3244/20000], Training Loss: 0.4755\n",
            "Epoch [3245/20000], Training Loss: 0.4698\n",
            "Epoch [3246/20000], Training Loss: 0.4868\n",
            "Epoch [3247/20000], Training Loss: 0.4911\n",
            "Epoch [3248/20000], Training Loss: 0.5060\n",
            "Epoch [3249/20000], Training Loss: 0.5197\n",
            "Epoch [3250/20000], Training Loss: 0.5016\n",
            "Epoch [3251/20000], Training Loss: 0.4865\n",
            "Epoch [3252/20000], Training Loss: 0.4756\n",
            "Epoch [3253/20000], Training Loss: 0.5034\n",
            "Epoch [3254/20000], Training Loss: 0.4444\n",
            "Epoch [3255/20000], Training Loss: 0.4953\n",
            "Epoch [3256/20000], Training Loss: 0.5286\n",
            "Epoch [3257/20000], Training Loss: 0.4799\n",
            "Epoch [3258/20000], Training Loss: 0.4457\n",
            "Epoch [3259/20000], Training Loss: 0.4707\n",
            "Epoch [3260/20000], Training Loss: 0.4952\n",
            "Epoch [3261/20000], Training Loss: 0.4973\n",
            "Epoch [3262/20000], Training Loss: 0.4363\n",
            "Epoch [3263/20000], Training Loss: 0.4900\n",
            "Epoch [3264/20000], Training Loss: 0.4840\n",
            "Epoch [3265/20000], Training Loss: 0.5298\n",
            "Epoch [3266/20000], Training Loss: 0.5162\n",
            "Epoch [3267/20000], Training Loss: 0.4629\n",
            "Epoch [3268/20000], Training Loss: 0.4777\n",
            "Epoch [3269/20000], Training Loss: 0.4785\n",
            "Epoch [3270/20000], Training Loss: 0.5078\n",
            "Epoch [3271/20000], Training Loss: 0.4461\n",
            "Epoch [3272/20000], Training Loss: 0.4641\n",
            "Epoch [3273/20000], Training Loss: 0.4794\n",
            "Epoch [3274/20000], Training Loss: 0.5144\n",
            "Epoch [3275/20000], Training Loss: 0.4864\n",
            "Epoch [3276/20000], Training Loss: 0.4866\n",
            "Epoch [3277/20000], Training Loss: 0.5218\n",
            "Epoch [3278/20000], Training Loss: 0.4548\n",
            "Epoch [3279/20000], Training Loss: 0.4537\n",
            "Epoch [3280/20000], Training Loss: 0.4986\n",
            "Epoch [3281/20000], Training Loss: 0.5040\n",
            "Epoch [3282/20000], Training Loss: 0.4771\n",
            "Epoch [3283/20000], Training Loss: 0.5104\n",
            "Epoch [3284/20000], Training Loss: 0.5568\n",
            "Epoch [3285/20000], Training Loss: 0.4999\n",
            "Epoch [3286/20000], Training Loss: 0.4551\n",
            "Epoch [3287/20000], Training Loss: 0.5061\n",
            "Epoch [3288/20000], Training Loss: 0.4850\n",
            "Epoch [3289/20000], Training Loss: 0.4997\n",
            "Epoch [3290/20000], Training Loss: 0.4712\n",
            "Epoch [3291/20000], Training Loss: 0.4639\n",
            "Epoch [3292/20000], Training Loss: 0.4488\n",
            "Epoch [3293/20000], Training Loss: 0.4820\n",
            "Epoch [3294/20000], Training Loss: 0.4865\n",
            "Epoch [3295/20000], Training Loss: 0.4763\n",
            "Epoch [3296/20000], Training Loss: 0.4632\n",
            "Epoch [3297/20000], Training Loss: 0.4716\n",
            "Epoch [3298/20000], Training Loss: 0.4620\n",
            "Epoch [3299/20000], Training Loss: 0.4767\n",
            "Epoch [3300/20000], Training Loss: 0.5044\n",
            "Epoch [3301/20000], Training Loss: 0.4692\n",
            "Epoch [3302/20000], Training Loss: 0.5251\n",
            "Epoch [3303/20000], Training Loss: 0.5173\n",
            "Epoch [3304/20000], Training Loss: 0.5264\n",
            "Epoch [3305/20000], Training Loss: 0.4623\n",
            "Epoch [3306/20000], Training Loss: 0.4529\n",
            "Epoch [3307/20000], Training Loss: 0.4556\n",
            "Epoch [3308/20000], Training Loss: 0.4940\n",
            "Epoch [3309/20000], Training Loss: 0.5019\n",
            "Epoch [3310/20000], Training Loss: 0.5103\n",
            "Epoch [3311/20000], Training Loss: 0.5282\n",
            "Epoch [3312/20000], Training Loss: 0.5184\n",
            "Epoch [3313/20000], Training Loss: 0.5294\n",
            "Epoch [3314/20000], Training Loss: 0.5094\n",
            "Epoch [3315/20000], Training Loss: 0.4552\n",
            "Epoch [3316/20000], Training Loss: 0.4673\n",
            "Epoch [3317/20000], Training Loss: 0.5277\n",
            "Epoch [3318/20000], Training Loss: 0.4579\n",
            "Epoch [3319/20000], Training Loss: 0.5002\n",
            "Epoch [3320/20000], Training Loss: 0.4355\n",
            "Epoch [3321/20000], Training Loss: 0.4486\n",
            "Epoch [3322/20000], Training Loss: 0.4984\n",
            "Epoch [3323/20000], Training Loss: 0.4842\n",
            "Epoch [3324/20000], Training Loss: 0.5008\n",
            "Epoch [3325/20000], Training Loss: 0.4700\n",
            "Epoch [3326/20000], Training Loss: 0.4632\n",
            "Epoch [3327/20000], Training Loss: 0.5010\n",
            "Epoch [3328/20000], Training Loss: 0.4769\n",
            "Epoch [3329/20000], Training Loss: 0.4703\n",
            "Epoch [3330/20000], Training Loss: 0.4401\n",
            "Epoch [3331/20000], Training Loss: 0.4930\n",
            "Epoch [3332/20000], Training Loss: 0.4677\n",
            "Epoch [3333/20000], Training Loss: 0.4620\n",
            "Epoch [3334/20000], Training Loss: 0.4618\n",
            "Epoch [3335/20000], Training Loss: 0.4596\n",
            "Epoch [3336/20000], Training Loss: 0.4701\n",
            "Epoch [3337/20000], Training Loss: 0.5528\n",
            "Epoch [3338/20000], Training Loss: 0.4998\n",
            "Epoch [3339/20000], Training Loss: 0.5181\n",
            "Epoch [3340/20000], Training Loss: 0.5297\n",
            "Epoch [3341/20000], Training Loss: 0.4803\n",
            "Epoch [3342/20000], Training Loss: 0.4766\n",
            "Epoch [3343/20000], Training Loss: 0.5009\n",
            "Epoch [3344/20000], Training Loss: 0.4976\n",
            "Epoch [3345/20000], Training Loss: 0.4924\n",
            "Epoch [3346/20000], Training Loss: 0.4600\n",
            "Epoch [3347/20000], Training Loss: 0.4857\n",
            "Epoch [3348/20000], Training Loss: 0.5252\n",
            "Epoch [3349/20000], Training Loss: 0.4401\n",
            "Epoch [3350/20000], Training Loss: 0.5009\n",
            "Epoch [3351/20000], Training Loss: 0.4790\n",
            "Epoch [3352/20000], Training Loss: 0.5464\n",
            "Epoch [3353/20000], Training Loss: 0.5317\n",
            "Epoch [3354/20000], Training Loss: 0.5311\n",
            "Epoch [3355/20000], Training Loss: 0.4652\n",
            "Epoch [3356/20000], Training Loss: 0.5160\n",
            "Epoch [3357/20000], Training Loss: 0.4754\n",
            "Epoch [3358/20000], Training Loss: 0.4432\n",
            "Epoch [3359/20000], Training Loss: 0.4738\n",
            "Epoch [3360/20000], Training Loss: 0.4371\n",
            "Epoch [3361/20000], Training Loss: 0.4909\n",
            "Epoch [3362/20000], Training Loss: 0.5184\n",
            "Epoch [3363/20000], Training Loss: 0.5342\n",
            "Epoch [3364/20000], Training Loss: 0.4567\n",
            "Epoch [3365/20000], Training Loss: 0.4670\n",
            "Epoch [3366/20000], Training Loss: 0.5039\n",
            "Epoch [3367/20000], Training Loss: 0.4695\n",
            "Epoch [3368/20000], Training Loss: 0.4800\n",
            "Epoch [3369/20000], Training Loss: 0.4725\n",
            "Epoch [3370/20000], Training Loss: 0.4726\n",
            "Epoch [3371/20000], Training Loss: 0.5062\n",
            "Epoch [3372/20000], Training Loss: 0.4757\n",
            "Epoch [3373/20000], Training Loss: 0.4634\n",
            "Epoch [3374/20000], Training Loss: 0.4941\n",
            "Epoch [3375/20000], Training Loss: 0.5001\n",
            "Epoch [3376/20000], Training Loss: 0.4815\n",
            "Epoch [3377/20000], Training Loss: 0.4935\n",
            "Epoch [3378/20000], Training Loss: 0.4586\n",
            "Epoch [3379/20000], Training Loss: 0.5080\n",
            "Epoch [3380/20000], Training Loss: 0.5040\n",
            "Epoch [3381/20000], Training Loss: 0.4384\n",
            "Epoch [3382/20000], Training Loss: 0.4830\n",
            "Epoch [3383/20000], Training Loss: 0.4760\n",
            "Epoch [3384/20000], Training Loss: 0.5222\n",
            "Epoch [3385/20000], Training Loss: 0.4511\n",
            "Epoch [3386/20000], Training Loss: 0.4633\n",
            "Epoch [3387/20000], Training Loss: 0.4696\n",
            "Epoch [3388/20000], Training Loss: 0.4687\n",
            "Epoch [3389/20000], Training Loss: 0.4686\n",
            "Epoch [3390/20000], Training Loss: 0.5036\n",
            "Epoch [3391/20000], Training Loss: 0.4787\n",
            "Epoch [3392/20000], Training Loss: 0.4761\n",
            "Epoch [3393/20000], Training Loss: 0.4301\n",
            "Epoch [3394/20000], Training Loss: 0.4526\n",
            "Epoch [3395/20000], Training Loss: 0.5284\n",
            "Epoch [3396/20000], Training Loss: 0.4640\n",
            "Epoch [3397/20000], Training Loss: 0.4747\n",
            "Epoch [3398/20000], Training Loss: 0.4639\n",
            "Epoch [3399/20000], Training Loss: 0.4739\n",
            "Epoch [3400/20000], Training Loss: 0.4853\n",
            "Epoch [3401/20000], Training Loss: 0.4535\n",
            "Epoch [3402/20000], Training Loss: 0.5309\n",
            "Epoch [3403/20000], Training Loss: 0.4825\n",
            "Epoch [3404/20000], Training Loss: 0.5153\n",
            "Epoch [3405/20000], Training Loss: 0.4729\n",
            "Epoch [3406/20000], Training Loss: 0.4507\n",
            "Epoch [3407/20000], Training Loss: 0.4729\n",
            "Epoch [3408/20000], Training Loss: 0.4892\n",
            "Epoch [3409/20000], Training Loss: 0.4298\n",
            "Epoch [3410/20000], Training Loss: 0.4546\n",
            "Epoch [3411/20000], Training Loss: 0.5051\n",
            "Epoch [3412/20000], Training Loss: 0.4786\n",
            "Epoch [3413/20000], Training Loss: 0.4731\n",
            "Epoch [3414/20000], Training Loss: 0.5607\n",
            "Epoch [3415/20000], Training Loss: 0.4888\n",
            "Epoch [3416/20000], Training Loss: 0.4731\n",
            "Epoch [3417/20000], Training Loss: 0.4975\n",
            "Epoch [3418/20000], Training Loss: 0.5043\n",
            "Epoch [3419/20000], Training Loss: 0.4450\n",
            "Epoch [3420/20000], Training Loss: 0.4675\n",
            "Epoch [3421/20000], Training Loss: 0.4750\n",
            "Epoch [3422/20000], Training Loss: 0.4544\n",
            "Epoch [3423/20000], Training Loss: 0.4676\n",
            "Epoch [3424/20000], Training Loss: 0.4565\n",
            "Epoch [3425/20000], Training Loss: 0.4904\n",
            "Epoch [3426/20000], Training Loss: 0.4481\n",
            "Epoch [3427/20000], Training Loss: 0.4761\n",
            "Epoch [3428/20000], Training Loss: 0.4822\n",
            "Epoch [3429/20000], Training Loss: 0.4576\n",
            "Epoch [3430/20000], Training Loss: 0.4892\n",
            "Epoch [3431/20000], Training Loss: 0.4851\n",
            "Epoch [3432/20000], Training Loss: 0.4559\n",
            "Epoch [3433/20000], Training Loss: 0.4891\n",
            "Epoch [3434/20000], Training Loss: 0.4958\n",
            "Epoch [3435/20000], Training Loss: 0.5344\n",
            "Epoch [3436/20000], Training Loss: 0.5441\n",
            "Epoch [3437/20000], Training Loss: 0.4979\n",
            "Epoch [3438/20000], Training Loss: 0.4958\n",
            "Epoch [3439/20000], Training Loss: 0.4803\n",
            "Epoch [3440/20000], Training Loss: 0.4833\n",
            "Epoch [3441/20000], Training Loss: 0.5222\n",
            "Epoch [3442/20000], Training Loss: 0.5119\n",
            "Epoch [3443/20000], Training Loss: 0.4942\n",
            "Epoch [3444/20000], Training Loss: 0.4755\n",
            "Epoch [3445/20000], Training Loss: 0.4826\n",
            "Epoch [3446/20000], Training Loss: 0.4920\n",
            "Epoch [3447/20000], Training Loss: 0.4604\n",
            "Epoch [3448/20000], Training Loss: 0.4707\n",
            "Epoch [3449/20000], Training Loss: 0.5412\n",
            "Epoch [3450/20000], Training Loss: 0.4836\n",
            "Epoch [3451/20000], Training Loss: 0.4480\n",
            "Epoch [3452/20000], Training Loss: 0.5077\n",
            "Epoch [3453/20000], Training Loss: 0.4605\n",
            "Epoch [3454/20000], Training Loss: 0.5434\n",
            "Epoch [3455/20000], Training Loss: 0.4917\n",
            "Epoch [3456/20000], Training Loss: 0.5229\n",
            "Epoch [3457/20000], Training Loss: 0.4447\n",
            "Epoch [3458/20000], Training Loss: 0.4626\n",
            "Epoch [3459/20000], Training Loss: 0.5115\n",
            "Epoch [3460/20000], Training Loss: 0.5593\n",
            "Epoch [3461/20000], Training Loss: 0.4930\n",
            "Epoch [3462/20000], Training Loss: 0.5058\n",
            "Epoch [3463/20000], Training Loss: 0.4703\n",
            "Epoch [3464/20000], Training Loss: 0.4812\n",
            "Epoch [3465/20000], Training Loss: 0.4358\n",
            "Epoch [3466/20000], Training Loss: 0.5164\n",
            "Epoch [3467/20000], Training Loss: 0.4855\n",
            "Epoch [3468/20000], Training Loss: 0.4476\n",
            "Epoch [3469/20000], Training Loss: 0.4551\n",
            "Epoch [3470/20000], Training Loss: 0.5303\n",
            "Epoch [3471/20000], Training Loss: 0.4886\n",
            "Epoch [3472/20000], Training Loss: 0.5178\n",
            "Epoch [3473/20000], Training Loss: 0.4640\n",
            "Epoch [3474/20000], Training Loss: 0.4594\n",
            "Epoch [3475/20000], Training Loss: 0.4585\n",
            "Epoch [3476/20000], Training Loss: 0.4790\n",
            "Epoch [3477/20000], Training Loss: 0.4776\n",
            "Epoch [3478/20000], Training Loss: 0.4932\n",
            "Epoch [3479/20000], Training Loss: 0.4866\n",
            "Epoch [3480/20000], Training Loss: 0.4662\n",
            "Epoch [3481/20000], Training Loss: 0.5343\n",
            "Epoch [3482/20000], Training Loss: 0.5046\n",
            "Epoch [3483/20000], Training Loss: 0.4948\n",
            "Epoch [3484/20000], Training Loss: 0.5246\n",
            "Epoch [3485/20000], Training Loss: 0.4994\n",
            "Epoch [3486/20000], Training Loss: 0.4773\n",
            "Epoch [3487/20000], Training Loss: 0.4928\n",
            "Epoch [3488/20000], Training Loss: 0.4997\n",
            "Epoch [3489/20000], Training Loss: 0.4749\n",
            "Epoch [3490/20000], Training Loss: 0.4659\n",
            "Epoch [3491/20000], Training Loss: 0.5248\n",
            "Epoch [3492/20000], Training Loss: 0.5146\n",
            "Epoch [3493/20000], Training Loss: 0.4970\n",
            "Epoch [3494/20000], Training Loss: 0.4989\n",
            "Epoch [3495/20000], Training Loss: 0.4181\n",
            "Epoch [3496/20000], Training Loss: 0.5102\n",
            "Epoch [3497/20000], Training Loss: 0.4727\n",
            "Epoch [3498/20000], Training Loss: 0.4911\n",
            "Epoch [3499/20000], Training Loss: 0.4794\n",
            "Epoch [3500/20000], Training Loss: 0.4472\n",
            "Epoch [3501/20000], Training Loss: 0.4869\n",
            "Epoch [3502/20000], Training Loss: 0.4986\n",
            "Epoch [3503/20000], Training Loss: 0.4819\n",
            "Epoch [3504/20000], Training Loss: 0.4644\n",
            "Epoch [3505/20000], Training Loss: 0.5064\n",
            "Epoch [3506/20000], Training Loss: 0.4745\n",
            "Epoch [3507/20000], Training Loss: 0.4949\n",
            "Epoch [3508/20000], Training Loss: 0.4754\n",
            "Epoch [3509/20000], Training Loss: 0.4369\n",
            "Epoch [3510/20000], Training Loss: 0.5303\n",
            "Epoch [3511/20000], Training Loss: 0.5293\n",
            "Epoch [3512/20000], Training Loss: 0.4756\n",
            "Epoch [3513/20000], Training Loss: 0.4667\n",
            "Epoch [3514/20000], Training Loss: 0.4787\n",
            "Epoch [3515/20000], Training Loss: 0.4898\n",
            "Epoch [3516/20000], Training Loss: 0.4603\n",
            "Epoch [3517/20000], Training Loss: 0.5025\n",
            "Epoch [3518/20000], Training Loss: 0.4599\n",
            "Epoch [3519/20000], Training Loss: 0.4389\n",
            "Epoch [3520/20000], Training Loss: 0.4747\n",
            "Epoch [3521/20000], Training Loss: 0.4321\n",
            "Epoch [3522/20000], Training Loss: 0.4941\n",
            "Epoch [3523/20000], Training Loss: 0.4643\n",
            "Epoch [3524/20000], Training Loss: 0.4995\n",
            "Epoch [3525/20000], Training Loss: 0.4751\n",
            "Epoch [3526/20000], Training Loss: 0.4797\n",
            "Epoch [3527/20000], Training Loss: 0.4701\n",
            "Epoch [3528/20000], Training Loss: 0.4808\n",
            "Epoch [3529/20000], Training Loss: 0.4738\n",
            "Epoch [3530/20000], Training Loss: 0.4671\n",
            "Epoch [3531/20000], Training Loss: 0.4726\n",
            "Epoch [3532/20000], Training Loss: 0.4994\n",
            "Epoch [3533/20000], Training Loss: 0.4932\n",
            "Epoch [3534/20000], Training Loss: 0.5160\n",
            "Epoch [3535/20000], Training Loss: 0.5159\n",
            "Epoch [3536/20000], Training Loss: 0.5009\n",
            "Epoch [3537/20000], Training Loss: 0.5340\n",
            "Epoch [3538/20000], Training Loss: 0.4820\n",
            "Epoch [3539/20000], Training Loss: 0.5417\n",
            "Epoch [3540/20000], Training Loss: 0.5120\n",
            "Epoch [3541/20000], Training Loss: 0.4739\n",
            "Epoch [3542/20000], Training Loss: 0.5168\n",
            "Epoch [3543/20000], Training Loss: 0.4752\n",
            "Epoch [3544/20000], Training Loss: 0.4813\n",
            "Epoch [3545/20000], Training Loss: 0.4907\n",
            "Epoch [3546/20000], Training Loss: 0.4731\n",
            "Epoch [3547/20000], Training Loss: 0.5234\n",
            "Epoch [3548/20000], Training Loss: 0.4673\n",
            "Epoch [3549/20000], Training Loss: 0.4898\n",
            "Epoch [3550/20000], Training Loss: 0.5135\n",
            "Epoch [3551/20000], Training Loss: 0.4728\n",
            "Epoch [3552/20000], Training Loss: 0.5083\n",
            "Epoch [3553/20000], Training Loss: 0.5071\n",
            "Epoch [3554/20000], Training Loss: 0.5320\n",
            "Epoch [3555/20000], Training Loss: 0.4845\n",
            "Epoch [3556/20000], Training Loss: 0.4930\n",
            "Epoch [3557/20000], Training Loss: 0.5095\n",
            "Epoch [3558/20000], Training Loss: 0.4989\n",
            "Epoch [3559/20000], Training Loss: 0.4443\n",
            "Epoch [3560/20000], Training Loss: 0.4339\n",
            "Epoch [3561/20000], Training Loss: 0.4491\n",
            "Epoch [3562/20000], Training Loss: 0.4358\n",
            "Epoch [3563/20000], Training Loss: 0.4487\n",
            "Epoch [3564/20000], Training Loss: 0.4836\n",
            "Epoch [3565/20000], Training Loss: 0.4544\n",
            "Epoch [3566/20000], Training Loss: 0.4786\n",
            "Epoch [3567/20000], Training Loss: 0.5111\n",
            "Epoch [3568/20000], Training Loss: 0.4774\n",
            "Epoch [3569/20000], Training Loss: 0.5127\n",
            "Epoch [3570/20000], Training Loss: 0.4747\n",
            "Epoch [3571/20000], Training Loss: 0.4826\n",
            "Epoch [3572/20000], Training Loss: 0.5094\n",
            "Epoch [3573/20000], Training Loss: 0.4755\n",
            "Epoch [3574/20000], Training Loss: 0.4741\n",
            "Epoch [3575/20000], Training Loss: 0.5510\n",
            "Epoch [3576/20000], Training Loss: 0.4886\n",
            "Epoch [3577/20000], Training Loss: 0.5002\n",
            "Epoch [3578/20000], Training Loss: 0.5478\n",
            "Epoch [3579/20000], Training Loss: 0.5173\n",
            "Epoch [3580/20000], Training Loss: 0.4976\n",
            "Epoch [3581/20000], Training Loss: 0.5400\n",
            "Epoch [3582/20000], Training Loss: 0.4785\n",
            "Epoch [3583/20000], Training Loss: 0.4547\n",
            "Epoch [3584/20000], Training Loss: 0.4479\n",
            "Epoch [3585/20000], Training Loss: 0.4877\n",
            "Epoch [3586/20000], Training Loss: 0.4927\n",
            "Epoch [3587/20000], Training Loss: 0.4444\n",
            "Epoch [3588/20000], Training Loss: 0.5050\n",
            "Epoch [3589/20000], Training Loss: 0.4713\n",
            "Epoch [3590/20000], Training Loss: 0.5118\n",
            "Epoch [3591/20000], Training Loss: 0.4933\n",
            "Epoch [3592/20000], Training Loss: 0.4418\n",
            "Epoch [3593/20000], Training Loss: 0.5006\n",
            "Epoch [3594/20000], Training Loss: 0.4914\n",
            "Epoch [3595/20000], Training Loss: 0.5039\n",
            "Epoch [3596/20000], Training Loss: 0.5199\n",
            "Epoch [3597/20000], Training Loss: 0.4872\n",
            "Epoch [3598/20000], Training Loss: 0.4855\n",
            "Epoch [3599/20000], Training Loss: 0.4773\n",
            "Epoch [3600/20000], Training Loss: 0.5047\n",
            "Epoch [3601/20000], Training Loss: 0.5179\n",
            "Epoch [3602/20000], Training Loss: 0.4627\n",
            "Epoch [3603/20000], Training Loss: 0.4546\n",
            "Epoch [3604/20000], Training Loss: 0.5190\n",
            "Epoch [3605/20000], Training Loss: 0.5005\n",
            "Epoch [3606/20000], Training Loss: 0.4796\n",
            "Epoch [3607/20000], Training Loss: 0.5308\n",
            "Epoch [3608/20000], Training Loss: 0.4783\n",
            "Epoch [3609/20000], Training Loss: 0.4729\n",
            "Epoch [3610/20000], Training Loss: 0.4797\n",
            "Epoch [3611/20000], Training Loss: 0.4807\n",
            "Epoch [3612/20000], Training Loss: 0.4587\n",
            "Epoch [3613/20000], Training Loss: 0.4870\n",
            "Epoch [3614/20000], Training Loss: 0.5076\n",
            "Epoch [3615/20000], Training Loss: 0.5214\n",
            "Epoch [3616/20000], Training Loss: 0.4960\n",
            "Epoch [3617/20000], Training Loss: 0.4814\n",
            "Epoch [3618/20000], Training Loss: 0.4819\n",
            "Epoch [3619/20000], Training Loss: 0.4793\n",
            "Epoch [3620/20000], Training Loss: 0.5002\n",
            "Epoch [3621/20000], Training Loss: 0.5097\n",
            "Epoch [3622/20000], Training Loss: 0.5171\n",
            "Epoch [3623/20000], Training Loss: 0.5299\n",
            "Epoch [3624/20000], Training Loss: 0.4527\n",
            "Epoch [3625/20000], Training Loss: 0.4540\n",
            "Epoch [3626/20000], Training Loss: 0.5171\n",
            "Epoch [3627/20000], Training Loss: 0.5046\n",
            "Epoch [3628/20000], Training Loss: 0.4900\n",
            "Epoch [3629/20000], Training Loss: 0.4388\n",
            "Epoch [3630/20000], Training Loss: 0.4402\n",
            "Epoch [3631/20000], Training Loss: 0.4613\n",
            "Epoch [3632/20000], Training Loss: 0.5041\n",
            "Epoch [3633/20000], Training Loss: 0.4936\n",
            "Epoch [3634/20000], Training Loss: 0.5365\n",
            "Epoch [3635/20000], Training Loss: 0.4918\n",
            "Epoch [3636/20000], Training Loss: 0.5235\n",
            "Epoch [3637/20000], Training Loss: 0.5094\n",
            "Epoch [3638/20000], Training Loss: 0.4949\n",
            "Epoch [3639/20000], Training Loss: 0.4569\n",
            "Epoch [3640/20000], Training Loss: 0.4587\n",
            "Epoch [3641/20000], Training Loss: 0.4736\n",
            "Epoch [3642/20000], Training Loss: 0.5201\n",
            "Epoch [3643/20000], Training Loss: 0.5186\n",
            "Epoch [3644/20000], Training Loss: 0.4944\n",
            "Epoch [3645/20000], Training Loss: 0.5061\n",
            "Epoch [3646/20000], Training Loss: 0.4859\n",
            "Epoch [3647/20000], Training Loss: 0.4768\n",
            "Epoch [3648/20000], Training Loss: 0.4563\n",
            "Epoch [3649/20000], Training Loss: 0.4817\n",
            "Epoch [3650/20000], Training Loss: 0.5181\n",
            "Epoch [3651/20000], Training Loss: 0.5477\n",
            "Epoch [3652/20000], Training Loss: 0.5269\n",
            "Epoch [3653/20000], Training Loss: 0.5122\n",
            "Epoch [3654/20000], Training Loss: 0.5360\n",
            "Epoch [3655/20000], Training Loss: 0.4804\n",
            "Epoch [3656/20000], Training Loss: 0.4692\n",
            "Epoch [3657/20000], Training Loss: 0.5512\n",
            "Epoch [3658/20000], Training Loss: 0.4576\n",
            "Epoch [3659/20000], Training Loss: 0.5481\n",
            "Epoch [3660/20000], Training Loss: 0.4800\n",
            "Epoch [3661/20000], Training Loss: 0.5317\n",
            "Epoch [3662/20000], Training Loss: 0.4647\n",
            "Epoch [3663/20000], Training Loss: 0.5240\n",
            "Epoch [3664/20000], Training Loss: 0.4564\n",
            "Epoch [3665/20000], Training Loss: 0.4843\n",
            "Epoch [3666/20000], Training Loss: 0.4892\n",
            "Epoch [3667/20000], Training Loss: 0.4743\n",
            "Epoch [3668/20000], Training Loss: 0.5526\n",
            "Epoch [3669/20000], Training Loss: 0.5050\n",
            "Epoch [3670/20000], Training Loss: 0.4600\n",
            "Epoch [3671/20000], Training Loss: 0.4881\n",
            "Epoch [3672/20000], Training Loss: 0.5130\n",
            "Epoch [3673/20000], Training Loss: 0.4657\n",
            "Epoch [3674/20000], Training Loss: 0.5299\n",
            "Epoch [3675/20000], Training Loss: 0.5170\n",
            "Epoch [3676/20000], Training Loss: 0.4956\n",
            "Epoch [3677/20000], Training Loss: 0.5256\n",
            "Epoch [3678/20000], Training Loss: 0.4910\n",
            "Epoch [3679/20000], Training Loss: 0.4931\n",
            "Epoch [3680/20000], Training Loss: 0.4710\n",
            "Epoch [3681/20000], Training Loss: 0.4658\n",
            "Epoch [3682/20000], Training Loss: 0.4826\n",
            "Epoch [3683/20000], Training Loss: 0.4960\n",
            "Epoch [3684/20000], Training Loss: 0.5320\n",
            "Epoch [3685/20000], Training Loss: 0.4668\n",
            "Epoch [3686/20000], Training Loss: 0.4641\n",
            "Epoch [3687/20000], Training Loss: 0.4845\n",
            "Epoch [3688/20000], Training Loss: 0.4814\n",
            "Epoch [3689/20000], Training Loss: 0.4864\n",
            "Epoch [3690/20000], Training Loss: 0.5493\n",
            "Epoch [3691/20000], Training Loss: 0.4764\n",
            "Epoch [3692/20000], Training Loss: 0.5165\n",
            "Epoch [3693/20000], Training Loss: 0.5243\n",
            "Epoch [3694/20000], Training Loss: 0.4817\n",
            "Epoch [3695/20000], Training Loss: 0.4685\n",
            "Epoch [3696/20000], Training Loss: 0.4450\n",
            "Epoch [3697/20000], Training Loss: 0.4935\n",
            "Epoch [3698/20000], Training Loss: 0.4831\n",
            "Epoch [3699/20000], Training Loss: 0.5137\n",
            "Epoch [3700/20000], Training Loss: 0.5062\n",
            "Epoch [3701/20000], Training Loss: 0.5258\n",
            "Epoch [3702/20000], Training Loss: 0.5249\n",
            "Epoch [3703/20000], Training Loss: 0.5067\n",
            "Epoch [3704/20000], Training Loss: 0.4916\n",
            "Epoch [3705/20000], Training Loss: 0.4813\n",
            "Epoch [3706/20000], Training Loss: 0.4867\n",
            "Epoch [3707/20000], Training Loss: 0.4930\n",
            "Epoch [3708/20000], Training Loss: 0.4507\n",
            "Epoch [3709/20000], Training Loss: 0.5193\n",
            "Epoch [3710/20000], Training Loss: 0.4784\n",
            "Epoch [3711/20000], Training Loss: 0.5245\n",
            "Epoch [3712/20000], Training Loss: 0.4691\n",
            "Epoch [3713/20000], Training Loss: 0.5267\n",
            "Epoch [3714/20000], Training Loss: 0.4943\n",
            "Epoch [3715/20000], Training Loss: 0.4991\n",
            "Epoch [3716/20000], Training Loss: 0.5492\n",
            "Epoch [3717/20000], Training Loss: 0.4904\n",
            "Epoch [3718/20000], Training Loss: 0.4685\n",
            "Epoch [3719/20000], Training Loss: 0.4783\n",
            "Epoch [3720/20000], Training Loss: 0.5029\n",
            "Epoch [3721/20000], Training Loss: 0.4873\n",
            "Epoch [3722/20000], Training Loss: 0.4713\n",
            "Epoch [3723/20000], Training Loss: 0.4349\n",
            "Epoch [3724/20000], Training Loss: 0.4550\n",
            "Epoch [3725/20000], Training Loss: 0.4818\n",
            "Epoch [3726/20000], Training Loss: 0.4655\n",
            "Epoch [3727/20000], Training Loss: 0.4773\n",
            "Epoch [3728/20000], Training Loss: 0.5269\n",
            "Epoch [3729/20000], Training Loss: 0.4948\n",
            "Epoch [3730/20000], Training Loss: 0.4892\n",
            "Epoch [3731/20000], Training Loss: 0.4707\n",
            "Epoch [3732/20000], Training Loss: 0.4712\n",
            "Epoch [3733/20000], Training Loss: 0.5342\n",
            "Epoch [3734/20000], Training Loss: 0.5003\n",
            "Epoch [3735/20000], Training Loss: 0.4855\n",
            "Epoch [3736/20000], Training Loss: 0.4483\n",
            "Epoch [3737/20000], Training Loss: 0.4977\n",
            "Epoch [3738/20000], Training Loss: 0.5402\n",
            "Epoch [3739/20000], Training Loss: 0.5200\n",
            "Epoch [3740/20000], Training Loss: 0.4993\n",
            "Epoch [3741/20000], Training Loss: 0.4822\n",
            "Epoch [3742/20000], Training Loss: 0.4916\n",
            "Epoch [3743/20000], Training Loss: 0.4718\n",
            "Epoch [3744/20000], Training Loss: 0.5189\n",
            "Epoch [3745/20000], Training Loss: 0.4712\n",
            "Epoch [3746/20000], Training Loss: 0.4613\n",
            "Epoch [3747/20000], Training Loss: 0.5011\n",
            "Epoch [3748/20000], Training Loss: 0.4767\n",
            "Epoch [3749/20000], Training Loss: 0.4611\n",
            "Epoch [3750/20000], Training Loss: 0.4668\n",
            "Epoch [3751/20000], Training Loss: 0.5053\n",
            "Epoch [3752/20000], Training Loss: 0.5449\n",
            "Epoch [3753/20000], Training Loss: 0.4876\n",
            "Epoch [3754/20000], Training Loss: 0.5099\n",
            "Epoch [3755/20000], Training Loss: 0.4565\n",
            "Epoch [3756/20000], Training Loss: 0.4669\n",
            "Epoch [3757/20000], Training Loss: 0.4601\n",
            "Epoch [3758/20000], Training Loss: 0.4976\n",
            "Epoch [3759/20000], Training Loss: 0.4443\n",
            "Epoch [3760/20000], Training Loss: 0.5139\n",
            "Epoch [3761/20000], Training Loss: 0.4612\n",
            "Epoch [3762/20000], Training Loss: 0.5364\n",
            "Epoch [3763/20000], Training Loss: 0.5060\n",
            "Epoch [3764/20000], Training Loss: 0.4775\n",
            "Epoch [3765/20000], Training Loss: 0.4947\n",
            "Epoch [3766/20000], Training Loss: 0.4927\n",
            "Epoch [3767/20000], Training Loss: 0.4614\n",
            "Epoch [3768/20000], Training Loss: 0.4835\n",
            "Epoch [3769/20000], Training Loss: 0.4686\n",
            "Epoch [3770/20000], Training Loss: 0.5265\n",
            "Epoch [3771/20000], Training Loss: 0.4915\n",
            "Epoch [3772/20000], Training Loss: 0.4516\n",
            "Epoch [3773/20000], Training Loss: 0.4993\n",
            "Epoch [3774/20000], Training Loss: 0.4367\n",
            "Epoch [3775/20000], Training Loss: 0.4652\n",
            "Epoch [3776/20000], Training Loss: 0.4964\n",
            "Epoch [3777/20000], Training Loss: 0.4386\n",
            "Epoch [3778/20000], Training Loss: 0.4677\n",
            "Epoch [3779/20000], Training Loss: 0.5003\n",
            "Epoch [3780/20000], Training Loss: 0.5010\n",
            "Epoch [3781/20000], Training Loss: 0.4896\n",
            "Epoch [3782/20000], Training Loss: 0.5220\n",
            "Epoch [3783/20000], Training Loss: 0.4902\n",
            "Epoch [3784/20000], Training Loss: 0.4958\n",
            "Epoch [3785/20000], Training Loss: 0.4983\n",
            "Epoch [3786/20000], Training Loss: 0.4706\n",
            "Epoch [3787/20000], Training Loss: 0.5022\n",
            "Epoch [3788/20000], Training Loss: 0.4684\n",
            "Epoch [3789/20000], Training Loss: 0.4683\n",
            "Epoch [3790/20000], Training Loss: 0.5062\n",
            "Epoch [3791/20000], Training Loss: 0.4834\n",
            "Epoch [3792/20000], Training Loss: 0.4875\n",
            "Epoch [3793/20000], Training Loss: 0.4849\n",
            "Epoch [3794/20000], Training Loss: 0.4528\n",
            "Epoch [3795/20000], Training Loss: 0.5186\n",
            "Epoch [3796/20000], Training Loss: 0.4549\n",
            "Epoch [3797/20000], Training Loss: 0.4993\n",
            "Epoch [3798/20000], Training Loss: 0.4777\n",
            "Epoch [3799/20000], Training Loss: 0.4945\n",
            "Epoch [3800/20000], Training Loss: 0.4901\n",
            "Epoch [3801/20000], Training Loss: 0.4619\n",
            "Epoch [3802/20000], Training Loss: 0.4645\n",
            "Epoch [3803/20000], Training Loss: 0.4787\n",
            "Epoch [3804/20000], Training Loss: 0.4610\n",
            "Epoch [3805/20000], Training Loss: 0.4587\n",
            "Epoch [3806/20000], Training Loss: 0.5010\n",
            "Epoch [3807/20000], Training Loss: 0.4881\n",
            "Epoch [3808/20000], Training Loss: 0.4969\n",
            "Epoch [3809/20000], Training Loss: 0.4659\n",
            "Epoch [3810/20000], Training Loss: 0.4840\n",
            "Epoch [3811/20000], Training Loss: 0.4523\n",
            "Epoch [3812/20000], Training Loss: 0.4779\n",
            "Epoch [3813/20000], Training Loss: 0.5042\n",
            "Epoch [3814/20000], Training Loss: 0.4751\n",
            "Epoch [3815/20000], Training Loss: 0.4575\n",
            "Epoch [3816/20000], Training Loss: 0.4789\n",
            "Epoch [3817/20000], Training Loss: 0.4886\n",
            "Epoch [3818/20000], Training Loss: 0.5303\n",
            "Epoch [3819/20000], Training Loss: 0.4717\n",
            "Epoch [3820/20000], Training Loss: 0.5409\n",
            "Epoch [3821/20000], Training Loss: 0.5386\n",
            "Epoch [3822/20000], Training Loss: 0.5384\n",
            "Epoch [3823/20000], Training Loss: 0.5151\n",
            "Epoch [3824/20000], Training Loss: 0.4767\n",
            "Epoch [3825/20000], Training Loss: 0.4805\n",
            "Epoch [3826/20000], Training Loss: 0.4930\n",
            "Epoch [3827/20000], Training Loss: 0.4642\n",
            "Epoch [3828/20000], Training Loss: 0.4801\n",
            "Epoch [3829/20000], Training Loss: 0.4768\n",
            "Epoch [3830/20000], Training Loss: 0.5071\n",
            "Epoch [3831/20000], Training Loss: 0.4965\n",
            "Epoch [3832/20000], Training Loss: 0.4476\n",
            "Epoch [3833/20000], Training Loss: 0.5007\n",
            "Epoch [3834/20000], Training Loss: 0.4916\n",
            "Epoch [3835/20000], Training Loss: 0.5114\n",
            "Epoch [3836/20000], Training Loss: 0.4912\n",
            "Epoch [3837/20000], Training Loss: 0.4714\n",
            "Epoch [3838/20000], Training Loss: 0.5114\n",
            "Epoch [3839/20000], Training Loss: 0.4702\n",
            "Epoch [3840/20000], Training Loss: 0.4765\n",
            "Epoch [3841/20000], Training Loss: 0.4864\n",
            "Epoch [3842/20000], Training Loss: 0.4885\n",
            "Epoch [3843/20000], Training Loss: 0.5069\n",
            "Epoch [3844/20000], Training Loss: 0.4656\n",
            "Epoch [3845/20000], Training Loss: 0.5334\n",
            "Epoch [3846/20000], Training Loss: 0.4381\n",
            "Epoch [3847/20000], Training Loss: 0.4753\n",
            "Epoch [3848/20000], Training Loss: 0.4843\n",
            "Epoch [3849/20000], Training Loss: 0.4977\n",
            "Epoch [3850/20000], Training Loss: 0.4414\n",
            "Epoch [3851/20000], Training Loss: 0.4652\n",
            "Epoch [3852/20000], Training Loss: 0.4804\n",
            "Epoch [3853/20000], Training Loss: 0.4884\n",
            "Epoch [3854/20000], Training Loss: 0.4455\n",
            "Epoch [3855/20000], Training Loss: 0.4610\n",
            "Epoch [3856/20000], Training Loss: 0.5014\n",
            "Epoch [3857/20000], Training Loss: 0.4733\n",
            "Epoch [3858/20000], Training Loss: 0.4498\n",
            "Epoch [3859/20000], Training Loss: 0.5161\n",
            "Epoch [3860/20000], Training Loss: 0.4592\n",
            "Epoch [3861/20000], Training Loss: 0.4940\n",
            "Epoch [3862/20000], Training Loss: 0.4564\n",
            "Epoch [3863/20000], Training Loss: 0.5006\n",
            "Epoch [3864/20000], Training Loss: 0.4643\n",
            "Epoch [3865/20000], Training Loss: 0.4928\n",
            "Epoch [3866/20000], Training Loss: 0.4780\n",
            "Epoch [3867/20000], Training Loss: 0.4640\n",
            "Epoch [3868/20000], Training Loss: 0.4959\n",
            "Epoch [3869/20000], Training Loss: 0.4999\n",
            "Epoch [3870/20000], Training Loss: 0.5081\n",
            "Epoch [3871/20000], Training Loss: 0.4979\n",
            "Epoch [3872/20000], Training Loss: 0.4673\n",
            "Epoch [3873/20000], Training Loss: 0.4727\n",
            "Epoch [3874/20000], Training Loss: 0.5079\n",
            "Epoch [3875/20000], Training Loss: 0.5365\n",
            "Epoch [3876/20000], Training Loss: 0.4331\n",
            "Epoch [3877/20000], Training Loss: 0.4810\n",
            "Epoch [3878/20000], Training Loss: 0.4935\n",
            "Epoch [3879/20000], Training Loss: 0.4653\n",
            "Epoch [3880/20000], Training Loss: 0.5044\n",
            "Epoch [3881/20000], Training Loss: 0.4585\n",
            "Epoch [3882/20000], Training Loss: 0.4969\n",
            "Epoch [3883/20000], Training Loss: 0.4733\n",
            "Epoch [3884/20000], Training Loss: 0.4853\n",
            "Epoch [3885/20000], Training Loss: 0.4945\n",
            "Epoch [3886/20000], Training Loss: 0.4887\n",
            "Epoch [3887/20000], Training Loss: 0.4835\n",
            "Epoch [3888/20000], Training Loss: 0.4940\n",
            "Epoch [3889/20000], Training Loss: 0.4947\n",
            "Epoch [3890/20000], Training Loss: 0.4928\n",
            "Epoch [3891/20000], Training Loss: 0.4461\n",
            "Epoch [3892/20000], Training Loss: 0.4420\n",
            "Epoch [3893/20000], Training Loss: 0.4935\n",
            "Epoch [3894/20000], Training Loss: 0.5004\n",
            "Epoch [3895/20000], Training Loss: 0.5281\n",
            "Epoch [3896/20000], Training Loss: 0.4840\n",
            "Epoch [3897/20000], Training Loss: 0.5007\n",
            "Epoch [3898/20000], Training Loss: 0.4560\n",
            "Epoch [3899/20000], Training Loss: 0.5138\n",
            "Epoch [3900/20000], Training Loss: 0.4848\n",
            "Epoch [3901/20000], Training Loss: 0.4675\n",
            "Epoch [3902/20000], Training Loss: 0.4993\n",
            "Epoch [3903/20000], Training Loss: 0.5243\n",
            "Epoch [3904/20000], Training Loss: 0.5227\n",
            "Epoch [3905/20000], Training Loss: 0.4953\n",
            "Epoch [3906/20000], Training Loss: 0.4460\n",
            "Epoch [3907/20000], Training Loss: 0.4847\n",
            "Epoch [3908/20000], Training Loss: 0.4659\n",
            "Epoch [3909/20000], Training Loss: 0.5215\n",
            "Epoch [3910/20000], Training Loss: 0.5104\n",
            "Epoch [3911/20000], Training Loss: 0.4959\n",
            "Epoch [3912/20000], Training Loss: 0.5202\n",
            "Epoch [3913/20000], Training Loss: 0.5152\n",
            "Epoch [3914/20000], Training Loss: 0.5078\n",
            "Epoch [3915/20000], Training Loss: 0.4509\n",
            "Epoch [3916/20000], Training Loss: 0.5185\n",
            "Epoch [3917/20000], Training Loss: 0.4864\n",
            "Epoch [3918/20000], Training Loss: 0.4636\n",
            "Epoch [3919/20000], Training Loss: 0.4875\n",
            "Epoch [3920/20000], Training Loss: 0.4922\n",
            "Epoch [3921/20000], Training Loss: 0.5117\n",
            "Epoch [3922/20000], Training Loss: 0.4237\n",
            "Epoch [3923/20000], Training Loss: 0.4711\n",
            "Epoch [3924/20000], Training Loss: 0.5064\n",
            "Epoch [3925/20000], Training Loss: 0.5404\n",
            "Epoch [3926/20000], Training Loss: 0.4660\n",
            "Epoch [3927/20000], Training Loss: 0.4583\n",
            "Epoch [3928/20000], Training Loss: 0.5157\n",
            "Epoch [3929/20000], Training Loss: 0.4974\n",
            "Epoch [3930/20000], Training Loss: 0.4982\n",
            "Epoch [3931/20000], Training Loss: 0.4693\n",
            "Epoch [3932/20000], Training Loss: 0.4855\n",
            "Epoch [3933/20000], Training Loss: 0.4680\n",
            "Epoch [3934/20000], Training Loss: 0.4627\n",
            "Epoch [3935/20000], Training Loss: 0.4477\n",
            "Epoch [3936/20000], Training Loss: 0.4649\n",
            "Epoch [3937/20000], Training Loss: 0.4568\n",
            "Epoch [3938/20000], Training Loss: 0.4558\n",
            "Epoch [3939/20000], Training Loss: 0.4876\n",
            "Epoch [3940/20000], Training Loss: 0.5069\n",
            "Epoch [3941/20000], Training Loss: 0.4640\n",
            "Epoch [3942/20000], Training Loss: 0.4704\n",
            "Epoch [3943/20000], Training Loss: 0.4779\n",
            "Epoch [3944/20000], Training Loss: 0.4762\n",
            "Epoch [3945/20000], Training Loss: 0.4826\n",
            "Epoch [3946/20000], Training Loss: 0.4707\n",
            "Epoch [3947/20000], Training Loss: 0.4590\n",
            "Epoch [3948/20000], Training Loss: 0.4803\n",
            "Epoch [3949/20000], Training Loss: 0.4636\n",
            "Epoch [3950/20000], Training Loss: 0.4842\n",
            "Epoch [3951/20000], Training Loss: 0.4675\n",
            "Epoch [3952/20000], Training Loss: 0.5340\n",
            "Epoch [3953/20000], Training Loss: 0.4930\n",
            "Epoch [3954/20000], Training Loss: 0.5332\n",
            "Epoch [3955/20000], Training Loss: 0.5076\n",
            "Epoch [3956/20000], Training Loss: 0.4727\n",
            "Epoch [3957/20000], Training Loss: 0.4506\n",
            "Epoch [3958/20000], Training Loss: 0.4473\n",
            "Epoch [3959/20000], Training Loss: 0.5077\n",
            "Epoch [3960/20000], Training Loss: 0.5076\n",
            "Epoch [3961/20000], Training Loss: 0.4556\n",
            "Epoch [3962/20000], Training Loss: 0.5114\n",
            "Epoch [3963/20000], Training Loss: 0.4519\n",
            "Epoch [3964/20000], Training Loss: 0.4666\n",
            "Epoch [3965/20000], Training Loss: 0.4546\n",
            "Epoch [3966/20000], Training Loss: 0.4845\n",
            "Epoch [3967/20000], Training Loss: 0.4968\n",
            "Epoch [3968/20000], Training Loss: 0.4711\n",
            "Epoch [3969/20000], Training Loss: 0.4441\n",
            "Epoch [3970/20000], Training Loss: 0.4806\n",
            "Epoch [3971/20000], Training Loss: 0.4599\n",
            "Epoch [3972/20000], Training Loss: 0.4979\n",
            "Epoch [3973/20000], Training Loss: 0.4824\n",
            "Epoch [3974/20000], Training Loss: 0.4813\n",
            "Epoch [3975/20000], Training Loss: 0.4764\n",
            "Epoch [3976/20000], Training Loss: 0.4598\n",
            "Epoch [3977/20000], Training Loss: 0.4890\n",
            "Epoch [3978/20000], Training Loss: 0.4289\n",
            "Epoch [3979/20000], Training Loss: 0.4935\n",
            "Epoch [3980/20000], Training Loss: 0.5229\n",
            "Epoch [3981/20000], Training Loss: 0.5083\n",
            "Epoch [3982/20000], Training Loss: 0.4914\n",
            "Epoch [3983/20000], Training Loss: 0.4773\n",
            "Epoch [3984/20000], Training Loss: 0.4846\n",
            "Epoch [3985/20000], Training Loss: 0.5022\n",
            "Epoch [3986/20000], Training Loss: 0.4971\n",
            "Epoch [3987/20000], Training Loss: 0.4886\n",
            "Epoch [3988/20000], Training Loss: 0.4720\n",
            "Epoch [3989/20000], Training Loss: 0.4695\n",
            "Epoch [3990/20000], Training Loss: 0.5033\n",
            "Epoch [3991/20000], Training Loss: 0.5108\n",
            "Epoch [3992/20000], Training Loss: 0.5135\n",
            "Epoch [3993/20000], Training Loss: 0.5145\n",
            "Epoch [3994/20000], Training Loss: 0.4805\n",
            "Epoch [3995/20000], Training Loss: 0.4750\n",
            "Epoch [3996/20000], Training Loss: 0.4755\n",
            "Epoch [3997/20000], Training Loss: 0.4765\n",
            "Epoch [3998/20000], Training Loss: 0.4985\n",
            "Epoch [3999/20000], Training Loss: 0.4710\n",
            "Epoch [4000/20000], Training Loss: 0.4974\n",
            "Epoch [4001/20000], Training Loss: 0.4903\n",
            "Epoch [4002/20000], Training Loss: 0.4884\n",
            "Epoch [4003/20000], Training Loss: 0.4555\n",
            "Epoch [4004/20000], Training Loss: 0.5344\n",
            "Epoch [4005/20000], Training Loss: 0.4419\n",
            "Epoch [4006/20000], Training Loss: 0.4894\n",
            "Epoch [4007/20000], Training Loss: 0.5000\n",
            "Epoch [4008/20000], Training Loss: 0.4737\n",
            "Epoch [4009/20000], Training Loss: 0.4745\n",
            "Epoch [4010/20000], Training Loss: 0.4825\n",
            "Epoch [4011/20000], Training Loss: 0.5485\n",
            "Epoch [4012/20000], Training Loss: 0.5188\n",
            "Epoch [4013/20000], Training Loss: 0.4782\n",
            "Epoch [4014/20000], Training Loss: 0.5050\n",
            "Epoch [4015/20000], Training Loss: 0.4744\n",
            "Epoch [4016/20000], Training Loss: 0.5053\n",
            "Epoch [4017/20000], Training Loss: 0.5080\n",
            "Epoch [4018/20000], Training Loss: 0.4644\n",
            "Epoch [4019/20000], Training Loss: 0.4782\n",
            "Epoch [4020/20000], Training Loss: 0.4311\n",
            "Epoch [4021/20000], Training Loss: 0.4761\n",
            "Epoch [4022/20000], Training Loss: 0.4748\n",
            "Epoch [4023/20000], Training Loss: 0.4975\n",
            "Epoch [4024/20000], Training Loss: 0.4659\n",
            "Epoch [4025/20000], Training Loss: 0.4526\n",
            "Epoch [4026/20000], Training Loss: 0.4922\n",
            "Epoch [4027/20000], Training Loss: 0.4473\n",
            "Epoch [4028/20000], Training Loss: 0.4742\n",
            "Epoch [4029/20000], Training Loss: 0.5128\n",
            "Epoch [4030/20000], Training Loss: 0.5083\n",
            "Epoch [4031/20000], Training Loss: 0.5574\n",
            "Epoch [4032/20000], Training Loss: 0.4862\n",
            "Epoch [4033/20000], Training Loss: 0.4901\n",
            "Epoch [4034/20000], Training Loss: 0.5069\n",
            "Epoch [4035/20000], Training Loss: 0.5025\n",
            "Epoch [4036/20000], Training Loss: 0.5097\n",
            "Epoch [4037/20000], Training Loss: 0.5374\n",
            "Epoch [4038/20000], Training Loss: 0.5067\n",
            "Epoch [4039/20000], Training Loss: 0.4795\n",
            "Epoch [4040/20000], Training Loss: 0.4927\n",
            "Epoch [4041/20000], Training Loss: 0.5005\n",
            "Epoch [4042/20000], Training Loss: 0.4885\n",
            "Epoch [4043/20000], Training Loss: 0.5039\n",
            "Epoch [4044/20000], Training Loss: 0.4689\n",
            "Epoch [4045/20000], Training Loss: 0.4699\n",
            "Epoch [4046/20000], Training Loss: 0.4802\n",
            "Epoch [4047/20000], Training Loss: 0.4511\n",
            "Epoch [4048/20000], Training Loss: 0.4998\n",
            "Epoch [4049/20000], Training Loss: 0.4484\n",
            "Epoch [4050/20000], Training Loss: 0.5054\n",
            "Epoch [4051/20000], Training Loss: 0.5389\n",
            "Epoch [4052/20000], Training Loss: 0.4864\n",
            "Epoch [4053/20000], Training Loss: 0.5031\n",
            "Epoch [4054/20000], Training Loss: 0.5080\n",
            "Epoch [4055/20000], Training Loss: 0.5156\n",
            "Epoch [4056/20000], Training Loss: 0.4447\n",
            "Epoch [4057/20000], Training Loss: 0.4885\n",
            "Epoch [4058/20000], Training Loss: 0.4936\n",
            "Epoch [4059/20000], Training Loss: 0.5471\n",
            "Epoch [4060/20000], Training Loss: 0.4445\n",
            "Epoch [4061/20000], Training Loss: 0.4588\n",
            "Epoch [4062/20000], Training Loss: 0.5081\n",
            "Epoch [4063/20000], Training Loss: 0.4963\n",
            "Epoch [4064/20000], Training Loss: 0.4783\n",
            "Epoch [4065/20000], Training Loss: 0.5110\n",
            "Epoch [4066/20000], Training Loss: 0.4492\n",
            "Epoch [4067/20000], Training Loss: 0.4928\n",
            "Epoch [4068/20000], Training Loss: 0.4861\n",
            "Epoch [4069/20000], Training Loss: 0.4888\n",
            "Epoch [4070/20000], Training Loss: 0.4723\n",
            "Epoch [4071/20000], Training Loss: 0.4992\n",
            "Epoch [4072/20000], Training Loss: 0.5188\n",
            "Epoch [4073/20000], Training Loss: 0.4772\n",
            "Epoch [4074/20000], Training Loss: 0.4963\n",
            "Epoch [4075/20000], Training Loss: 0.4875\n",
            "Epoch [4076/20000], Training Loss: 0.4801\n",
            "Epoch [4077/20000], Training Loss: 0.4932\n",
            "Epoch [4078/20000], Training Loss: 0.4906\n",
            "Epoch [4079/20000], Training Loss: 0.4745\n",
            "Epoch [4080/20000], Training Loss: 0.4817\n",
            "Epoch [4081/20000], Training Loss: 0.5053\n",
            "Epoch [4082/20000], Training Loss: 0.4817\n",
            "Epoch [4083/20000], Training Loss: 0.5169\n",
            "Epoch [4084/20000], Training Loss: 0.4595\n",
            "Epoch [4085/20000], Training Loss: 0.5050\n",
            "Epoch [4086/20000], Training Loss: 0.5222\n",
            "Epoch [4087/20000], Training Loss: 0.4911\n",
            "Epoch [4088/20000], Training Loss: 0.4654\n",
            "Epoch [4089/20000], Training Loss: 0.4788\n",
            "Epoch [4090/20000], Training Loss: 0.4778\n",
            "Epoch [4091/20000], Training Loss: 0.4569\n",
            "Epoch [4092/20000], Training Loss: 0.4738\n",
            "Epoch [4093/20000], Training Loss: 0.4972\n",
            "Epoch [4094/20000], Training Loss: 0.4626\n",
            "Epoch [4095/20000], Training Loss: 0.4879\n",
            "Epoch [4096/20000], Training Loss: 0.4513\n",
            "Epoch [4097/20000], Training Loss: 0.4686\n",
            "Epoch [4098/20000], Training Loss: 0.5063\n",
            "Epoch [4099/20000], Training Loss: 0.4739\n",
            "Epoch [4100/20000], Training Loss: 0.5435\n",
            "Epoch [4101/20000], Training Loss: 0.4983\n",
            "Epoch [4102/20000], Training Loss: 0.4882\n",
            "Epoch [4103/20000], Training Loss: 0.4877\n",
            "Epoch [4104/20000], Training Loss: 0.4930\n",
            "Epoch [4105/20000], Training Loss: 0.5045\n",
            "Epoch [4106/20000], Training Loss: 0.5154\n",
            "Epoch [4107/20000], Training Loss: 0.5098\n",
            "Epoch [4108/20000], Training Loss: 0.4642\n",
            "Epoch [4109/20000], Training Loss: 0.4887\n",
            "Epoch [4110/20000], Training Loss: 0.5101\n",
            "Epoch [4111/20000], Training Loss: 0.4523\n",
            "Epoch [4112/20000], Training Loss: 0.5201\n",
            "Epoch [4113/20000], Training Loss: 0.4538\n",
            "Epoch [4114/20000], Training Loss: 0.4668\n",
            "Epoch [4115/20000], Training Loss: 0.4811\n",
            "Epoch [4116/20000], Training Loss: 0.5292\n",
            "Epoch [4117/20000], Training Loss: 0.4782\n",
            "Epoch [4118/20000], Training Loss: 0.4282\n",
            "Epoch [4119/20000], Training Loss: 0.4945\n",
            "Epoch [4120/20000], Training Loss: 0.4598\n",
            "Epoch [4121/20000], Training Loss: 0.5055\n",
            "Epoch [4122/20000], Training Loss: 0.5036\n",
            "Epoch [4123/20000], Training Loss: 0.5254\n",
            "Epoch [4124/20000], Training Loss: 0.4901\n",
            "Epoch [4125/20000], Training Loss: 0.4663\n",
            "Epoch [4126/20000], Training Loss: 0.4783\n",
            "Epoch [4127/20000], Training Loss: 0.4875\n",
            "Epoch [4128/20000], Training Loss: 0.4998\n",
            "Epoch [4129/20000], Training Loss: 0.5311\n",
            "Epoch [4130/20000], Training Loss: 0.4625\n",
            "Epoch [4131/20000], Training Loss: 0.4908\n",
            "Epoch [4132/20000], Training Loss: 0.4871\n",
            "Epoch [4133/20000], Training Loss: 0.4969\n",
            "Epoch [4134/20000], Training Loss: 0.5201\n",
            "Epoch [4135/20000], Training Loss: 0.4930\n",
            "Epoch [4136/20000], Training Loss: 0.4771\n",
            "Epoch [4137/20000], Training Loss: 0.5017\n",
            "Epoch [4138/20000], Training Loss: 0.4680\n",
            "Epoch [4139/20000], Training Loss: 0.4867\n",
            "Epoch [4140/20000], Training Loss: 0.5242\n",
            "Epoch [4141/20000], Training Loss: 0.4584\n",
            "Epoch [4142/20000], Training Loss: 0.4649\n",
            "Epoch [4143/20000], Training Loss: 0.4575\n",
            "Epoch [4144/20000], Training Loss: 0.4773\n",
            "Epoch [4145/20000], Training Loss: 0.4512\n",
            "Epoch [4146/20000], Training Loss: 0.4472\n",
            "Epoch [4147/20000], Training Loss: 0.4913\n",
            "Epoch [4148/20000], Training Loss: 0.5114\n",
            "Epoch [4149/20000], Training Loss: 0.5038\n",
            "Epoch [4150/20000], Training Loss: 0.5035\n",
            "Epoch [4151/20000], Training Loss: 0.4932\n",
            "Epoch [4152/20000], Training Loss: 0.4438\n",
            "Epoch [4153/20000], Training Loss: 0.5003\n",
            "Epoch [4154/20000], Training Loss: 0.5136\n",
            "Epoch [4155/20000], Training Loss: 0.5402\n",
            "Epoch [4156/20000], Training Loss: 0.5183\n",
            "Epoch [4157/20000], Training Loss: 0.4530\n",
            "Epoch [4158/20000], Training Loss: 0.4596\n",
            "Epoch [4159/20000], Training Loss: 0.5476\n",
            "Epoch [4160/20000], Training Loss: 0.4828\n",
            "Epoch [4161/20000], Training Loss: 0.4884\n",
            "Epoch [4162/20000], Training Loss: 0.5096\n",
            "Epoch [4163/20000], Training Loss: 0.4825\n",
            "Epoch [4164/20000], Training Loss: 0.4925\n",
            "Epoch [4165/20000], Training Loss: 0.5212\n",
            "Epoch [4166/20000], Training Loss: 0.4764\n",
            "Epoch [4167/20000], Training Loss: 0.5488\n",
            "Epoch [4168/20000], Training Loss: 0.4935\n",
            "Epoch [4169/20000], Training Loss: 0.4923\n",
            "Epoch [4170/20000], Training Loss: 0.5225\n",
            "Epoch [4171/20000], Training Loss: 0.4564\n",
            "Epoch [4172/20000], Training Loss: 0.4769\n",
            "Epoch [4173/20000], Training Loss: 0.4695\n",
            "Epoch [4174/20000], Training Loss: 0.4730\n",
            "Epoch [4175/20000], Training Loss: 0.4814\n",
            "Epoch [4176/20000], Training Loss: 0.5509\n",
            "Epoch [4177/20000], Training Loss: 0.4741\n",
            "Epoch [4178/20000], Training Loss: 0.4999\n",
            "Epoch [4179/20000], Training Loss: 0.5035\n",
            "Epoch [4180/20000], Training Loss: 0.4537\n",
            "Epoch [4181/20000], Training Loss: 0.5306\n",
            "Epoch [4182/20000], Training Loss: 0.5209\n",
            "Epoch [4183/20000], Training Loss: 0.5085\n",
            "Epoch [4184/20000], Training Loss: 0.5226\n",
            "Epoch [4185/20000], Training Loss: 0.5230\n",
            "Epoch [4186/20000], Training Loss: 0.4914\n",
            "Epoch [4187/20000], Training Loss: 0.5011\n",
            "Epoch [4188/20000], Training Loss: 0.4876\n",
            "Epoch [4189/20000], Training Loss: 0.4695\n",
            "Epoch [4190/20000], Training Loss: 0.4664\n",
            "Epoch [4191/20000], Training Loss: 0.5013\n",
            "Epoch [4192/20000], Training Loss: 0.4835\n",
            "Epoch [4193/20000], Training Loss: 0.4781\n",
            "Epoch [4194/20000], Training Loss: 0.4825\n",
            "Epoch [4195/20000], Training Loss: 0.5205\n",
            "Epoch [4196/20000], Training Loss: 0.4501\n",
            "Epoch [4197/20000], Training Loss: 0.4891\n",
            "Epoch [4198/20000], Training Loss: 0.4834\n",
            "Epoch [4199/20000], Training Loss: 0.5113\n",
            "Epoch [4200/20000], Training Loss: 0.4860\n",
            "Epoch [4201/20000], Training Loss: 0.4729\n",
            "Epoch [4202/20000], Training Loss: 0.4716\n",
            "Epoch [4203/20000], Training Loss: 0.4398\n",
            "Epoch [4204/20000], Training Loss: 0.4814\n",
            "Epoch [4205/20000], Training Loss: 0.5105\n",
            "Epoch [4206/20000], Training Loss: 0.4907\n",
            "Epoch [4207/20000], Training Loss: 0.4910\n",
            "Epoch [4208/20000], Training Loss: 0.4649\n",
            "Epoch [4209/20000], Training Loss: 0.4543\n",
            "Epoch [4210/20000], Training Loss: 0.4590\n",
            "Epoch [4211/20000], Training Loss: 0.4732\n",
            "Epoch [4212/20000], Training Loss: 0.4975\n",
            "Epoch [4213/20000], Training Loss: 0.4735\n",
            "Epoch [4214/20000], Training Loss: 0.4570\n",
            "Epoch [4215/20000], Training Loss: 0.4913\n",
            "Epoch [4216/20000], Training Loss: 0.5258\n",
            "Epoch [4217/20000], Training Loss: 0.4795\n",
            "Epoch [4218/20000], Training Loss: 0.4525\n",
            "Epoch [4219/20000], Training Loss: 0.5132\n",
            "Epoch [4220/20000], Training Loss: 0.4668\n",
            "Epoch [4221/20000], Training Loss: 0.4838\n",
            "Epoch [4222/20000], Training Loss: 0.4698\n",
            "Epoch [4223/20000], Training Loss: 0.4839\n",
            "Epoch [4224/20000], Training Loss: 0.4437\n",
            "Epoch [4225/20000], Training Loss: 0.4768\n",
            "Epoch [4226/20000], Training Loss: 0.5079\n",
            "Epoch [4227/20000], Training Loss: 0.4629\n",
            "Epoch [4228/20000], Training Loss: 0.5194\n",
            "Epoch [4229/20000], Training Loss: 0.4713\n",
            "Epoch [4230/20000], Training Loss: 0.4950\n",
            "Epoch [4231/20000], Training Loss: 0.4738\n",
            "Epoch [4232/20000], Training Loss: 0.5098\n",
            "Epoch [4233/20000], Training Loss: 0.5450\n",
            "Epoch [4234/20000], Training Loss: 0.4856\n",
            "Epoch [4235/20000], Training Loss: 0.5191\n",
            "Epoch [4236/20000], Training Loss: 0.4692\n",
            "Epoch [4237/20000], Training Loss: 0.4819\n",
            "Epoch [4238/20000], Training Loss: 0.4841\n",
            "Epoch [4239/20000], Training Loss: 0.5083\n",
            "Epoch [4240/20000], Training Loss: 0.4941\n",
            "Epoch [4241/20000], Training Loss: 0.4767\n",
            "Epoch [4242/20000], Training Loss: 0.4460\n",
            "Epoch [4243/20000], Training Loss: 0.4780\n",
            "Epoch [4244/20000], Training Loss: 0.5003\n",
            "Epoch [4245/20000], Training Loss: 0.5113\n",
            "Epoch [4246/20000], Training Loss: 0.4710\n",
            "Epoch [4247/20000], Training Loss: 0.4642\n",
            "Epoch [4248/20000], Training Loss: 0.5315\n",
            "Epoch [4249/20000], Training Loss: 0.5245\n",
            "Epoch [4250/20000], Training Loss: 0.5402\n",
            "Epoch [4251/20000], Training Loss: 0.5276\n",
            "Epoch [4252/20000], Training Loss: 0.5158\n",
            "Epoch [4253/20000], Training Loss: 0.4640\n",
            "Epoch [4254/20000], Training Loss: 0.4625\n",
            "Epoch [4255/20000], Training Loss: 0.5327\n",
            "Epoch [4256/20000], Training Loss: 0.5030\n",
            "Epoch [4257/20000], Training Loss: 0.4580\n",
            "Epoch [4258/20000], Training Loss: 0.4828\n",
            "Epoch [4259/20000], Training Loss: 0.4668\n",
            "Epoch [4260/20000], Training Loss: 0.5014\n",
            "Epoch [4261/20000], Training Loss: 0.4469\n",
            "Epoch [4262/20000], Training Loss: 0.5020\n",
            "Epoch [4263/20000], Training Loss: 0.4714\n",
            "Epoch [4264/20000], Training Loss: 0.4411\n",
            "Epoch [4265/20000], Training Loss: 0.4840\n",
            "Epoch [4266/20000], Training Loss: 0.4816\n",
            "Epoch [4267/20000], Training Loss: 0.4652\n",
            "Epoch [4268/20000], Training Loss: 0.4804\n",
            "Epoch [4269/20000], Training Loss: 0.4812\n",
            "Epoch [4270/20000], Training Loss: 0.4969\n",
            "Epoch [4271/20000], Training Loss: 0.4855\n",
            "Epoch [4272/20000], Training Loss: 0.4850\n",
            "Epoch [4273/20000], Training Loss: 0.4443\n",
            "Epoch [4274/20000], Training Loss: 0.5379\n",
            "Epoch [4275/20000], Training Loss: 0.4707\n",
            "Epoch [4276/20000], Training Loss: 0.4708\n",
            "Epoch [4277/20000], Training Loss: 0.5133\n",
            "Epoch [4278/20000], Training Loss: 0.4604\n",
            "Epoch [4279/20000], Training Loss: 0.4681\n",
            "Epoch [4280/20000], Training Loss: 0.5290\n",
            "Epoch [4281/20000], Training Loss: 0.4874\n",
            "Epoch [4282/20000], Training Loss: 0.5156\n",
            "Epoch [4283/20000], Training Loss: 0.4999\n",
            "Epoch [4284/20000], Training Loss: 0.4502\n",
            "Epoch [4285/20000], Training Loss: 0.5062\n",
            "Epoch [4286/20000], Training Loss: 0.5095\n",
            "Epoch [4287/20000], Training Loss: 0.4845\n",
            "Epoch [4288/20000], Training Loss: 0.4563\n",
            "Epoch [4289/20000], Training Loss: 0.4823\n",
            "Epoch [4290/20000], Training Loss: 0.4522\n",
            "Epoch [4291/20000], Training Loss: 0.4589\n",
            "Epoch [4292/20000], Training Loss: 0.5118\n",
            "Epoch [4293/20000], Training Loss: 0.5104\n",
            "Epoch [4294/20000], Training Loss: 0.5046\n",
            "Epoch [4295/20000], Training Loss: 0.4704\n",
            "Epoch [4296/20000], Training Loss: 0.4972\n",
            "Epoch [4297/20000], Training Loss: 0.4646\n",
            "Epoch [4298/20000], Training Loss: 0.4891\n",
            "Epoch [4299/20000], Training Loss: 0.4878\n",
            "Epoch [4300/20000], Training Loss: 0.5099\n",
            "Epoch [4301/20000], Training Loss: 0.4550\n",
            "Epoch [4302/20000], Training Loss: 0.5207\n",
            "Epoch [4303/20000], Training Loss: 0.4788\n",
            "Epoch [4304/20000], Training Loss: 0.4836\n",
            "Epoch [4305/20000], Training Loss: 0.4851\n",
            "Epoch [4306/20000], Training Loss: 0.5296\n",
            "Epoch [4307/20000], Training Loss: 0.4439\n",
            "Epoch [4308/20000], Training Loss: 0.4919\n",
            "Epoch [4309/20000], Training Loss: 0.5036\n",
            "Epoch [4310/20000], Training Loss: 0.4683\n",
            "Epoch [4311/20000], Training Loss: 0.5175\n",
            "Epoch [4312/20000], Training Loss: 0.4908\n",
            "Epoch [4313/20000], Training Loss: 0.4975\n",
            "Epoch [4314/20000], Training Loss: 0.5300\n",
            "Epoch [4315/20000], Training Loss: 0.4939\n",
            "Epoch [4316/20000], Training Loss: 0.4705\n",
            "Epoch [4317/20000], Training Loss: 0.4572\n",
            "Epoch [4318/20000], Training Loss: 0.5414\n",
            "Epoch [4319/20000], Training Loss: 0.5218\n",
            "Epoch [4320/20000], Training Loss: 0.4944\n",
            "Epoch [4321/20000], Training Loss: 0.4807\n",
            "Epoch [4322/20000], Training Loss: 0.5168\n",
            "Epoch [4323/20000], Training Loss: 0.4852\n",
            "Epoch [4324/20000], Training Loss: 0.4916\n",
            "Epoch [4325/20000], Training Loss: 0.4973\n",
            "Epoch [4326/20000], Training Loss: 0.4893\n",
            "Epoch [4327/20000], Training Loss: 0.4784\n",
            "Epoch [4328/20000], Training Loss: 0.5094\n",
            "Epoch [4329/20000], Training Loss: 0.5091\n",
            "Epoch [4330/20000], Training Loss: 0.4838\n",
            "Epoch [4331/20000], Training Loss: 0.4727\n",
            "Epoch [4332/20000], Training Loss: 0.4545\n",
            "Epoch [4333/20000], Training Loss: 0.4632\n",
            "Epoch [4334/20000], Training Loss: 0.4976\n",
            "Epoch [4335/20000], Training Loss: 0.5020\n",
            "Epoch [4336/20000], Training Loss: 0.5025\n",
            "Epoch [4337/20000], Training Loss: 0.5109\n",
            "Epoch [4338/20000], Training Loss: 0.4674\n",
            "Epoch [4339/20000], Training Loss: 0.4592\n",
            "Epoch [4340/20000], Training Loss: 0.4950\n",
            "Epoch [4341/20000], Training Loss: 0.5192\n",
            "Epoch [4342/20000], Training Loss: 0.4687\n",
            "Epoch [4343/20000], Training Loss: 0.4811\n",
            "Epoch [4344/20000], Training Loss: 0.5179\n",
            "Epoch [4345/20000], Training Loss: 0.5132\n",
            "Epoch [4346/20000], Training Loss: 0.4529\n",
            "Epoch [4347/20000], Training Loss: 0.4888\n",
            "Epoch [4348/20000], Training Loss: 0.4893\n",
            "Epoch [4349/20000], Training Loss: 0.4351\n",
            "Epoch [4350/20000], Training Loss: 0.5105\n",
            "Epoch [4351/20000], Training Loss: 0.5081\n",
            "Epoch [4352/20000], Training Loss: 0.5066\n",
            "Epoch [4353/20000], Training Loss: 0.5006\n",
            "Epoch [4354/20000], Training Loss: 0.4830\n",
            "Epoch [4355/20000], Training Loss: 0.4868\n",
            "Epoch [4356/20000], Training Loss: 0.4488\n",
            "Epoch [4357/20000], Training Loss: 0.4839\n",
            "Epoch [4358/20000], Training Loss: 0.5143\n",
            "Epoch [4359/20000], Training Loss: 0.4673\n",
            "Epoch [4360/20000], Training Loss: 0.5026\n",
            "Epoch [4361/20000], Training Loss: 0.4967\n",
            "Epoch [4362/20000], Training Loss: 0.5222\n",
            "Epoch [4363/20000], Training Loss: 0.4852\n",
            "Epoch [4364/20000], Training Loss: 0.4705\n",
            "Epoch [4365/20000], Training Loss: 0.4919\n",
            "Epoch [4366/20000], Training Loss: 0.5172\n",
            "Epoch [4367/20000], Training Loss: 0.4944\n",
            "Epoch [4368/20000], Training Loss: 0.4923\n",
            "Epoch [4369/20000], Training Loss: 0.4662\n",
            "Epoch [4370/20000], Training Loss: 0.4672\n",
            "Epoch [4371/20000], Training Loss: 0.5088\n",
            "Epoch [4372/20000], Training Loss: 0.4814\n",
            "Epoch [4373/20000], Training Loss: 0.4493\n",
            "Epoch [4374/20000], Training Loss: 0.4449\n",
            "Epoch [4375/20000], Training Loss: 0.4810\n",
            "Epoch [4376/20000], Training Loss: 0.4734\n",
            "Epoch [4377/20000], Training Loss: 0.4883\n",
            "Epoch [4378/20000], Training Loss: 0.4878\n",
            "Epoch [4379/20000], Training Loss: 0.4293\n",
            "Epoch [4380/20000], Training Loss: 0.5198\n",
            "Epoch [4381/20000], Training Loss: 0.4846\n",
            "Epoch [4382/20000], Training Loss: 0.4509\n",
            "Epoch [4383/20000], Training Loss: 0.4664\n",
            "Epoch [4384/20000], Training Loss: 0.4951\n",
            "Epoch [4385/20000], Training Loss: 0.4871\n",
            "Epoch [4386/20000], Training Loss: 0.5222\n",
            "Epoch [4387/20000], Training Loss: 0.4899\n",
            "Epoch [4388/20000], Training Loss: 0.4966\n",
            "Epoch [4389/20000], Training Loss: 0.4912\n",
            "Epoch [4390/20000], Training Loss: 0.4711\n",
            "Epoch [4391/20000], Training Loss: 0.5252\n",
            "Epoch [4392/20000], Training Loss: 0.4850\n",
            "Epoch [4393/20000], Training Loss: 0.5087\n",
            "Epoch [4394/20000], Training Loss: 0.4850\n",
            "Epoch [4395/20000], Training Loss: 0.4917\n",
            "Epoch [4396/20000], Training Loss: 0.4797\n",
            "Epoch [4397/20000], Training Loss: 0.4537\n",
            "Epoch [4398/20000], Training Loss: 0.4572\n",
            "Epoch [4399/20000], Training Loss: 0.5099\n",
            "Epoch [4400/20000], Training Loss: 0.4835\n",
            "Epoch [4401/20000], Training Loss: 0.4957\n",
            "Epoch [4402/20000], Training Loss: 0.5403\n",
            "Epoch [4403/20000], Training Loss: 0.4523\n",
            "Epoch [4404/20000], Training Loss: 0.4574\n",
            "Epoch [4405/20000], Training Loss: 0.5107\n",
            "Epoch [4406/20000], Training Loss: 0.4853\n",
            "Epoch [4407/20000], Training Loss: 0.5326\n",
            "Epoch [4408/20000], Training Loss: 0.5460\n",
            "Epoch [4409/20000], Training Loss: 0.4939\n",
            "Epoch [4410/20000], Training Loss: 0.5128\n",
            "Epoch [4411/20000], Training Loss: 0.4575\n",
            "Epoch [4412/20000], Training Loss: 0.4578\n",
            "Epoch [4413/20000], Training Loss: 0.5000\n",
            "Epoch [4414/20000], Training Loss: 0.4704\n",
            "Epoch [4415/20000], Training Loss: 0.4901\n",
            "Epoch [4416/20000], Training Loss: 0.4384\n",
            "Epoch [4417/20000], Training Loss: 0.4595\n",
            "Epoch [4418/20000], Training Loss: 0.4938\n",
            "Epoch [4419/20000], Training Loss: 0.4471\n",
            "Epoch [4420/20000], Training Loss: 0.4984\n",
            "Epoch [4421/20000], Training Loss: 0.4696\n",
            "Epoch [4422/20000], Training Loss: 0.4502\n",
            "Epoch [4423/20000], Training Loss: 0.4647\n",
            "Epoch [4424/20000], Training Loss: 0.5245\n",
            "Epoch [4425/20000], Training Loss: 0.5428\n",
            "Epoch [4426/20000], Training Loss: 0.4706\n",
            "Epoch [4427/20000], Training Loss: 0.5260\n",
            "Epoch [4428/20000], Training Loss: 0.4904\n",
            "Epoch [4429/20000], Training Loss: 0.5015\n",
            "Epoch [4430/20000], Training Loss: 0.5051\n",
            "Epoch [4431/20000], Training Loss: 0.4519\n",
            "Epoch [4432/20000], Training Loss: 0.5413\n",
            "Epoch [4433/20000], Training Loss: 0.5360\n",
            "Epoch [4434/20000], Training Loss: 0.5064\n",
            "Epoch [4435/20000], Training Loss: 0.4598\n",
            "Epoch [4436/20000], Training Loss: 0.4943\n",
            "Epoch [4437/20000], Training Loss: 0.5054\n",
            "Epoch [4438/20000], Training Loss: 0.4417\n",
            "Epoch [4439/20000], Training Loss: 0.4551\n",
            "Epoch [4440/20000], Training Loss: 0.4680\n",
            "Epoch [4441/20000], Training Loss: 0.5536\n",
            "Epoch [4442/20000], Training Loss: 0.4917\n",
            "Epoch [4443/20000], Training Loss: 0.5120\n",
            "Epoch [4444/20000], Training Loss: 0.4649\n",
            "Epoch [4445/20000], Training Loss: 0.5131\n",
            "Epoch [4446/20000], Training Loss: 0.5110\n",
            "Epoch [4447/20000], Training Loss: 0.4715\n",
            "Epoch [4448/20000], Training Loss: 0.5116\n",
            "Epoch [4449/20000], Training Loss: 0.4935\n",
            "Epoch [4450/20000], Training Loss: 0.4922\n",
            "Epoch [4451/20000], Training Loss: 0.5136\n",
            "Epoch [4452/20000], Training Loss: 0.4744\n",
            "Epoch [4453/20000], Training Loss: 0.5521\n",
            "Epoch [4454/20000], Training Loss: 0.5098\n",
            "Epoch [4455/20000], Training Loss: 0.4978\n",
            "Epoch [4456/20000], Training Loss: 0.4769\n",
            "Epoch [4457/20000], Training Loss: 0.5415\n",
            "Epoch [4458/20000], Training Loss: 0.5179\n",
            "Epoch [4459/20000], Training Loss: 0.4997\n",
            "Epoch [4460/20000], Training Loss: 0.5212\n",
            "Epoch [4461/20000], Training Loss: 0.4700\n",
            "Epoch [4462/20000], Training Loss: 0.5069\n",
            "Epoch [4463/20000], Training Loss: 0.5046\n",
            "Epoch [4464/20000], Training Loss: 0.5149\n",
            "Epoch [4465/20000], Training Loss: 0.4637\n",
            "Epoch [4466/20000], Training Loss: 0.5550\n",
            "Epoch [4467/20000], Training Loss: 0.5008\n",
            "Epoch [4468/20000], Training Loss: 0.4603\n",
            "Epoch [4469/20000], Training Loss: 0.4793\n",
            "Epoch [4470/20000], Training Loss: 0.4645\n",
            "Epoch [4471/20000], Training Loss: 0.4729\n",
            "Epoch [4472/20000], Training Loss: 0.5043\n",
            "Epoch [4473/20000], Training Loss: 0.4402\n",
            "Epoch [4474/20000], Training Loss: 0.5054\n",
            "Epoch [4475/20000], Training Loss: 0.4870\n",
            "Epoch [4476/20000], Training Loss: 0.4533\n",
            "Epoch [4477/20000], Training Loss: 0.4741\n",
            "Epoch [4478/20000], Training Loss: 0.4814\n",
            "Epoch [4479/20000], Training Loss: 0.4929\n",
            "Epoch [4480/20000], Training Loss: 0.4698\n",
            "Epoch [4481/20000], Training Loss: 0.5059\n",
            "Epoch [4482/20000], Training Loss: 0.5267\n",
            "Epoch [4483/20000], Training Loss: 0.4732\n",
            "Epoch [4484/20000], Training Loss: 0.5215\n",
            "Epoch [4485/20000], Training Loss: 0.4608\n",
            "Epoch [4486/20000], Training Loss: 0.5012\n",
            "Epoch [4487/20000], Training Loss: 0.5340\n",
            "Epoch [4488/20000], Training Loss: 0.5083\n",
            "Epoch [4489/20000], Training Loss: 0.4948\n",
            "Epoch [4490/20000], Training Loss: 0.5153\n",
            "Epoch [4491/20000], Training Loss: 0.5215\n",
            "Epoch [4492/20000], Training Loss: 0.4899\n",
            "Epoch [4493/20000], Training Loss: 0.4908\n",
            "Epoch [4494/20000], Training Loss: 0.5215\n",
            "Epoch [4495/20000], Training Loss: 0.4601\n",
            "Epoch [4496/20000], Training Loss: 0.4857\n",
            "Epoch [4497/20000], Training Loss: 0.5080\n",
            "Epoch [4498/20000], Training Loss: 0.5396\n",
            "Epoch [4499/20000], Training Loss: 0.5120\n",
            "Epoch [4500/20000], Training Loss: 0.4451\n",
            "Epoch [4501/20000], Training Loss: 0.4382\n",
            "Epoch [4502/20000], Training Loss: 0.4831\n",
            "Epoch [4503/20000], Training Loss: 0.4963\n",
            "Epoch [4504/20000], Training Loss: 0.4952\n",
            "Epoch [4505/20000], Training Loss: 0.4789\n",
            "Epoch [4506/20000], Training Loss: 0.4607\n",
            "Epoch [4507/20000], Training Loss: 0.4924\n",
            "Epoch [4508/20000], Training Loss: 0.5043\n",
            "Epoch [4509/20000], Training Loss: 0.4843\n",
            "Epoch [4510/20000], Training Loss: 0.4576\n",
            "Epoch [4511/20000], Training Loss: 0.5025\n",
            "Epoch [4512/20000], Training Loss: 0.5052\n",
            "Epoch [4513/20000], Training Loss: 0.5301\n",
            "Epoch [4514/20000], Training Loss: 0.4757\n",
            "Epoch [4515/20000], Training Loss: 0.4728\n",
            "Epoch [4516/20000], Training Loss: 0.4727\n",
            "Epoch [4517/20000], Training Loss: 0.4571\n",
            "Epoch [4518/20000], Training Loss: 0.5232\n",
            "Epoch [4519/20000], Training Loss: 0.4761\n",
            "Epoch [4520/20000], Training Loss: 0.4644\n",
            "Epoch [4521/20000], Training Loss: 0.4769\n",
            "Epoch [4522/20000], Training Loss: 0.5288\n",
            "Epoch [4523/20000], Training Loss: 0.4968\n",
            "Epoch [4524/20000], Training Loss: 0.4818\n",
            "Epoch [4525/20000], Training Loss: 0.4875\n",
            "Epoch [4526/20000], Training Loss: 0.5140\n",
            "Epoch [4527/20000], Training Loss: 0.4404\n",
            "Epoch [4528/20000], Training Loss: 0.5020\n",
            "Epoch [4529/20000], Training Loss: 0.4635\n",
            "Epoch [4530/20000], Training Loss: 0.4588\n",
            "Epoch [4531/20000], Training Loss: 0.5050\n",
            "Epoch [4532/20000], Training Loss: 0.5079\n",
            "Epoch [4533/20000], Training Loss: 0.4656\n",
            "Epoch [4534/20000], Training Loss: 0.4717\n",
            "Epoch [4535/20000], Training Loss: 0.4874\n",
            "Epoch [4536/20000], Training Loss: 0.4839\n",
            "Epoch [4537/20000], Training Loss: 0.5213\n",
            "Epoch [4538/20000], Training Loss: 0.4709\n",
            "Epoch [4539/20000], Training Loss: 0.4877\n",
            "Epoch [4540/20000], Training Loss: 0.4550\n",
            "Epoch [4541/20000], Training Loss: 0.5445\n",
            "Epoch [4542/20000], Training Loss: 0.4670\n",
            "Epoch [4543/20000], Training Loss: 0.5122\n",
            "Epoch [4544/20000], Training Loss: 0.4664\n",
            "Epoch [4545/20000], Training Loss: 0.5250\n",
            "Epoch [4546/20000], Training Loss: 0.4941\n",
            "Epoch [4547/20000], Training Loss: 0.4906\n",
            "Epoch [4548/20000], Training Loss: 0.4580\n",
            "Epoch [4549/20000], Training Loss: 0.4825\n",
            "Epoch [4550/20000], Training Loss: 0.4751\n",
            "Epoch [4551/20000], Training Loss: 0.5011\n",
            "Epoch [4552/20000], Training Loss: 0.5014\n",
            "Epoch [4553/20000], Training Loss: 0.5159\n",
            "Epoch [4554/20000], Training Loss: 0.4462\n",
            "Epoch [4555/20000], Training Loss: 0.5227\n",
            "Epoch [4556/20000], Training Loss: 0.4847\n",
            "Epoch [4557/20000], Training Loss: 0.5287\n",
            "Epoch [4558/20000], Training Loss: 0.4926\n",
            "Epoch [4559/20000], Training Loss: 0.4815\n",
            "Epoch [4560/20000], Training Loss: 0.4645\n",
            "Epoch [4561/20000], Training Loss: 0.5293\n",
            "Epoch [4562/20000], Training Loss: 0.5100\n",
            "Epoch [4563/20000], Training Loss: 0.4575\n",
            "Epoch [4564/20000], Training Loss: 0.4649\n",
            "Epoch [4565/20000], Training Loss: 0.4563\n",
            "Epoch [4566/20000], Training Loss: 0.5214\n",
            "Epoch [4567/20000], Training Loss: 0.4695\n",
            "Epoch [4568/20000], Training Loss: 0.5109\n",
            "Epoch [4569/20000], Training Loss: 0.4713\n",
            "Epoch [4570/20000], Training Loss: 0.4836\n",
            "Epoch [4571/20000], Training Loss: 0.4917\n",
            "Epoch [4572/20000], Training Loss: 0.5020\n",
            "Epoch [4573/20000], Training Loss: 0.4387\n",
            "Epoch [4574/20000], Training Loss: 0.5007\n",
            "Epoch [4575/20000], Training Loss: 0.5103\n",
            "Epoch [4576/20000], Training Loss: 0.4782\n",
            "Epoch [4577/20000], Training Loss: 0.5000\n",
            "Epoch [4578/20000], Training Loss: 0.4878\n",
            "Epoch [4579/20000], Training Loss: 0.4635\n",
            "Epoch [4580/20000], Training Loss: 0.4962\n",
            "Epoch [4581/20000], Training Loss: 0.5466\n",
            "Epoch [4582/20000], Training Loss: 0.4618\n",
            "Epoch [4583/20000], Training Loss: 0.4522\n",
            "Epoch [4584/20000], Training Loss: 0.4690\n",
            "Epoch [4585/20000], Training Loss: 0.4860\n",
            "Epoch [4586/20000], Training Loss: 0.5041\n",
            "Epoch [4587/20000], Training Loss: 0.5131\n",
            "Epoch [4588/20000], Training Loss: 0.4659\n",
            "Epoch [4589/20000], Training Loss: 0.4933\n",
            "Epoch [4590/20000], Training Loss: 0.4749\n",
            "Epoch [4591/20000], Training Loss: 0.5248\n",
            "Epoch [4592/20000], Training Loss: 0.4671\n",
            "Epoch [4593/20000], Training Loss: 0.4649\n",
            "Epoch [4594/20000], Training Loss: 0.4617\n",
            "Epoch [4595/20000], Training Loss: 0.4925\n",
            "Epoch [4596/20000], Training Loss: 0.5399\n",
            "Epoch [4597/20000], Training Loss: 0.4946\n",
            "Epoch [4598/20000], Training Loss: 0.4900\n",
            "Epoch [4599/20000], Training Loss: 0.4771\n",
            "Epoch [4600/20000], Training Loss: 0.5060\n",
            "Epoch [4601/20000], Training Loss: 0.4563\n",
            "Epoch [4602/20000], Training Loss: 0.4917\n",
            "Epoch [4603/20000], Training Loss: 0.4852\n",
            "Epoch [4604/20000], Training Loss: 0.4998\n",
            "Epoch [4605/20000], Training Loss: 0.4747\n",
            "Epoch [4606/20000], Training Loss: 0.4457\n",
            "Epoch [4607/20000], Training Loss: 0.4951\n",
            "Epoch [4608/20000], Training Loss: 0.4894\n",
            "Epoch [4609/20000], Training Loss: 0.5438\n",
            "Epoch [4610/20000], Training Loss: 0.5229\n",
            "Epoch [4611/20000], Training Loss: 0.4823\n",
            "Epoch [4612/20000], Training Loss: 0.5448\n",
            "Epoch [4613/20000], Training Loss: 0.5069\n",
            "Epoch [4614/20000], Training Loss: 0.5014\n",
            "Epoch [4615/20000], Training Loss: 0.5071\n",
            "Epoch [4616/20000], Training Loss: 0.5183\n",
            "Epoch [4617/20000], Training Loss: 0.4490\n",
            "Epoch [4618/20000], Training Loss: 0.4729\n",
            "Epoch [4619/20000], Training Loss: 0.5155\n",
            "Epoch [4620/20000], Training Loss: 0.4886\n",
            "Epoch [4621/20000], Training Loss: 0.5236\n",
            "Epoch [4622/20000], Training Loss: 0.4928\n",
            "Epoch [4623/20000], Training Loss: 0.4761\n",
            "Epoch [4624/20000], Training Loss: 0.5197\n",
            "Epoch [4625/20000], Training Loss: 0.4799\n",
            "Epoch [4626/20000], Training Loss: 0.4872\n",
            "Epoch [4627/20000], Training Loss: 0.4510\n",
            "Epoch [4628/20000], Training Loss: 0.4788\n",
            "Epoch [4629/20000], Training Loss: 0.5031\n",
            "Epoch [4630/20000], Training Loss: 0.4769\n",
            "Epoch [4631/20000], Training Loss: 0.5075\n",
            "Epoch [4632/20000], Training Loss: 0.4911\n",
            "Epoch [4633/20000], Training Loss: 0.5204\n",
            "Epoch [4634/20000], Training Loss: 0.4915\n",
            "Epoch [4635/20000], Training Loss: 0.5239\n",
            "Epoch [4636/20000], Training Loss: 0.4937\n",
            "Epoch [4637/20000], Training Loss: 0.4940\n",
            "Epoch [4638/20000], Training Loss: 0.4409\n",
            "Epoch [4639/20000], Training Loss: 0.5238\n",
            "Epoch [4640/20000], Training Loss: 0.5188\n",
            "Epoch [4641/20000], Training Loss: 0.5019\n",
            "Epoch [4642/20000], Training Loss: 0.4655\n",
            "Epoch [4643/20000], Training Loss: 0.5025\n",
            "Epoch [4644/20000], Training Loss: 0.4663\n",
            "Epoch [4645/20000], Training Loss: 0.4982\n",
            "Epoch [4646/20000], Training Loss: 0.4655\n",
            "Epoch [4647/20000], Training Loss: 0.4786\n",
            "Epoch [4648/20000], Training Loss: 0.4795\n",
            "Epoch [4649/20000], Training Loss: 0.4483\n",
            "Epoch [4650/20000], Training Loss: 0.4987\n",
            "Epoch [4651/20000], Training Loss: 0.5045\n",
            "Epoch [4652/20000], Training Loss: 0.4945\n",
            "Epoch [4653/20000], Training Loss: 0.4855\n",
            "Epoch [4654/20000], Training Loss: 0.4950\n",
            "Epoch [4655/20000], Training Loss: 0.4753\n",
            "Epoch [4656/20000], Training Loss: 0.4934\n",
            "Epoch [4657/20000], Training Loss: 0.4708\n",
            "Epoch [4658/20000], Training Loss: 0.5154\n",
            "Epoch [4659/20000], Training Loss: 0.5094\n",
            "Epoch [4660/20000], Training Loss: 0.5020\n",
            "Epoch [4661/20000], Training Loss: 0.5043\n",
            "Epoch [4662/20000], Training Loss: 0.4569\n",
            "Epoch [4663/20000], Training Loss: 0.4930\n",
            "Epoch [4664/20000], Training Loss: 0.4925\n",
            "Epoch [4665/20000], Training Loss: 0.4744\n",
            "Epoch [4666/20000], Training Loss: 0.5069\n",
            "Epoch [4667/20000], Training Loss: 0.4874\n",
            "Epoch [4668/20000], Training Loss: 0.5053\n",
            "Epoch [4669/20000], Training Loss: 0.5407\n",
            "Epoch [4670/20000], Training Loss: 0.4973\n",
            "Epoch [4671/20000], Training Loss: 0.4772\n",
            "Epoch [4672/20000], Training Loss: 0.4882\n",
            "Epoch [4673/20000], Training Loss: 0.5041\n",
            "Epoch [4674/20000], Training Loss: 0.4956\n",
            "Epoch [4675/20000], Training Loss: 0.4909\n",
            "Epoch [4676/20000], Training Loss: 0.5002\n",
            "Epoch [4677/20000], Training Loss: 0.4905\n",
            "Epoch [4678/20000], Training Loss: 0.4951\n",
            "Epoch [4679/20000], Training Loss: 0.5067\n",
            "Epoch [4680/20000], Training Loss: 0.5228\n",
            "Epoch [4681/20000], Training Loss: 0.4836\n",
            "Epoch [4682/20000], Training Loss: 0.4402\n",
            "Epoch [4683/20000], Training Loss: 0.4930\n",
            "Epoch [4684/20000], Training Loss: 0.4685\n",
            "Epoch [4685/20000], Training Loss: 0.4456\n",
            "Epoch [4686/20000], Training Loss: 0.4621\n",
            "Epoch [4687/20000], Training Loss: 0.4735\n",
            "Epoch [4688/20000], Training Loss: 0.5090\n",
            "Epoch [4689/20000], Training Loss: 0.4982\n",
            "Epoch [4690/20000], Training Loss: 0.4857\n",
            "Epoch [4691/20000], Training Loss: 0.4919\n",
            "Epoch [4692/20000], Training Loss: 0.5088\n",
            "Epoch [4693/20000], Training Loss: 0.4609\n",
            "Epoch [4694/20000], Training Loss: 0.4981\n",
            "Epoch [4695/20000], Training Loss: 0.5127\n",
            "Epoch [4696/20000], Training Loss: 0.4948\n",
            "Epoch [4697/20000], Training Loss: 0.5020\n",
            "Epoch [4698/20000], Training Loss: 0.4922\n",
            "Epoch [4699/20000], Training Loss: 0.5086\n",
            "Epoch [4700/20000], Training Loss: 0.5303\n",
            "Epoch [4701/20000], Training Loss: 0.4861\n",
            "Epoch [4702/20000], Training Loss: 0.4796\n",
            "Epoch [4703/20000], Training Loss: 0.4855\n",
            "Epoch [4704/20000], Training Loss: 0.4888\n",
            "Epoch [4705/20000], Training Loss: 0.4664\n",
            "Epoch [4706/20000], Training Loss: 0.4949\n",
            "Epoch [4707/20000], Training Loss: 0.4886\n",
            "Epoch [4708/20000], Training Loss: 0.4988\n",
            "Epoch [4709/20000], Training Loss: 0.5042\n",
            "Epoch [4710/20000], Training Loss: 0.4988\n",
            "Epoch [4711/20000], Training Loss: 0.4964\n",
            "Epoch [4712/20000], Training Loss: 0.5191\n",
            "Epoch [4713/20000], Training Loss: 0.4821\n",
            "Epoch [4714/20000], Training Loss: 0.4477\n",
            "Epoch [4715/20000], Training Loss: 0.5066\n",
            "Epoch [4716/20000], Training Loss: 0.5465\n",
            "Epoch [4717/20000], Training Loss: 0.4617\n",
            "Epoch [4718/20000], Training Loss: 0.4999\n",
            "Epoch [4719/20000], Training Loss: 0.4523\n",
            "Epoch [4720/20000], Training Loss: 0.4945\n",
            "Epoch [4721/20000], Training Loss: 0.4711\n",
            "Epoch [4722/20000], Training Loss: 0.4712\n",
            "Epoch [4723/20000], Training Loss: 0.5039\n",
            "Epoch [4724/20000], Training Loss: 0.5182\n",
            "Epoch [4725/20000], Training Loss: 0.4733\n",
            "Epoch [4726/20000], Training Loss: 0.4780\n",
            "Epoch [4727/20000], Training Loss: 0.4640\n",
            "Epoch [4728/20000], Training Loss: 0.4851\n",
            "Epoch [4729/20000], Training Loss: 0.5076\n",
            "Epoch [4730/20000], Training Loss: 0.5165\n",
            "Epoch [4731/20000], Training Loss: 0.5023\n",
            "Epoch [4732/20000], Training Loss: 0.5356\n",
            "Epoch [4733/20000], Training Loss: 0.5050\n",
            "Epoch [4734/20000], Training Loss: 0.4595\n",
            "Epoch [4735/20000], Training Loss: 0.4651\n",
            "Epoch [4736/20000], Training Loss: 0.5063\n",
            "Epoch [4737/20000], Training Loss: 0.4473\n",
            "Epoch [4738/20000], Training Loss: 0.4481\n",
            "Epoch [4739/20000], Training Loss: 0.5037\n",
            "Epoch [4740/20000], Training Loss: 0.4775\n",
            "Epoch [4741/20000], Training Loss: 0.4903\n",
            "Epoch [4742/20000], Training Loss: 0.4626\n",
            "Epoch [4743/20000], Training Loss: 0.4894\n",
            "Epoch [4744/20000], Training Loss: 0.4867\n",
            "Epoch [4745/20000], Training Loss: 0.4932\n",
            "Epoch [4746/20000], Training Loss: 0.5202\n",
            "Epoch [4747/20000], Training Loss: 0.4517\n",
            "Epoch [4748/20000], Training Loss: 0.5000\n",
            "Epoch [4749/20000], Training Loss: 0.4623\n",
            "Epoch [4750/20000], Training Loss: 0.4988\n",
            "Epoch [4751/20000], Training Loss: 0.4998\n",
            "Epoch [4752/20000], Training Loss: 0.4830\n",
            "Epoch [4753/20000], Training Loss: 0.5220\n",
            "Epoch [4754/20000], Training Loss: 0.4715\n",
            "Epoch [4755/20000], Training Loss: 0.4538\n",
            "Epoch [4756/20000], Training Loss: 0.4897\n",
            "Epoch [4757/20000], Training Loss: 0.4805\n",
            "Epoch [4758/20000], Training Loss: 0.5054\n",
            "Epoch [4759/20000], Training Loss: 0.5139\n",
            "Epoch [4760/20000], Training Loss: 0.4468\n",
            "Epoch [4761/20000], Training Loss: 0.5028\n",
            "Epoch [4762/20000], Training Loss: 0.4770\n",
            "Epoch [4763/20000], Training Loss: 0.4655\n",
            "Epoch [4764/20000], Training Loss: 0.4685\n",
            "Epoch [4765/20000], Training Loss: 0.4677\n",
            "Epoch [4766/20000], Training Loss: 0.4543\n",
            "Epoch [4767/20000], Training Loss: 0.4627\n",
            "Epoch [4768/20000], Training Loss: 0.5085\n",
            "Epoch [4769/20000], Training Loss: 0.4887\n",
            "Epoch [4770/20000], Training Loss: 0.4880\n",
            "Epoch [4771/20000], Training Loss: 0.4622\n",
            "Epoch [4772/20000], Training Loss: 0.5032\n",
            "Epoch [4773/20000], Training Loss: 0.5274\n",
            "Epoch [4774/20000], Training Loss: 0.5296\n",
            "Epoch [4775/20000], Training Loss: 0.4777\n",
            "Epoch [4776/20000], Training Loss: 0.5097\n",
            "Epoch [4777/20000], Training Loss: 0.5063\n",
            "Epoch [4778/20000], Training Loss: 0.5284\n",
            "Epoch [4779/20000], Training Loss: 0.4945\n",
            "Epoch [4780/20000], Training Loss: 0.4850\n",
            "Epoch [4781/20000], Training Loss: 0.5073\n",
            "Epoch [4782/20000], Training Loss: 0.4872\n",
            "Epoch [4783/20000], Training Loss: 0.4888\n",
            "Epoch [4784/20000], Training Loss: 0.4937\n",
            "Epoch [4785/20000], Training Loss: 0.4852\n",
            "Epoch [4786/20000], Training Loss: 0.4914\n",
            "Epoch [4787/20000], Training Loss: 0.4904\n",
            "Epoch [4788/20000], Training Loss: 0.5492\n",
            "Epoch [4789/20000], Training Loss: 0.4928\n",
            "Epoch [4790/20000], Training Loss: 0.5085\n",
            "Epoch [4791/20000], Training Loss: 0.5019\n",
            "Epoch [4792/20000], Training Loss: 0.4557\n",
            "Epoch [4793/20000], Training Loss: 0.4877\n",
            "Epoch [4794/20000], Training Loss: 0.4878\n",
            "Epoch [4795/20000], Training Loss: 0.4917\n",
            "Epoch [4796/20000], Training Loss: 0.5043\n",
            "Epoch [4797/20000], Training Loss: 0.4753\n",
            "Epoch [4798/20000], Training Loss: 0.5056\n",
            "Epoch [4799/20000], Training Loss: 0.4680\n",
            "Epoch [4800/20000], Training Loss: 0.5154\n",
            "Epoch [4801/20000], Training Loss: 0.5007\n",
            "Epoch [4802/20000], Training Loss: 0.5172\n",
            "Epoch [4803/20000], Training Loss: 0.4766\n",
            "Epoch [4804/20000], Training Loss: 0.5339\n",
            "Epoch [4805/20000], Training Loss: 0.4879\n",
            "Epoch [4806/20000], Training Loss: 0.4926\n",
            "Epoch [4807/20000], Training Loss: 0.4922\n",
            "Epoch [4808/20000], Training Loss: 0.5242\n",
            "Epoch [4809/20000], Training Loss: 0.4686\n",
            "Epoch [4810/20000], Training Loss: 0.4769\n",
            "Epoch [4811/20000], Training Loss: 0.5148\n",
            "Epoch [4812/20000], Training Loss: 0.4779\n",
            "Epoch [4813/20000], Training Loss: 0.5160\n",
            "Epoch [4814/20000], Training Loss: 0.5386\n",
            "Epoch [4815/20000], Training Loss: 0.4899\n",
            "Epoch [4816/20000], Training Loss: 0.5060\n",
            "Epoch [4817/20000], Training Loss: 0.4870\n",
            "Epoch [4818/20000], Training Loss: 0.4704\n",
            "Epoch [4819/20000], Training Loss: 0.5017\n",
            "Epoch [4820/20000], Training Loss: 0.4904\n",
            "Epoch [4821/20000], Training Loss: 0.4968\n",
            "Epoch [4822/20000], Training Loss: 0.4813\n",
            "Epoch [4823/20000], Training Loss: 0.4590\n",
            "Epoch [4824/20000], Training Loss: 0.5278\n",
            "Epoch [4825/20000], Training Loss: 0.4849\n",
            "Epoch [4826/20000], Training Loss: 0.4734\n",
            "Epoch [4827/20000], Training Loss: 0.5014\n",
            "Epoch [4828/20000], Training Loss: 0.4977\n",
            "Epoch [4829/20000], Training Loss: 0.4536\n",
            "Epoch [4830/20000], Training Loss: 0.4462\n",
            "Epoch [4831/20000], Training Loss: 0.4797\n",
            "Epoch [4832/20000], Training Loss: 0.4702\n",
            "Epoch [4833/20000], Training Loss: 0.5031\n",
            "Epoch [4834/20000], Training Loss: 0.4664\n",
            "Epoch [4835/20000], Training Loss: 0.5140\n",
            "Epoch [4836/20000], Training Loss: 0.4511\n",
            "Epoch [4837/20000], Training Loss: 0.4735\n",
            "Epoch [4838/20000], Training Loss: 0.4535\n",
            "Epoch [4839/20000], Training Loss: 0.4544\n",
            "Epoch [4840/20000], Training Loss: 0.4993\n",
            "Epoch [4841/20000], Training Loss: 0.4336\n",
            "Epoch [4842/20000], Training Loss: 0.4733\n",
            "Epoch [4843/20000], Training Loss: 0.5009\n",
            "Epoch [4844/20000], Training Loss: 0.4616\n",
            "Epoch [4845/20000], Training Loss: 0.4724\n",
            "Epoch [4846/20000], Training Loss: 0.5582\n",
            "Epoch [4847/20000], Training Loss: 0.4456\n",
            "Epoch [4848/20000], Training Loss: 0.4697\n",
            "Epoch [4849/20000], Training Loss: 0.5186\n",
            "Epoch [4850/20000], Training Loss: 0.4815\n",
            "Epoch [4851/20000], Training Loss: 0.4761\n",
            "Epoch [4852/20000], Training Loss: 0.4669\n",
            "Epoch [4853/20000], Training Loss: 0.5496\n",
            "Epoch [4854/20000], Training Loss: 0.4782\n",
            "Epoch [4855/20000], Training Loss: 0.4672\n",
            "Epoch [4856/20000], Training Loss: 0.4853\n",
            "Epoch [4857/20000], Training Loss: 0.5117\n",
            "Epoch [4858/20000], Training Loss: 0.4802\n",
            "Epoch [4859/20000], Training Loss: 0.4679\n",
            "Epoch [4860/20000], Training Loss: 0.4875\n",
            "Epoch [4861/20000], Training Loss: 0.5051\n",
            "Epoch [4862/20000], Training Loss: 0.4796\n",
            "Epoch [4863/20000], Training Loss: 0.4908\n",
            "Epoch [4864/20000], Training Loss: 0.4882\n",
            "Epoch [4865/20000], Training Loss: 0.5170\n",
            "Epoch [4866/20000], Training Loss: 0.5042\n",
            "Epoch [4867/20000], Training Loss: 0.4800\n",
            "Epoch [4868/20000], Training Loss: 0.4676\n",
            "Epoch [4869/20000], Training Loss: 0.4896\n",
            "Epoch [4870/20000], Training Loss: 0.4613\n",
            "Epoch [4871/20000], Training Loss: 0.4701\n",
            "Epoch [4872/20000], Training Loss: 0.5031\n",
            "Epoch [4873/20000], Training Loss: 0.4823\n",
            "Epoch [4874/20000], Training Loss: 0.4278\n",
            "Epoch [4875/20000], Training Loss: 0.4755\n",
            "Epoch [4876/20000], Training Loss: 0.5081\n",
            "Epoch [4877/20000], Training Loss: 0.4623\n",
            "Epoch [4878/20000], Training Loss: 0.5098\n",
            "Epoch [4879/20000], Training Loss: 0.4345\n",
            "Epoch [4880/20000], Training Loss: 0.4971\n",
            "Epoch [4881/20000], Training Loss: 0.5073\n",
            "Epoch [4882/20000], Training Loss: 0.4860\n",
            "Epoch [4883/20000], Training Loss: 0.5088\n",
            "Epoch [4884/20000], Training Loss: 0.5028\n",
            "Epoch [4885/20000], Training Loss: 0.5112\n",
            "Epoch [4886/20000], Training Loss: 0.4585\n",
            "Epoch [4887/20000], Training Loss: 0.4897\n",
            "Epoch [4888/20000], Training Loss: 0.4629\n",
            "Epoch [4889/20000], Training Loss: 0.5039\n",
            "Epoch [4890/20000], Training Loss: 0.4739\n",
            "Epoch [4891/20000], Training Loss: 0.4691\n",
            "Epoch [4892/20000], Training Loss: 0.4820\n",
            "Epoch [4893/20000], Training Loss: 0.4741\n",
            "Epoch [4894/20000], Training Loss: 0.5080\n",
            "Epoch [4895/20000], Training Loss: 0.4867\n",
            "Epoch [4896/20000], Training Loss: 0.4554\n",
            "Epoch [4897/20000], Training Loss: 0.4855\n",
            "Epoch [4898/20000], Training Loss: 0.5080\n",
            "Epoch [4899/20000], Training Loss: 0.4493\n",
            "Epoch [4900/20000], Training Loss: 0.4732\n",
            "Epoch [4901/20000], Training Loss: 0.5074\n",
            "Epoch [4902/20000], Training Loss: 0.4806\n",
            "Epoch [4903/20000], Training Loss: 0.4723\n",
            "Epoch [4904/20000], Training Loss: 0.4666\n",
            "Epoch [4905/20000], Training Loss: 0.5257\n",
            "Epoch [4906/20000], Training Loss: 0.4802\n",
            "Epoch [4907/20000], Training Loss: 0.5012\n",
            "Epoch [4908/20000], Training Loss: 0.5002\n",
            "Epoch [4909/20000], Training Loss: 0.4615\n",
            "Epoch [4910/20000], Training Loss: 0.4992\n",
            "Epoch [4911/20000], Training Loss: 0.4754\n",
            "Epoch [4912/20000], Training Loss: 0.4689\n",
            "Epoch [4913/20000], Training Loss: 0.4836\n",
            "Epoch [4914/20000], Training Loss: 0.5023\n",
            "Epoch [4915/20000], Training Loss: 0.5212\n",
            "Epoch [4916/20000], Training Loss: 0.5029\n",
            "Epoch [4917/20000], Training Loss: 0.4768\n",
            "Epoch [4918/20000], Training Loss: 0.5394\n",
            "Epoch [4919/20000], Training Loss: 0.4684\n",
            "Epoch [4920/20000], Training Loss: 0.4699\n",
            "Epoch [4921/20000], Training Loss: 0.4634\n",
            "Epoch [4922/20000], Training Loss: 0.4585\n",
            "Epoch [4923/20000], Training Loss: 0.4716\n",
            "Epoch [4924/20000], Training Loss: 0.4419\n",
            "Epoch [4925/20000], Training Loss: 0.5058\n",
            "Epoch [4926/20000], Training Loss: 0.4906\n",
            "Epoch [4927/20000], Training Loss: 0.5157\n",
            "Epoch [4928/20000], Training Loss: 0.5136\n",
            "Epoch [4929/20000], Training Loss: 0.5374\n",
            "Epoch [4930/20000], Training Loss: 0.4488\n",
            "Epoch [4931/20000], Training Loss: 0.4723\n",
            "Epoch [4932/20000], Training Loss: 0.4634\n",
            "Epoch [4933/20000], Training Loss: 0.4740\n",
            "Epoch [4934/20000], Training Loss: 0.4871\n",
            "Epoch [4935/20000], Training Loss: 0.5050\n",
            "Epoch [4936/20000], Training Loss: 0.4441\n",
            "Epoch [4937/20000], Training Loss: 0.4575\n",
            "Epoch [4938/20000], Training Loss: 0.5089\n",
            "Epoch [4939/20000], Training Loss: 0.4789\n",
            "Epoch [4940/20000], Training Loss: 0.5164\n",
            "Epoch [4941/20000], Training Loss: 0.4699\n",
            "Epoch [4942/20000], Training Loss: 0.4842\n",
            "Epoch [4943/20000], Training Loss: 0.4955\n",
            "Epoch [4944/20000], Training Loss: 0.5124\n",
            "Epoch [4945/20000], Training Loss: 0.4966\n",
            "Epoch [4946/20000], Training Loss: 0.4700\n",
            "Epoch [4947/20000], Training Loss: 0.4971\n",
            "Epoch [4948/20000], Training Loss: 0.5027\n",
            "Epoch [4949/20000], Training Loss: 0.5233\n",
            "Epoch [4950/20000], Training Loss: 0.4687\n",
            "Epoch [4951/20000], Training Loss: 0.4784\n",
            "Epoch [4952/20000], Training Loss: 0.4750\n",
            "Epoch [4953/20000], Training Loss: 0.5046\n",
            "Epoch [4954/20000], Training Loss: 0.5164\n",
            "Epoch [4955/20000], Training Loss: 0.4704\n",
            "Epoch [4956/20000], Training Loss: 0.5034\n",
            "Epoch [4957/20000], Training Loss: 0.4954\n",
            "Epoch [4958/20000], Training Loss: 0.4656\n",
            "Epoch [4959/20000], Training Loss: 0.5005\n",
            "Epoch [4960/20000], Training Loss: 0.5339\n",
            "Epoch [4961/20000], Training Loss: 0.4602\n",
            "Epoch [4962/20000], Training Loss: 0.4746\n",
            "Epoch [4963/20000], Training Loss: 0.5028\n",
            "Epoch [4964/20000], Training Loss: 0.4982\n",
            "Epoch [4965/20000], Training Loss: 0.4783\n",
            "Epoch [4966/20000], Training Loss: 0.4888\n",
            "Epoch [4967/20000], Training Loss: 0.5035\n",
            "Epoch [4968/20000], Training Loss: 0.4460\n",
            "Epoch [4969/20000], Training Loss: 0.4475\n",
            "Epoch [4970/20000], Training Loss: 0.4304\n",
            "Epoch [4971/20000], Training Loss: 0.5200\n",
            "Epoch [4972/20000], Training Loss: 0.5127\n",
            "Epoch [4973/20000], Training Loss: 0.4786\n",
            "Epoch [4974/20000], Training Loss: 0.4718\n",
            "Epoch [4975/20000], Training Loss: 0.4847\n",
            "Epoch [4976/20000], Training Loss: 0.4810\n",
            "Epoch [4977/20000], Training Loss: 0.4335\n",
            "Epoch [4978/20000], Training Loss: 0.4655\n",
            "Epoch [4979/20000], Training Loss: 0.4827\n",
            "Epoch [4980/20000], Training Loss: 0.4938\n",
            "Epoch [4981/20000], Training Loss: 0.4813\n",
            "Epoch [4982/20000], Training Loss: 0.4780\n",
            "Epoch [4983/20000], Training Loss: 0.5187\n",
            "Epoch [4984/20000], Training Loss: 0.4932\n",
            "Epoch [4985/20000], Training Loss: 0.4994\n",
            "Epoch [4986/20000], Training Loss: 0.4634\n",
            "Epoch [4987/20000], Training Loss: 0.4911\n",
            "Epoch [4988/20000], Training Loss: 0.5129\n",
            "Epoch [4989/20000], Training Loss: 0.4900\n",
            "Epoch [4990/20000], Training Loss: 0.4712\n",
            "Epoch [4991/20000], Training Loss: 0.4777\n",
            "Epoch [4992/20000], Training Loss: 0.4880\n",
            "Epoch [4993/20000], Training Loss: 0.5009\n",
            "Epoch [4994/20000], Training Loss: 0.5089\n",
            "Epoch [4995/20000], Training Loss: 0.4732\n",
            "Epoch [4996/20000], Training Loss: 0.5328\n",
            "Epoch [4997/20000], Training Loss: 0.4811\n",
            "Epoch [4998/20000], Training Loss: 0.4643\n",
            "Epoch [4999/20000], Training Loss: 0.4893\n",
            "Epoch [5000/20000], Training Loss: 0.4713\n",
            "Epoch [5001/20000], Training Loss: 0.4991\n",
            "Epoch [5002/20000], Training Loss: 0.4903\n",
            "Epoch [5003/20000], Training Loss: 0.5049\n",
            "Epoch [5004/20000], Training Loss: 0.5114\n",
            "Epoch [5005/20000], Training Loss: 0.5265\n",
            "Epoch [5006/20000], Training Loss: 0.5215\n",
            "Epoch [5007/20000], Training Loss: 0.4905\n",
            "Epoch [5008/20000], Training Loss: 0.5197\n",
            "Epoch [5009/20000], Training Loss: 0.4893\n",
            "Epoch [5010/20000], Training Loss: 0.4660\n",
            "Epoch [5011/20000], Training Loss: 0.4551\n",
            "Epoch [5012/20000], Training Loss: 0.4968\n",
            "Epoch [5013/20000], Training Loss: 0.5545\n",
            "Epoch [5014/20000], Training Loss: 0.4892\n",
            "Epoch [5015/20000], Training Loss: 0.5046\n",
            "Epoch [5016/20000], Training Loss: 0.4662\n",
            "Epoch [5017/20000], Training Loss: 0.4867\n",
            "Epoch [5018/20000], Training Loss: 0.4893\n",
            "Epoch [5019/20000], Training Loss: 0.5215\n",
            "Epoch [5020/20000], Training Loss: 0.5121\n",
            "Epoch [5021/20000], Training Loss: 0.5124\n",
            "Epoch [5022/20000], Training Loss: 0.4800\n",
            "Epoch [5023/20000], Training Loss: 0.4924\n",
            "Epoch [5024/20000], Training Loss: 0.4633\n",
            "Epoch [5025/20000], Training Loss: 0.5453\n",
            "Epoch [5026/20000], Training Loss: 0.4840\n",
            "Epoch [5027/20000], Training Loss: 0.4857\n",
            "Epoch [5028/20000], Training Loss: 0.5007\n",
            "Epoch [5029/20000], Training Loss: 0.4441\n",
            "Epoch [5030/20000], Training Loss: 0.5309\n",
            "Epoch [5031/20000], Training Loss: 0.4640\n",
            "Epoch [5032/20000], Training Loss: 0.4878\n",
            "Epoch [5033/20000], Training Loss: 0.4740\n",
            "Epoch [5034/20000], Training Loss: 0.5512\n",
            "Epoch [5035/20000], Training Loss: 0.4476\n",
            "Epoch [5036/20000], Training Loss: 0.5590\n",
            "Epoch [5037/20000], Training Loss: 0.5130\n",
            "Epoch [5038/20000], Training Loss: 0.5418\n",
            "Epoch [5039/20000], Training Loss: 0.4653\n",
            "Epoch [5040/20000], Training Loss: 0.4690\n",
            "Epoch [5041/20000], Training Loss: 0.5183\n",
            "Epoch [5042/20000], Training Loss: 0.5408\n",
            "Epoch [5043/20000], Training Loss: 0.4871\n",
            "Epoch [5044/20000], Training Loss: 0.4915\n",
            "Epoch [5045/20000], Training Loss: 0.5004\n",
            "Epoch [5046/20000], Training Loss: 0.4403\n",
            "Epoch [5047/20000], Training Loss: 0.4616\n",
            "Epoch [5048/20000], Training Loss: 0.5049\n",
            "Epoch [5049/20000], Training Loss: 0.5014\n",
            "Epoch [5050/20000], Training Loss: 0.5471\n",
            "Epoch [5051/20000], Training Loss: 0.4761\n",
            "Epoch [5052/20000], Training Loss: 0.4989\n",
            "Epoch [5053/20000], Training Loss: 0.4731\n",
            "Epoch [5054/20000], Training Loss: 0.4938\n",
            "Epoch [5055/20000], Training Loss: 0.4633\n",
            "Epoch [5056/20000], Training Loss: 0.4878\n",
            "Epoch [5057/20000], Training Loss: 0.4973\n",
            "Epoch [5058/20000], Training Loss: 0.4485\n",
            "Epoch [5059/20000], Training Loss: 0.5031\n",
            "Epoch [5060/20000], Training Loss: 0.4941\n",
            "Epoch [5061/20000], Training Loss: 0.4904\n",
            "Epoch [5062/20000], Training Loss: 0.4990\n",
            "Epoch [5063/20000], Training Loss: 0.5289\n",
            "Epoch [5064/20000], Training Loss: 0.4836\n",
            "Epoch [5065/20000], Training Loss: 0.5272\n",
            "Epoch [5066/20000], Training Loss: 0.5275\n",
            "Epoch [5067/20000], Training Loss: 0.5067\n",
            "Epoch [5068/20000], Training Loss: 0.4834\n",
            "Epoch [5069/20000], Training Loss: 0.4713\n",
            "Epoch [5070/20000], Training Loss: 0.4708\n",
            "Epoch [5071/20000], Training Loss: 0.4918\n",
            "Epoch [5072/20000], Training Loss: 0.4521\n",
            "Epoch [5073/20000], Training Loss: 0.5610\n",
            "Epoch [5074/20000], Training Loss: 0.4766\n",
            "Epoch [5075/20000], Training Loss: 0.5034\n",
            "Epoch [5076/20000], Training Loss: 0.4504\n",
            "Epoch [5077/20000], Training Loss: 0.4843\n",
            "Epoch [5078/20000], Training Loss: 0.5131\n",
            "Epoch [5079/20000], Training Loss: 0.4831\n",
            "Epoch [5080/20000], Training Loss: 0.4665\n",
            "Epoch [5081/20000], Training Loss: 0.4593\n",
            "Epoch [5082/20000], Training Loss: 0.4476\n",
            "Epoch [5083/20000], Training Loss: 0.5196\n",
            "Epoch [5084/20000], Training Loss: 0.4917\n",
            "Epoch [5085/20000], Training Loss: 0.5170\n",
            "Epoch [5086/20000], Training Loss: 0.4912\n",
            "Epoch [5087/20000], Training Loss: 0.4463\n",
            "Epoch [5088/20000], Training Loss: 0.5078\n",
            "Epoch [5089/20000], Training Loss: 0.5099\n",
            "Epoch [5090/20000], Training Loss: 0.4812\n",
            "Epoch [5091/20000], Training Loss: 0.4501\n",
            "Epoch [5092/20000], Training Loss: 0.4659\n",
            "Epoch [5093/20000], Training Loss: 0.5172\n",
            "Epoch [5094/20000], Training Loss: 0.5058\n",
            "Epoch [5095/20000], Training Loss: 0.4445\n",
            "Epoch [5096/20000], Training Loss: 0.4525\n",
            "Epoch [5097/20000], Training Loss: 0.4550\n",
            "Epoch [5098/20000], Training Loss: 0.5376\n",
            "Epoch [5099/20000], Training Loss: 0.4536\n",
            "Epoch [5100/20000], Training Loss: 0.4922\n",
            "Epoch [5101/20000], Training Loss: 0.4903\n",
            "Epoch [5102/20000], Training Loss: 0.5075\n",
            "Epoch [5103/20000], Training Loss: 0.5303\n",
            "Epoch [5104/20000], Training Loss: 0.5175\n",
            "Epoch [5105/20000], Training Loss: 0.5133\n",
            "Epoch [5106/20000], Training Loss: 0.4725\n",
            "Epoch [5107/20000], Training Loss: 0.4755\n",
            "Epoch [5108/20000], Training Loss: 0.5238\n",
            "Epoch [5109/20000], Training Loss: 0.4806\n",
            "Epoch [5110/20000], Training Loss: 0.4837\n",
            "Epoch [5111/20000], Training Loss: 0.4424\n",
            "Epoch [5112/20000], Training Loss: 0.4698\n",
            "Epoch [5113/20000], Training Loss: 0.5277\n",
            "Epoch [5114/20000], Training Loss: 0.5074\n",
            "Epoch [5115/20000], Training Loss: 0.4706\n",
            "Epoch [5116/20000], Training Loss: 0.4938\n",
            "Epoch [5117/20000], Training Loss: 0.4656\n",
            "Epoch [5118/20000], Training Loss: 0.4700\n",
            "Epoch [5119/20000], Training Loss: 0.4634\n",
            "Epoch [5120/20000], Training Loss: 0.5101\n",
            "Epoch [5121/20000], Training Loss: 0.5108\n",
            "Epoch [5122/20000], Training Loss: 0.4868\n",
            "Epoch [5123/20000], Training Loss: 0.5151\n",
            "Epoch [5124/20000], Training Loss: 0.4702\n",
            "Epoch [5125/20000], Training Loss: 0.4874\n",
            "Epoch [5126/20000], Training Loss: 0.4753\n",
            "Epoch [5127/20000], Training Loss: 0.5138\n",
            "Epoch [5128/20000], Training Loss: 0.5119\n",
            "Epoch [5129/20000], Training Loss: 0.5102\n",
            "Epoch [5130/20000], Training Loss: 0.4931\n",
            "Epoch [5131/20000], Training Loss: 0.4511\n",
            "Epoch [5132/20000], Training Loss: 0.4658\n",
            "Epoch [5133/20000], Training Loss: 0.4680\n",
            "Epoch [5134/20000], Training Loss: 0.5283\n",
            "Epoch [5135/20000], Training Loss: 0.4614\n",
            "Epoch [5136/20000], Training Loss: 0.4904\n",
            "Epoch [5137/20000], Training Loss: 0.4600\n",
            "Epoch [5138/20000], Training Loss: 0.4546\n",
            "Epoch [5139/20000], Training Loss: 0.4906\n",
            "Epoch [5140/20000], Training Loss: 0.4563\n",
            "Epoch [5141/20000], Training Loss: 0.5424\n",
            "Epoch [5142/20000], Training Loss: 0.4718\n",
            "Epoch [5143/20000], Training Loss: 0.5020\n",
            "Epoch [5144/20000], Training Loss: 0.5147\n",
            "Epoch [5145/20000], Training Loss: 0.4927\n",
            "Epoch [5146/20000], Training Loss: 0.4876\n",
            "Epoch [5147/20000], Training Loss: 0.5255\n",
            "Epoch [5148/20000], Training Loss: 0.4683\n",
            "Epoch [5149/20000], Training Loss: 0.5102\n",
            "Epoch [5150/20000], Training Loss: 0.5360\n",
            "Epoch [5151/20000], Training Loss: 0.4874\n",
            "Epoch [5152/20000], Training Loss: 0.5022\n",
            "Epoch [5153/20000], Training Loss: 0.4712\n",
            "Epoch [5154/20000], Training Loss: 0.5146\n",
            "Epoch [5155/20000], Training Loss: 0.5038\n",
            "Epoch [5156/20000], Training Loss: 0.4862\n",
            "Epoch [5157/20000], Training Loss: 0.4661\n",
            "Epoch [5158/20000], Training Loss: 0.4991\n",
            "Epoch [5159/20000], Training Loss: 0.5225\n",
            "Epoch [5160/20000], Training Loss: 0.4955\n",
            "Epoch [5161/20000], Training Loss: 0.4538\n",
            "Epoch [5162/20000], Training Loss: 0.4566\n",
            "Epoch [5163/20000], Training Loss: 0.4768\n",
            "Epoch [5164/20000], Training Loss: 0.5075\n",
            "Epoch [5165/20000], Training Loss: 0.4914\n",
            "Epoch [5166/20000], Training Loss: 0.5306\n",
            "Epoch [5167/20000], Training Loss: 0.5143\n",
            "Epoch [5168/20000], Training Loss: 0.5009\n",
            "Epoch [5169/20000], Training Loss: 0.4730\n",
            "Epoch [5170/20000], Training Loss: 0.5186\n",
            "Epoch [5171/20000], Training Loss: 0.4666\n",
            "Epoch [5172/20000], Training Loss: 0.5113\n",
            "Epoch [5173/20000], Training Loss: 0.4933\n",
            "Epoch [5174/20000], Training Loss: 0.4442\n",
            "Epoch [5175/20000], Training Loss: 0.5188\n",
            "Epoch [5176/20000], Training Loss: 0.4406\n",
            "Epoch [5177/20000], Training Loss: 0.5081\n",
            "Epoch [5178/20000], Training Loss: 0.4889\n",
            "Epoch [5179/20000], Training Loss: 0.4665\n",
            "Epoch [5180/20000], Training Loss: 0.5012\n",
            "Epoch [5181/20000], Training Loss: 0.5042\n",
            "Epoch [5182/20000], Training Loss: 0.4770\n",
            "Epoch [5183/20000], Training Loss: 0.4827\n",
            "Epoch [5184/20000], Training Loss: 0.4824\n",
            "Epoch [5185/20000], Training Loss: 0.5160\n",
            "Epoch [5186/20000], Training Loss: 0.5132\n",
            "Epoch [5187/20000], Training Loss: 0.5122\n",
            "Epoch [5188/20000], Training Loss: 0.5027\n",
            "Epoch [5189/20000], Training Loss: 0.4853\n",
            "Epoch [5190/20000], Training Loss: 0.4984\n",
            "Epoch [5191/20000], Training Loss: 0.4902\n",
            "Epoch [5192/20000], Training Loss: 0.5659\n",
            "Epoch [5193/20000], Training Loss: 0.4920\n",
            "Epoch [5194/20000], Training Loss: 0.5106\n",
            "Epoch [5195/20000], Training Loss: 0.4627\n",
            "Epoch [5196/20000], Training Loss: 0.5065\n",
            "Epoch [5197/20000], Training Loss: 0.5199\n",
            "Epoch [5198/20000], Training Loss: 0.4796\n",
            "Epoch [5199/20000], Training Loss: 0.5091\n",
            "Epoch [5200/20000], Training Loss: 0.5169\n",
            "Epoch [5201/20000], Training Loss: 0.4699\n",
            "Epoch [5202/20000], Training Loss: 0.4944\n",
            "Epoch [5203/20000], Training Loss: 0.4474\n",
            "Epoch [5204/20000], Training Loss: 0.4716\n",
            "Epoch [5205/20000], Training Loss: 0.4779\n",
            "Epoch [5206/20000], Training Loss: 0.5014\n",
            "Epoch [5207/20000], Training Loss: 0.5345\n",
            "Epoch [5208/20000], Training Loss: 0.4669\n",
            "Epoch [5209/20000], Training Loss: 0.4924\n",
            "Epoch [5210/20000], Training Loss: 0.4716\n",
            "Epoch [5211/20000], Training Loss: 0.4793\n",
            "Epoch [5212/20000], Training Loss: 0.4854\n",
            "Epoch [5213/20000], Training Loss: 0.4964\n",
            "Epoch [5214/20000], Training Loss: 0.5072\n",
            "Epoch [5215/20000], Training Loss: 0.4642\n",
            "Epoch [5216/20000], Training Loss: 0.4669\n",
            "Epoch [5217/20000], Training Loss: 0.5723\n",
            "Epoch [5218/20000], Training Loss: 0.5491\n",
            "Epoch [5219/20000], Training Loss: 0.4648\n",
            "Epoch [5220/20000], Training Loss: 0.4620\n",
            "Epoch [5221/20000], Training Loss: 0.5155\n",
            "Epoch [5222/20000], Training Loss: 0.5152\n",
            "Epoch [5223/20000], Training Loss: 0.5007\n",
            "Epoch [5224/20000], Training Loss: 0.5042\n",
            "Epoch [5225/20000], Training Loss: 0.4990\n",
            "Epoch [5226/20000], Training Loss: 0.5032\n",
            "Epoch [5227/20000], Training Loss: 0.4771\n",
            "Epoch [5228/20000], Training Loss: 0.5189\n",
            "Epoch [5229/20000], Training Loss: 0.4850\n",
            "Epoch [5230/20000], Training Loss: 0.5094\n",
            "Epoch [5231/20000], Training Loss: 0.4824\n",
            "Epoch [5232/20000], Training Loss: 0.5231\n",
            "Epoch [5233/20000], Training Loss: 0.4397\n",
            "Epoch [5234/20000], Training Loss: 0.4485\n",
            "Epoch [5235/20000], Training Loss: 0.5043\n",
            "Epoch [5236/20000], Training Loss: 0.4579\n",
            "Epoch [5237/20000], Training Loss: 0.4557\n",
            "Epoch [5238/20000], Training Loss: 0.5074\n",
            "Epoch [5239/20000], Training Loss: 0.5117\n",
            "Epoch [5240/20000], Training Loss: 0.4819\n",
            "Epoch [5241/20000], Training Loss: 0.4852\n",
            "Epoch [5242/20000], Training Loss: 0.5352\n",
            "Epoch [5243/20000], Training Loss: 0.5086\n",
            "Epoch [5244/20000], Training Loss: 0.4434\n",
            "Epoch [5245/20000], Training Loss: 0.4743\n",
            "Epoch [5246/20000], Training Loss: 0.4820\n",
            "Epoch [5247/20000], Training Loss: 0.4851\n",
            "Epoch [5248/20000], Training Loss: 0.4518\n",
            "Epoch [5249/20000], Training Loss: 0.4808\n",
            "Epoch [5250/20000], Training Loss: 0.4811\n",
            "Epoch [5251/20000], Training Loss: 0.4416\n",
            "Epoch [5252/20000], Training Loss: 0.5035\n",
            "Epoch [5253/20000], Training Loss: 0.4728\n",
            "Epoch [5254/20000], Training Loss: 0.5309\n",
            "Epoch [5255/20000], Training Loss: 0.4976\n",
            "Epoch [5256/20000], Training Loss: 0.4774\n",
            "Epoch [5257/20000], Training Loss: 0.4827\n",
            "Epoch [5258/20000], Training Loss: 0.4746\n",
            "Epoch [5259/20000], Training Loss: 0.4861\n",
            "Epoch [5260/20000], Training Loss: 0.4573\n",
            "Epoch [5261/20000], Training Loss: 0.4837\n",
            "Epoch [5262/20000], Training Loss: 0.4966\n",
            "Epoch [5263/20000], Training Loss: 0.4858\n",
            "Epoch [5264/20000], Training Loss: 0.5049\n",
            "Epoch [5265/20000], Training Loss: 0.4708\n",
            "Epoch [5266/20000], Training Loss: 0.4858\n",
            "Epoch [5267/20000], Training Loss: 0.4848\n",
            "Epoch [5268/20000], Training Loss: 0.5278\n",
            "Epoch [5269/20000], Training Loss: 0.5057\n",
            "Epoch [5270/20000], Training Loss: 0.4927\n",
            "Epoch [5271/20000], Training Loss: 0.4928\n",
            "Epoch [5272/20000], Training Loss: 0.4908\n",
            "Epoch [5273/20000], Training Loss: 0.5105\n",
            "Epoch [5274/20000], Training Loss: 0.4587\n",
            "Epoch [5275/20000], Training Loss: 0.4930\n",
            "Epoch [5276/20000], Training Loss: 0.4521\n",
            "Epoch [5277/20000], Training Loss: 0.4638\n",
            "Epoch [5278/20000], Training Loss: 0.4856\n",
            "Epoch [5279/20000], Training Loss: 0.5023\n",
            "Epoch [5280/20000], Training Loss: 0.4778\n",
            "Epoch [5281/20000], Training Loss: 0.4865\n",
            "Epoch [5282/20000], Training Loss: 0.4778\n",
            "Epoch [5283/20000], Training Loss: 0.4615\n",
            "Epoch [5284/20000], Training Loss: 0.4775\n",
            "Epoch [5285/20000], Training Loss: 0.5146\n",
            "Epoch [5286/20000], Training Loss: 0.5123\n",
            "Epoch [5287/20000], Training Loss: 0.4708\n",
            "Epoch [5288/20000], Training Loss: 0.4905\n",
            "Epoch [5289/20000], Training Loss: 0.4528\n",
            "Epoch [5290/20000], Training Loss: 0.4841\n",
            "Epoch [5291/20000], Training Loss: 0.4932\n",
            "Epoch [5292/20000], Training Loss: 0.4562\n",
            "Epoch [5293/20000], Training Loss: 0.5126\n",
            "Epoch [5294/20000], Training Loss: 0.4904\n",
            "Epoch [5295/20000], Training Loss: 0.4775\n",
            "Epoch [5296/20000], Training Loss: 0.4743\n",
            "Epoch [5297/20000], Training Loss: 0.4617\n",
            "Epoch [5298/20000], Training Loss: 0.4442\n",
            "Epoch [5299/20000], Training Loss: 0.4700\n",
            "Epoch [5300/20000], Training Loss: 0.4785\n",
            "Epoch [5301/20000], Training Loss: 0.4946\n",
            "Epoch [5302/20000], Training Loss: 0.5016\n",
            "Epoch [5303/20000], Training Loss: 0.4606\n",
            "Epoch [5304/20000], Training Loss: 0.4520\n",
            "Epoch [5305/20000], Training Loss: 0.4979\n",
            "Epoch [5306/20000], Training Loss: 0.4433\n",
            "Epoch [5307/20000], Training Loss: 0.5295\n",
            "Epoch [5308/20000], Training Loss: 0.4922\n",
            "Epoch [5309/20000], Training Loss: 0.5085\n",
            "Epoch [5310/20000], Training Loss: 0.4753\n",
            "Epoch [5311/20000], Training Loss: 0.5225\n",
            "Epoch [5312/20000], Training Loss: 0.4884\n",
            "Epoch [5313/20000], Training Loss: 0.4990\n",
            "Epoch [5314/20000], Training Loss: 0.4792\n",
            "Epoch [5315/20000], Training Loss: 0.4919\n",
            "Epoch [5316/20000], Training Loss: 0.4763\n",
            "Epoch [5317/20000], Training Loss: 0.5157\n",
            "Epoch [5318/20000], Training Loss: 0.5033\n",
            "Epoch [5319/20000], Training Loss: 0.4963\n",
            "Epoch [5320/20000], Training Loss: 0.4876\n",
            "Epoch [5321/20000], Training Loss: 0.5026\n",
            "Epoch [5322/20000], Training Loss: 0.4571\n",
            "Epoch [5323/20000], Training Loss: 0.5218\n",
            "Epoch [5324/20000], Training Loss: 0.4671\n",
            "Epoch [5325/20000], Training Loss: 0.4836\n",
            "Epoch [5326/20000], Training Loss: 0.5105\n",
            "Epoch [5327/20000], Training Loss: 0.5134\n",
            "Epoch [5328/20000], Training Loss: 0.5145\n",
            "Epoch [5329/20000], Training Loss: 0.4980\n",
            "Epoch [5330/20000], Training Loss: 0.4886\n",
            "Epoch [5331/20000], Training Loss: 0.5140\n",
            "Epoch [5332/20000], Training Loss: 0.4559\n",
            "Epoch [5333/20000], Training Loss: 0.5218\n",
            "Epoch [5334/20000], Training Loss: 0.4954\n",
            "Epoch [5335/20000], Training Loss: 0.4791\n",
            "Epoch [5336/20000], Training Loss: 0.4554\n",
            "Epoch [5337/20000], Training Loss: 0.5043\n",
            "Epoch [5338/20000], Training Loss: 0.5053\n",
            "Epoch [5339/20000], Training Loss: 0.5373\n",
            "Epoch [5340/20000], Training Loss: 0.5326\n",
            "Epoch [5341/20000], Training Loss: 0.5183\n",
            "Epoch [5342/20000], Training Loss: 0.4675\n",
            "Epoch [5343/20000], Training Loss: 0.4928\n",
            "Epoch [5344/20000], Training Loss: 0.4613\n",
            "Epoch [5345/20000], Training Loss: 0.4942\n",
            "Epoch [5346/20000], Training Loss: 0.5353\n",
            "Epoch [5347/20000], Training Loss: 0.4939\n",
            "Epoch [5348/20000], Training Loss: 0.5293\n",
            "Epoch [5349/20000], Training Loss: 0.5014\n",
            "Epoch [5350/20000], Training Loss: 0.4819\n",
            "Epoch [5351/20000], Training Loss: 0.5407\n",
            "Epoch [5352/20000], Training Loss: 0.5308\n",
            "Epoch [5353/20000], Training Loss: 0.5135\n",
            "Epoch [5354/20000], Training Loss: 0.4758\n",
            "Epoch [5355/20000], Training Loss: 0.4887\n",
            "Epoch [5356/20000], Training Loss: 0.4880\n",
            "Epoch [5357/20000], Training Loss: 0.5115\n",
            "Epoch [5358/20000], Training Loss: 0.5519\n",
            "Epoch [5359/20000], Training Loss: 0.5015\n",
            "Epoch [5360/20000], Training Loss: 0.4702\n",
            "Epoch [5361/20000], Training Loss: 0.5447\n",
            "Epoch [5362/20000], Training Loss: 0.4934\n",
            "Epoch [5363/20000], Training Loss: 0.4508\n",
            "Epoch [5364/20000], Training Loss: 0.5145\n",
            "Epoch [5365/20000], Training Loss: 0.4720\n",
            "Epoch [5366/20000], Training Loss: 0.4920\n",
            "Epoch [5367/20000], Training Loss: 0.4659\n",
            "Epoch [5368/20000], Training Loss: 0.4772\n",
            "Epoch [5369/20000], Training Loss: 0.4569\n",
            "Epoch [5370/20000], Training Loss: 0.4854\n",
            "Epoch [5371/20000], Training Loss: 0.4513\n",
            "Epoch [5372/20000], Training Loss: 0.4838\n",
            "Epoch [5373/20000], Training Loss: 0.4917\n",
            "Epoch [5374/20000], Training Loss: 0.4896\n",
            "Epoch [5375/20000], Training Loss: 0.4427\n",
            "Epoch [5376/20000], Training Loss: 0.5185\n",
            "Epoch [5377/20000], Training Loss: 0.4689\n",
            "Epoch [5378/20000], Training Loss: 0.5099\n",
            "Epoch [5379/20000], Training Loss: 0.5190\n",
            "Epoch [5380/20000], Training Loss: 0.5215\n",
            "Epoch [5381/20000], Training Loss: 0.5080\n",
            "Epoch [5382/20000], Training Loss: 0.5110\n",
            "Epoch [5383/20000], Training Loss: 0.5108\n",
            "Epoch [5384/20000], Training Loss: 0.4936\n",
            "Epoch [5385/20000], Training Loss: 0.4990\n",
            "Epoch [5386/20000], Training Loss: 0.4834\n",
            "Epoch [5387/20000], Training Loss: 0.5173\n",
            "Epoch [5388/20000], Training Loss: 0.5194\n",
            "Epoch [5389/20000], Training Loss: 0.4626\n",
            "Epoch [5390/20000], Training Loss: 0.4971\n",
            "Epoch [5391/20000], Training Loss: 0.5010\n",
            "Epoch [5392/20000], Training Loss: 0.5224\n",
            "Epoch [5393/20000], Training Loss: 0.4560\n",
            "Epoch [5394/20000], Training Loss: 0.4956\n",
            "Epoch [5395/20000], Training Loss: 0.4562\n",
            "Epoch [5396/20000], Training Loss: 0.4384\n",
            "Epoch [5397/20000], Training Loss: 0.5093\n",
            "Epoch [5398/20000], Training Loss: 0.5358\n",
            "Epoch [5399/20000], Training Loss: 0.5167\n",
            "Epoch [5400/20000], Training Loss: 0.4625\n",
            "Epoch [5401/20000], Training Loss: 0.4580\n",
            "Epoch [5402/20000], Training Loss: 0.5164\n",
            "Epoch [5403/20000], Training Loss: 0.5118\n",
            "Epoch [5404/20000], Training Loss: 0.4508\n",
            "Epoch [5405/20000], Training Loss: 0.4512\n",
            "Epoch [5406/20000], Training Loss: 0.4872\n",
            "Epoch [5407/20000], Training Loss: 0.5020\n",
            "Epoch [5408/20000], Training Loss: 0.5092\n",
            "Epoch [5409/20000], Training Loss: 0.4727\n",
            "Epoch [5410/20000], Training Loss: 0.4695\n",
            "Epoch [5411/20000], Training Loss: 0.4983\n",
            "Epoch [5412/20000], Training Loss: 0.4980\n",
            "Epoch [5413/20000], Training Loss: 0.5166\n",
            "Epoch [5414/20000], Training Loss: 0.5023\n",
            "Epoch [5415/20000], Training Loss: 0.5363\n",
            "Epoch [5416/20000], Training Loss: 0.4877\n",
            "Epoch [5417/20000], Training Loss: 0.4830\n",
            "Epoch [5418/20000], Training Loss: 0.4508\n",
            "Epoch [5419/20000], Training Loss: 0.4599\n",
            "Epoch [5420/20000], Training Loss: 0.4423\n",
            "Epoch [5421/20000], Training Loss: 0.4266\n",
            "Epoch [5422/20000], Training Loss: 0.5179\n",
            "Epoch [5423/20000], Training Loss: 0.4949\n",
            "Epoch [5424/20000], Training Loss: 0.5087\n",
            "Epoch [5425/20000], Training Loss: 0.4836\n",
            "Epoch [5426/20000], Training Loss: 0.4823\n",
            "Epoch [5427/20000], Training Loss: 0.4790\n",
            "Epoch [5428/20000], Training Loss: 0.5111\n",
            "Epoch [5429/20000], Training Loss: 0.4771\n",
            "Epoch [5430/20000], Training Loss: 0.5134\n",
            "Epoch [5431/20000], Training Loss: 0.4587\n",
            "Epoch [5432/20000], Training Loss: 0.5035\n",
            "Epoch [5433/20000], Training Loss: 0.4956\n",
            "Epoch [5434/20000], Training Loss: 0.4675\n",
            "Epoch [5435/20000], Training Loss: 0.4786\n",
            "Epoch [5436/20000], Training Loss: 0.4678\n",
            "Epoch [5437/20000], Training Loss: 0.4508\n",
            "Epoch [5438/20000], Training Loss: 0.4865\n",
            "Epoch [5439/20000], Training Loss: 0.4591\n",
            "Epoch [5440/20000], Training Loss: 0.4545\n",
            "Epoch [5441/20000], Training Loss: 0.4777\n",
            "Epoch [5442/20000], Training Loss: 0.4891\n",
            "Epoch [5443/20000], Training Loss: 0.4904\n",
            "Epoch [5444/20000], Training Loss: 0.4682\n",
            "Epoch [5445/20000], Training Loss: 0.5176\n",
            "Epoch [5446/20000], Training Loss: 0.5110\n",
            "Epoch [5447/20000], Training Loss: 0.4753\n",
            "Epoch [5448/20000], Training Loss: 0.4891\n",
            "Epoch [5449/20000], Training Loss: 0.4774\n",
            "Epoch [5450/20000], Training Loss: 0.4541\n",
            "Epoch [5451/20000], Training Loss: 0.4930\n",
            "Epoch [5452/20000], Training Loss: 0.4916\n",
            "Epoch [5453/20000], Training Loss: 0.4716\n",
            "Epoch [5454/20000], Training Loss: 0.5377\n",
            "Epoch [5455/20000], Training Loss: 0.4963\n",
            "Epoch [5456/20000], Training Loss: 0.4628\n",
            "Epoch [5457/20000], Training Loss: 0.4702\n",
            "Epoch [5458/20000], Training Loss: 0.5071\n",
            "Epoch [5459/20000], Training Loss: 0.5099\n",
            "Epoch [5460/20000], Training Loss: 0.5263\n",
            "Epoch [5461/20000], Training Loss: 0.4966\n",
            "Epoch [5462/20000], Training Loss: 0.4861\n",
            "Epoch [5463/20000], Training Loss: 0.4880\n",
            "Epoch [5464/20000], Training Loss: 0.4860\n",
            "Epoch [5465/20000], Training Loss: 0.4846\n",
            "Epoch [5466/20000], Training Loss: 0.5042\n",
            "Epoch [5467/20000], Training Loss: 0.4802\n",
            "Epoch [5468/20000], Training Loss: 0.4649\n",
            "Epoch [5469/20000], Training Loss: 0.4607\n",
            "Epoch [5470/20000], Training Loss: 0.4890\n",
            "Epoch [5471/20000], Training Loss: 0.4770\n",
            "Epoch [5472/20000], Training Loss: 0.5033\n",
            "Epoch [5473/20000], Training Loss: 0.4599\n",
            "Epoch [5474/20000], Training Loss: 0.4749\n",
            "Epoch [5475/20000], Training Loss: 0.4822\n",
            "Epoch [5476/20000], Training Loss: 0.5511\n",
            "Epoch [5477/20000], Training Loss: 0.5033\n",
            "Epoch [5478/20000], Training Loss: 0.4644\n",
            "Epoch [5479/20000], Training Loss: 0.4442\n",
            "Epoch [5480/20000], Training Loss: 0.4689\n",
            "Epoch [5481/20000], Training Loss: 0.5335\n",
            "Epoch [5482/20000], Training Loss: 0.5440\n",
            "Epoch [5483/20000], Training Loss: 0.4680\n",
            "Epoch [5484/20000], Training Loss: 0.5477\n",
            "Epoch [5485/20000], Training Loss: 0.4704\n",
            "Epoch [5486/20000], Training Loss: 0.4939\n",
            "Epoch [5487/20000], Training Loss: 0.4530\n",
            "Epoch [5488/20000], Training Loss: 0.4938\n",
            "Epoch [5489/20000], Training Loss: 0.4734\n",
            "Epoch [5490/20000], Training Loss: 0.4680\n",
            "Epoch [5491/20000], Training Loss: 0.4698\n",
            "Epoch [5492/20000], Training Loss: 0.5077\n",
            "Epoch [5493/20000], Training Loss: 0.4720\n",
            "Epoch [5494/20000], Training Loss: 0.4764\n",
            "Epoch [5495/20000], Training Loss: 0.4510\n",
            "Epoch [5496/20000], Training Loss: 0.5044\n",
            "Epoch [5497/20000], Training Loss: 0.4882\n",
            "Epoch [5498/20000], Training Loss: 0.4616\n",
            "Epoch [5499/20000], Training Loss: 0.4579\n",
            "Epoch [5500/20000], Training Loss: 0.4636\n",
            "Epoch [5501/20000], Training Loss: 0.5136\n",
            "Epoch [5502/20000], Training Loss: 0.5306\n",
            "Epoch [5503/20000], Training Loss: 0.4913\n",
            "Epoch [5504/20000], Training Loss: 0.4541\n",
            "Epoch [5505/20000], Training Loss: 0.5232\n",
            "Epoch [5506/20000], Training Loss: 0.4664\n",
            "Epoch [5507/20000], Training Loss: 0.5454\n",
            "Epoch [5508/20000], Training Loss: 0.4923\n",
            "Epoch [5509/20000], Training Loss: 0.5091\n",
            "Epoch [5510/20000], Training Loss: 0.4345\n",
            "Epoch [5511/20000], Training Loss: 0.4950\n",
            "Epoch [5512/20000], Training Loss: 0.4638\n",
            "Epoch [5513/20000], Training Loss: 0.4789\n",
            "Epoch [5514/20000], Training Loss: 0.5051\n",
            "Epoch [5515/20000], Training Loss: 0.5057\n",
            "Epoch [5516/20000], Training Loss: 0.4721\n",
            "Epoch [5517/20000], Training Loss: 0.4647\n",
            "Epoch [5518/20000], Training Loss: 0.4426\n",
            "Epoch [5519/20000], Training Loss: 0.4991\n",
            "Epoch [5520/20000], Training Loss: 0.4719\n",
            "Epoch [5521/20000], Training Loss: 0.4922\n",
            "Epoch [5522/20000], Training Loss: 0.5044\n",
            "Epoch [5523/20000], Training Loss: 0.4785\n",
            "Epoch [5524/20000], Training Loss: 0.4780\n",
            "Epoch [5525/20000], Training Loss: 0.4852\n",
            "Epoch [5526/20000], Training Loss: 0.4373\n",
            "Epoch [5527/20000], Training Loss: 0.4615\n",
            "Epoch [5528/20000], Training Loss: 0.4814\n",
            "Epoch [5529/20000], Training Loss: 0.4677\n",
            "Epoch [5530/20000], Training Loss: 0.4406\n",
            "Epoch [5531/20000], Training Loss: 0.5183\n",
            "Epoch [5532/20000], Training Loss: 0.5135\n",
            "Epoch [5533/20000], Training Loss: 0.4676\n",
            "Epoch [5534/20000], Training Loss: 0.4705\n",
            "Epoch [5535/20000], Training Loss: 0.4966\n",
            "Epoch [5536/20000], Training Loss: 0.5116\n",
            "Epoch [5537/20000], Training Loss: 0.5482\n",
            "Epoch [5538/20000], Training Loss: 0.4679\n",
            "Epoch [5539/20000], Training Loss: 0.4589\n",
            "Epoch [5540/20000], Training Loss: 0.4550\n",
            "Epoch [5541/20000], Training Loss: 0.4699\n",
            "Epoch [5542/20000], Training Loss: 0.5002\n",
            "Epoch [5543/20000], Training Loss: 0.5237\n",
            "Epoch [5544/20000], Training Loss: 0.5299\n",
            "Epoch [5545/20000], Training Loss: 0.5128\n",
            "Epoch [5546/20000], Training Loss: 0.4848\n",
            "Epoch [5547/20000], Training Loss: 0.4673\n",
            "Epoch [5548/20000], Training Loss: 0.5141\n",
            "Epoch [5549/20000], Training Loss: 0.5299\n",
            "Epoch [5550/20000], Training Loss: 0.5014\n",
            "Epoch [5551/20000], Training Loss: 0.4895\n",
            "Epoch [5552/20000], Training Loss: 0.4527\n",
            "Epoch [5553/20000], Training Loss: 0.4776\n",
            "Epoch [5554/20000], Training Loss: 0.4699\n",
            "Epoch [5555/20000], Training Loss: 0.4908\n",
            "Epoch [5556/20000], Training Loss: 0.5138\n",
            "Epoch [5557/20000], Training Loss: 0.4911\n",
            "Epoch [5558/20000], Training Loss: 0.4612\n",
            "Epoch [5559/20000], Training Loss: 0.4883\n",
            "Epoch [5560/20000], Training Loss: 0.4844\n",
            "Epoch [5561/20000], Training Loss: 0.4758\n",
            "Epoch [5562/20000], Training Loss: 0.5352\n",
            "Epoch [5563/20000], Training Loss: 0.4925\n",
            "Epoch [5564/20000], Training Loss: 0.4500\n",
            "Epoch [5565/20000], Training Loss: 0.5081\n",
            "Epoch [5566/20000], Training Loss: 0.4767\n",
            "Epoch [5567/20000], Training Loss: 0.4806\n",
            "Epoch [5568/20000], Training Loss: 0.5192\n",
            "Epoch [5569/20000], Training Loss: 0.4884\n",
            "Epoch [5570/20000], Training Loss: 0.4882\n",
            "Epoch [5571/20000], Training Loss: 0.5153\n",
            "Epoch [5572/20000], Training Loss: 0.5009\n",
            "Epoch [5573/20000], Training Loss: 0.4703\n",
            "Epoch [5574/20000], Training Loss: 0.4687\n",
            "Epoch [5575/20000], Training Loss: 0.5031\n",
            "Epoch [5576/20000], Training Loss: 0.4747\n",
            "Epoch [5577/20000], Training Loss: 0.5151\n",
            "Epoch [5578/20000], Training Loss: 0.5023\n",
            "Epoch [5579/20000], Training Loss: 0.4689\n",
            "Epoch [5580/20000], Training Loss: 0.4555\n",
            "Epoch [5581/20000], Training Loss: 0.4843\n",
            "Epoch [5582/20000], Training Loss: 0.5091\n",
            "Epoch [5583/20000], Training Loss: 0.4727\n",
            "Epoch [5584/20000], Training Loss: 0.4961\n",
            "Epoch [5585/20000], Training Loss: 0.4327\n",
            "Epoch [5586/20000], Training Loss: 0.4942\n",
            "Epoch [5587/20000], Training Loss: 0.4661\n",
            "Epoch [5588/20000], Training Loss: 0.4729\n",
            "Epoch [5589/20000], Training Loss: 0.5090\n",
            "Epoch [5590/20000], Training Loss: 0.4387\n",
            "Epoch [5591/20000], Training Loss: 0.5218\n",
            "Epoch [5592/20000], Training Loss: 0.5033\n",
            "Epoch [5593/20000], Training Loss: 0.4637\n",
            "Epoch [5594/20000], Training Loss: 0.4556\n",
            "Epoch [5595/20000], Training Loss: 0.4666\n",
            "Epoch [5596/20000], Training Loss: 0.4595\n",
            "Epoch [5597/20000], Training Loss: 0.4692\n",
            "Epoch [5598/20000], Training Loss: 0.4799\n",
            "Epoch [5599/20000], Training Loss: 0.4672\n",
            "Epoch [5600/20000], Training Loss: 0.5064\n",
            "Epoch [5601/20000], Training Loss: 0.4240\n",
            "Epoch [5602/20000], Training Loss: 0.4684\n",
            "Epoch [5603/20000], Training Loss: 0.4943\n",
            "Epoch [5604/20000], Training Loss: 0.5326\n",
            "Epoch [5605/20000], Training Loss: 0.4498\n",
            "Epoch [5606/20000], Training Loss: 0.4685\n",
            "Epoch [5607/20000], Training Loss: 0.5253\n",
            "Epoch [5608/20000], Training Loss: 0.5187\n",
            "Epoch [5609/20000], Training Loss: 0.4874\n",
            "Epoch [5610/20000], Training Loss: 0.4665\n",
            "Epoch [5611/20000], Training Loss: 0.4855\n",
            "Epoch [5612/20000], Training Loss: 0.4808\n",
            "Epoch [5613/20000], Training Loss: 0.4557\n",
            "Epoch [5614/20000], Training Loss: 0.4944\n",
            "Epoch [5615/20000], Training Loss: 0.4570\n",
            "Epoch [5616/20000], Training Loss: 0.4596\n",
            "Epoch [5617/20000], Training Loss: 0.5015\n",
            "Epoch [5618/20000], Training Loss: 0.4921\n",
            "Epoch [5619/20000], Training Loss: 0.4845\n",
            "Epoch [5620/20000], Training Loss: 0.4884\n",
            "Epoch [5621/20000], Training Loss: 0.4854\n",
            "Epoch [5622/20000], Training Loss: 0.4704\n",
            "Epoch [5623/20000], Training Loss: 0.5057\n",
            "Epoch [5624/20000], Training Loss: 0.4663\n",
            "Epoch [5625/20000], Training Loss: 0.4979\n",
            "Epoch [5626/20000], Training Loss: 0.4533\n",
            "Epoch [5627/20000], Training Loss: 0.5043\n",
            "Epoch [5628/20000], Training Loss: 0.4720\n",
            "Epoch [5629/20000], Training Loss: 0.5076\n",
            "Epoch [5630/20000], Training Loss: 0.4855\n",
            "Epoch [5631/20000], Training Loss: 0.4662\n",
            "Epoch [5632/20000], Training Loss: 0.4700\n",
            "Epoch [5633/20000], Training Loss: 0.4722\n",
            "Epoch [5634/20000], Training Loss: 0.5166\n",
            "Epoch [5635/20000], Training Loss: 0.5079\n",
            "Epoch [5636/20000], Training Loss: 0.5007\n",
            "Epoch [5637/20000], Training Loss: 0.4628\n",
            "Epoch [5638/20000], Training Loss: 0.4879\n",
            "Epoch [5639/20000], Training Loss: 0.4816\n",
            "Epoch [5640/20000], Training Loss: 0.5342\n",
            "Epoch [5641/20000], Training Loss: 0.5426\n",
            "Epoch [5642/20000], Training Loss: 0.4775\n",
            "Epoch [5643/20000], Training Loss: 0.4712\n",
            "Epoch [5644/20000], Training Loss: 0.4772\n",
            "Epoch [5645/20000], Training Loss: 0.4993\n",
            "Epoch [5646/20000], Training Loss: 0.4741\n",
            "Epoch [5647/20000], Training Loss: 0.5084\n",
            "Epoch [5648/20000], Training Loss: 0.5047\n",
            "Epoch [5649/20000], Training Loss: 0.4529\n",
            "Epoch [5650/20000], Training Loss: 0.4527\n",
            "Epoch [5651/20000], Training Loss: 0.5041\n",
            "Epoch [5652/20000], Training Loss: 0.5212\n",
            "Epoch [5653/20000], Training Loss: 0.4572\n",
            "Epoch [5654/20000], Training Loss: 0.4331\n",
            "Epoch [5655/20000], Training Loss: 0.4683\n",
            "Epoch [5656/20000], Training Loss: 0.4751\n",
            "Epoch [5657/20000], Training Loss: 0.5009\n",
            "Epoch [5658/20000], Training Loss: 0.4814\n",
            "Epoch [5659/20000], Training Loss: 0.4858\n",
            "Epoch [5660/20000], Training Loss: 0.4683\n",
            "Epoch [5661/20000], Training Loss: 0.4595\n",
            "Epoch [5662/20000], Training Loss: 0.4905\n",
            "Epoch [5663/20000], Training Loss: 0.5141\n",
            "Epoch [5664/20000], Training Loss: 0.5040\n",
            "Epoch [5665/20000], Training Loss: 0.4770\n",
            "Epoch [5666/20000], Training Loss: 0.5143\n",
            "Epoch [5667/20000], Training Loss: 0.4947\n",
            "Epoch [5668/20000], Training Loss: 0.4650\n",
            "Epoch [5669/20000], Training Loss: 0.4745\n",
            "Epoch [5670/20000], Training Loss: 0.5233\n",
            "Epoch [5671/20000], Training Loss: 0.4816\n",
            "Epoch [5672/20000], Training Loss: 0.4525\n",
            "Epoch [5673/20000], Training Loss: 0.5023\n",
            "Epoch [5674/20000], Training Loss: 0.4407\n",
            "Epoch [5675/20000], Training Loss: 0.4808\n",
            "Epoch [5676/20000], Training Loss: 0.4691\n",
            "Epoch [5677/20000], Training Loss: 0.4362\n",
            "Epoch [5678/20000], Training Loss: 0.5263\n",
            "Epoch [5679/20000], Training Loss: 0.4610\n",
            "Epoch [5680/20000], Training Loss: 0.4512\n",
            "Epoch [5681/20000], Training Loss: 0.4753\n",
            "Epoch [5682/20000], Training Loss: 0.5509\n",
            "Epoch [5683/20000], Training Loss: 0.4947\n",
            "Epoch [5684/20000], Training Loss: 0.5081\n",
            "Epoch [5685/20000], Training Loss: 0.4888\n",
            "Epoch [5686/20000], Training Loss: 0.4988\n",
            "Epoch [5687/20000], Training Loss: 0.5167\n",
            "Epoch [5688/20000], Training Loss: 0.4965\n",
            "Epoch [5689/20000], Training Loss: 0.4868\n",
            "Epoch [5690/20000], Training Loss: 0.5403\n",
            "Epoch [5691/20000], Training Loss: 0.4980\n",
            "Epoch [5692/20000], Training Loss: 0.4515\n",
            "Epoch [5693/20000], Training Loss: 0.4908\n",
            "Epoch [5694/20000], Training Loss: 0.4621\n",
            "Epoch [5695/20000], Training Loss: 0.4379\n",
            "Epoch [5696/20000], Training Loss: 0.4822\n",
            "Epoch [5697/20000], Training Loss: 0.4770\n",
            "Epoch [5698/20000], Training Loss: 0.5226\n",
            "Epoch [5699/20000], Training Loss: 0.4536\n",
            "Epoch [5700/20000], Training Loss: 0.4819\n",
            "Epoch [5701/20000], Training Loss: 0.4825\n",
            "Epoch [5702/20000], Training Loss: 0.4541\n",
            "Epoch [5703/20000], Training Loss: 0.4678\n",
            "Epoch [5704/20000], Training Loss: 0.4731\n",
            "Epoch [5705/20000], Training Loss: 0.5101\n",
            "Epoch [5706/20000], Training Loss: 0.5310\n",
            "Epoch [5707/20000], Training Loss: 0.4881\n",
            "Epoch [5708/20000], Training Loss: 0.4852\n",
            "Epoch [5709/20000], Training Loss: 0.4456\n",
            "Epoch [5710/20000], Training Loss: 0.4608\n",
            "Epoch [5711/20000], Training Loss: 0.5306\n",
            "Epoch [5712/20000], Training Loss: 0.4535\n",
            "Epoch [5713/20000], Training Loss: 0.5251\n",
            "Epoch [5714/20000], Training Loss: 0.4681\n",
            "Epoch [5715/20000], Training Loss: 0.5048\n",
            "Epoch [5716/20000], Training Loss: 0.4764\n",
            "Epoch [5717/20000], Training Loss: 0.4832\n",
            "Epoch [5718/20000], Training Loss: 0.4737\n",
            "Epoch [5719/20000], Training Loss: 0.4250\n",
            "Epoch [5720/20000], Training Loss: 0.4639\n",
            "Epoch [5721/20000], Training Loss: 0.5105\n",
            "Epoch [5722/20000], Training Loss: 0.4795\n",
            "Epoch [5723/20000], Training Loss: 0.4540\n",
            "Epoch [5724/20000], Training Loss: 0.4893\n",
            "Epoch [5725/20000], Training Loss: 0.4554\n",
            "Epoch [5726/20000], Training Loss: 0.5249\n",
            "Epoch [5727/20000], Training Loss: 0.5263\n",
            "Epoch [5728/20000], Training Loss: 0.5217\n",
            "Epoch [5729/20000], Training Loss: 0.4858\n",
            "Epoch [5730/20000], Training Loss: 0.4780\n",
            "Epoch [5731/20000], Training Loss: 0.5313\n",
            "Epoch [5732/20000], Training Loss: 0.4777\n",
            "Epoch [5733/20000], Training Loss: 0.4815\n",
            "Epoch [5734/20000], Training Loss: 0.5254\n",
            "Epoch [5735/20000], Training Loss: 0.4975\n",
            "Epoch [5736/20000], Training Loss: 0.5327\n",
            "Epoch [5737/20000], Training Loss: 0.5050\n",
            "Epoch [5738/20000], Training Loss: 0.4928\n",
            "Epoch [5739/20000], Training Loss: 0.5076\n",
            "Epoch [5740/20000], Training Loss: 0.4983\n",
            "Epoch [5741/20000], Training Loss: 0.4863\n",
            "Epoch [5742/20000], Training Loss: 0.4448\n",
            "Epoch [5743/20000], Training Loss: 0.4646\n",
            "Epoch [5744/20000], Training Loss: 0.4835\n",
            "Epoch [5745/20000], Training Loss: 0.4668\n",
            "Epoch [5746/20000], Training Loss: 0.4690\n",
            "Epoch [5747/20000], Training Loss: 0.5191\n",
            "Epoch [5748/20000], Training Loss: 0.5151\n",
            "Epoch [5749/20000], Training Loss: 0.5010\n",
            "Epoch [5750/20000], Training Loss: 0.4796\n",
            "Epoch [5751/20000], Training Loss: 0.4843\n",
            "Epoch [5752/20000], Training Loss: 0.4633\n",
            "Epoch [5753/20000], Training Loss: 0.4670\n",
            "Epoch [5754/20000], Training Loss: 0.4466\n",
            "Epoch [5755/20000], Training Loss: 0.4955\n",
            "Epoch [5756/20000], Training Loss: 0.4580\n",
            "Epoch [5757/20000], Training Loss: 0.4878\n",
            "Epoch [5758/20000], Training Loss: 0.5255\n",
            "Epoch [5759/20000], Training Loss: 0.4908\n",
            "Epoch [5760/20000], Training Loss: 0.4879\n",
            "Epoch [5761/20000], Training Loss: 0.4600\n",
            "Epoch [5762/20000], Training Loss: 0.5124\n",
            "Epoch [5763/20000], Training Loss: 0.4891\n",
            "Epoch [5764/20000], Training Loss: 0.4649\n",
            "Epoch [5765/20000], Training Loss: 0.4771\n",
            "Epoch [5766/20000], Training Loss: 0.5229\n",
            "Epoch [5767/20000], Training Loss: 0.4835\n",
            "Epoch [5768/20000], Training Loss: 0.5237\n",
            "Epoch [5769/20000], Training Loss: 0.4870\n",
            "Epoch [5770/20000], Training Loss: 0.5006\n",
            "Epoch [5771/20000], Training Loss: 0.5058\n",
            "Epoch [5772/20000], Training Loss: 0.5339\n",
            "Epoch [5773/20000], Training Loss: 0.5068\n",
            "Epoch [5774/20000], Training Loss: 0.4455\n",
            "Epoch [5775/20000], Training Loss: 0.4667\n",
            "Epoch [5776/20000], Training Loss: 0.5006\n",
            "Epoch [5777/20000], Training Loss: 0.4648\n",
            "Epoch [5778/20000], Training Loss: 0.4357\n",
            "Epoch [5779/20000], Training Loss: 0.4743\n",
            "Epoch [5780/20000], Training Loss: 0.4743\n",
            "Epoch [5781/20000], Training Loss: 0.4757\n",
            "Epoch [5782/20000], Training Loss: 0.5020\n",
            "Epoch [5783/20000], Training Loss: 0.4740\n",
            "Epoch [5784/20000], Training Loss: 0.4611\n",
            "Epoch [5785/20000], Training Loss: 0.4621\n",
            "Epoch [5786/20000], Training Loss: 0.4693\n",
            "Epoch [5787/20000], Training Loss: 0.4906\n",
            "Epoch [5788/20000], Training Loss: 0.4913\n",
            "Epoch [5789/20000], Training Loss: 0.4507\n",
            "Epoch [5790/20000], Training Loss: 0.4744\n",
            "Epoch [5791/20000], Training Loss: 0.4697\n",
            "Epoch [5792/20000], Training Loss: 0.4785\n",
            "Epoch [5793/20000], Training Loss: 0.5146\n",
            "Epoch [5794/20000], Training Loss: 0.5541\n",
            "Epoch [5795/20000], Training Loss: 0.4478\n",
            "Epoch [5796/20000], Training Loss: 0.4418\n",
            "Epoch [5797/20000], Training Loss: 0.4990\n",
            "Epoch [5798/20000], Training Loss: 0.5050\n",
            "Epoch [5799/20000], Training Loss: 0.4562\n",
            "Epoch [5800/20000], Training Loss: 0.5312\n",
            "Epoch [5801/20000], Training Loss: 0.4519\n",
            "Epoch [5802/20000], Training Loss: 0.5000\n",
            "Epoch [5803/20000], Training Loss: 0.5141\n",
            "Epoch [5804/20000], Training Loss: 0.5074\n",
            "Epoch [5805/20000], Training Loss: 0.4559\n",
            "Epoch [5806/20000], Training Loss: 0.5068\n",
            "Epoch [5807/20000], Training Loss: 0.4725\n",
            "Epoch [5808/20000], Training Loss: 0.4569\n",
            "Epoch [5809/20000], Training Loss: 0.4910\n",
            "Epoch [5810/20000], Training Loss: 0.4901\n",
            "Epoch [5811/20000], Training Loss: 0.4858\n",
            "Epoch [5812/20000], Training Loss: 0.4838\n",
            "Epoch [5813/20000], Training Loss: 0.4840\n",
            "Epoch [5814/20000], Training Loss: 0.5019\n",
            "Epoch [5815/20000], Training Loss: 0.4823\n",
            "Epoch [5816/20000], Training Loss: 0.5226\n",
            "Epoch [5817/20000], Training Loss: 0.4497\n",
            "Epoch [5818/20000], Training Loss: 0.4737\n",
            "Epoch [5819/20000], Training Loss: 0.4639\n",
            "Epoch [5820/20000], Training Loss: 0.4890\n",
            "Epoch [5821/20000], Training Loss: 0.4913\n",
            "Epoch [5822/20000], Training Loss: 0.4477\n",
            "Epoch [5823/20000], Training Loss: 0.4522\n",
            "Epoch [5824/20000], Training Loss: 0.4590\n",
            "Epoch [5825/20000], Training Loss: 0.4599\n",
            "Epoch [5826/20000], Training Loss: 0.4908\n",
            "Epoch [5827/20000], Training Loss: 0.4908\n",
            "Epoch [5828/20000], Training Loss: 0.4861\n",
            "Epoch [5829/20000], Training Loss: 0.4601\n",
            "Epoch [5830/20000], Training Loss: 0.5082\n",
            "Epoch [5831/20000], Training Loss: 0.4883\n",
            "Epoch [5832/20000], Training Loss: 0.4829\n",
            "Epoch [5833/20000], Training Loss: 0.4908\n",
            "Epoch [5834/20000], Training Loss: 0.5338\n",
            "Epoch [5835/20000], Training Loss: 0.4873\n",
            "Epoch [5836/20000], Training Loss: 0.5265\n",
            "Epoch [5837/20000], Training Loss: 0.4730\n",
            "Epoch [5838/20000], Training Loss: 0.5014\n",
            "Epoch [5839/20000], Training Loss: 0.4639\n",
            "Epoch [5840/20000], Training Loss: 0.4813\n",
            "Epoch [5841/20000], Training Loss: 0.4922\n",
            "Epoch [5842/20000], Training Loss: 0.5160\n",
            "Epoch [5843/20000], Training Loss: 0.4987\n",
            "Epoch [5844/20000], Training Loss: 0.4969\n",
            "Epoch [5845/20000], Training Loss: 0.4653\n",
            "Epoch [5846/20000], Training Loss: 0.4523\n",
            "Epoch [5847/20000], Training Loss: 0.4729\n",
            "Epoch [5848/20000], Training Loss: 0.4682\n",
            "Epoch [5849/20000], Training Loss: 0.4935\n",
            "Epoch [5850/20000], Training Loss: 0.4857\n",
            "Epoch [5851/20000], Training Loss: 0.4713\n",
            "Epoch [5852/20000], Training Loss: 0.4870\n",
            "Epoch [5853/20000], Training Loss: 0.4737\n",
            "Epoch [5854/20000], Training Loss: 0.4792\n",
            "Epoch [5855/20000], Training Loss: 0.4456\n",
            "Epoch [5856/20000], Training Loss: 0.4426\n",
            "Epoch [5857/20000], Training Loss: 0.5221\n",
            "Epoch [5858/20000], Training Loss: 0.5333\n",
            "Epoch [5859/20000], Training Loss: 0.5175\n",
            "Epoch [5860/20000], Training Loss: 0.4517\n",
            "Epoch [5861/20000], Training Loss: 0.4525\n",
            "Epoch [5862/20000], Training Loss: 0.4585\n",
            "Epoch [5863/20000], Training Loss: 0.5168\n",
            "Epoch [5864/20000], Training Loss: 0.4549\n",
            "Epoch [5865/20000], Training Loss: 0.4728\n",
            "Epoch [5866/20000], Training Loss: 0.4654\n",
            "Epoch [5867/20000], Training Loss: 0.4647\n",
            "Epoch [5868/20000], Training Loss: 0.5013\n",
            "Epoch [5869/20000], Training Loss: 0.4756\n",
            "Epoch [5870/20000], Training Loss: 0.4728\n",
            "Epoch [5871/20000], Training Loss: 0.4598\n",
            "Epoch [5872/20000], Training Loss: 0.5033\n",
            "Epoch [5873/20000], Training Loss: 0.4820\n",
            "Epoch [5874/20000], Training Loss: 0.4731\n",
            "Epoch [5875/20000], Training Loss: 0.5012\n",
            "Epoch [5876/20000], Training Loss: 0.4528\n",
            "Epoch [5877/20000], Training Loss: 0.4536\n",
            "Epoch [5878/20000], Training Loss: 0.4967\n",
            "Epoch [5879/20000], Training Loss: 0.4668\n",
            "Epoch [5880/20000], Training Loss: 0.5003\n",
            "Epoch [5881/20000], Training Loss: 0.4685\n",
            "Epoch [5882/20000], Training Loss: 0.4792\n",
            "Epoch [5883/20000], Training Loss: 0.4722\n",
            "Epoch [5884/20000], Training Loss: 0.4812\n",
            "Epoch [5885/20000], Training Loss: 0.5035\n",
            "Epoch [5886/20000], Training Loss: 0.4524\n",
            "Epoch [5887/20000], Training Loss: 0.5311\n",
            "Epoch [5888/20000], Training Loss: 0.5135\n",
            "Epoch [5889/20000], Training Loss: 0.4324\n",
            "Epoch [5890/20000], Training Loss: 0.4865\n",
            "Epoch [5891/20000], Training Loss: 0.4700\n",
            "Epoch [5892/20000], Training Loss: 0.4451\n",
            "Epoch [5893/20000], Training Loss: 0.4666\n",
            "Epoch [5894/20000], Training Loss: 0.4672\n",
            "Epoch [5895/20000], Training Loss: 0.4828\n",
            "Epoch [5896/20000], Training Loss: 0.4886\n",
            "Epoch [5897/20000], Training Loss: 0.5067\n",
            "Epoch [5898/20000], Training Loss: 0.4728\n",
            "Epoch [5899/20000], Training Loss: 0.4707\n",
            "Epoch [5900/20000], Training Loss: 0.4749\n",
            "Epoch [5901/20000], Training Loss: 0.5211\n",
            "Epoch [5902/20000], Training Loss: 0.5491\n",
            "Epoch [5903/20000], Training Loss: 0.5036\n",
            "Epoch [5904/20000], Training Loss: 0.4822\n",
            "Epoch [5905/20000], Training Loss: 0.4938\n",
            "Epoch [5906/20000], Training Loss: 0.4416\n",
            "Epoch [5907/20000], Training Loss: 0.5048\n",
            "Epoch [5908/20000], Training Loss: 0.4775\n",
            "Epoch [5909/20000], Training Loss: 0.4918\n",
            "Epoch [5910/20000], Training Loss: 0.5180\n",
            "Epoch [5911/20000], Training Loss: 0.4757\n",
            "Epoch [5912/20000], Training Loss: 0.4953\n",
            "Epoch [5913/20000], Training Loss: 0.4697\n",
            "Epoch [5914/20000], Training Loss: 0.4572\n",
            "Epoch [5915/20000], Training Loss: 0.4886\n",
            "Epoch [5916/20000], Training Loss: 0.5157\n",
            "Epoch [5917/20000], Training Loss: 0.4688\n",
            "Epoch [5918/20000], Training Loss: 0.4599\n",
            "Epoch [5919/20000], Training Loss: 0.4905\n",
            "Epoch [5920/20000], Training Loss: 0.4317\n",
            "Epoch [5921/20000], Training Loss: 0.4625\n",
            "Epoch [5922/20000], Training Loss: 0.4897\n",
            "Epoch [5923/20000], Training Loss: 0.4900\n",
            "Epoch [5924/20000], Training Loss: 0.4567\n",
            "Epoch [5925/20000], Training Loss: 0.5136\n",
            "Epoch [5926/20000], Training Loss: 0.4983\n",
            "Epoch [5927/20000], Training Loss: 0.4974\n",
            "Epoch [5928/20000], Training Loss: 0.4575\n",
            "Epoch [5929/20000], Training Loss: 0.5118\n",
            "Epoch [5930/20000], Training Loss: 0.4564\n",
            "Epoch [5931/20000], Training Loss: 0.4897\n",
            "Epoch [5932/20000], Training Loss: 0.4586\n",
            "Epoch [5933/20000], Training Loss: 0.4609\n",
            "Epoch [5934/20000], Training Loss: 0.4841\n",
            "Epoch [5935/20000], Training Loss: 0.4776\n",
            "Epoch [5936/20000], Training Loss: 0.5212\n",
            "Epoch [5937/20000], Training Loss: 0.4901\n",
            "Epoch [5938/20000], Training Loss: 0.4710\n",
            "Epoch [5939/20000], Training Loss: 0.4774\n",
            "Epoch [5940/20000], Training Loss: 0.5035\n",
            "Epoch [5941/20000], Training Loss: 0.4527\n",
            "Epoch [5942/20000], Training Loss: 0.4617\n",
            "Epoch [5943/20000], Training Loss: 0.4428\n",
            "Epoch [5944/20000], Training Loss: 0.5000\n",
            "Epoch [5945/20000], Training Loss: 0.5227\n",
            "Epoch [5946/20000], Training Loss: 0.5012\n",
            "Epoch [5947/20000], Training Loss: 0.4814\n",
            "Epoch [5948/20000], Training Loss: 0.4538\n",
            "Epoch [5949/20000], Training Loss: 0.5233\n",
            "Epoch [5950/20000], Training Loss: 0.4871\n",
            "Epoch [5951/20000], Training Loss: 0.4913\n",
            "Epoch [5952/20000], Training Loss: 0.4897\n",
            "Epoch [5953/20000], Training Loss: 0.4561\n",
            "Epoch [5954/20000], Training Loss: 0.5422\n",
            "Epoch [5955/20000], Training Loss: 0.4603\n",
            "Epoch [5956/20000], Training Loss: 0.4779\n",
            "Epoch [5957/20000], Training Loss: 0.5076\n",
            "Epoch [5958/20000], Training Loss: 0.5005\n",
            "Epoch [5959/20000], Training Loss: 0.4517\n",
            "Epoch [5960/20000], Training Loss: 0.5325\n",
            "Epoch [5961/20000], Training Loss: 0.4647\n",
            "Epoch [5962/20000], Training Loss: 0.4840\n",
            "Epoch [5963/20000], Training Loss: 0.4544\n",
            "Epoch [5964/20000], Training Loss: 0.4773\n",
            "Epoch [5965/20000], Training Loss: 0.4899\n",
            "Epoch [5966/20000], Training Loss: 0.5237\n",
            "Epoch [5967/20000], Training Loss: 0.5180\n",
            "Epoch [5968/20000], Training Loss: 0.4447\n",
            "Epoch [5969/20000], Training Loss: 0.4733\n",
            "Epoch [5970/20000], Training Loss: 0.4829\n",
            "Epoch [5971/20000], Training Loss: 0.4560\n",
            "Epoch [5972/20000], Training Loss: 0.4951\n",
            "Epoch [5973/20000], Training Loss: 0.5336\n",
            "Epoch [5974/20000], Training Loss: 0.4984\n",
            "Epoch [5975/20000], Training Loss: 0.5284\n",
            "Epoch [5976/20000], Training Loss: 0.4817\n",
            "Epoch [5977/20000], Training Loss: 0.4920\n",
            "Epoch [5978/20000], Training Loss: 0.4876\n",
            "Epoch [5979/20000], Training Loss: 0.4836\n",
            "Epoch [5980/20000], Training Loss: 0.5099\n",
            "Epoch [5981/20000], Training Loss: 0.4569\n",
            "Epoch [5982/20000], Training Loss: 0.4880\n",
            "Epoch [5983/20000], Training Loss: 0.5208\n",
            "Epoch [5984/20000], Training Loss: 0.4842\n",
            "Epoch [5985/20000], Training Loss: 0.4784\n",
            "Epoch [5986/20000], Training Loss: 0.5063\n",
            "Epoch [5987/20000], Training Loss: 0.4752\n",
            "Epoch [5988/20000], Training Loss: 0.5014\n",
            "Epoch [5989/20000], Training Loss: 0.4617\n",
            "Epoch [5990/20000], Training Loss: 0.4589\n",
            "Epoch [5991/20000], Training Loss: 0.4959\n",
            "Epoch [5992/20000], Training Loss: 0.4880\n",
            "Epoch [5993/20000], Training Loss: 0.4841\n",
            "Epoch [5994/20000], Training Loss: 0.5306\n",
            "Epoch [5995/20000], Training Loss: 0.5357\n",
            "Epoch [5996/20000], Training Loss: 0.5064\n",
            "Epoch [5997/20000], Training Loss: 0.5134\n",
            "Epoch [5998/20000], Training Loss: 0.4467\n",
            "Epoch [5999/20000], Training Loss: 0.4966\n",
            "Epoch [6000/20000], Training Loss: 0.4240\n",
            "Epoch [6001/20000], Training Loss: 0.4807\n",
            "Epoch [6002/20000], Training Loss: 0.5054\n",
            "Epoch [6003/20000], Training Loss: 0.4719\n",
            "Epoch [6004/20000], Training Loss: 0.4971\n",
            "Epoch [6005/20000], Training Loss: 0.5487\n",
            "Epoch [6006/20000], Training Loss: 0.4509\n",
            "Epoch [6007/20000], Training Loss: 0.4951\n",
            "Epoch [6008/20000], Training Loss: 0.5183\n",
            "Epoch [6009/20000], Training Loss: 0.4976\n",
            "Epoch [6010/20000], Training Loss: 0.4845\n",
            "Epoch [6011/20000], Training Loss: 0.4990\n",
            "Epoch [6012/20000], Training Loss: 0.4741\n",
            "Epoch [6013/20000], Training Loss: 0.5196\n",
            "Epoch [6014/20000], Training Loss: 0.4565\n",
            "Epoch [6015/20000], Training Loss: 0.4849\n",
            "Epoch [6016/20000], Training Loss: 0.4681\n",
            "Epoch [6017/20000], Training Loss: 0.4513\n",
            "Epoch [6018/20000], Training Loss: 0.5384\n",
            "Epoch [6019/20000], Training Loss: 0.4757\n",
            "Epoch [6020/20000], Training Loss: 0.4891\n",
            "Epoch [6021/20000], Training Loss: 0.4674\n",
            "Epoch [6022/20000], Training Loss: 0.4609\n",
            "Epoch [6023/20000], Training Loss: 0.5120\n",
            "Epoch [6024/20000], Training Loss: 0.4704\n",
            "Epoch [6025/20000], Training Loss: 0.5192\n",
            "Epoch [6026/20000], Training Loss: 0.4671\n",
            "Epoch [6027/20000], Training Loss: 0.4874\n",
            "Epoch [6028/20000], Training Loss: 0.4681\n",
            "Epoch [6029/20000], Training Loss: 0.4415\n",
            "Epoch [6030/20000], Training Loss: 0.5062\n",
            "Epoch [6031/20000], Training Loss: 0.4809\n",
            "Epoch [6032/20000], Training Loss: 0.5404\n",
            "Epoch [6033/20000], Training Loss: 0.5075\n",
            "Epoch [6034/20000], Training Loss: 0.4795\n",
            "Epoch [6035/20000], Training Loss: 0.5026\n",
            "Epoch [6036/20000], Training Loss: 0.4797\n",
            "Epoch [6037/20000], Training Loss: 0.4512\n",
            "Epoch [6038/20000], Training Loss: 0.4258\n",
            "Epoch [6039/20000], Training Loss: 0.4625\n",
            "Epoch [6040/20000], Training Loss: 0.4961\n",
            "Epoch [6041/20000], Training Loss: 0.5145\n",
            "Epoch [6042/20000], Training Loss: 0.4885\n",
            "Epoch [6043/20000], Training Loss: 0.5179\n",
            "Epoch [6044/20000], Training Loss: 0.4924\n",
            "Epoch [6045/20000], Training Loss: 0.5142\n",
            "Epoch [6046/20000], Training Loss: 0.4997\n",
            "Epoch [6047/20000], Training Loss: 0.4578\n",
            "Epoch [6048/20000], Training Loss: 0.5436\n",
            "Epoch [6049/20000], Training Loss: 0.4897\n",
            "Epoch [6050/20000], Training Loss: 0.4760\n",
            "Epoch [6051/20000], Training Loss: 0.4872\n",
            "Epoch [6052/20000], Training Loss: 0.5118\n",
            "Epoch [6053/20000], Training Loss: 0.5029\n",
            "Epoch [6054/20000], Training Loss: 0.5177\n",
            "Epoch [6055/20000], Training Loss: 0.4777\n",
            "Epoch [6056/20000], Training Loss: 0.4565\n",
            "Epoch [6057/20000], Training Loss: 0.4690\n",
            "Epoch [6058/20000], Training Loss: 0.5145\n",
            "Epoch [6059/20000], Training Loss: 0.4993\n",
            "Epoch [6060/20000], Training Loss: 0.4838\n",
            "Epoch [6061/20000], Training Loss: 0.5077\n",
            "Epoch [6062/20000], Training Loss: 0.5089\n",
            "Epoch [6063/20000], Training Loss: 0.4808\n",
            "Epoch [6064/20000], Training Loss: 0.4761\n",
            "Epoch [6065/20000], Training Loss: 0.4793\n",
            "Epoch [6066/20000], Training Loss: 0.4974\n",
            "Epoch [6067/20000], Training Loss: 0.5107\n",
            "Epoch [6068/20000], Training Loss: 0.5185\n",
            "Epoch [6069/20000], Training Loss: 0.4826\n",
            "Epoch [6070/20000], Training Loss: 0.5449\n",
            "Epoch [6071/20000], Training Loss: 0.4583\n",
            "Epoch [6072/20000], Training Loss: 0.5462\n",
            "Epoch [6073/20000], Training Loss: 0.4743\n",
            "Epoch [6074/20000], Training Loss: 0.5190\n",
            "Epoch [6075/20000], Training Loss: 0.5110\n",
            "Epoch [6076/20000], Training Loss: 0.4759\n",
            "Epoch [6077/20000], Training Loss: 0.4849\n",
            "Epoch [6078/20000], Training Loss: 0.4947\n",
            "Epoch [6079/20000], Training Loss: 0.4613\n",
            "Epoch [6080/20000], Training Loss: 0.4630\n",
            "Epoch [6081/20000], Training Loss: 0.4722\n",
            "Epoch [6082/20000], Training Loss: 0.4969\n",
            "Epoch [6083/20000], Training Loss: 0.4758\n",
            "Epoch [6084/20000], Training Loss: 0.5255\n",
            "Epoch [6085/20000], Training Loss: 0.4628\n",
            "Epoch [6086/20000], Training Loss: 0.4880\n",
            "Epoch [6087/20000], Training Loss: 0.4585\n",
            "Epoch [6088/20000], Training Loss: 0.4975\n",
            "Epoch [6089/20000], Training Loss: 0.5199\n",
            "Epoch [6090/20000], Training Loss: 0.4887\n",
            "Epoch [6091/20000], Training Loss: 0.5000\n",
            "Epoch [6092/20000], Training Loss: 0.4625\n",
            "Epoch [6093/20000], Training Loss: 0.4645\n",
            "Epoch [6094/20000], Training Loss: 0.5236\n",
            "Epoch [6095/20000], Training Loss: 0.4783\n",
            "Epoch [6096/20000], Training Loss: 0.5116\n",
            "Epoch [6097/20000], Training Loss: 0.5390\n",
            "Epoch [6098/20000], Training Loss: 0.5162\n",
            "Epoch [6099/20000], Training Loss: 0.4765\n",
            "Epoch [6100/20000], Training Loss: 0.4709\n",
            "Epoch [6101/20000], Training Loss: 0.4742\n",
            "Epoch [6102/20000], Training Loss: 0.5122\n",
            "Epoch [6103/20000], Training Loss: 0.5073\n",
            "Epoch [6104/20000], Training Loss: 0.4654\n",
            "Epoch [6105/20000], Training Loss: 0.4925\n",
            "Epoch [6106/20000], Training Loss: 0.5182\n",
            "Epoch [6107/20000], Training Loss: 0.5235\n",
            "Epoch [6108/20000], Training Loss: 0.4893\n",
            "Epoch [6109/20000], Training Loss: 0.4293\n",
            "Epoch [6110/20000], Training Loss: 0.5106\n",
            "Epoch [6111/20000], Training Loss: 0.5008\n",
            "Epoch [6112/20000], Training Loss: 0.5475\n",
            "Epoch [6113/20000], Training Loss: 0.5228\n",
            "Epoch [6114/20000], Training Loss: 0.4907\n",
            "Epoch [6115/20000], Training Loss: 0.4856\n",
            "Epoch [6116/20000], Training Loss: 0.4769\n",
            "Epoch [6117/20000], Training Loss: 0.4707\n",
            "Epoch [6118/20000], Training Loss: 0.4947\n",
            "Epoch [6119/20000], Training Loss: 0.5128\n",
            "Epoch [6120/20000], Training Loss: 0.4615\n",
            "Epoch [6121/20000], Training Loss: 0.4744\n",
            "Epoch [6122/20000], Training Loss: 0.5053\n",
            "Epoch [6123/20000], Training Loss: 0.4904\n",
            "Epoch [6124/20000], Training Loss: 0.4720\n",
            "Epoch [6125/20000], Training Loss: 0.4818\n",
            "Epoch [6126/20000], Training Loss: 0.4813\n",
            "Epoch [6127/20000], Training Loss: 0.4900\n",
            "Epoch [6128/20000], Training Loss: 0.4532\n",
            "Epoch [6129/20000], Training Loss: 0.5120\n",
            "Epoch [6130/20000], Training Loss: 0.4975\n",
            "Epoch [6131/20000], Training Loss: 0.5411\n",
            "Epoch [6132/20000], Training Loss: 0.4962\n",
            "Epoch [6133/20000], Training Loss: 0.4556\n",
            "Epoch [6134/20000], Training Loss: 0.5309\n",
            "Epoch [6135/20000], Training Loss: 0.4935\n",
            "Epoch [6136/20000], Training Loss: 0.4396\n",
            "Epoch [6137/20000], Training Loss: 0.4723\n",
            "Epoch [6138/20000], Training Loss: 0.4591\n",
            "Epoch [6139/20000], Training Loss: 0.4665\n",
            "Epoch [6140/20000], Training Loss: 0.5359\n",
            "Epoch [6141/20000], Training Loss: 0.4509\n",
            "Epoch [6142/20000], Training Loss: 0.4746\n",
            "Epoch [6143/20000], Training Loss: 0.4712\n",
            "Epoch [6144/20000], Training Loss: 0.5006\n",
            "Epoch [6145/20000], Training Loss: 0.5107\n",
            "Epoch [6146/20000], Training Loss: 0.4946\n",
            "Epoch [6147/20000], Training Loss: 0.4836\n",
            "Epoch [6148/20000], Training Loss: 0.4648\n",
            "Epoch [6149/20000], Training Loss: 0.5017\n",
            "Epoch [6150/20000], Training Loss: 0.5149\n",
            "Epoch [6151/20000], Training Loss: 0.4451\n",
            "Epoch [6152/20000], Training Loss: 0.4803\n",
            "Epoch [6153/20000], Training Loss: 0.4557\n",
            "Epoch [6154/20000], Training Loss: 0.5382\n",
            "Epoch [6155/20000], Training Loss: 0.4545\n",
            "Epoch [6156/20000], Training Loss: 0.4754\n",
            "Epoch [6157/20000], Training Loss: 0.4996\n",
            "Epoch [6158/20000], Training Loss: 0.5088\n",
            "Epoch [6159/20000], Training Loss: 0.4619\n",
            "Epoch [6160/20000], Training Loss: 0.4633\n",
            "Epoch [6161/20000], Training Loss: 0.5305\n",
            "Epoch [6162/20000], Training Loss: 0.4770\n",
            "Epoch [6163/20000], Training Loss: 0.4724\n",
            "Epoch [6164/20000], Training Loss: 0.4403\n",
            "Epoch [6165/20000], Training Loss: 0.4630\n",
            "Epoch [6166/20000], Training Loss: 0.4845\n",
            "Epoch [6167/20000], Training Loss: 0.4773\n",
            "Epoch [6168/20000], Training Loss: 0.4813\n",
            "Epoch [6169/20000], Training Loss: 0.4705\n",
            "Epoch [6170/20000], Training Loss: 0.4649\n",
            "Epoch [6171/20000], Training Loss: 0.4835\n",
            "Epoch [6172/20000], Training Loss: 0.4318\n",
            "Epoch [6173/20000], Training Loss: 0.4587\n",
            "Epoch [6174/20000], Training Loss: 0.4795\n",
            "Epoch [6175/20000], Training Loss: 0.4994\n",
            "Epoch [6176/20000], Training Loss: 0.4503\n",
            "Epoch [6177/20000], Training Loss: 0.4475\n",
            "Epoch [6178/20000], Training Loss: 0.4719\n",
            "Epoch [6179/20000], Training Loss: 0.4739\n",
            "Epoch [6180/20000], Training Loss: 0.4639\n",
            "Epoch [6181/20000], Training Loss: 0.4874\n",
            "Epoch [6182/20000], Training Loss: 0.4441\n",
            "Epoch [6183/20000], Training Loss: 0.4735\n",
            "Epoch [6184/20000], Training Loss: 0.4656\n",
            "Epoch [6185/20000], Training Loss: 0.4420\n",
            "Epoch [6186/20000], Training Loss: 0.5117\n",
            "Epoch [6187/20000], Training Loss: 0.5041\n",
            "Epoch [6188/20000], Training Loss: 0.4955\n",
            "Epoch [6189/20000], Training Loss: 0.4728\n",
            "Epoch [6190/20000], Training Loss: 0.4929\n",
            "Epoch [6191/20000], Training Loss: 0.5062\n",
            "Epoch [6192/20000], Training Loss: 0.4845\n",
            "Epoch [6193/20000], Training Loss: 0.4925\n",
            "Epoch [6194/20000], Training Loss: 0.4627\n",
            "Epoch [6195/20000], Training Loss: 0.5597\n",
            "Epoch [6196/20000], Training Loss: 0.5274\n",
            "Epoch [6197/20000], Training Loss: 0.5240\n",
            "Epoch [6198/20000], Training Loss: 0.4706\n",
            "Epoch [6199/20000], Training Loss: 0.4995\n",
            "Epoch [6200/20000], Training Loss: 0.4582\n",
            "Epoch [6201/20000], Training Loss: 0.4598\n",
            "Epoch [6202/20000], Training Loss: 0.4667\n",
            "Epoch [6203/20000], Training Loss: 0.4916\n",
            "Epoch [6204/20000], Training Loss: 0.4335\n",
            "Epoch [6205/20000], Training Loss: 0.5114\n",
            "Epoch [6206/20000], Training Loss: 0.4498\n",
            "Epoch [6207/20000], Training Loss: 0.5381\n",
            "Epoch [6208/20000], Training Loss: 0.4937\n",
            "Epoch [6209/20000], Training Loss: 0.4835\n",
            "Epoch [6210/20000], Training Loss: 0.5020\n",
            "Epoch [6211/20000], Training Loss: 0.5283\n",
            "Epoch [6212/20000], Training Loss: 0.5310\n",
            "Epoch [6213/20000], Training Loss: 0.4740\n",
            "Epoch [6214/20000], Training Loss: 0.4758\n",
            "Epoch [6215/20000], Training Loss: 0.5240\n",
            "Epoch [6216/20000], Training Loss: 0.4788\n",
            "Epoch [6217/20000], Training Loss: 0.5211\n",
            "Epoch [6218/20000], Training Loss: 0.5298\n",
            "Epoch [6219/20000], Training Loss: 0.4413\n",
            "Epoch [6220/20000], Training Loss: 0.4502\n",
            "Epoch [6221/20000], Training Loss: 0.4876\n",
            "Epoch [6222/20000], Training Loss: 0.5121\n",
            "Epoch [6223/20000], Training Loss: 0.5036\n",
            "Epoch [6224/20000], Training Loss: 0.4479\n",
            "Epoch [6225/20000], Training Loss: 0.5010\n",
            "Epoch [6226/20000], Training Loss: 0.5138\n",
            "Epoch [6227/20000], Training Loss: 0.4496\n",
            "Epoch [6228/20000], Training Loss: 0.5085\n",
            "Epoch [6229/20000], Training Loss: 0.5129\n",
            "Epoch [6230/20000], Training Loss: 0.5171\n",
            "Epoch [6231/20000], Training Loss: 0.4675\n",
            "Epoch [6232/20000], Training Loss: 0.5115\n",
            "Epoch [6233/20000], Training Loss: 0.4486\n",
            "Epoch [6234/20000], Training Loss: 0.4974\n",
            "Epoch [6235/20000], Training Loss: 0.4747\n",
            "Epoch [6236/20000], Training Loss: 0.4692\n",
            "Epoch [6237/20000], Training Loss: 0.4744\n",
            "Epoch [6238/20000], Training Loss: 0.5110\n",
            "Epoch [6239/20000], Training Loss: 0.4583\n",
            "Epoch [6240/20000], Training Loss: 0.4812\n",
            "Epoch [6241/20000], Training Loss: 0.5072\n",
            "Epoch [6242/20000], Training Loss: 0.5051\n",
            "Epoch [6243/20000], Training Loss: 0.4592\n",
            "Epoch [6244/20000], Training Loss: 0.5042\n",
            "Epoch [6245/20000], Training Loss: 0.5176\n",
            "Epoch [6246/20000], Training Loss: 0.4549\n",
            "Epoch [6247/20000], Training Loss: 0.5122\n",
            "Epoch [6248/20000], Training Loss: 0.4833\n",
            "Epoch [6249/20000], Training Loss: 0.5223\n",
            "Epoch [6250/20000], Training Loss: 0.5086\n",
            "Epoch [6251/20000], Training Loss: 0.4945\n",
            "Epoch [6252/20000], Training Loss: 0.5051\n",
            "Epoch [6253/20000], Training Loss: 0.4559\n",
            "Epoch [6254/20000], Training Loss: 0.5154\n",
            "Epoch [6255/20000], Training Loss: 0.4906\n",
            "Epoch [6256/20000], Training Loss: 0.4505\n",
            "Epoch [6257/20000], Training Loss: 0.4859\n",
            "Epoch [6258/20000], Training Loss: 0.4747\n",
            "Epoch [6259/20000], Training Loss: 0.4815\n",
            "Epoch [6260/20000], Training Loss: 0.4598\n",
            "Epoch [6261/20000], Training Loss: 0.4896\n",
            "Epoch [6262/20000], Training Loss: 0.5028\n",
            "Epoch [6263/20000], Training Loss: 0.4576\n",
            "Epoch [6264/20000], Training Loss: 0.5114\n",
            "Epoch [6265/20000], Training Loss: 0.4871\n",
            "Epoch [6266/20000], Training Loss: 0.4620\n",
            "Epoch [6267/20000], Training Loss: 0.4783\n",
            "Epoch [6268/20000], Training Loss: 0.4834\n",
            "Epoch [6269/20000], Training Loss: 0.5108\n",
            "Epoch [6270/20000], Training Loss: 0.4638\n",
            "Epoch [6271/20000], Training Loss: 0.4714\n",
            "Epoch [6272/20000], Training Loss: 0.5253\n",
            "Epoch [6273/20000], Training Loss: 0.4749\n",
            "Epoch [6274/20000], Training Loss: 0.4737\n",
            "Epoch [6275/20000], Training Loss: 0.5104\n",
            "Epoch [6276/20000], Training Loss: 0.5139\n",
            "Epoch [6277/20000], Training Loss: 0.4707\n",
            "Epoch [6278/20000], Training Loss: 0.4635\n",
            "Epoch [6279/20000], Training Loss: 0.5180\n",
            "Epoch [6280/20000], Training Loss: 0.4865\n",
            "Epoch [6281/20000], Training Loss: 0.4825\n",
            "Epoch [6282/20000], Training Loss: 0.4776\n",
            "Epoch [6283/20000], Training Loss: 0.4717\n",
            "Epoch [6284/20000], Training Loss: 0.4567\n",
            "Epoch [6285/20000], Training Loss: 0.4806\n",
            "Epoch [6286/20000], Training Loss: 0.5105\n",
            "Epoch [6287/20000], Training Loss: 0.4632\n",
            "Epoch [6288/20000], Training Loss: 0.4560\n",
            "Epoch [6289/20000], Training Loss: 0.4784\n",
            "Epoch [6290/20000], Training Loss: 0.5161\n",
            "Epoch [6291/20000], Training Loss: 0.4666\n",
            "Epoch [6292/20000], Training Loss: 0.5058\n",
            "Epoch [6293/20000], Training Loss: 0.5249\n",
            "Epoch [6294/20000], Training Loss: 0.5186\n",
            "Epoch [6295/20000], Training Loss: 0.4813\n",
            "Epoch [6296/20000], Training Loss: 0.4940\n",
            "Epoch [6297/20000], Training Loss: 0.4994\n",
            "Epoch [6298/20000], Training Loss: 0.4875\n",
            "Epoch [6299/20000], Training Loss: 0.4872\n",
            "Epoch [6300/20000], Training Loss: 0.4962\n",
            "Epoch [6301/20000], Training Loss: 0.4654\n",
            "Epoch [6302/20000], Training Loss: 0.5040\n",
            "Epoch [6303/20000], Training Loss: 0.4738\n",
            "Epoch [6304/20000], Training Loss: 0.4583\n",
            "Epoch [6305/20000], Training Loss: 0.5276\n",
            "Epoch [6306/20000], Training Loss: 0.4803\n",
            "Epoch [6307/20000], Training Loss: 0.5744\n",
            "Epoch [6308/20000], Training Loss: 0.4758\n",
            "Epoch [6309/20000], Training Loss: 0.4893\n",
            "Epoch [6310/20000], Training Loss: 0.4929\n",
            "Epoch [6311/20000], Training Loss: 0.4817\n",
            "Epoch [6312/20000], Training Loss: 0.5123\n",
            "Epoch [6313/20000], Training Loss: 0.4719\n",
            "Epoch [6314/20000], Training Loss: 0.4620\n",
            "Epoch [6315/20000], Training Loss: 0.4790\n",
            "Epoch [6316/20000], Training Loss: 0.5114\n",
            "Epoch [6317/20000], Training Loss: 0.4732\n",
            "Epoch [6318/20000], Training Loss: 0.4896\n",
            "Epoch [6319/20000], Training Loss: 0.4975\n",
            "Epoch [6320/20000], Training Loss: 0.5050\n",
            "Epoch [6321/20000], Training Loss: 0.4933\n",
            "Epoch [6322/20000], Training Loss: 0.4821\n",
            "Epoch [6323/20000], Training Loss: 0.5065\n",
            "Epoch [6324/20000], Training Loss: 0.4972\n",
            "Epoch [6325/20000], Training Loss: 0.4959\n",
            "Epoch [6326/20000], Training Loss: 0.5298\n",
            "Epoch [6327/20000], Training Loss: 0.4642\n",
            "Epoch [6328/20000], Training Loss: 0.4487\n",
            "Epoch [6329/20000], Training Loss: 0.5056\n",
            "Epoch [6330/20000], Training Loss: 0.4865\n",
            "Epoch [6331/20000], Training Loss: 0.5198\n",
            "Epoch [6332/20000], Training Loss: 0.4982\n",
            "Epoch [6333/20000], Training Loss: 0.4871\n",
            "Epoch [6334/20000], Training Loss: 0.5040\n",
            "Epoch [6335/20000], Training Loss: 0.4836\n",
            "Epoch [6336/20000], Training Loss: 0.4431\n",
            "Epoch [6337/20000], Training Loss: 0.5592\n",
            "Epoch [6338/20000], Training Loss: 0.4544\n",
            "Epoch [6339/20000], Training Loss: 0.4976\n",
            "Epoch [6340/20000], Training Loss: 0.4662\n",
            "Epoch [6341/20000], Training Loss: 0.4748\n",
            "Epoch [6342/20000], Training Loss: 0.4966\n",
            "Epoch [6343/20000], Training Loss: 0.4532\n",
            "Epoch [6344/20000], Training Loss: 0.4420\n",
            "Epoch [6345/20000], Training Loss: 0.4919\n",
            "Epoch [6346/20000], Training Loss: 0.4992\n",
            "Epoch [6347/20000], Training Loss: 0.4768\n",
            "Epoch [6348/20000], Training Loss: 0.4605\n",
            "Epoch [6349/20000], Training Loss: 0.5159\n",
            "Epoch [6350/20000], Training Loss: 0.5153\n",
            "Epoch [6351/20000], Training Loss: 0.5405\n",
            "Epoch [6352/20000], Training Loss: 0.5022\n",
            "Epoch [6353/20000], Training Loss: 0.5020\n",
            "Epoch [6354/20000], Training Loss: 0.4378\n",
            "Epoch [6355/20000], Training Loss: 0.5042\n",
            "Epoch [6356/20000], Training Loss: 0.5437\n",
            "Epoch [6357/20000], Training Loss: 0.5458\n",
            "Epoch [6358/20000], Training Loss: 0.4269\n",
            "Epoch [6359/20000], Training Loss: 0.4639\n",
            "Epoch [6360/20000], Training Loss: 0.4915\n",
            "Epoch [6361/20000], Training Loss: 0.4773\n",
            "Epoch [6362/20000], Training Loss: 0.4798\n",
            "Epoch [6363/20000], Training Loss: 0.5187\n",
            "Epoch [6364/20000], Training Loss: 0.4830\n",
            "Epoch [6365/20000], Training Loss: 0.4610\n",
            "Epoch [6366/20000], Training Loss: 0.5043\n",
            "Epoch [6367/20000], Training Loss: 0.4938\n",
            "Epoch [6368/20000], Training Loss: 0.4644\n",
            "Epoch [6369/20000], Training Loss: 0.4562\n",
            "Epoch [6370/20000], Training Loss: 0.4708\n",
            "Epoch [6371/20000], Training Loss: 0.4554\n",
            "Epoch [6372/20000], Training Loss: 0.4848\n",
            "Epoch [6373/20000], Training Loss: 0.4764\n",
            "Epoch [6374/20000], Training Loss: 0.4592\n",
            "Epoch [6375/20000], Training Loss: 0.4837\n",
            "Epoch [6376/20000], Training Loss: 0.4919\n",
            "Epoch [6377/20000], Training Loss: 0.4741\n",
            "Epoch [6378/20000], Training Loss: 0.4729\n",
            "Epoch [6379/20000], Training Loss: 0.4456\n",
            "Epoch [6380/20000], Training Loss: 0.4889\n",
            "Epoch [6381/20000], Training Loss: 0.5183\n",
            "Epoch [6382/20000], Training Loss: 0.5240\n",
            "Epoch [6383/20000], Training Loss: 0.4590\n",
            "Epoch [6384/20000], Training Loss: 0.4765\n",
            "Epoch [6385/20000], Training Loss: 0.4956\n",
            "Epoch [6386/20000], Training Loss: 0.4945\n",
            "Epoch [6387/20000], Training Loss: 0.4802\n",
            "Epoch [6388/20000], Training Loss: 0.4939\n",
            "Epoch [6389/20000], Training Loss: 0.4885\n",
            "Epoch [6390/20000], Training Loss: 0.5096\n",
            "Epoch [6391/20000], Training Loss: 0.4940\n",
            "Epoch [6392/20000], Training Loss: 0.5323\n",
            "Epoch [6393/20000], Training Loss: 0.4723\n",
            "Epoch [6394/20000], Training Loss: 0.5121\n",
            "Epoch [6395/20000], Training Loss: 0.4610\n",
            "Epoch [6396/20000], Training Loss: 0.5044\n",
            "Epoch [6397/20000], Training Loss: 0.5053\n",
            "Epoch [6398/20000], Training Loss: 0.5000\n",
            "Epoch [6399/20000], Training Loss: 0.4473\n",
            "Epoch [6400/20000], Training Loss: 0.4935\n",
            "Epoch [6401/20000], Training Loss: 0.4647\n",
            "Epoch [6402/20000], Training Loss: 0.4929\n",
            "Epoch [6403/20000], Training Loss: 0.4651\n",
            "Epoch [6404/20000], Training Loss: 0.4645\n",
            "Epoch [6405/20000], Training Loss: 0.4824\n",
            "Epoch [6406/20000], Training Loss: 0.4877\n",
            "Epoch [6407/20000], Training Loss: 0.4487\n",
            "Epoch [6408/20000], Training Loss: 0.5196\n",
            "Epoch [6409/20000], Training Loss: 0.5046\n",
            "Epoch [6410/20000], Training Loss: 0.4605\n",
            "Epoch [6411/20000], Training Loss: 0.4608\n",
            "Epoch [6412/20000], Training Loss: 0.4423\n",
            "Epoch [6413/20000], Training Loss: 0.5104\n",
            "Epoch [6414/20000], Training Loss: 0.4775\n",
            "Epoch [6415/20000], Training Loss: 0.4772\n",
            "Epoch [6416/20000], Training Loss: 0.5321\n",
            "Epoch [6417/20000], Training Loss: 0.4904\n",
            "Epoch [6418/20000], Training Loss: 0.4759\n",
            "Epoch [6419/20000], Training Loss: 0.4853\n",
            "Epoch [6420/20000], Training Loss: 0.4896\n",
            "Epoch [6421/20000], Training Loss: 0.4613\n",
            "Epoch [6422/20000], Training Loss: 0.4656\n",
            "Epoch [6423/20000], Training Loss: 0.4598\n",
            "Epoch [6424/20000], Training Loss: 0.5139\n",
            "Epoch [6425/20000], Training Loss: 0.4645\n",
            "Epoch [6426/20000], Training Loss: 0.4940\n",
            "Epoch [6427/20000], Training Loss: 0.5410\n",
            "Epoch [6428/20000], Training Loss: 0.5057\n",
            "Epoch [6429/20000], Training Loss: 0.4702\n",
            "Epoch [6430/20000], Training Loss: 0.4649\n",
            "Epoch [6431/20000], Training Loss: 0.4864\n",
            "Epoch [6432/20000], Training Loss: 0.5348\n",
            "Epoch [6433/20000], Training Loss: 0.4415\n",
            "Epoch [6434/20000], Training Loss: 0.5343\n",
            "Epoch [6435/20000], Training Loss: 0.4900\n",
            "Epoch [6436/20000], Training Loss: 0.5210\n",
            "Epoch [6437/20000], Training Loss: 0.5124\n",
            "Epoch [6438/20000], Training Loss: 0.4550\n",
            "Epoch [6439/20000], Training Loss: 0.4571\n",
            "Epoch [6440/20000], Training Loss: 0.5372\n",
            "Epoch [6441/20000], Training Loss: 0.4933\n",
            "Epoch [6442/20000], Training Loss: 0.4464\n",
            "Epoch [6443/20000], Training Loss: 0.5261\n",
            "Epoch [6444/20000], Training Loss: 0.4719\n",
            "Epoch [6445/20000], Training Loss: 0.4717\n",
            "Epoch [6446/20000], Training Loss: 0.5087\n",
            "Epoch [6447/20000], Training Loss: 0.4972\n",
            "Epoch [6448/20000], Training Loss: 0.4679\n",
            "Epoch [6449/20000], Training Loss: 0.5592\n",
            "Epoch [6450/20000], Training Loss: 0.5148\n",
            "Epoch [6451/20000], Training Loss: 0.5243\n",
            "Epoch [6452/20000], Training Loss: 0.5417\n",
            "Epoch [6453/20000], Training Loss: 0.4940\n",
            "Epoch [6454/20000], Training Loss: 0.4785\n",
            "Epoch [6455/20000], Training Loss: 0.5189\n",
            "Epoch [6456/20000], Training Loss: 0.4974\n",
            "Epoch [6457/20000], Training Loss: 0.4615\n",
            "Epoch [6458/20000], Training Loss: 0.4488\n",
            "Epoch [6459/20000], Training Loss: 0.4735\n",
            "Epoch [6460/20000], Training Loss: 0.4861\n",
            "Epoch [6461/20000], Training Loss: 0.4843\n",
            "Epoch [6462/20000], Training Loss: 0.4926\n",
            "Epoch [6463/20000], Training Loss: 0.5128\n",
            "Epoch [6464/20000], Training Loss: 0.4615\n",
            "Epoch [6465/20000], Training Loss: 0.4895\n",
            "Epoch [6466/20000], Training Loss: 0.5102\n",
            "Epoch [6467/20000], Training Loss: 0.5069\n",
            "Epoch [6468/20000], Training Loss: 0.4837\n",
            "Epoch [6469/20000], Training Loss: 0.4850\n",
            "Epoch [6470/20000], Training Loss: 0.4516\n",
            "Epoch [6471/20000], Training Loss: 0.5031\n",
            "Epoch [6472/20000], Training Loss: 0.4919\n",
            "Epoch [6473/20000], Training Loss: 0.4971\n",
            "Epoch [6474/20000], Training Loss: 0.5370\n",
            "Epoch [6475/20000], Training Loss: 0.4915\n",
            "Epoch [6476/20000], Training Loss: 0.4839\n",
            "Epoch [6477/20000], Training Loss: 0.4737\n",
            "Epoch [6478/20000], Training Loss: 0.5352\n",
            "Epoch [6479/20000], Training Loss: 0.4522\n",
            "Epoch [6480/20000], Training Loss: 0.4699\n",
            "Epoch [6481/20000], Training Loss: 0.5292\n",
            "Epoch [6482/20000], Training Loss: 0.5210\n",
            "Epoch [6483/20000], Training Loss: 0.5293\n",
            "Epoch [6484/20000], Training Loss: 0.4885\n",
            "Epoch [6485/20000], Training Loss: 0.4469\n",
            "Epoch [6486/20000], Training Loss: 0.4831\n",
            "Epoch [6487/20000], Training Loss: 0.5296\n",
            "Epoch [6488/20000], Training Loss: 0.4874\n",
            "Epoch [6489/20000], Training Loss: 0.4562\n",
            "Epoch [6490/20000], Training Loss: 0.4782\n",
            "Epoch [6491/20000], Training Loss: 0.5047\n",
            "Epoch [6492/20000], Training Loss: 0.5163\n",
            "Epoch [6493/20000], Training Loss: 0.5307\n",
            "Epoch [6494/20000], Training Loss: 0.5277\n",
            "Epoch [6495/20000], Training Loss: 0.4843\n",
            "Epoch [6496/20000], Training Loss: 0.5042\n",
            "Epoch [6497/20000], Training Loss: 0.5011\n",
            "Epoch [6498/20000], Training Loss: 0.5211\n",
            "Epoch [6499/20000], Training Loss: 0.4502\n",
            "Epoch [6500/20000], Training Loss: 0.4594\n",
            "Epoch [6501/20000], Training Loss: 0.4767\n",
            "Epoch [6502/20000], Training Loss: 0.4940\n",
            "Epoch [6503/20000], Training Loss: 0.4909\n",
            "Epoch [6504/20000], Training Loss: 0.5176\n",
            "Epoch [6505/20000], Training Loss: 0.4759\n",
            "Epoch [6506/20000], Training Loss: 0.4530\n",
            "Epoch [6507/20000], Training Loss: 0.4881\n",
            "Epoch [6508/20000], Training Loss: 0.4767\n",
            "Epoch [6509/20000], Training Loss: 0.5179\n",
            "Epoch [6510/20000], Training Loss: 0.4609\n",
            "Epoch [6511/20000], Training Loss: 0.4881\n",
            "Epoch [6512/20000], Training Loss: 0.4732\n",
            "Epoch [6513/20000], Training Loss: 0.4911\n",
            "Epoch [6514/20000], Training Loss: 0.5078\n",
            "Epoch [6515/20000], Training Loss: 0.4572\n",
            "Epoch [6516/20000], Training Loss: 0.4768\n",
            "Epoch [6517/20000], Training Loss: 0.4911\n",
            "Epoch [6518/20000], Training Loss: 0.4892\n",
            "Epoch [6519/20000], Training Loss: 0.4752\n",
            "Epoch [6520/20000], Training Loss: 0.4755\n",
            "Epoch [6521/20000], Training Loss: 0.5154\n",
            "Epoch [6522/20000], Training Loss: 0.4823\n",
            "Epoch [6523/20000], Training Loss: 0.4664\n",
            "Epoch [6524/20000], Training Loss: 0.5172\n",
            "Epoch [6525/20000], Training Loss: 0.5323\n",
            "Epoch [6526/20000], Training Loss: 0.4702\n",
            "Epoch [6527/20000], Training Loss: 0.5251\n",
            "Epoch [6528/20000], Training Loss: 0.5126\n",
            "Epoch [6529/20000], Training Loss: 0.4933\n",
            "Epoch [6530/20000], Training Loss: 0.4774\n",
            "Epoch [6531/20000], Training Loss: 0.4505\n",
            "Epoch [6532/20000], Training Loss: 0.4862\n",
            "Epoch [6533/20000], Training Loss: 0.4809\n",
            "Epoch [6534/20000], Training Loss: 0.5009\n",
            "Epoch [6535/20000], Training Loss: 0.5076\n",
            "Epoch [6536/20000], Training Loss: 0.5382\n",
            "Epoch [6537/20000], Training Loss: 0.4686\n",
            "Epoch [6538/20000], Training Loss: 0.5178\n",
            "Epoch [6539/20000], Training Loss: 0.4447\n",
            "Epoch [6540/20000], Training Loss: 0.4983\n",
            "Epoch [6541/20000], Training Loss: 0.4884\n",
            "Epoch [6542/20000], Training Loss: 0.5021\n",
            "Epoch [6543/20000], Training Loss: 0.4659\n",
            "Epoch [6544/20000], Training Loss: 0.4721\n",
            "Epoch [6545/20000], Training Loss: 0.4851\n",
            "Epoch [6546/20000], Training Loss: 0.4747\n",
            "Epoch [6547/20000], Training Loss: 0.4654\n",
            "Epoch [6548/20000], Training Loss: 0.4689\n",
            "Epoch [6549/20000], Training Loss: 0.5214\n",
            "Epoch [6550/20000], Training Loss: 0.4938\n",
            "Epoch [6551/20000], Training Loss: 0.5213\n",
            "Epoch [6552/20000], Training Loss: 0.4952\n",
            "Epoch [6553/20000], Training Loss: 0.5191\n",
            "Epoch [6554/20000], Training Loss: 0.5176\n",
            "Epoch [6555/20000], Training Loss: 0.5276\n",
            "Epoch [6556/20000], Training Loss: 0.5097\n",
            "Epoch [6557/20000], Training Loss: 0.4568\n",
            "Epoch [6558/20000], Training Loss: 0.4657\n",
            "Epoch [6559/20000], Training Loss: 0.4808\n",
            "Epoch [6560/20000], Training Loss: 0.5012\n",
            "Epoch [6561/20000], Training Loss: 0.5229\n",
            "Epoch [6562/20000], Training Loss: 0.4955\n",
            "Epoch [6563/20000], Training Loss: 0.5306\n",
            "Epoch [6564/20000], Training Loss: 0.4659\n",
            "Epoch [6565/20000], Training Loss: 0.5034\n",
            "Epoch [6566/20000], Training Loss: 0.5109\n",
            "Epoch [6567/20000], Training Loss: 0.5007\n",
            "Epoch [6568/20000], Training Loss: 0.5340\n",
            "Epoch [6569/20000], Training Loss: 0.5092\n",
            "Epoch [6570/20000], Training Loss: 0.4496\n",
            "Epoch [6571/20000], Training Loss: 0.4613\n",
            "Epoch [6572/20000], Training Loss: 0.4941\n",
            "Epoch [6573/20000], Training Loss: 0.5319\n",
            "Epoch [6574/20000], Training Loss: 0.5350\n",
            "Epoch [6575/20000], Training Loss: 0.4269\n",
            "Epoch [6576/20000], Training Loss: 0.5031\n",
            "Epoch [6577/20000], Training Loss: 0.4434\n",
            "Epoch [6578/20000], Training Loss: 0.5387\n",
            "Epoch [6579/20000], Training Loss: 0.5002\n",
            "Epoch [6580/20000], Training Loss: 0.5194\n",
            "Epoch [6581/20000], Training Loss: 0.4992\n",
            "Epoch [6582/20000], Training Loss: 0.4517\n",
            "Epoch [6583/20000], Training Loss: 0.4806\n",
            "Epoch [6584/20000], Training Loss: 0.4830\n",
            "Epoch [6585/20000], Training Loss: 0.5166\n",
            "Epoch [6586/20000], Training Loss: 0.4421\n",
            "Epoch [6587/20000], Training Loss: 0.4599\n",
            "Epoch [6588/20000], Training Loss: 0.4619\n",
            "Epoch [6589/20000], Training Loss: 0.4794\n",
            "Epoch [6590/20000], Training Loss: 0.5522\n",
            "Epoch [6591/20000], Training Loss: 0.4597\n",
            "Epoch [6592/20000], Training Loss: 0.4643\n",
            "Epoch [6593/20000], Training Loss: 0.4746\n",
            "Epoch [6594/20000], Training Loss: 0.4663\n",
            "Epoch [6595/20000], Training Loss: 0.4986\n",
            "Epoch [6596/20000], Training Loss: 0.5017\n",
            "Epoch [6597/20000], Training Loss: 0.5066\n",
            "Epoch [6598/20000], Training Loss: 0.5110\n",
            "Epoch [6599/20000], Training Loss: 0.4838\n",
            "Epoch [6600/20000], Training Loss: 0.4895\n",
            "Epoch [6601/20000], Training Loss: 0.5087\n",
            "Epoch [6602/20000], Training Loss: 0.4746\n",
            "Epoch [6603/20000], Training Loss: 0.4857\n",
            "Epoch [6604/20000], Training Loss: 0.5072\n",
            "Epoch [6605/20000], Training Loss: 0.4832\n",
            "Epoch [6606/20000], Training Loss: 0.4922\n",
            "Epoch [6607/20000], Training Loss: 0.4898\n",
            "Epoch [6608/20000], Training Loss: 0.4575\n",
            "Epoch [6609/20000], Training Loss: 0.4823\n",
            "Epoch [6610/20000], Training Loss: 0.4992\n",
            "Epoch [6611/20000], Training Loss: 0.4729\n",
            "Epoch [6612/20000], Training Loss: 0.4753\n",
            "Epoch [6613/20000], Training Loss: 0.4815\n",
            "Epoch [6614/20000], Training Loss: 0.5361\n",
            "Epoch [6615/20000], Training Loss: 0.4799\n",
            "Epoch [6616/20000], Training Loss: 0.4866\n",
            "Epoch [6617/20000], Training Loss: 0.5171\n",
            "Epoch [6618/20000], Training Loss: 0.4786\n",
            "Epoch [6619/20000], Training Loss: 0.4693\n",
            "Epoch [6620/20000], Training Loss: 0.4859\n",
            "Epoch [6621/20000], Training Loss: 0.4819\n",
            "Epoch [6622/20000], Training Loss: 0.4873\n",
            "Epoch [6623/20000], Training Loss: 0.4734\n",
            "Epoch [6624/20000], Training Loss: 0.4810\n",
            "Epoch [6625/20000], Training Loss: 0.5142\n",
            "Epoch [6626/20000], Training Loss: 0.4751\n",
            "Epoch [6627/20000], Training Loss: 0.4882\n",
            "Epoch [6628/20000], Training Loss: 0.5241\n",
            "Epoch [6629/20000], Training Loss: 0.4890\n",
            "Epoch [6630/20000], Training Loss: 0.5110\n",
            "Epoch [6631/20000], Training Loss: 0.5015\n",
            "Epoch [6632/20000], Training Loss: 0.4846\n",
            "Epoch [6633/20000], Training Loss: 0.4928\n",
            "Epoch [6634/20000], Training Loss: 0.4930\n",
            "Epoch [6635/20000], Training Loss: 0.4969\n",
            "Epoch [6636/20000], Training Loss: 0.5098\n",
            "Epoch [6637/20000], Training Loss: 0.4709\n",
            "Epoch [6638/20000], Training Loss: 0.5030\n",
            "Epoch [6639/20000], Training Loss: 0.4340\n",
            "Epoch [6640/20000], Training Loss: 0.5360\n",
            "Epoch [6641/20000], Training Loss: 0.4648\n",
            "Epoch [6642/20000], Training Loss: 0.5441\n",
            "Epoch [6643/20000], Training Loss: 0.4862\n",
            "Epoch [6644/20000], Training Loss: 0.5137\n",
            "Epoch [6645/20000], Training Loss: 0.5028\n",
            "Epoch [6646/20000], Training Loss: 0.4851\n",
            "Epoch [6647/20000], Training Loss: 0.5002\n",
            "Epoch [6648/20000], Training Loss: 0.4887\n",
            "Epoch [6649/20000], Training Loss: 0.4346\n",
            "Epoch [6650/20000], Training Loss: 0.4180\n",
            "Epoch [6651/20000], Training Loss: 0.4648\n",
            "Epoch [6652/20000], Training Loss: 0.4768\n",
            "Epoch [6653/20000], Training Loss: 0.4689\n",
            "Epoch [6654/20000], Training Loss: 0.4715\n",
            "Epoch [6655/20000], Training Loss: 0.4919\n",
            "Epoch [6656/20000], Training Loss: 0.4642\n",
            "Epoch [6657/20000], Training Loss: 0.4765\n",
            "Epoch [6658/20000], Training Loss: 0.5078\n",
            "Epoch [6659/20000], Training Loss: 0.4651\n",
            "Epoch [6660/20000], Training Loss: 0.4484\n",
            "Epoch [6661/20000], Training Loss: 0.5298\n",
            "Epoch [6662/20000], Training Loss: 0.4441\n",
            "Epoch [6663/20000], Training Loss: 0.4816\n",
            "Epoch [6664/20000], Training Loss: 0.4817\n",
            "Epoch [6665/20000], Training Loss: 0.5371\n",
            "Epoch [6666/20000], Training Loss: 0.5050\n",
            "Epoch [6667/20000], Training Loss: 0.4806\n",
            "Epoch [6668/20000], Training Loss: 0.4913\n",
            "Epoch [6669/20000], Training Loss: 0.4796\n",
            "Epoch [6670/20000], Training Loss: 0.4542\n",
            "Epoch [6671/20000], Training Loss: 0.4315\n",
            "Epoch [6672/20000], Training Loss: 0.4800\n",
            "Epoch [6673/20000], Training Loss: 0.4938\n",
            "Epoch [6674/20000], Training Loss: 0.5295\n",
            "Epoch [6675/20000], Training Loss: 0.5015\n",
            "Epoch [6676/20000], Training Loss: 0.5492\n",
            "Epoch [6677/20000], Training Loss: 0.4580\n",
            "Epoch [6678/20000], Training Loss: 0.4527\n",
            "Epoch [6679/20000], Training Loss: 0.4554\n",
            "Epoch [6680/20000], Training Loss: 0.4717\n",
            "Epoch [6681/20000], Training Loss: 0.5266\n",
            "Epoch [6682/20000], Training Loss: 0.4860\n",
            "Epoch [6683/20000], Training Loss: 0.4936\n",
            "Epoch [6684/20000], Training Loss: 0.4740\n",
            "Epoch [6685/20000], Training Loss: 0.5062\n",
            "Epoch [6686/20000], Training Loss: 0.5286\n",
            "Epoch [6687/20000], Training Loss: 0.4611\n",
            "Epoch [6688/20000], Training Loss: 0.4995\n",
            "Epoch [6689/20000], Training Loss: 0.5092\n",
            "Epoch [6690/20000], Training Loss: 0.4420\n",
            "Epoch [6691/20000], Training Loss: 0.5025\n",
            "Epoch [6692/20000], Training Loss: 0.5634\n",
            "Epoch [6693/20000], Training Loss: 0.4725\n",
            "Epoch [6694/20000], Training Loss: 0.4998\n",
            "Epoch [6695/20000], Training Loss: 0.4973\n",
            "Epoch [6696/20000], Training Loss: 0.4697\n",
            "Epoch [6697/20000], Training Loss: 0.4769\n",
            "Epoch [6698/20000], Training Loss: 0.5205\n",
            "Epoch [6699/20000], Training Loss: 0.4763\n",
            "Epoch [6700/20000], Training Loss: 0.4995\n",
            "Epoch [6701/20000], Training Loss: 0.4638\n",
            "Epoch [6702/20000], Training Loss: 0.4619\n",
            "Epoch [6703/20000], Training Loss: 0.5260\n",
            "Epoch [6704/20000], Training Loss: 0.5289\n",
            "Epoch [6705/20000], Training Loss: 0.4877\n",
            "Epoch [6706/20000], Training Loss: 0.4680\n",
            "Epoch [6707/20000], Training Loss: 0.5258\n",
            "Epoch [6708/20000], Training Loss: 0.5059\n",
            "Epoch [6709/20000], Training Loss: 0.4793\n",
            "Epoch [6710/20000], Training Loss: 0.5021\n",
            "Epoch [6711/20000], Training Loss: 0.4709\n",
            "Epoch [6712/20000], Training Loss: 0.5063\n",
            "Epoch [6713/20000], Training Loss: 0.4881\n",
            "Epoch [6714/20000], Training Loss: 0.5096\n",
            "Epoch [6715/20000], Training Loss: 0.4860\n",
            "Epoch [6716/20000], Training Loss: 0.4874\n",
            "Epoch [6717/20000], Training Loss: 0.4445\n",
            "Epoch [6718/20000], Training Loss: 0.4632\n",
            "Epoch [6719/20000], Training Loss: 0.5111\n",
            "Epoch [6720/20000], Training Loss: 0.5275\n",
            "Epoch [6721/20000], Training Loss: 0.4529\n",
            "Epoch [6722/20000], Training Loss: 0.4626\n",
            "Epoch [6723/20000], Training Loss: 0.4640\n",
            "Epoch [6724/20000], Training Loss: 0.4699\n",
            "Epoch [6725/20000], Training Loss: 0.4926\n",
            "Epoch [6726/20000], Training Loss: 0.4614\n",
            "Epoch [6727/20000], Training Loss: 0.5156\n",
            "Epoch [6728/20000], Training Loss: 0.5232\n",
            "Epoch [6729/20000], Training Loss: 0.4616\n",
            "Epoch [6730/20000], Training Loss: 0.4639\n",
            "Epoch [6731/20000], Training Loss: 0.5215\n",
            "Epoch [6732/20000], Training Loss: 0.4575\n",
            "Epoch [6733/20000], Training Loss: 0.5263\n",
            "Epoch [6734/20000], Training Loss: 0.4529\n",
            "Epoch [6735/20000], Training Loss: 0.4495\n",
            "Epoch [6736/20000], Training Loss: 0.5015\n",
            "Epoch [6737/20000], Training Loss: 0.4865\n",
            "Epoch [6738/20000], Training Loss: 0.5332\n",
            "Epoch [6739/20000], Training Loss: 0.5242\n",
            "Epoch [6740/20000], Training Loss: 0.4984\n",
            "Epoch [6741/20000], Training Loss: 0.5242\n",
            "Epoch [6742/20000], Training Loss: 0.4825\n",
            "Epoch [6743/20000], Training Loss: 0.4883\n",
            "Epoch [6744/20000], Training Loss: 0.5003\n",
            "Epoch [6745/20000], Training Loss: 0.4665\n",
            "Epoch [6746/20000], Training Loss: 0.4645\n",
            "Epoch [6747/20000], Training Loss: 0.4563\n",
            "Epoch [6748/20000], Training Loss: 0.4980\n",
            "Epoch [6749/20000], Training Loss: 0.4921\n",
            "Epoch [6750/20000], Training Loss: 0.4955\n",
            "Epoch [6751/20000], Training Loss: 0.4914\n",
            "Epoch [6752/20000], Training Loss: 0.4603\n",
            "Epoch [6753/20000], Training Loss: 0.4413\n",
            "Epoch [6754/20000], Training Loss: 0.4450\n",
            "Epoch [6755/20000], Training Loss: 0.4568\n",
            "Epoch [6756/20000], Training Loss: 0.4992\n",
            "Epoch [6757/20000], Training Loss: 0.4734\n",
            "Epoch [6758/20000], Training Loss: 0.4816\n",
            "Epoch [6759/20000], Training Loss: 0.4848\n",
            "Epoch [6760/20000], Training Loss: 0.5079\n",
            "Epoch [6761/20000], Training Loss: 0.4744\n",
            "Epoch [6762/20000], Training Loss: 0.5142\n",
            "Epoch [6763/20000], Training Loss: 0.4804\n",
            "Epoch [6764/20000], Training Loss: 0.4606\n",
            "Epoch [6765/20000], Training Loss: 0.5200\n",
            "Epoch [6766/20000], Training Loss: 0.4993\n",
            "Epoch [6767/20000], Training Loss: 0.4634\n",
            "Epoch [6768/20000], Training Loss: 0.5382\n",
            "Epoch [6769/20000], Training Loss: 0.4656\n",
            "Epoch [6770/20000], Training Loss: 0.5512\n",
            "Epoch [6771/20000], Training Loss: 0.5156\n",
            "Epoch [6772/20000], Training Loss: 0.5085\n",
            "Epoch [6773/20000], Training Loss: 0.5111\n",
            "Epoch [6774/20000], Training Loss: 0.4797\n",
            "Epoch [6775/20000], Training Loss: 0.4677\n",
            "Epoch [6776/20000], Training Loss: 0.4407\n",
            "Epoch [6777/20000], Training Loss: 0.5459\n",
            "Epoch [6778/20000], Training Loss: 0.4585\n",
            "Epoch [6779/20000], Training Loss: 0.4599\n",
            "Epoch [6780/20000], Training Loss: 0.4328\n",
            "Epoch [6781/20000], Training Loss: 0.5475\n",
            "Epoch [6782/20000], Training Loss: 0.4943\n",
            "Epoch [6783/20000], Training Loss: 0.4911\n",
            "Epoch [6784/20000], Training Loss: 0.4816\n",
            "Epoch [6785/20000], Training Loss: 0.4677\n",
            "Epoch [6786/20000], Training Loss: 0.5213\n",
            "Epoch [6787/20000], Training Loss: 0.5097\n",
            "Epoch [6788/20000], Training Loss: 0.4553\n",
            "Epoch [6789/20000], Training Loss: 0.4910\n",
            "Epoch [6790/20000], Training Loss: 0.4685\n",
            "Epoch [6791/20000], Training Loss: 0.4712\n",
            "Epoch [6792/20000], Training Loss: 0.4938\n",
            "Epoch [6793/20000], Training Loss: 0.5306\n",
            "Epoch [6794/20000], Training Loss: 0.5245\n",
            "Epoch [6795/20000], Training Loss: 0.4495\n",
            "Epoch [6796/20000], Training Loss: 0.5011\n",
            "Epoch [6797/20000], Training Loss: 0.4641\n",
            "Epoch [6798/20000], Training Loss: 0.5137\n",
            "Epoch [6799/20000], Training Loss: 0.4871\n",
            "Epoch [6800/20000], Training Loss: 0.5220\n",
            "Epoch [6801/20000], Training Loss: 0.4565\n",
            "Epoch [6802/20000], Training Loss: 0.4632\n",
            "Epoch [6803/20000], Training Loss: 0.5298\n",
            "Epoch [6804/20000], Training Loss: 0.4995\n",
            "Epoch [6805/20000], Training Loss: 0.4637\n",
            "Epoch [6806/20000], Training Loss: 0.5006\n",
            "Epoch [6807/20000], Training Loss: 0.4669\n",
            "Epoch [6808/20000], Training Loss: 0.4820\n",
            "Epoch [6809/20000], Training Loss: 0.5298\n",
            "Epoch [6810/20000], Training Loss: 0.5194\n",
            "Epoch [6811/20000], Training Loss: 0.4649\n",
            "Epoch [6812/20000], Training Loss: 0.5526\n",
            "Epoch [6813/20000], Training Loss: 0.4800\n",
            "Epoch [6814/20000], Training Loss: 0.5177\n",
            "Epoch [6815/20000], Training Loss: 0.5099\n",
            "Epoch [6816/20000], Training Loss: 0.5700\n",
            "Epoch [6817/20000], Training Loss: 0.4603\n",
            "Epoch [6818/20000], Training Loss: 0.4824\n",
            "Epoch [6819/20000], Training Loss: 0.4880\n",
            "Epoch [6820/20000], Training Loss: 0.4740\n",
            "Epoch [6821/20000], Training Loss: 0.4884\n",
            "Epoch [6822/20000], Training Loss: 0.4891\n",
            "Epoch [6823/20000], Training Loss: 0.4570\n",
            "Epoch [6824/20000], Training Loss: 0.4769\n",
            "Epoch [6825/20000], Training Loss: 0.4664\n",
            "Epoch [6826/20000], Training Loss: 0.4922\n",
            "Epoch [6827/20000], Training Loss: 0.4878\n",
            "Epoch [6828/20000], Training Loss: 0.4926\n",
            "Epoch [6829/20000], Training Loss: 0.4789\n",
            "Epoch [6830/20000], Training Loss: 0.4755\n",
            "Epoch [6831/20000], Training Loss: 0.5404\n",
            "Epoch [6832/20000], Training Loss: 0.4906\n",
            "Epoch [6833/20000], Training Loss: 0.4932\n",
            "Epoch [6834/20000], Training Loss: 0.4460\n",
            "Epoch [6835/20000], Training Loss: 0.5002\n",
            "Epoch [6836/20000], Training Loss: 0.4774\n",
            "Epoch [6837/20000], Training Loss: 0.4899\n",
            "Epoch [6838/20000], Training Loss: 0.4208\n",
            "Epoch [6839/20000], Training Loss: 0.4993\n",
            "Epoch [6840/20000], Training Loss: 0.4991\n",
            "Epoch [6841/20000], Training Loss: 0.4992\n",
            "Epoch [6842/20000], Training Loss: 0.4878\n",
            "Epoch [6843/20000], Training Loss: 0.4930\n",
            "Epoch [6844/20000], Training Loss: 0.4582\n",
            "Epoch [6845/20000], Training Loss: 0.4880\n",
            "Epoch [6846/20000], Training Loss: 0.4668\n",
            "Epoch [6847/20000], Training Loss: 0.4586\n",
            "Epoch [6848/20000], Training Loss: 0.4435\n",
            "Epoch [6849/20000], Training Loss: 0.5564\n",
            "Epoch [6850/20000], Training Loss: 0.5107\n",
            "Epoch [6851/20000], Training Loss: 0.4729\n",
            "Epoch [6852/20000], Training Loss: 0.4869\n",
            "Epoch [6853/20000], Training Loss: 0.5095\n",
            "Epoch [6854/20000], Training Loss: 0.5129\n",
            "Epoch [6855/20000], Training Loss: 0.4828\n",
            "Epoch [6856/20000], Training Loss: 0.4822\n",
            "Epoch [6857/20000], Training Loss: 0.4882\n",
            "Epoch [6858/20000], Training Loss: 0.4633\n",
            "Epoch [6859/20000], Training Loss: 0.4456\n",
            "Epoch [6860/20000], Training Loss: 0.4349\n",
            "Epoch [6861/20000], Training Loss: 0.5478\n",
            "Epoch [6862/20000], Training Loss: 0.5639\n",
            "Epoch [6863/20000], Training Loss: 0.4592\n",
            "Epoch [6864/20000], Training Loss: 0.4881\n",
            "Epoch [6865/20000], Training Loss: 0.4544\n",
            "Epoch [6866/20000], Training Loss: 0.4929\n",
            "Epoch [6867/20000], Training Loss: 0.5138\n",
            "Epoch [6868/20000], Training Loss: 0.4972\n",
            "Epoch [6869/20000], Training Loss: 0.5276\n",
            "Epoch [6870/20000], Training Loss: 0.4522\n",
            "Epoch [6871/20000], Training Loss: 0.4707\n",
            "Epoch [6872/20000], Training Loss: 0.5012\n",
            "Epoch [6873/20000], Training Loss: 0.4823\n",
            "Epoch [6874/20000], Training Loss: 0.4825\n",
            "Epoch [6875/20000], Training Loss: 0.4812\n",
            "Epoch [6876/20000], Training Loss: 0.4698\n",
            "Epoch [6877/20000], Training Loss: 0.5184\n",
            "Epoch [6878/20000], Training Loss: 0.5395\n",
            "Epoch [6879/20000], Training Loss: 0.4755\n",
            "Epoch [6880/20000], Training Loss: 0.4954\n",
            "Epoch [6881/20000], Training Loss: 0.5203\n",
            "Epoch [6882/20000], Training Loss: 0.5015\n",
            "Epoch [6883/20000], Training Loss: 0.4551\n",
            "Epoch [6884/20000], Training Loss: 0.4814\n",
            "Epoch [6885/20000], Training Loss: 0.4928\n",
            "Epoch [6886/20000], Training Loss: 0.4694\n",
            "Epoch [6887/20000], Training Loss: 0.5309\n",
            "Epoch [6888/20000], Training Loss: 0.4922\n",
            "Epoch [6889/20000], Training Loss: 0.4696\n",
            "Epoch [6890/20000], Training Loss: 0.5219\n",
            "Epoch [6891/20000], Training Loss: 0.4806\n",
            "Epoch [6892/20000], Training Loss: 0.4966\n",
            "Epoch [6893/20000], Training Loss: 0.4566\n",
            "Epoch [6894/20000], Training Loss: 0.4601\n",
            "Epoch [6895/20000], Training Loss: 0.5206\n",
            "Epoch [6896/20000], Training Loss: 0.4918\n",
            "Epoch [6897/20000], Training Loss: 0.4368\n",
            "Epoch [6898/20000], Training Loss: 0.5362\n",
            "Epoch [6899/20000], Training Loss: 0.5024\n",
            "Epoch [6900/20000], Training Loss: 0.4800\n",
            "Epoch [6901/20000], Training Loss: 0.5324\n",
            "Epoch [6902/20000], Training Loss: 0.4868\n",
            "Epoch [6903/20000], Training Loss: 0.5167\n",
            "Epoch [6904/20000], Training Loss: 0.5624\n",
            "Epoch [6905/20000], Training Loss: 0.4942\n",
            "Epoch [6906/20000], Training Loss: 0.4936\n",
            "Epoch [6907/20000], Training Loss: 0.4729\n",
            "Epoch [6908/20000], Training Loss: 0.5117\n",
            "Epoch [6909/20000], Training Loss: 0.4957\n",
            "Epoch [6910/20000], Training Loss: 0.4881\n",
            "Epoch [6911/20000], Training Loss: 0.4988\n",
            "Epoch [6912/20000], Training Loss: 0.4955\n",
            "Epoch [6913/20000], Training Loss: 0.5227\n",
            "Epoch [6914/20000], Training Loss: 0.4888\n",
            "Epoch [6915/20000], Training Loss: 0.5014\n",
            "Epoch [6916/20000], Training Loss: 0.5251\n",
            "Epoch [6917/20000], Training Loss: 0.5534\n",
            "Epoch [6918/20000], Training Loss: 0.5231\n",
            "Epoch [6919/20000], Training Loss: 0.4971\n",
            "Epoch [6920/20000], Training Loss: 0.4695\n",
            "Epoch [6921/20000], Training Loss: 0.5058\n",
            "Epoch [6922/20000], Training Loss: 0.4998\n",
            "Epoch [6923/20000], Training Loss: 0.4718\n",
            "Epoch [6924/20000], Training Loss: 0.4515\n",
            "Epoch [6925/20000], Training Loss: 0.4822\n",
            "Epoch [6926/20000], Training Loss: 0.4807\n",
            "Epoch [6927/20000], Training Loss: 0.4874\n",
            "Epoch [6928/20000], Training Loss: 0.5148\n",
            "Epoch [6929/20000], Training Loss: 0.4530\n",
            "Epoch [6930/20000], Training Loss: 0.4829\n",
            "Epoch [6931/20000], Training Loss: 0.4708\n",
            "Epoch [6932/20000], Training Loss: 0.4798\n",
            "Epoch [6933/20000], Training Loss: 0.5222\n",
            "Epoch [6934/20000], Training Loss: 0.4833\n",
            "Epoch [6935/20000], Training Loss: 0.5002\n",
            "Epoch [6936/20000], Training Loss: 0.4630\n",
            "Epoch [6937/20000], Training Loss: 0.4727\n",
            "Epoch [6938/20000], Training Loss: 0.4991\n",
            "Epoch [6939/20000], Training Loss: 0.5097\n",
            "Epoch [6940/20000], Training Loss: 0.5059\n",
            "Epoch [6941/20000], Training Loss: 0.4480\n",
            "Epoch [6942/20000], Training Loss: 0.4893\n",
            "Epoch [6943/20000], Training Loss: 0.4480\n",
            "Epoch [6944/20000], Training Loss: 0.4738\n",
            "Epoch [6945/20000], Training Loss: 0.5353\n",
            "Epoch [6946/20000], Training Loss: 0.5090\n",
            "Epoch [6947/20000], Training Loss: 0.4898\n",
            "Epoch [6948/20000], Training Loss: 0.4755\n",
            "Epoch [6949/20000], Training Loss: 0.4829\n",
            "Epoch [6950/20000], Training Loss: 0.4881\n",
            "Epoch [6951/20000], Training Loss: 0.4691\n",
            "Epoch [6952/20000], Training Loss: 0.4489\n",
            "Epoch [6953/20000], Training Loss: 0.5128\n",
            "Epoch [6954/20000], Training Loss: 0.4565\n",
            "Epoch [6955/20000], Training Loss: 0.4514\n",
            "Epoch [6956/20000], Training Loss: 0.4571\n",
            "Epoch [6957/20000], Training Loss: 0.4734\n",
            "Epoch [6958/20000], Training Loss: 0.5263\n",
            "Epoch [6959/20000], Training Loss: 0.4837\n",
            "Epoch [6960/20000], Training Loss: 0.5196\n",
            "Epoch [6961/20000], Training Loss: 0.5025\n",
            "Epoch [6962/20000], Training Loss: 0.5552\n",
            "Epoch [6963/20000], Training Loss: 0.4588\n",
            "Epoch [6964/20000], Training Loss: 0.4758\n",
            "Epoch [6965/20000], Training Loss: 0.4508\n",
            "Epoch [6966/20000], Training Loss: 0.4966\n",
            "Epoch [6967/20000], Training Loss: 0.4845\n",
            "Epoch [6968/20000], Training Loss: 0.4763\n",
            "Epoch [6969/20000], Training Loss: 0.5113\n",
            "Epoch [6970/20000], Training Loss: 0.5071\n",
            "Epoch [6971/20000], Training Loss: 0.4871\n",
            "Epoch [6972/20000], Training Loss: 0.5069\n",
            "Epoch [6973/20000], Training Loss: 0.4532\n",
            "Epoch [6974/20000], Training Loss: 0.4718\n",
            "Epoch [6975/20000], Training Loss: 0.4855\n",
            "Epoch [6976/20000], Training Loss: 0.4831\n",
            "Epoch [6977/20000], Training Loss: 0.4760\n",
            "Epoch [6978/20000], Training Loss: 0.4623\n",
            "Epoch [6979/20000], Training Loss: 0.4969\n",
            "Epoch [6980/20000], Training Loss: 0.4901\n",
            "Epoch [6981/20000], Training Loss: 0.4737\n",
            "Epoch [6982/20000], Training Loss: 0.4760\n",
            "Epoch [6983/20000], Training Loss: 0.4729\n",
            "Epoch [6984/20000], Training Loss: 0.5242\n",
            "Epoch [6985/20000], Training Loss: 0.4838\n",
            "Epoch [6986/20000], Training Loss: 0.4804\n",
            "Epoch [6987/20000], Training Loss: 0.4596\n",
            "Epoch [6988/20000], Training Loss: 0.4386\n",
            "Epoch [6989/20000], Training Loss: 0.4446\n",
            "Epoch [6990/20000], Training Loss: 0.4566\n",
            "Epoch [6991/20000], Training Loss: 0.4863\n",
            "Epoch [6992/20000], Training Loss: 0.5171\n",
            "Epoch [6993/20000], Training Loss: 0.4870\n",
            "Epoch [6994/20000], Training Loss: 0.5002\n",
            "Epoch [6995/20000], Training Loss: 0.4891\n",
            "Epoch [6996/20000], Training Loss: 0.4947\n",
            "Epoch [6997/20000], Training Loss: 0.4822\n",
            "Epoch [6998/20000], Training Loss: 0.5002\n",
            "Epoch [6999/20000], Training Loss: 0.5177\n",
            "Epoch [7000/20000], Training Loss: 0.4783\n",
            "Epoch [7001/20000], Training Loss: 0.4871\n",
            "Epoch [7002/20000], Training Loss: 0.4583\n",
            "Epoch [7003/20000], Training Loss: 0.5385\n",
            "Epoch [7004/20000], Training Loss: 0.4577\n",
            "Epoch [7005/20000], Training Loss: 0.4984\n",
            "Epoch [7006/20000], Training Loss: 0.4780\n",
            "Epoch [7007/20000], Training Loss: 0.5137\n",
            "Epoch [7008/20000], Training Loss: 0.4830\n",
            "Epoch [7009/20000], Training Loss: 0.4764\n",
            "Epoch [7010/20000], Training Loss: 0.5059\n",
            "Epoch [7011/20000], Training Loss: 0.5139\n",
            "Epoch [7012/20000], Training Loss: 0.4783\n",
            "Epoch [7013/20000], Training Loss: 0.4782\n",
            "Epoch [7014/20000], Training Loss: 0.4915\n",
            "Epoch [7015/20000], Training Loss: 0.5255\n",
            "Epoch [7016/20000], Training Loss: 0.4831\n",
            "Epoch [7017/20000], Training Loss: 0.4991\n",
            "Epoch [7018/20000], Training Loss: 0.4875\n",
            "Epoch [7019/20000], Training Loss: 0.4729\n",
            "Epoch [7020/20000], Training Loss: 0.4873\n",
            "Epoch [7021/20000], Training Loss: 0.4397\n",
            "Epoch [7022/20000], Training Loss: 0.4936\n",
            "Epoch [7023/20000], Training Loss: 0.5205\n",
            "Epoch [7024/20000], Training Loss: 0.4920\n",
            "Epoch [7025/20000], Training Loss: 0.4784\n",
            "Epoch [7026/20000], Training Loss: 0.4749\n",
            "Epoch [7027/20000], Training Loss: 0.4957\n",
            "Epoch [7028/20000], Training Loss: 0.5227\n",
            "Epoch [7029/20000], Training Loss: 0.5049\n",
            "Epoch [7030/20000], Training Loss: 0.5372\n",
            "Epoch [7031/20000], Training Loss: 0.4591\n",
            "Epoch [7032/20000], Training Loss: 0.4744\n",
            "Epoch [7033/20000], Training Loss: 0.4962\n",
            "Epoch [7034/20000], Training Loss: 0.4946\n",
            "Epoch [7035/20000], Training Loss: 0.4764\n",
            "Epoch [7036/20000], Training Loss: 0.4687\n",
            "Epoch [7037/20000], Training Loss: 0.4968\n",
            "Epoch [7038/20000], Training Loss: 0.4775\n",
            "Epoch [7039/20000], Training Loss: 0.4682\n",
            "Epoch [7040/20000], Training Loss: 0.4935\n",
            "Epoch [7041/20000], Training Loss: 0.4658\n",
            "Epoch [7042/20000], Training Loss: 0.5252\n",
            "Epoch [7043/20000], Training Loss: 0.4735\n",
            "Epoch [7044/20000], Training Loss: 0.4774\n",
            "Epoch [7045/20000], Training Loss: 0.4615\n",
            "Epoch [7046/20000], Training Loss: 0.4545\n",
            "Epoch [7047/20000], Training Loss: 0.4667\n",
            "Epoch [7048/20000], Training Loss: 0.4885\n",
            "Epoch [7049/20000], Training Loss: 0.4497\n",
            "Epoch [7050/20000], Training Loss: 0.4897\n",
            "Epoch [7051/20000], Training Loss: 0.5586\n",
            "Epoch [7052/20000], Training Loss: 0.4566\n",
            "Epoch [7053/20000], Training Loss: 0.4851\n",
            "Epoch [7054/20000], Training Loss: 0.4575\n",
            "Epoch [7055/20000], Training Loss: 0.5088\n",
            "Epoch [7056/20000], Training Loss: 0.5170\n",
            "Epoch [7057/20000], Training Loss: 0.4623\n",
            "Epoch [7058/20000], Training Loss: 0.4224\n",
            "Epoch [7059/20000], Training Loss: 0.4955\n",
            "Epoch [7060/20000], Training Loss: 0.4769\n",
            "Epoch [7061/20000], Training Loss: 0.4710\n",
            "Epoch [7062/20000], Training Loss: 0.5395\n",
            "Epoch [7063/20000], Training Loss: 0.4875\n",
            "Epoch [7064/20000], Training Loss: 0.4823\n",
            "Epoch [7065/20000], Training Loss: 0.4900\n",
            "Epoch [7066/20000], Training Loss: 0.4737\n",
            "Epoch [7067/20000], Training Loss: 0.4501\n",
            "Epoch [7068/20000], Training Loss: 0.4811\n",
            "Epoch [7069/20000], Training Loss: 0.4909\n",
            "Epoch [7070/20000], Training Loss: 0.4888\n",
            "Epoch [7071/20000], Training Loss: 0.5142\n",
            "Epoch [7072/20000], Training Loss: 0.4788\n",
            "Epoch [7073/20000], Training Loss: 0.4649\n",
            "Epoch [7074/20000], Training Loss: 0.5123\n",
            "Epoch [7075/20000], Training Loss: 0.5237\n",
            "Epoch [7076/20000], Training Loss: 0.4855\n",
            "Epoch [7077/20000], Training Loss: 0.5103\n",
            "Epoch [7078/20000], Training Loss: 0.4949\n",
            "Epoch [7079/20000], Training Loss: 0.5049\n",
            "Epoch [7080/20000], Training Loss: 0.4588\n",
            "Epoch [7081/20000], Training Loss: 0.4485\n",
            "Epoch [7082/20000], Training Loss: 0.5144\n",
            "Epoch [7083/20000], Training Loss: 0.5089\n",
            "Epoch [7084/20000], Training Loss: 0.4545\n",
            "Epoch [7085/20000], Training Loss: 0.4853\n",
            "Epoch [7086/20000], Training Loss: 0.4797\n",
            "Epoch [7087/20000], Training Loss: 0.4908\n",
            "Epoch [7088/20000], Training Loss: 0.5345\n",
            "Epoch [7089/20000], Training Loss: 0.4817\n",
            "Epoch [7090/20000], Training Loss: 0.5261\n",
            "Epoch [7091/20000], Training Loss: 0.5065\n",
            "Epoch [7092/20000], Training Loss: 0.5239\n",
            "Epoch [7093/20000], Training Loss: 0.4754\n",
            "Epoch [7094/20000], Training Loss: 0.4757\n",
            "Epoch [7095/20000], Training Loss: 0.5003\n",
            "Epoch [7096/20000], Training Loss: 0.5122\n",
            "Epoch [7097/20000], Training Loss: 0.4950\n",
            "Epoch [7098/20000], Training Loss: 0.4629\n",
            "Epoch [7099/20000], Training Loss: 0.5329\n",
            "Epoch [7100/20000], Training Loss: 0.4812\n",
            "Epoch [7101/20000], Training Loss: 0.4433\n",
            "Epoch [7102/20000], Training Loss: 0.4838\n",
            "Epoch [7103/20000], Training Loss: 0.5240\n",
            "Epoch [7104/20000], Training Loss: 0.5028\n",
            "Epoch [7105/20000], Training Loss: 0.4466\n",
            "Epoch [7106/20000], Training Loss: 0.4840\n",
            "Epoch [7107/20000], Training Loss: 0.5049\n",
            "Epoch [7108/20000], Training Loss: 0.4747\n",
            "Epoch [7109/20000], Training Loss: 0.5389\n",
            "Epoch [7110/20000], Training Loss: 0.5091\n",
            "Epoch [7111/20000], Training Loss: 0.5131\n",
            "Epoch [7112/20000], Training Loss: 0.4585\n",
            "Epoch [7113/20000], Training Loss: 0.4808\n",
            "Epoch [7114/20000], Training Loss: 0.4495\n",
            "Epoch [7115/20000], Training Loss: 0.4840\n",
            "Epoch [7116/20000], Training Loss: 0.4802\n",
            "Epoch [7117/20000], Training Loss: 0.5300\n",
            "Epoch [7118/20000], Training Loss: 0.5303\n",
            "Epoch [7119/20000], Training Loss: 0.4804\n",
            "Epoch [7120/20000], Training Loss: 0.5162\n",
            "Epoch [7121/20000], Training Loss: 0.4525\n",
            "Epoch [7122/20000], Training Loss: 0.5001\n",
            "Epoch [7123/20000], Training Loss: 0.5205\n",
            "Epoch [7124/20000], Training Loss: 0.4915\n",
            "Epoch [7125/20000], Training Loss: 0.5288\n",
            "Epoch [7126/20000], Training Loss: 0.5112\n",
            "Epoch [7127/20000], Training Loss: 0.5012\n",
            "Epoch [7128/20000], Training Loss: 0.5272\n",
            "Epoch [7129/20000], Training Loss: 0.4982\n",
            "Epoch [7130/20000], Training Loss: 0.5124\n",
            "Epoch [7131/20000], Training Loss: 0.4688\n",
            "Epoch [7132/20000], Training Loss: 0.4877\n",
            "Epoch [7133/20000], Training Loss: 0.4494\n",
            "Epoch [7134/20000], Training Loss: 0.4946\n",
            "Epoch [7135/20000], Training Loss: 0.5407\n",
            "Epoch [7136/20000], Training Loss: 0.4735\n",
            "Epoch [7137/20000], Training Loss: 0.5393\n",
            "Epoch [7138/20000], Training Loss: 0.4771\n",
            "Epoch [7139/20000], Training Loss: 0.4655\n",
            "Epoch [7140/20000], Training Loss: 0.4739\n",
            "Epoch [7141/20000], Training Loss: 0.5168\n",
            "Epoch [7142/20000], Training Loss: 0.4552\n",
            "Epoch [7143/20000], Training Loss: 0.4914\n",
            "Epoch [7144/20000], Training Loss: 0.4765\n",
            "Epoch [7145/20000], Training Loss: 0.4902\n",
            "Epoch [7146/20000], Training Loss: 0.4919\n",
            "Epoch [7147/20000], Training Loss: 0.5010\n",
            "Epoch [7148/20000], Training Loss: 0.5229\n",
            "Epoch [7149/20000], Training Loss: 0.5443\n",
            "Epoch [7150/20000], Training Loss: 0.4904\n",
            "Epoch [7151/20000], Training Loss: 0.4892\n",
            "Epoch [7152/20000], Training Loss: 0.5531\n",
            "Epoch [7153/20000], Training Loss: 0.4807\n",
            "Epoch [7154/20000], Training Loss: 0.5331\n",
            "Epoch [7155/20000], Training Loss: 0.4721\n",
            "Epoch [7156/20000], Training Loss: 0.4958\n",
            "Epoch [7157/20000], Training Loss: 0.5068\n",
            "Epoch [7158/20000], Training Loss: 0.5275\n",
            "Epoch [7159/20000], Training Loss: 0.4670\n",
            "Epoch [7160/20000], Training Loss: 0.4606\n",
            "Epoch [7161/20000], Training Loss: 0.5143\n",
            "Epoch [7162/20000], Training Loss: 0.4594\n",
            "Epoch [7163/20000], Training Loss: 0.4858\n",
            "Epoch [7164/20000], Training Loss: 0.5181\n",
            "Epoch [7165/20000], Training Loss: 0.5022\n",
            "Epoch [7166/20000], Training Loss: 0.4670\n",
            "Epoch [7167/20000], Training Loss: 0.5033\n",
            "Epoch [7168/20000], Training Loss: 0.4679\n",
            "Epoch [7169/20000], Training Loss: 0.4849\n",
            "Epoch [7170/20000], Training Loss: 0.4878\n",
            "Epoch [7171/20000], Training Loss: 0.4977\n",
            "Epoch [7172/20000], Training Loss: 0.5019\n",
            "Epoch [7173/20000], Training Loss: 0.5407\n",
            "Epoch [7174/20000], Training Loss: 0.5092\n",
            "Epoch [7175/20000], Training Loss: 0.5033\n",
            "Epoch [7176/20000], Training Loss: 0.5149\n",
            "Epoch [7177/20000], Training Loss: 0.4822\n",
            "Epoch [7178/20000], Training Loss: 0.4552\n",
            "Epoch [7179/20000], Training Loss: 0.4720\n",
            "Epoch [7180/20000], Training Loss: 0.4852\n",
            "Epoch [7181/20000], Training Loss: 0.5524\n",
            "Epoch [7182/20000], Training Loss: 0.5080\n",
            "Epoch [7183/20000], Training Loss: 0.4757\n",
            "Epoch [7184/20000], Training Loss: 0.5112\n",
            "Epoch [7185/20000], Training Loss: 0.4829\n",
            "Epoch [7186/20000], Training Loss: 0.4571\n",
            "Epoch [7187/20000], Training Loss: 0.5182\n",
            "Epoch [7188/20000], Training Loss: 0.4841\n",
            "Epoch [7189/20000], Training Loss: 0.5099\n",
            "Epoch [7190/20000], Training Loss: 0.4600\n",
            "Epoch [7191/20000], Training Loss: 0.4657\n",
            "Epoch [7192/20000], Training Loss: 0.4678\n",
            "Epoch [7193/20000], Training Loss: 0.5147\n",
            "Epoch [7194/20000], Training Loss: 0.5178\n",
            "Epoch [7195/20000], Training Loss: 0.5087\n",
            "Epoch [7196/20000], Training Loss: 0.4947\n",
            "Epoch [7197/20000], Training Loss: 0.5176\n",
            "Epoch [7198/20000], Training Loss: 0.4785\n",
            "Epoch [7199/20000], Training Loss: 0.4747\n",
            "Epoch [7200/20000], Training Loss: 0.4353\n",
            "Epoch [7201/20000], Training Loss: 0.5056\n",
            "Epoch [7202/20000], Training Loss: 0.4680\n",
            "Epoch [7203/20000], Training Loss: 0.4687\n",
            "Epoch [7204/20000], Training Loss: 0.4726\n",
            "Epoch [7205/20000], Training Loss: 0.5043\n",
            "Epoch [7206/20000], Training Loss: 0.4742\n",
            "Epoch [7207/20000], Training Loss: 0.4915\n",
            "Epoch [7208/20000], Training Loss: 0.5288\n",
            "Epoch [7209/20000], Training Loss: 0.5139\n",
            "Epoch [7210/20000], Training Loss: 0.4806\n",
            "Epoch [7211/20000], Training Loss: 0.4831\n",
            "Epoch [7212/20000], Training Loss: 0.5143\n",
            "Epoch [7213/20000], Training Loss: 0.4927\n",
            "Epoch [7214/20000], Training Loss: 0.4848\n",
            "Epoch [7215/20000], Training Loss: 0.5258\n",
            "Epoch [7216/20000], Training Loss: 0.5223\n",
            "Epoch [7217/20000], Training Loss: 0.5002\n",
            "Epoch [7218/20000], Training Loss: 0.4980\n",
            "Epoch [7219/20000], Training Loss: 0.4562\n",
            "Epoch [7220/20000], Training Loss: 0.4673\n",
            "Epoch [7221/20000], Training Loss: 0.4565\n",
            "Epoch [7222/20000], Training Loss: 0.4548\n",
            "Epoch [7223/20000], Training Loss: 0.4879\n",
            "Epoch [7224/20000], Training Loss: 0.4775\n",
            "Epoch [7225/20000], Training Loss: 0.5038\n",
            "Epoch [7226/20000], Training Loss: 0.4812\n",
            "Epoch [7227/20000], Training Loss: 0.4583\n",
            "Epoch [7228/20000], Training Loss: 0.5298\n",
            "Epoch [7229/20000], Training Loss: 0.4601\n",
            "Epoch [7230/20000], Training Loss: 0.4587\n",
            "Epoch [7231/20000], Training Loss: 0.5044\n",
            "Epoch [7232/20000], Training Loss: 0.5580\n",
            "Epoch [7233/20000], Training Loss: 0.5187\n",
            "Epoch [7234/20000], Training Loss: 0.4796\n",
            "Epoch [7235/20000], Training Loss: 0.4604\n",
            "Epoch [7236/20000], Training Loss: 0.4809\n",
            "Epoch [7237/20000], Training Loss: 0.4890\n",
            "Epoch [7238/20000], Training Loss: 0.5131\n",
            "Epoch [7239/20000], Training Loss: 0.4809\n",
            "Epoch [7240/20000], Training Loss: 0.4459\n",
            "Epoch [7241/20000], Training Loss: 0.4488\n",
            "Epoch [7242/20000], Training Loss: 0.5067\n",
            "Epoch [7243/20000], Training Loss: 0.4697\n",
            "Epoch [7244/20000], Training Loss: 0.4947\n",
            "Epoch [7245/20000], Training Loss: 0.5037\n",
            "Epoch [7246/20000], Training Loss: 0.4697\n",
            "Epoch [7247/20000], Training Loss: 0.4644\n",
            "Epoch [7248/20000], Training Loss: 0.4964\n",
            "Epoch [7249/20000], Training Loss: 0.5042\n",
            "Epoch [7250/20000], Training Loss: 0.4815\n",
            "Epoch [7251/20000], Training Loss: 0.4843\n",
            "Epoch [7252/20000], Training Loss: 0.4928\n",
            "Epoch [7253/20000], Training Loss: 0.4705\n",
            "Epoch [7254/20000], Training Loss: 0.4871\n",
            "Epoch [7255/20000], Training Loss: 0.4951\n",
            "Epoch [7256/20000], Training Loss: 0.5245\n",
            "Epoch [7257/20000], Training Loss: 0.4617\n",
            "Epoch [7258/20000], Training Loss: 0.5160\n",
            "Epoch [7259/20000], Training Loss: 0.4635\n",
            "Epoch [7260/20000], Training Loss: 0.4554\n",
            "Epoch [7261/20000], Training Loss: 0.4979\n",
            "Epoch [7262/20000], Training Loss: 0.4748\n",
            "Epoch [7263/20000], Training Loss: 0.5058\n",
            "Epoch [7264/20000], Training Loss: 0.4555\n",
            "Epoch [7265/20000], Training Loss: 0.5100\n",
            "Epoch [7266/20000], Training Loss: 0.4407\n",
            "Epoch [7267/20000], Training Loss: 0.4800\n",
            "Epoch [7268/20000], Training Loss: 0.4928\n",
            "Epoch [7269/20000], Training Loss: 0.4695\n",
            "Epoch [7270/20000], Training Loss: 0.5164\n",
            "Epoch [7271/20000], Training Loss: 0.4817\n",
            "Epoch [7272/20000], Training Loss: 0.4782\n",
            "Epoch [7273/20000], Training Loss: 0.4801\n",
            "Epoch [7274/20000], Training Loss: 0.4595\n",
            "Epoch [7275/20000], Training Loss: 0.5144\n",
            "Epoch [7276/20000], Training Loss: 0.4876\n",
            "Epoch [7277/20000], Training Loss: 0.5094\n",
            "Epoch [7278/20000], Training Loss: 0.5025\n",
            "Epoch [7279/20000], Training Loss: 0.4706\n",
            "Epoch [7280/20000], Training Loss: 0.5191\n",
            "Epoch [7281/20000], Training Loss: 0.5152\n",
            "Epoch [7282/20000], Training Loss: 0.4613\n",
            "Epoch [7283/20000], Training Loss: 0.4768\n",
            "Epoch [7284/20000], Training Loss: 0.4882\n",
            "Epoch [7285/20000], Training Loss: 0.4192\n",
            "Epoch [7286/20000], Training Loss: 0.4991\n",
            "Epoch [7287/20000], Training Loss: 0.4733\n",
            "Epoch [7288/20000], Training Loss: 0.4782\n",
            "Epoch [7289/20000], Training Loss: 0.5047\n",
            "Epoch [7290/20000], Training Loss: 0.5141\n",
            "Epoch [7291/20000], Training Loss: 0.4661\n",
            "Epoch [7292/20000], Training Loss: 0.4688\n",
            "Epoch [7293/20000], Training Loss: 0.5049\n",
            "Epoch [7294/20000], Training Loss: 0.4658\n",
            "Epoch [7295/20000], Training Loss: 0.4541\n",
            "Epoch [7296/20000], Training Loss: 0.4826\n",
            "Epoch [7297/20000], Training Loss: 0.5526\n",
            "Epoch [7298/20000], Training Loss: 0.4784\n",
            "Epoch [7299/20000], Training Loss: 0.4682\n",
            "Epoch [7300/20000], Training Loss: 0.5182\n",
            "Epoch [7301/20000], Training Loss: 0.4980\n",
            "Epoch [7302/20000], Training Loss: 0.5011\n",
            "Epoch [7303/20000], Training Loss: 0.4901\n",
            "Epoch [7304/20000], Training Loss: 0.5036\n",
            "Epoch [7305/20000], Training Loss: 0.4965\n",
            "Epoch [7306/20000], Training Loss: 0.4647\n",
            "Epoch [7307/20000], Training Loss: 0.5208\n",
            "Epoch [7308/20000], Training Loss: 0.4786\n",
            "Epoch [7309/20000], Training Loss: 0.5034\n",
            "Epoch [7310/20000], Training Loss: 0.4983\n",
            "Epoch [7311/20000], Training Loss: 0.4882\n",
            "Epoch [7312/20000], Training Loss: 0.4838\n",
            "Epoch [7313/20000], Training Loss: 0.4644\n",
            "Epoch [7314/20000], Training Loss: 0.5205\n",
            "Epoch [7315/20000], Training Loss: 0.5166\n",
            "Epoch [7316/20000], Training Loss: 0.5050\n",
            "Epoch [7317/20000], Training Loss: 0.4516\n",
            "Epoch [7318/20000], Training Loss: 0.4829\n",
            "Epoch [7319/20000], Training Loss: 0.4908\n",
            "Epoch [7320/20000], Training Loss: 0.5473\n",
            "Epoch [7321/20000], Training Loss: 0.4497\n",
            "Epoch [7322/20000], Training Loss: 0.4863\n",
            "Epoch [7323/20000], Training Loss: 0.4647\n",
            "Epoch [7324/20000], Training Loss: 0.4732\n",
            "Epoch [7325/20000], Training Loss: 0.4833\n",
            "Epoch [7326/20000], Training Loss: 0.4725\n",
            "Epoch [7327/20000], Training Loss: 0.5226\n",
            "Epoch [7328/20000], Training Loss: 0.5011\n",
            "Epoch [7329/20000], Training Loss: 0.4780\n",
            "Epoch [7330/20000], Training Loss: 0.5329\n",
            "Epoch [7331/20000], Training Loss: 0.4886\n",
            "Epoch [7332/20000], Training Loss: 0.4780\n",
            "Epoch [7333/20000], Training Loss: 0.5070\n",
            "Epoch [7334/20000], Training Loss: 0.4384\n",
            "Epoch [7335/20000], Training Loss: 0.4998\n",
            "Epoch [7336/20000], Training Loss: 0.4900\n",
            "Epoch [7337/20000], Training Loss: 0.5002\n",
            "Epoch [7338/20000], Training Loss: 0.4842\n",
            "Epoch [7339/20000], Training Loss: 0.4962\n",
            "Epoch [7340/20000], Training Loss: 0.5343\n",
            "Epoch [7341/20000], Training Loss: 0.5046\n",
            "Epoch [7342/20000], Training Loss: 0.5251\n",
            "Epoch [7343/20000], Training Loss: 0.4930\n",
            "Epoch [7344/20000], Training Loss: 0.4641\n",
            "Epoch [7345/20000], Training Loss: 0.4848\n",
            "Epoch [7346/20000], Training Loss: 0.4483\n",
            "Epoch [7347/20000], Training Loss: 0.4645\n",
            "Epoch [7348/20000], Training Loss: 0.5078\n",
            "Epoch [7349/20000], Training Loss: 0.4725\n",
            "Epoch [7350/20000], Training Loss: 0.4853\n",
            "Epoch [7351/20000], Training Loss: 0.4661\n",
            "Epoch [7352/20000], Training Loss: 0.4572\n",
            "Epoch [7353/20000], Training Loss: 0.4899\n",
            "Epoch [7354/20000], Training Loss: 0.4454\n",
            "Epoch [7355/20000], Training Loss: 0.4657\n",
            "Epoch [7356/20000], Training Loss: 0.4606\n",
            "Epoch [7357/20000], Training Loss: 0.4350\n",
            "Epoch [7358/20000], Training Loss: 0.4425\n",
            "Epoch [7359/20000], Training Loss: 0.5089\n",
            "Epoch [7360/20000], Training Loss: 0.4974\n",
            "Epoch [7361/20000], Training Loss: 0.5127\n",
            "Epoch [7362/20000], Training Loss: 0.4879\n",
            "Epoch [7363/20000], Training Loss: 0.5175\n",
            "Epoch [7364/20000], Training Loss: 0.4652\n",
            "Epoch [7365/20000], Training Loss: 0.4743\n",
            "Epoch [7366/20000], Training Loss: 0.5032\n",
            "Epoch [7367/20000], Training Loss: 0.5051\n",
            "Epoch [7368/20000], Training Loss: 0.4780\n",
            "Epoch [7369/20000], Training Loss: 0.4667\n",
            "Epoch [7370/20000], Training Loss: 0.5145\n",
            "Epoch [7371/20000], Training Loss: 0.4906\n",
            "Epoch [7372/20000], Training Loss: 0.4951\n",
            "Epoch [7373/20000], Training Loss: 0.5079\n",
            "Epoch [7374/20000], Training Loss: 0.5563\n",
            "Epoch [7375/20000], Training Loss: 0.4978\n",
            "Epoch [7376/20000], Training Loss: 0.5032\n",
            "Epoch [7377/20000], Training Loss: 0.5374\n",
            "Epoch [7378/20000], Training Loss: 0.5263\n",
            "Epoch [7379/20000], Training Loss: 0.4653\n",
            "Epoch [7380/20000], Training Loss: 0.5414\n",
            "Epoch [7381/20000], Training Loss: 0.4980\n",
            "Epoch [7382/20000], Training Loss: 0.5325\n",
            "Epoch [7383/20000], Training Loss: 0.4621\n",
            "Epoch [7384/20000], Training Loss: 0.5094\n",
            "Epoch [7385/20000], Training Loss: 0.4945\n",
            "Epoch [7386/20000], Training Loss: 0.5107\n",
            "Epoch [7387/20000], Training Loss: 0.4455\n",
            "Epoch [7388/20000], Training Loss: 0.4500\n",
            "Epoch [7389/20000], Training Loss: 0.4626\n",
            "Epoch [7390/20000], Training Loss: 0.4532\n",
            "Epoch [7391/20000], Training Loss: 0.4322\n",
            "Epoch [7392/20000], Training Loss: 0.5125\n",
            "Epoch [7393/20000], Training Loss: 0.5261\n",
            "Epoch [7394/20000], Training Loss: 0.5045\n",
            "Epoch [7395/20000], Training Loss: 0.4853\n",
            "Epoch [7396/20000], Training Loss: 0.5162\n",
            "Epoch [7397/20000], Training Loss: 0.5231\n",
            "Epoch [7398/20000], Training Loss: 0.5472\n",
            "Epoch [7399/20000], Training Loss: 0.4817\n",
            "Epoch [7400/20000], Training Loss: 0.4939\n",
            "Epoch [7401/20000], Training Loss: 0.5466\n",
            "Epoch [7402/20000], Training Loss: 0.4957\n",
            "Epoch [7403/20000], Training Loss: 0.5327\n",
            "Epoch [7404/20000], Training Loss: 0.4703\n",
            "Epoch [7405/20000], Training Loss: 0.4804\n",
            "Epoch [7406/20000], Training Loss: 0.4629\n",
            "Epoch [7407/20000], Training Loss: 0.5401\n",
            "Epoch [7408/20000], Training Loss: 0.4729\n",
            "Epoch [7409/20000], Training Loss: 0.4343\n",
            "Epoch [7410/20000], Training Loss: 0.5055\n",
            "Epoch [7411/20000], Training Loss: 0.4648\n",
            "Epoch [7412/20000], Training Loss: 0.4931\n",
            "Epoch [7413/20000], Training Loss: 0.4590\n",
            "Epoch [7414/20000], Training Loss: 0.4715\n",
            "Epoch [7415/20000], Training Loss: 0.4380\n",
            "Epoch [7416/20000], Training Loss: 0.5050\n",
            "Epoch [7417/20000], Training Loss: 0.5184\n",
            "Epoch [7418/20000], Training Loss: 0.5185\n",
            "Epoch [7419/20000], Training Loss: 0.4714\n",
            "Epoch [7420/20000], Training Loss: 0.5103\n",
            "Epoch [7421/20000], Training Loss: 0.4913\n",
            "Epoch [7422/20000], Training Loss: 0.4587\n",
            "Epoch [7423/20000], Training Loss: 0.5085\n",
            "Epoch [7424/20000], Training Loss: 0.4689\n",
            "Epoch [7425/20000], Training Loss: 0.4658\n",
            "Epoch [7426/20000], Training Loss: 0.4866\n",
            "Epoch [7427/20000], Training Loss: 0.5148\n",
            "Epoch [7428/20000], Training Loss: 0.5032\n",
            "Epoch [7429/20000], Training Loss: 0.4860\n",
            "Epoch [7430/20000], Training Loss: 0.4841\n",
            "Epoch [7431/20000], Training Loss: 0.5121\n",
            "Epoch [7432/20000], Training Loss: 0.5069\n",
            "Epoch [7433/20000], Training Loss: 0.4846\n",
            "Epoch [7434/20000], Training Loss: 0.4833\n",
            "Epoch [7435/20000], Training Loss: 0.4792\n",
            "Epoch [7436/20000], Training Loss: 0.5065\n",
            "Epoch [7437/20000], Training Loss: 0.4545\n",
            "Epoch [7438/20000], Training Loss: 0.5050\n",
            "Epoch [7439/20000], Training Loss: 0.5093\n",
            "Epoch [7440/20000], Training Loss: 0.4917\n",
            "Epoch [7441/20000], Training Loss: 0.5070\n",
            "Epoch [7442/20000], Training Loss: 0.4723\n",
            "Epoch [7443/20000], Training Loss: 0.4821\n",
            "Epoch [7444/20000], Training Loss: 0.4965\n",
            "Epoch [7445/20000], Training Loss: 0.4850\n",
            "Epoch [7446/20000], Training Loss: 0.4603\n",
            "Epoch [7447/20000], Training Loss: 0.4899\n",
            "Epoch [7448/20000], Training Loss: 0.4707\n",
            "Epoch [7449/20000], Training Loss: 0.5339\n",
            "Epoch [7450/20000], Training Loss: 0.5413\n",
            "Epoch [7451/20000], Training Loss: 0.5240\n",
            "Epoch [7452/20000], Training Loss: 0.4927\n",
            "Epoch [7453/20000], Training Loss: 0.4727\n",
            "Epoch [7454/20000], Training Loss: 0.5114\n",
            "Epoch [7455/20000], Training Loss: 0.4897\n",
            "Epoch [7456/20000], Training Loss: 0.5217\n",
            "Epoch [7457/20000], Training Loss: 0.5039\n",
            "Epoch [7458/20000], Training Loss: 0.4490\n",
            "Epoch [7459/20000], Training Loss: 0.4706\n",
            "Epoch [7460/20000], Training Loss: 0.5066\n",
            "Epoch [7461/20000], Training Loss: 0.5108\n",
            "Epoch [7462/20000], Training Loss: 0.4707\n",
            "Epoch [7463/20000], Training Loss: 0.4898\n",
            "Epoch [7464/20000], Training Loss: 0.4995\n",
            "Epoch [7465/20000], Training Loss: 0.4719\n",
            "Epoch [7466/20000], Training Loss: 0.4937\n",
            "Epoch [7467/20000], Training Loss: 0.4510\n",
            "Epoch [7468/20000], Training Loss: 0.5045\n",
            "Epoch [7469/20000], Training Loss: 0.4874\n",
            "Epoch [7470/20000], Training Loss: 0.5181\n",
            "Epoch [7471/20000], Training Loss: 0.4850\n",
            "Epoch [7472/20000], Training Loss: 0.4858\n",
            "Epoch [7473/20000], Training Loss: 0.4611\n",
            "Epoch [7474/20000], Training Loss: 0.5126\n",
            "Epoch [7475/20000], Training Loss: 0.5206\n",
            "Epoch [7476/20000], Training Loss: 0.4638\n",
            "Epoch [7477/20000], Training Loss: 0.4310\n",
            "Epoch [7478/20000], Training Loss: 0.4998\n",
            "Epoch [7479/20000], Training Loss: 0.5098\n",
            "Epoch [7480/20000], Training Loss: 0.5181\n",
            "Epoch [7481/20000], Training Loss: 0.4514\n",
            "Epoch [7482/20000], Training Loss: 0.4808\n",
            "Epoch [7483/20000], Training Loss: 0.4527\n",
            "Epoch [7484/20000], Training Loss: 0.5202\n",
            "Epoch [7485/20000], Training Loss: 0.4703\n",
            "Epoch [7486/20000], Training Loss: 0.5424\n",
            "Epoch [7487/20000], Training Loss: 0.4788\n",
            "Epoch [7488/20000], Training Loss: 0.4663\n",
            "Epoch [7489/20000], Training Loss: 0.4658\n",
            "Epoch [7490/20000], Training Loss: 0.4661\n",
            "Epoch [7491/20000], Training Loss: 0.4894\n",
            "Epoch [7492/20000], Training Loss: 0.5013\n",
            "Epoch [7493/20000], Training Loss: 0.4942\n",
            "Epoch [7494/20000], Training Loss: 0.4841\n",
            "Epoch [7495/20000], Training Loss: 0.4490\n",
            "Epoch [7496/20000], Training Loss: 0.4493\n",
            "Epoch [7497/20000], Training Loss: 0.4836\n",
            "Epoch [7498/20000], Training Loss: 0.4456\n",
            "Epoch [7499/20000], Training Loss: 0.4546\n",
            "Epoch [7500/20000], Training Loss: 0.4703\n",
            "Epoch [7501/20000], Training Loss: 0.5013\n",
            "Epoch [7502/20000], Training Loss: 0.4937\n",
            "Epoch [7503/20000], Training Loss: 0.4706\n",
            "Epoch [7504/20000], Training Loss: 0.5431\n",
            "Epoch [7505/20000], Training Loss: 0.4728\n",
            "Epoch [7506/20000], Training Loss: 0.4457\n",
            "Epoch [7507/20000], Training Loss: 0.5498\n",
            "Epoch [7508/20000], Training Loss: 0.4818\n",
            "Epoch [7509/20000], Training Loss: 0.4568\n",
            "Epoch [7510/20000], Training Loss: 0.5077\n",
            "Epoch [7511/20000], Training Loss: 0.4977\n",
            "Epoch [7512/20000], Training Loss: 0.5012\n",
            "Epoch [7513/20000], Training Loss: 0.4988\n",
            "Epoch [7514/20000], Training Loss: 0.5363\n",
            "Epoch [7515/20000], Training Loss: 0.4674\n",
            "Epoch [7516/20000], Training Loss: 0.4760\n",
            "Epoch [7517/20000], Training Loss: 0.4865\n",
            "Epoch [7518/20000], Training Loss: 0.4614\n",
            "Epoch [7519/20000], Training Loss: 0.4889\n",
            "Epoch [7520/20000], Training Loss: 0.5187\n",
            "Epoch [7521/20000], Training Loss: 0.5041\n",
            "Epoch [7522/20000], Training Loss: 0.5352\n",
            "Epoch [7523/20000], Training Loss: 0.4820\n",
            "Epoch [7524/20000], Training Loss: 0.4471\n",
            "Epoch [7525/20000], Training Loss: 0.4875\n",
            "Epoch [7526/20000], Training Loss: 0.5120\n",
            "Epoch [7527/20000], Training Loss: 0.4763\n",
            "Epoch [7528/20000], Training Loss: 0.5026\n",
            "Epoch [7529/20000], Training Loss: 0.4847\n",
            "Epoch [7530/20000], Training Loss: 0.5226\n",
            "Epoch [7531/20000], Training Loss: 0.5278\n",
            "Epoch [7532/20000], Training Loss: 0.4617\n",
            "Epoch [7533/20000], Training Loss: 0.5247\n",
            "Epoch [7534/20000], Training Loss: 0.4735\n",
            "Epoch [7535/20000], Training Loss: 0.5016\n",
            "Epoch [7536/20000], Training Loss: 0.5478\n",
            "Epoch [7537/20000], Training Loss: 0.5306\n",
            "Epoch [7538/20000], Training Loss: 0.5111\n",
            "Epoch [7539/20000], Training Loss: 0.4688\n",
            "Epoch [7540/20000], Training Loss: 0.4474\n",
            "Epoch [7541/20000], Training Loss: 0.4905\n",
            "Epoch [7542/20000], Training Loss: 0.4911\n",
            "Epoch [7543/20000], Training Loss: 0.4918\n",
            "Epoch [7544/20000], Training Loss: 0.5457\n",
            "Epoch [7545/20000], Training Loss: 0.4846\n",
            "Epoch [7546/20000], Training Loss: 0.5440\n",
            "Epoch [7547/20000], Training Loss: 0.4454\n",
            "Epoch [7548/20000], Training Loss: 0.4651\n",
            "Epoch [7549/20000], Training Loss: 0.4687\n",
            "Epoch [7550/20000], Training Loss: 0.4809\n",
            "Epoch [7551/20000], Training Loss: 0.5047\n",
            "Epoch [7552/20000], Training Loss: 0.5338\n",
            "Epoch [7553/20000], Training Loss: 0.4996\n",
            "Epoch [7554/20000], Training Loss: 0.4790\n",
            "Epoch [7555/20000], Training Loss: 0.4491\n",
            "Epoch [7556/20000], Training Loss: 0.4884\n",
            "Epoch [7557/20000], Training Loss: 0.5085\n",
            "Epoch [7558/20000], Training Loss: 0.5249\n",
            "Epoch [7559/20000], Training Loss: 0.4780\n",
            "Epoch [7560/20000], Training Loss: 0.5087\n",
            "Epoch [7561/20000], Training Loss: 0.5299\n",
            "Epoch [7562/20000], Training Loss: 0.5192\n",
            "Epoch [7563/20000], Training Loss: 0.4358\n",
            "Epoch [7564/20000], Training Loss: 0.4660\n",
            "Epoch [7565/20000], Training Loss: 0.4797\n",
            "Epoch [7566/20000], Training Loss: 0.4819\n",
            "Epoch [7567/20000], Training Loss: 0.4919\n",
            "Epoch [7568/20000], Training Loss: 0.4481\n",
            "Epoch [7569/20000], Training Loss: 0.5186\n",
            "Epoch [7570/20000], Training Loss: 0.5409\n",
            "Epoch [7571/20000], Training Loss: 0.4954\n",
            "Epoch [7572/20000], Training Loss: 0.4905\n",
            "Epoch [7573/20000], Training Loss: 0.4911\n",
            "Epoch [7574/20000], Training Loss: 0.4941\n",
            "Epoch [7575/20000], Training Loss: 0.4781\n",
            "Epoch [7576/20000], Training Loss: 0.5011\n",
            "Epoch [7577/20000], Training Loss: 0.4953\n",
            "Epoch [7578/20000], Training Loss: 0.4919\n",
            "Epoch [7579/20000], Training Loss: 0.4963\n",
            "Epoch [7580/20000], Training Loss: 0.4977\n",
            "Epoch [7581/20000], Training Loss: 0.4831\n",
            "Epoch [7582/20000], Training Loss: 0.5076\n",
            "Epoch [7583/20000], Training Loss: 0.4546\n",
            "Epoch [7584/20000], Training Loss: 0.5071\n",
            "Epoch [7585/20000], Training Loss: 0.4366\n",
            "Epoch [7586/20000], Training Loss: 0.4662\n",
            "Epoch [7587/20000], Training Loss: 0.4557\n",
            "Epoch [7588/20000], Training Loss: 0.4819\n",
            "Epoch [7589/20000], Training Loss: 0.4964\n",
            "Epoch [7590/20000], Training Loss: 0.4574\n",
            "Epoch [7591/20000], Training Loss: 0.4982\n",
            "Epoch [7592/20000], Training Loss: 0.4835\n",
            "Epoch [7593/20000], Training Loss: 0.4751\n",
            "Epoch [7594/20000], Training Loss: 0.5525\n",
            "Epoch [7595/20000], Training Loss: 0.4616\n",
            "Epoch [7596/20000], Training Loss: 0.5058\n",
            "Epoch [7597/20000], Training Loss: 0.4957\n",
            "Epoch [7598/20000], Training Loss: 0.4681\n",
            "Epoch [7599/20000], Training Loss: 0.4783\n",
            "Epoch [7600/20000], Training Loss: 0.4563\n",
            "Epoch [7601/20000], Training Loss: 0.4646\n",
            "Epoch [7602/20000], Training Loss: 0.4516\n",
            "Epoch [7603/20000], Training Loss: 0.4945\n",
            "Epoch [7604/20000], Training Loss: 0.5225\n",
            "Epoch [7605/20000], Training Loss: 0.5039\n",
            "Epoch [7606/20000], Training Loss: 0.4882\n",
            "Epoch [7607/20000], Training Loss: 0.4858\n",
            "Epoch [7608/20000], Training Loss: 0.5224\n",
            "Epoch [7609/20000], Training Loss: 0.5324\n",
            "Epoch [7610/20000], Training Loss: 0.5000\n",
            "Epoch [7611/20000], Training Loss: 0.4565\n",
            "Epoch [7612/20000], Training Loss: 0.4668\n",
            "Epoch [7613/20000], Training Loss: 0.5112\n",
            "Epoch [7614/20000], Training Loss: 0.4768\n",
            "Epoch [7615/20000], Training Loss: 0.4810\n",
            "Epoch [7616/20000], Training Loss: 0.4541\n",
            "Epoch [7617/20000], Training Loss: 0.5017\n",
            "Epoch [7618/20000], Training Loss: 0.4844\n",
            "Epoch [7619/20000], Training Loss: 0.4785\n",
            "Epoch [7620/20000], Training Loss: 0.4743\n",
            "Epoch [7621/20000], Training Loss: 0.4875\n",
            "Epoch [7622/20000], Training Loss: 0.4731\n",
            "Epoch [7623/20000], Training Loss: 0.5313\n",
            "Epoch [7624/20000], Training Loss: 0.5202\n",
            "Epoch [7625/20000], Training Loss: 0.4728\n",
            "Epoch [7626/20000], Training Loss: 0.4483\n",
            "Epoch [7627/20000], Training Loss: 0.4949\n",
            "Epoch [7628/20000], Training Loss: 0.5029\n",
            "Epoch [7629/20000], Training Loss: 0.4815\n",
            "Epoch [7630/20000], Training Loss: 0.5140\n",
            "Epoch [7631/20000], Training Loss: 0.4811\n",
            "Epoch [7632/20000], Training Loss: 0.4636\n",
            "Epoch [7633/20000], Training Loss: 0.4905\n",
            "Epoch [7634/20000], Training Loss: 0.4990\n",
            "Epoch [7635/20000], Training Loss: 0.5081\n",
            "Epoch [7636/20000], Training Loss: 0.4741\n",
            "Epoch [7637/20000], Training Loss: 0.4828\n",
            "Epoch [7638/20000], Training Loss: 0.4977\n",
            "Epoch [7639/20000], Training Loss: 0.4829\n",
            "Epoch [7640/20000], Training Loss: 0.4842\n",
            "Epoch [7641/20000], Training Loss: 0.4966\n",
            "Epoch [7642/20000], Training Loss: 0.4756\n",
            "Epoch [7643/20000], Training Loss: 0.5026\n",
            "Epoch [7644/20000], Training Loss: 0.5183\n",
            "Epoch [7645/20000], Training Loss: 0.5074\n",
            "Epoch [7646/20000], Training Loss: 0.4664\n",
            "Epoch [7647/20000], Training Loss: 0.5194\n",
            "Epoch [7648/20000], Training Loss: 0.4729\n",
            "Epoch [7649/20000], Training Loss: 0.5174\n",
            "Epoch [7650/20000], Training Loss: 0.4920\n",
            "Epoch [7651/20000], Training Loss: 0.4973\n",
            "Epoch [7652/20000], Training Loss: 0.4734\n",
            "Epoch [7653/20000], Training Loss: 0.4763\n",
            "Epoch [7654/20000], Training Loss: 0.4399\n",
            "Epoch [7655/20000], Training Loss: 0.4738\n",
            "Epoch [7656/20000], Training Loss: 0.4592\n",
            "Epoch [7657/20000], Training Loss: 0.5212\n",
            "Epoch [7658/20000], Training Loss: 0.4652\n",
            "Epoch [7659/20000], Training Loss: 0.4913\n",
            "Epoch [7660/20000], Training Loss: 0.4884\n",
            "Epoch [7661/20000], Training Loss: 0.4891\n",
            "Epoch [7662/20000], Training Loss: 0.4653\n",
            "Epoch [7663/20000], Training Loss: 0.4486\n",
            "Epoch [7664/20000], Training Loss: 0.4996\n",
            "Epoch [7665/20000], Training Loss: 0.4957\n",
            "Epoch [7666/20000], Training Loss: 0.4753\n",
            "Epoch [7667/20000], Training Loss: 0.4772\n",
            "Epoch [7668/20000], Training Loss: 0.4966\n",
            "Epoch [7669/20000], Training Loss: 0.4711\n",
            "Epoch [7670/20000], Training Loss: 0.4756\n",
            "Epoch [7671/20000], Training Loss: 0.5547\n",
            "Epoch [7672/20000], Training Loss: 0.5142\n",
            "Epoch [7673/20000], Training Loss: 0.4764\n",
            "Epoch [7674/20000], Training Loss: 0.4791\n",
            "Epoch [7675/20000], Training Loss: 0.4800\n",
            "Epoch [7676/20000], Training Loss: 0.4411\n",
            "Epoch [7677/20000], Training Loss: 0.4555\n",
            "Epoch [7678/20000], Training Loss: 0.5353\n",
            "Epoch [7679/20000], Training Loss: 0.4803\n",
            "Epoch [7680/20000], Training Loss: 0.4993\n",
            "Epoch [7681/20000], Training Loss: 0.5014\n",
            "Epoch [7682/20000], Training Loss: 0.5148\n",
            "Epoch [7683/20000], Training Loss: 0.4918\n",
            "Epoch [7684/20000], Training Loss: 0.4987\n",
            "Epoch [7685/20000], Training Loss: 0.4988\n",
            "Epoch [7686/20000], Training Loss: 0.4977\n",
            "Epoch [7687/20000], Training Loss: 0.4477\n",
            "Epoch [7688/20000], Training Loss: 0.4844\n",
            "Epoch [7689/20000], Training Loss: 0.5016\n",
            "Epoch [7690/20000], Training Loss: 0.4415\n",
            "Epoch [7691/20000], Training Loss: 0.4784\n",
            "Epoch [7692/20000], Training Loss: 0.5151\n",
            "Epoch [7693/20000], Training Loss: 0.4771\n",
            "Epoch [7694/20000], Training Loss: 0.5264\n",
            "Epoch [7695/20000], Training Loss: 0.4752\n",
            "Epoch [7696/20000], Training Loss: 0.5021\n",
            "Epoch [7697/20000], Training Loss: 0.4873\n",
            "Epoch [7698/20000], Training Loss: 0.4545\n",
            "Epoch [7699/20000], Training Loss: 0.5017\n",
            "Epoch [7700/20000], Training Loss: 0.5067\n",
            "Epoch [7701/20000], Training Loss: 0.4842\n",
            "Epoch [7702/20000], Training Loss: 0.5067\n",
            "Epoch [7703/20000], Training Loss: 0.4573\n",
            "Epoch [7704/20000], Training Loss: 0.5019\n",
            "Epoch [7705/20000], Training Loss: 0.4967\n",
            "Epoch [7706/20000], Training Loss: 0.4822\n",
            "Epoch [7707/20000], Training Loss: 0.4772\n",
            "Epoch [7708/20000], Training Loss: 0.4820\n",
            "Epoch [7709/20000], Training Loss: 0.4655\n",
            "Epoch [7710/20000], Training Loss: 0.4861\n",
            "Epoch [7711/20000], Training Loss: 0.4505\n",
            "Epoch [7712/20000], Training Loss: 0.4634\n",
            "Epoch [7713/20000], Training Loss: 0.4896\n",
            "Epoch [7714/20000], Training Loss: 0.4323\n",
            "Epoch [7715/20000], Training Loss: 0.4985\n",
            "Epoch [7716/20000], Training Loss: 0.4675\n",
            "Epoch [7717/20000], Training Loss: 0.5317\n",
            "Epoch [7718/20000], Training Loss: 0.5068\n",
            "Epoch [7719/20000], Training Loss: 0.5029\n",
            "Epoch [7720/20000], Training Loss: 0.4789\n",
            "Epoch [7721/20000], Training Loss: 0.4866\n",
            "Epoch [7722/20000], Training Loss: 0.4898\n",
            "Epoch [7723/20000], Training Loss: 0.4906\n",
            "Epoch [7724/20000], Training Loss: 0.5117\n",
            "Epoch [7725/20000], Training Loss: 0.5108\n",
            "Epoch [7726/20000], Training Loss: 0.4895\n",
            "Epoch [7727/20000], Training Loss: 0.5279\n",
            "Epoch [7728/20000], Training Loss: 0.5056\n",
            "Epoch [7729/20000], Training Loss: 0.5286\n",
            "Epoch [7730/20000], Training Loss: 0.5557\n",
            "Epoch [7731/20000], Training Loss: 0.4840\n",
            "Epoch [7732/20000], Training Loss: 0.4851\n",
            "Epoch [7733/20000], Training Loss: 0.4475\n",
            "Epoch [7734/20000], Training Loss: 0.4865\n",
            "Epoch [7735/20000], Training Loss: 0.5034\n",
            "Epoch [7736/20000], Training Loss: 0.4746\n",
            "Epoch [7737/20000], Training Loss: 0.5396\n",
            "Epoch [7738/20000], Training Loss: 0.5095\n",
            "Epoch [7739/20000], Training Loss: 0.4791\n",
            "Epoch [7740/20000], Training Loss: 0.4367\n",
            "Epoch [7741/20000], Training Loss: 0.4807\n",
            "Epoch [7742/20000], Training Loss: 0.4775\n",
            "Epoch [7743/20000], Training Loss: 0.5003\n",
            "Epoch [7744/20000], Training Loss: 0.4965\n",
            "Epoch [7745/20000], Training Loss: 0.4751\n",
            "Epoch [7746/20000], Training Loss: 0.4949\n",
            "Epoch [7747/20000], Training Loss: 0.5006\n",
            "Epoch [7748/20000], Training Loss: 0.4589\n",
            "Epoch [7749/20000], Training Loss: 0.5068\n",
            "Epoch [7750/20000], Training Loss: 0.5260\n",
            "Epoch [7751/20000], Training Loss: 0.5303\n",
            "Epoch [7752/20000], Training Loss: 0.4819\n",
            "Epoch [7753/20000], Training Loss: 0.4564\n",
            "Epoch [7754/20000], Training Loss: 0.4771\n",
            "Epoch [7755/20000], Training Loss: 0.4809\n",
            "Epoch [7756/20000], Training Loss: 0.4724\n",
            "Epoch [7757/20000], Training Loss: 0.4947\n",
            "Epoch [7758/20000], Training Loss: 0.4857\n",
            "Epoch [7759/20000], Training Loss: 0.4772\n",
            "Epoch [7760/20000], Training Loss: 0.4763\n",
            "Epoch [7761/20000], Training Loss: 0.4461\n",
            "Epoch [7762/20000], Training Loss: 0.4778\n",
            "Epoch [7763/20000], Training Loss: 0.4771\n",
            "Epoch [7764/20000], Training Loss: 0.4672\n",
            "Epoch [7765/20000], Training Loss: 0.4646\n",
            "Epoch [7766/20000], Training Loss: 0.4537\n",
            "Epoch [7767/20000], Training Loss: 0.5122\n",
            "Epoch [7768/20000], Training Loss: 0.4675\n",
            "Epoch [7769/20000], Training Loss: 0.4790\n",
            "Epoch [7770/20000], Training Loss: 0.5032\n",
            "Epoch [7771/20000], Training Loss: 0.4761\n",
            "Epoch [7772/20000], Training Loss: 0.4754\n",
            "Epoch [7773/20000], Training Loss: 0.5100\n",
            "Epoch [7774/20000], Training Loss: 0.4798\n",
            "Epoch [7775/20000], Training Loss: 0.5124\n",
            "Epoch [7776/20000], Training Loss: 0.4795\n",
            "Epoch [7777/20000], Training Loss: 0.4762\n",
            "Epoch [7778/20000], Training Loss: 0.4521\n",
            "Epoch [7779/20000], Training Loss: 0.4705\n",
            "Epoch [7780/20000], Training Loss: 0.5159\n",
            "Epoch [7781/20000], Training Loss: 0.4718\n",
            "Epoch [7782/20000], Training Loss: 0.5109\n",
            "Epoch [7783/20000], Training Loss: 0.4275\n",
            "Epoch [7784/20000], Training Loss: 0.5021\n",
            "Epoch [7785/20000], Training Loss: 0.4961\n",
            "Epoch [7786/20000], Training Loss: 0.5134\n",
            "Epoch [7787/20000], Training Loss: 0.4795\n",
            "Epoch [7788/20000], Training Loss: 0.5286\n",
            "Epoch [7789/20000], Training Loss: 0.4979\n",
            "Epoch [7790/20000], Training Loss: 0.4950\n",
            "Epoch [7791/20000], Training Loss: 0.4840\n",
            "Epoch [7792/20000], Training Loss: 0.4857\n",
            "Epoch [7793/20000], Training Loss: 0.4984\n",
            "Epoch [7794/20000], Training Loss: 0.4807\n",
            "Epoch [7795/20000], Training Loss: 0.5181\n",
            "Epoch [7796/20000], Training Loss: 0.4821\n",
            "Epoch [7797/20000], Training Loss: 0.5482\n",
            "Epoch [7798/20000], Training Loss: 0.4812\n",
            "Epoch [7799/20000], Training Loss: 0.4499\n",
            "Epoch [7800/20000], Training Loss: 0.4703\n",
            "Epoch [7801/20000], Training Loss: 0.5542\n",
            "Epoch [7802/20000], Training Loss: 0.5211\n",
            "Epoch [7803/20000], Training Loss: 0.5370\n",
            "Epoch [7804/20000], Training Loss: 0.4927\n",
            "Epoch [7805/20000], Training Loss: 0.4471\n",
            "Epoch [7806/20000], Training Loss: 0.4721\n",
            "Epoch [7807/20000], Training Loss: 0.4685\n",
            "Epoch [7808/20000], Training Loss: 0.5052\n",
            "Epoch [7809/20000], Training Loss: 0.4868\n",
            "Epoch [7810/20000], Training Loss: 0.4470\n",
            "Epoch [7811/20000], Training Loss: 0.4978\n",
            "Epoch [7812/20000], Training Loss: 0.5002\n",
            "Epoch [7813/20000], Training Loss: 0.4972\n",
            "Epoch [7814/20000], Training Loss: 0.5011\n",
            "Epoch [7815/20000], Training Loss: 0.4603\n",
            "Epoch [7816/20000], Training Loss: 0.4801\n",
            "Epoch [7817/20000], Training Loss: 0.4661\n",
            "Epoch [7818/20000], Training Loss: 0.4976\n",
            "Epoch [7819/20000], Training Loss: 0.4607\n",
            "Epoch [7820/20000], Training Loss: 0.4828\n",
            "Epoch [7821/20000], Training Loss: 0.4585\n",
            "Epoch [7822/20000], Training Loss: 0.5074\n",
            "Epoch [7823/20000], Training Loss: 0.4401\n",
            "Epoch [7824/20000], Training Loss: 0.5436\n",
            "Epoch [7825/20000], Training Loss: 0.4917\n",
            "Epoch [7826/20000], Training Loss: 0.4596\n",
            "Epoch [7827/20000], Training Loss: 0.4975\n",
            "Epoch [7828/20000], Training Loss: 0.5414\n",
            "Epoch [7829/20000], Training Loss: 0.4828\n",
            "Epoch [7830/20000], Training Loss: 0.4702\n",
            "Epoch [7831/20000], Training Loss: 0.5055\n",
            "Epoch [7832/20000], Training Loss: 0.5207\n",
            "Epoch [7833/20000], Training Loss: 0.4491\n",
            "Epoch [7834/20000], Training Loss: 0.5036\n",
            "Epoch [7835/20000], Training Loss: 0.4621\n",
            "Epoch [7836/20000], Training Loss: 0.4628\n",
            "Epoch [7837/20000], Training Loss: 0.4783\n",
            "Epoch [7838/20000], Training Loss: 0.4564\n",
            "Epoch [7839/20000], Training Loss: 0.4820\n",
            "Epoch [7840/20000], Training Loss: 0.5093\n",
            "Epoch [7841/20000], Training Loss: 0.4991\n",
            "Epoch [7842/20000], Training Loss: 0.4703\n",
            "Epoch [7843/20000], Training Loss: 0.5079\n",
            "Epoch [7844/20000], Training Loss: 0.5142\n",
            "Epoch [7845/20000], Training Loss: 0.4749\n",
            "Epoch [7846/20000], Training Loss: 0.5176\n",
            "Epoch [7847/20000], Training Loss: 0.5336\n",
            "Epoch [7848/20000], Training Loss: 0.4935\n",
            "Epoch [7849/20000], Training Loss: 0.5188\n",
            "Epoch [7850/20000], Training Loss: 0.5218\n",
            "Epoch [7851/20000], Training Loss: 0.5194\n",
            "Epoch [7852/20000], Training Loss: 0.4769\n",
            "Epoch [7853/20000], Training Loss: 0.4798\n",
            "Epoch [7854/20000], Training Loss: 0.4534\n",
            "Epoch [7855/20000], Training Loss: 0.4769\n",
            "Epoch [7856/20000], Training Loss: 0.5180\n",
            "Epoch [7857/20000], Training Loss: 0.5502\n",
            "Epoch [7858/20000], Training Loss: 0.4945\n",
            "Epoch [7859/20000], Training Loss: 0.4749\n",
            "Epoch [7860/20000], Training Loss: 0.5230\n",
            "Epoch [7861/20000], Training Loss: 0.4798\n",
            "Epoch [7862/20000], Training Loss: 0.4842\n",
            "Epoch [7863/20000], Training Loss: 0.4620\n",
            "Epoch [7864/20000], Training Loss: 0.4464\n",
            "Epoch [7865/20000], Training Loss: 0.4587\n",
            "Epoch [7866/20000], Training Loss: 0.4898\n",
            "Epoch [7867/20000], Training Loss: 0.4804\n",
            "Epoch [7868/20000], Training Loss: 0.5007\n",
            "Epoch [7869/20000], Training Loss: 0.4376\n",
            "Epoch [7870/20000], Training Loss: 0.4802\n",
            "Epoch [7871/20000], Training Loss: 0.4869\n",
            "Epoch [7872/20000], Training Loss: 0.4755\n",
            "Epoch [7873/20000], Training Loss: 0.4631\n",
            "Epoch [7874/20000], Training Loss: 0.5308\n",
            "Epoch [7875/20000], Training Loss: 0.4815\n",
            "Epoch [7876/20000], Training Loss: 0.4752\n",
            "Epoch [7877/20000], Training Loss: 0.4783\n",
            "Epoch [7878/20000], Training Loss: 0.4400\n",
            "Epoch [7879/20000], Training Loss: 0.5005\n",
            "Epoch [7880/20000], Training Loss: 0.5118\n",
            "Epoch [7881/20000], Training Loss: 0.4384\n",
            "Epoch [7882/20000], Training Loss: 0.4764\n",
            "Epoch [7883/20000], Training Loss: 0.5109\n",
            "Epoch [7884/20000], Training Loss: 0.4706\n",
            "Epoch [7885/20000], Training Loss: 0.4822\n",
            "Epoch [7886/20000], Training Loss: 0.5093\n",
            "Epoch [7887/20000], Training Loss: 0.5151\n",
            "Epoch [7888/20000], Training Loss: 0.4798\n",
            "Epoch [7889/20000], Training Loss: 0.4995\n",
            "Epoch [7890/20000], Training Loss: 0.4882\n",
            "Epoch [7891/20000], Training Loss: 0.4653\n",
            "Epoch [7892/20000], Training Loss: 0.5272\n",
            "Epoch [7893/20000], Training Loss: 0.5121\n",
            "Epoch [7894/20000], Training Loss: 0.4571\n",
            "Epoch [7895/20000], Training Loss: 0.4915\n",
            "Epoch [7896/20000], Training Loss: 0.5206\n",
            "Epoch [7897/20000], Training Loss: 0.5287\n",
            "Epoch [7898/20000], Training Loss: 0.4869\n",
            "Epoch [7899/20000], Training Loss: 0.4868\n",
            "Epoch [7900/20000], Training Loss: 0.5119\n",
            "Epoch [7901/20000], Training Loss: 0.5034\n",
            "Epoch [7902/20000], Training Loss: 0.4976\n",
            "Epoch [7903/20000], Training Loss: 0.4504\n",
            "Epoch [7904/20000], Training Loss: 0.4982\n",
            "Epoch [7905/20000], Training Loss: 0.4548\n",
            "Epoch [7906/20000], Training Loss: 0.4921\n",
            "Epoch [7907/20000], Training Loss: 0.4759\n",
            "Epoch [7908/20000], Training Loss: 0.4547\n",
            "Epoch [7909/20000], Training Loss: 0.4816\n",
            "Epoch [7910/20000], Training Loss: 0.4935\n",
            "Epoch [7911/20000], Training Loss: 0.4739\n",
            "Epoch [7912/20000], Training Loss: 0.4749\n",
            "Epoch [7913/20000], Training Loss: 0.5159\n",
            "Epoch [7914/20000], Training Loss: 0.4675\n",
            "Epoch [7915/20000], Training Loss: 0.5181\n",
            "Epoch [7916/20000], Training Loss: 0.5194\n",
            "Epoch [7917/20000], Training Loss: 0.4989\n",
            "Epoch [7918/20000], Training Loss: 0.4814\n",
            "Epoch [7919/20000], Training Loss: 0.5058\n",
            "Epoch [7920/20000], Training Loss: 0.4855\n",
            "Epoch [7921/20000], Training Loss: 0.4505\n",
            "Epoch [7922/20000], Training Loss: 0.4707\n",
            "Epoch [7923/20000], Training Loss: 0.4681\n",
            "Epoch [7924/20000], Training Loss: 0.4582\n",
            "Epoch [7925/20000], Training Loss: 0.5170\n",
            "Epoch [7926/20000], Training Loss: 0.4817\n",
            "Epoch [7927/20000], Training Loss: 0.4526\n",
            "Epoch [7928/20000], Training Loss: 0.4849\n",
            "Epoch [7929/20000], Training Loss: 0.4385\n",
            "Epoch [7930/20000], Training Loss: 0.4823\n",
            "Epoch [7931/20000], Training Loss: 0.4648\n",
            "Epoch [7932/20000], Training Loss: 0.4893\n",
            "Epoch [7933/20000], Training Loss: 0.5220\n",
            "Epoch [7934/20000], Training Loss: 0.4674\n",
            "Epoch [7935/20000], Training Loss: 0.4867\n",
            "Epoch [7936/20000], Training Loss: 0.4835\n",
            "Epoch [7937/20000], Training Loss: 0.4969\n",
            "Epoch [7938/20000], Training Loss: 0.5075\n",
            "Epoch [7939/20000], Training Loss: 0.4857\n",
            "Epoch [7940/20000], Training Loss: 0.5410\n",
            "Epoch [7941/20000], Training Loss: 0.4922\n",
            "Epoch [7942/20000], Training Loss: 0.4874\n",
            "Epoch [7943/20000], Training Loss: 0.4720\n",
            "Epoch [7944/20000], Training Loss: 0.4325\n",
            "Epoch [7945/20000], Training Loss: 0.4627\n",
            "Epoch [7946/20000], Training Loss: 0.4751\n",
            "Epoch [7947/20000], Training Loss: 0.4894\n",
            "Epoch [7948/20000], Training Loss: 0.4846\n",
            "Epoch [7949/20000], Training Loss: 0.4579\n",
            "Epoch [7950/20000], Training Loss: 0.4746\n",
            "Epoch [7951/20000], Training Loss: 0.4890\n",
            "Epoch [7952/20000], Training Loss: 0.5074\n",
            "Epoch [7953/20000], Training Loss: 0.4795\n",
            "Epoch [7954/20000], Training Loss: 0.4600\n",
            "Epoch [7955/20000], Training Loss: 0.4870\n",
            "Epoch [7956/20000], Training Loss: 0.4884\n",
            "Epoch [7957/20000], Training Loss: 0.5076\n",
            "Epoch [7958/20000], Training Loss: 0.5249\n",
            "Epoch [7959/20000], Training Loss: 0.4656\n",
            "Epoch [7960/20000], Training Loss: 0.4405\n",
            "Epoch [7961/20000], Training Loss: 0.4990\n",
            "Epoch [7962/20000], Training Loss: 0.5191\n",
            "Epoch [7963/20000], Training Loss: 0.5134\n",
            "Epoch [7964/20000], Training Loss: 0.4765\n",
            "Epoch [7965/20000], Training Loss: 0.5038\n",
            "Epoch [7966/20000], Training Loss: 0.4956\n",
            "Epoch [7967/20000], Training Loss: 0.4727\n",
            "Epoch [7968/20000], Training Loss: 0.4890\n",
            "Epoch [7969/20000], Training Loss: 0.4575\n",
            "Epoch [7970/20000], Training Loss: 0.4657\n",
            "Epoch [7971/20000], Training Loss: 0.4691\n",
            "Epoch [7972/20000], Training Loss: 0.4695\n",
            "Epoch [7973/20000], Training Loss: 0.4493\n",
            "Epoch [7974/20000], Training Loss: 0.5304\n",
            "Epoch [7975/20000], Training Loss: 0.4980\n",
            "Epoch [7976/20000], Training Loss: 0.4850\n",
            "Epoch [7977/20000], Training Loss: 0.4871\n",
            "Epoch [7978/20000], Training Loss: 0.5006\n",
            "Epoch [7979/20000], Training Loss: 0.4746\n",
            "Epoch [7980/20000], Training Loss: 0.4824\n",
            "Epoch [7981/20000], Training Loss: 0.4930\n",
            "Epoch [7982/20000], Training Loss: 0.5245\n",
            "Epoch [7983/20000], Training Loss: 0.4743\n",
            "Epoch [7984/20000], Training Loss: 0.4701\n",
            "Epoch [7985/20000], Training Loss: 0.4587\n",
            "Epoch [7986/20000], Training Loss: 0.5366\n",
            "Epoch [7987/20000], Training Loss: 0.5168\n",
            "Epoch [7988/20000], Training Loss: 0.4666\n",
            "Epoch [7989/20000], Training Loss: 0.4923\n",
            "Epoch [7990/20000], Training Loss: 0.5006\n",
            "Epoch [7991/20000], Training Loss: 0.4524\n",
            "Epoch [7992/20000], Training Loss: 0.4762\n",
            "Epoch [7993/20000], Training Loss: 0.4578\n",
            "Epoch [7994/20000], Training Loss: 0.4967\n",
            "Epoch [7995/20000], Training Loss: 0.4730\n",
            "Epoch [7996/20000], Training Loss: 0.4601\n",
            "Epoch [7997/20000], Training Loss: 0.4970\n",
            "Epoch [7998/20000], Training Loss: 0.4875\n",
            "Epoch [7999/20000], Training Loss: 0.4892\n",
            "Epoch [8000/20000], Training Loss: 0.4534\n",
            "Epoch [8001/20000], Training Loss: 0.4444\n",
            "Epoch [8002/20000], Training Loss: 0.4877\n",
            "Epoch [8003/20000], Training Loss: 0.5191\n",
            "Epoch [8004/20000], Training Loss: 0.4791\n",
            "Epoch [8005/20000], Training Loss: 0.4818\n",
            "Epoch [8006/20000], Training Loss: 0.4613\n",
            "Epoch [8007/20000], Training Loss: 0.4787\n",
            "Epoch [8008/20000], Training Loss: 0.5136\n",
            "Epoch [8009/20000], Training Loss: 0.4791\n",
            "Epoch [8010/20000], Training Loss: 0.5271\n",
            "Epoch [8011/20000], Training Loss: 0.4673\n",
            "Epoch [8012/20000], Training Loss: 0.5219\n",
            "Epoch [8013/20000], Training Loss: 0.5052\n",
            "Epoch [8014/20000], Training Loss: 0.5035\n",
            "Epoch [8015/20000], Training Loss: 0.4857\n",
            "Epoch [8016/20000], Training Loss: 0.4569\n",
            "Epoch [8017/20000], Training Loss: 0.4832\n",
            "Epoch [8018/20000], Training Loss: 0.4713\n",
            "Epoch [8019/20000], Training Loss: 0.4495\n",
            "Epoch [8020/20000], Training Loss: 0.4510\n",
            "Epoch [8021/20000], Training Loss: 0.5162\n",
            "Epoch [8022/20000], Training Loss: 0.4552\n",
            "Epoch [8023/20000], Training Loss: 0.4889\n",
            "Epoch [8024/20000], Training Loss: 0.4666\n",
            "Epoch [8025/20000], Training Loss: 0.5157\n",
            "Epoch [8026/20000], Training Loss: 0.4879\n",
            "Epoch [8027/20000], Training Loss: 0.4503\n",
            "Epoch [8028/20000], Training Loss: 0.4935\n",
            "Epoch [8029/20000], Training Loss: 0.4569\n",
            "Epoch [8030/20000], Training Loss: 0.5137\n",
            "Epoch [8031/20000], Training Loss: 0.4991\n",
            "Epoch [8032/20000], Training Loss: 0.4563\n",
            "Epoch [8033/20000], Training Loss: 0.4904\n",
            "Epoch [8034/20000], Training Loss: 0.4993\n",
            "Epoch [8035/20000], Training Loss: 0.5127\n",
            "Epoch [8036/20000], Training Loss: 0.5224\n",
            "Epoch [8037/20000], Training Loss: 0.5277\n",
            "Epoch [8038/20000], Training Loss: 0.4899\n",
            "Epoch [8039/20000], Training Loss: 0.4716\n",
            "Epoch [8040/20000], Training Loss: 0.4827\n",
            "Epoch [8041/20000], Training Loss: 0.4365\n",
            "Epoch [8042/20000], Training Loss: 0.4949\n",
            "Epoch [8043/20000], Training Loss: 0.5046\n",
            "Epoch [8044/20000], Training Loss: 0.5796\n",
            "Epoch [8045/20000], Training Loss: 0.4994\n",
            "Epoch [8046/20000], Training Loss: 0.4449\n",
            "Epoch [8047/20000], Training Loss: 0.5214\n",
            "Epoch [8048/20000], Training Loss: 0.4869\n",
            "Epoch [8049/20000], Training Loss: 0.5194\n",
            "Epoch [8050/20000], Training Loss: 0.5161\n",
            "Epoch [8051/20000], Training Loss: 0.4908\n",
            "Epoch [8052/20000], Training Loss: 0.4738\n",
            "Epoch [8053/20000], Training Loss: 0.4697\n",
            "Epoch [8054/20000], Training Loss: 0.4468\n",
            "Epoch [8055/20000], Training Loss: 0.4487\n",
            "Epoch [8056/20000], Training Loss: 0.4778\n",
            "Epoch [8057/20000], Training Loss: 0.5498\n",
            "Epoch [8058/20000], Training Loss: 0.4623\n",
            "Epoch [8059/20000], Training Loss: 0.5231\n",
            "Epoch [8060/20000], Training Loss: 0.5240\n",
            "Epoch [8061/20000], Training Loss: 0.4620\n",
            "Epoch [8062/20000], Training Loss: 0.4596\n",
            "Epoch [8063/20000], Training Loss: 0.4400\n",
            "Epoch [8064/20000], Training Loss: 0.4922\n",
            "Epoch [8065/20000], Training Loss: 0.5102\n",
            "Epoch [8066/20000], Training Loss: 0.5075\n",
            "Epoch [8067/20000], Training Loss: 0.4912\n",
            "Epoch [8068/20000], Training Loss: 0.4548\n",
            "Epoch [8069/20000], Training Loss: 0.4727\n",
            "Epoch [8070/20000], Training Loss: 0.5115\n",
            "Epoch [8071/20000], Training Loss: 0.4706\n",
            "Epoch [8072/20000], Training Loss: 0.5005\n",
            "Epoch [8073/20000], Training Loss: 0.4851\n",
            "Epoch [8074/20000], Training Loss: 0.5331\n",
            "Epoch [8075/20000], Training Loss: 0.4800\n",
            "Epoch [8076/20000], Training Loss: 0.5312\n",
            "Epoch [8077/20000], Training Loss: 0.4821\n",
            "Epoch [8078/20000], Training Loss: 0.5053\n",
            "Epoch [8079/20000], Training Loss: 0.5125\n",
            "Epoch [8080/20000], Training Loss: 0.4466\n",
            "Epoch [8081/20000], Training Loss: 0.4591\n",
            "Epoch [8082/20000], Training Loss: 0.4907\n",
            "Epoch [8083/20000], Training Loss: 0.5665\n",
            "Epoch [8084/20000], Training Loss: 0.5136\n",
            "Epoch [8085/20000], Training Loss: 0.4923\n",
            "Epoch [8086/20000], Training Loss: 0.5164\n",
            "Epoch [8087/20000], Training Loss: 0.5145\n",
            "Epoch [8088/20000], Training Loss: 0.5069\n",
            "Epoch [8089/20000], Training Loss: 0.5121\n",
            "Epoch [8090/20000], Training Loss: 0.4814\n",
            "Epoch [8091/20000], Training Loss: 0.4484\n",
            "Epoch [8092/20000], Training Loss: 0.5125\n",
            "Epoch [8093/20000], Training Loss: 0.5142\n",
            "Epoch [8094/20000], Training Loss: 0.4769\n",
            "Epoch [8095/20000], Training Loss: 0.4579\n",
            "Epoch [8096/20000], Training Loss: 0.4612\n",
            "Epoch [8097/20000], Training Loss: 0.5166\n",
            "Epoch [8098/20000], Training Loss: 0.4825\n",
            "Epoch [8099/20000], Training Loss: 0.5040\n",
            "Epoch [8100/20000], Training Loss: 0.4932\n",
            "Epoch [8101/20000], Training Loss: 0.4637\n",
            "Epoch [8102/20000], Training Loss: 0.4992\n",
            "Epoch [8103/20000], Training Loss: 0.5004\n",
            "Epoch [8104/20000], Training Loss: 0.4999\n",
            "Epoch [8105/20000], Training Loss: 0.4536\n",
            "Epoch [8106/20000], Training Loss: 0.5153\n",
            "Epoch [8107/20000], Training Loss: 0.4594\n",
            "Epoch [8108/20000], Training Loss: 0.4916\n",
            "Epoch [8109/20000], Training Loss: 0.4951\n",
            "Epoch [8110/20000], Training Loss: 0.4803\n",
            "Epoch [8111/20000], Training Loss: 0.5042\n",
            "Epoch [8112/20000], Training Loss: 0.4922\n",
            "Epoch [8113/20000], Training Loss: 0.4682\n",
            "Epoch [8114/20000], Training Loss: 0.5078\n",
            "Epoch [8115/20000], Training Loss: 0.4960\n",
            "Epoch [8116/20000], Training Loss: 0.4621\n",
            "Epoch [8117/20000], Training Loss: 0.4786\n",
            "Epoch [8118/20000], Training Loss: 0.5106\n",
            "Epoch [8119/20000], Training Loss: 0.4749\n",
            "Epoch [8120/20000], Training Loss: 0.4978\n",
            "Epoch [8121/20000], Training Loss: 0.4960\n",
            "Epoch [8122/20000], Training Loss: 0.5072\n",
            "Epoch [8123/20000], Training Loss: 0.4941\n",
            "Epoch [8124/20000], Training Loss: 0.4949\n",
            "Epoch [8125/20000], Training Loss: 0.5038\n",
            "Epoch [8126/20000], Training Loss: 0.5387\n",
            "Epoch [8127/20000], Training Loss: 0.4946\n",
            "Epoch [8128/20000], Training Loss: 0.4481\n",
            "Epoch [8129/20000], Training Loss: 0.4935\n",
            "Epoch [8130/20000], Training Loss: 0.4446\n",
            "Epoch [8131/20000], Training Loss: 0.4642\n",
            "Epoch [8132/20000], Training Loss: 0.5056\n",
            "Epoch [8133/20000], Training Loss: 0.4497\n",
            "Epoch [8134/20000], Training Loss: 0.4773\n",
            "Epoch [8135/20000], Training Loss: 0.4929\n",
            "Epoch [8136/20000], Training Loss: 0.4593\n",
            "Epoch [8137/20000], Training Loss: 0.4858\n",
            "Epoch [8138/20000], Training Loss: 0.4931\n",
            "Epoch [8139/20000], Training Loss: 0.4867\n",
            "Epoch [8140/20000], Training Loss: 0.5199\n",
            "Epoch [8141/20000], Training Loss: 0.4867\n",
            "Epoch [8142/20000], Training Loss: 0.5130\n",
            "Epoch [8143/20000], Training Loss: 0.4658\n",
            "Epoch [8144/20000], Training Loss: 0.4657\n",
            "Epoch [8145/20000], Training Loss: 0.5359\n",
            "Epoch [8146/20000], Training Loss: 0.4402\n",
            "Epoch [8147/20000], Training Loss: 0.4879\n",
            "Epoch [8148/20000], Training Loss: 0.4939\n",
            "Epoch [8149/20000], Training Loss: 0.5061\n",
            "Epoch [8150/20000], Training Loss: 0.5063\n",
            "Epoch [8151/20000], Training Loss: 0.4894\n",
            "Epoch [8152/20000], Training Loss: 0.4682\n",
            "Epoch [8153/20000], Training Loss: 0.4737\n",
            "Epoch [8154/20000], Training Loss: 0.4966\n",
            "Epoch [8155/20000], Training Loss: 0.5047\n",
            "Epoch [8156/20000], Training Loss: 0.4654\n",
            "Epoch [8157/20000], Training Loss: 0.5275\n",
            "Epoch [8158/20000], Training Loss: 0.4704\n",
            "Epoch [8159/20000], Training Loss: 0.4543\n",
            "Epoch [8160/20000], Training Loss: 0.4737\n",
            "Epoch [8161/20000], Training Loss: 0.4698\n",
            "Epoch [8162/20000], Training Loss: 0.4861\n",
            "Epoch [8163/20000], Training Loss: 0.4794\n",
            "Epoch [8164/20000], Training Loss: 0.5332\n",
            "Epoch [8165/20000], Training Loss: 0.5021\n",
            "Epoch [8166/20000], Training Loss: 0.4802\n",
            "Epoch [8167/20000], Training Loss: 0.4742\n",
            "Epoch [8168/20000], Training Loss: 0.4769\n",
            "Epoch [8169/20000], Training Loss: 0.4847\n",
            "Epoch [8170/20000], Training Loss: 0.4724\n",
            "Epoch [8171/20000], Training Loss: 0.5139\n",
            "Epoch [8172/20000], Training Loss: 0.4645\n",
            "Epoch [8173/20000], Training Loss: 0.4417\n",
            "Epoch [8174/20000], Training Loss: 0.4960\n",
            "Epoch [8175/20000], Training Loss: 0.4544\n",
            "Epoch [8176/20000], Training Loss: 0.4881\n",
            "Epoch [8177/20000], Training Loss: 0.5272\n",
            "Epoch [8178/20000], Training Loss: 0.4629\n",
            "Epoch [8179/20000], Training Loss: 0.4740\n",
            "Epoch [8180/20000], Training Loss: 0.4514\n",
            "Epoch [8181/20000], Training Loss: 0.5022\n",
            "Epoch [8182/20000], Training Loss: 0.4797\n",
            "Epoch [8183/20000], Training Loss: 0.4281\n",
            "Epoch [8184/20000], Training Loss: 0.4811\n",
            "Epoch [8185/20000], Training Loss: 0.5151\n",
            "Epoch [8186/20000], Training Loss: 0.5066\n",
            "Epoch [8187/20000], Training Loss: 0.4943\n",
            "Epoch [8188/20000], Training Loss: 0.4997\n",
            "Epoch [8189/20000], Training Loss: 0.4972\n",
            "Epoch [8190/20000], Training Loss: 0.4952\n",
            "Epoch [8191/20000], Training Loss: 0.4694\n",
            "Epoch [8192/20000], Training Loss: 0.5409\n",
            "Epoch [8193/20000], Training Loss: 0.4448\n",
            "Epoch [8194/20000], Training Loss: 0.4651\n",
            "Epoch [8195/20000], Training Loss: 0.5136\n",
            "Epoch [8196/20000], Training Loss: 0.5333\n",
            "Epoch [8197/20000], Training Loss: 0.5304\n",
            "Epoch [8198/20000], Training Loss: 0.5144\n",
            "Epoch [8199/20000], Training Loss: 0.5083\n",
            "Epoch [8200/20000], Training Loss: 0.4966\n",
            "Epoch [8201/20000], Training Loss: 0.4933\n",
            "Epoch [8202/20000], Training Loss: 0.5135\n",
            "Epoch [8203/20000], Training Loss: 0.4560\n",
            "Epoch [8204/20000], Training Loss: 0.4727\n",
            "Epoch [8205/20000], Training Loss: 0.5094\n",
            "Epoch [8206/20000], Training Loss: 0.5009\n",
            "Epoch [8207/20000], Training Loss: 0.5119\n",
            "Epoch [8208/20000], Training Loss: 0.4643\n",
            "Epoch [8209/20000], Training Loss: 0.5077\n",
            "Epoch [8210/20000], Training Loss: 0.4377\n",
            "Epoch [8211/20000], Training Loss: 0.4669\n",
            "Epoch [8212/20000], Training Loss: 0.4883\n",
            "Epoch [8213/20000], Training Loss: 0.4489\n",
            "Epoch [8214/20000], Training Loss: 0.4509\n",
            "Epoch [8215/20000], Training Loss: 0.4792\n",
            "Epoch [8216/20000], Training Loss: 0.4889\n",
            "Epoch [8217/20000], Training Loss: 0.4679\n",
            "Epoch [8218/20000], Training Loss: 0.5287\n",
            "Epoch [8219/20000], Training Loss: 0.5123\n",
            "Epoch [8220/20000], Training Loss: 0.4793\n",
            "Epoch [8221/20000], Training Loss: 0.4464\n",
            "Epoch [8222/20000], Training Loss: 0.5033\n",
            "Epoch [8223/20000], Training Loss: 0.4696\n",
            "Epoch [8224/20000], Training Loss: 0.4390\n",
            "Epoch [8225/20000], Training Loss: 0.4630\n",
            "Epoch [8226/20000], Training Loss: 0.4675\n",
            "Epoch [8227/20000], Training Loss: 0.4402\n",
            "Epoch [8228/20000], Training Loss: 0.5143\n",
            "Epoch [8229/20000], Training Loss: 0.4858\n",
            "Epoch [8230/20000], Training Loss: 0.5081\n",
            "Epoch [8231/20000], Training Loss: 0.5264\n",
            "Epoch [8232/20000], Training Loss: 0.4594\n",
            "Epoch [8233/20000], Training Loss: 0.5008\n",
            "Epoch [8234/20000], Training Loss: 0.5209\n",
            "Epoch [8235/20000], Training Loss: 0.5202\n",
            "Epoch [8236/20000], Training Loss: 0.5048\n",
            "Epoch [8237/20000], Training Loss: 0.4725\n",
            "Epoch [8238/20000], Training Loss: 0.4631\n",
            "Epoch [8239/20000], Training Loss: 0.4938\n",
            "Epoch [8240/20000], Training Loss: 0.4860\n",
            "Epoch [8241/20000], Training Loss: 0.4650\n",
            "Epoch [8242/20000], Training Loss: 0.4772\n",
            "Epoch [8243/20000], Training Loss: 0.4899\n",
            "Epoch [8244/20000], Training Loss: 0.5269\n",
            "Epoch [8245/20000], Training Loss: 0.5452\n",
            "Epoch [8246/20000], Training Loss: 0.4608\n",
            "Epoch [8247/20000], Training Loss: 0.4971\n",
            "Epoch [8248/20000], Training Loss: 0.4429\n",
            "Epoch [8249/20000], Training Loss: 0.5021\n",
            "Epoch [8250/20000], Training Loss: 0.4826\n",
            "Epoch [8251/20000], Training Loss: 0.4990\n",
            "Epoch [8252/20000], Training Loss: 0.4544\n",
            "Epoch [8253/20000], Training Loss: 0.4759\n",
            "Epoch [8254/20000], Training Loss: 0.4833\n",
            "Epoch [8255/20000], Training Loss: 0.4517\n",
            "Epoch [8256/20000], Training Loss: 0.5015\n",
            "Epoch [8257/20000], Training Loss: 0.5195\n",
            "Epoch [8258/20000], Training Loss: 0.4797\n",
            "Epoch [8259/20000], Training Loss: 0.5466\n",
            "Epoch [8260/20000], Training Loss: 0.4888\n",
            "Epoch [8261/20000], Training Loss: 0.4814\n",
            "Epoch [8262/20000], Training Loss: 0.5237\n",
            "Epoch [8263/20000], Training Loss: 0.5171\n",
            "Epoch [8264/20000], Training Loss: 0.4838\n",
            "Epoch [8265/20000], Training Loss: 0.4899\n",
            "Epoch [8266/20000], Training Loss: 0.4802\n",
            "Epoch [8267/20000], Training Loss: 0.4751\n",
            "Epoch [8268/20000], Training Loss: 0.4667\n",
            "Epoch [8269/20000], Training Loss: 0.4498\n",
            "Epoch [8270/20000], Training Loss: 0.5200\n",
            "Epoch [8271/20000], Training Loss: 0.4989\n",
            "Epoch [8272/20000], Training Loss: 0.4748\n",
            "Epoch [8273/20000], Training Loss: 0.5361\n",
            "Epoch [8274/20000], Training Loss: 0.4707\n",
            "Epoch [8275/20000], Training Loss: 0.4905\n",
            "Epoch [8276/20000], Training Loss: 0.4754\n",
            "Epoch [8277/20000], Training Loss: 0.4664\n",
            "Epoch [8278/20000], Training Loss: 0.4524\n",
            "Epoch [8279/20000], Training Loss: 0.4680\n",
            "Epoch [8280/20000], Training Loss: 0.4574\n",
            "Epoch [8281/20000], Training Loss: 0.4889\n",
            "Epoch [8282/20000], Training Loss: 0.4717\n",
            "Epoch [8283/20000], Training Loss: 0.5315\n",
            "Epoch [8284/20000], Training Loss: 0.4940\n",
            "Epoch [8285/20000], Training Loss: 0.4511\n",
            "Epoch [8286/20000], Training Loss: 0.5358\n",
            "Epoch [8287/20000], Training Loss: 0.5230\n",
            "Epoch [8288/20000], Training Loss: 0.4743\n",
            "Epoch [8289/20000], Training Loss: 0.4551\n",
            "Epoch [8290/20000], Training Loss: 0.4780\n",
            "Epoch [8291/20000], Training Loss: 0.4945\n",
            "Epoch [8292/20000], Training Loss: 0.4822\n",
            "Epoch [8293/20000], Training Loss: 0.5092\n",
            "Epoch [8294/20000], Training Loss: 0.4796\n",
            "Epoch [8295/20000], Training Loss: 0.5391\n",
            "Epoch [8296/20000], Training Loss: 0.5036\n",
            "Epoch [8297/20000], Training Loss: 0.4792\n",
            "Epoch [8298/20000], Training Loss: 0.5049\n",
            "Epoch [8299/20000], Training Loss: 0.4848\n",
            "Epoch [8300/20000], Training Loss: 0.4904\n",
            "Epoch [8301/20000], Training Loss: 0.4608\n",
            "Epoch [8302/20000], Training Loss: 0.4532\n",
            "Epoch [8303/20000], Training Loss: 0.5402\n",
            "Epoch [8304/20000], Training Loss: 0.5056\n",
            "Epoch [8305/20000], Training Loss: 0.5008\n",
            "Epoch [8306/20000], Training Loss: 0.4637\n",
            "Epoch [8307/20000], Training Loss: 0.5149\n",
            "Epoch [8308/20000], Training Loss: 0.4652\n",
            "Epoch [8309/20000], Training Loss: 0.4717\n",
            "Epoch [8310/20000], Training Loss: 0.4703\n",
            "Epoch [8311/20000], Training Loss: 0.5057\n",
            "Epoch [8312/20000], Training Loss: 0.4504\n",
            "Epoch [8313/20000], Training Loss: 0.4596\n",
            "Epoch [8314/20000], Training Loss: 0.4821\n",
            "Epoch [8315/20000], Training Loss: 0.4696\n",
            "Epoch [8316/20000], Training Loss: 0.5046\n",
            "Epoch [8317/20000], Training Loss: 0.4876\n",
            "Epoch [8318/20000], Training Loss: 0.5414\n",
            "Epoch [8319/20000], Training Loss: 0.4524\n",
            "Epoch [8320/20000], Training Loss: 0.5279\n",
            "Epoch [8321/20000], Training Loss: 0.5173\n",
            "Epoch [8322/20000], Training Loss: 0.4520\n",
            "Epoch [8323/20000], Training Loss: 0.4551\n",
            "Epoch [8324/20000], Training Loss: 0.4926\n",
            "Epoch [8325/20000], Training Loss: 0.4804\n",
            "Epoch [8326/20000], Training Loss: 0.4975\n",
            "Epoch [8327/20000], Training Loss: 0.5004\n",
            "Epoch [8328/20000], Training Loss: 0.5009\n",
            "Epoch [8329/20000], Training Loss: 0.4783\n",
            "Epoch [8330/20000], Training Loss: 0.5079\n",
            "Epoch [8331/20000], Training Loss: 0.5183\n",
            "Epoch [8332/20000], Training Loss: 0.4275\n",
            "Epoch [8333/20000], Training Loss: 0.5044\n",
            "Epoch [8334/20000], Training Loss: 0.4895\n",
            "Epoch [8335/20000], Training Loss: 0.4778\n",
            "Epoch [8336/20000], Training Loss: 0.5336\n",
            "Epoch [8337/20000], Training Loss: 0.4536\n",
            "Epoch [8338/20000], Training Loss: 0.4882\n",
            "Epoch [8339/20000], Training Loss: 0.4835\n",
            "Epoch [8340/20000], Training Loss: 0.4884\n",
            "Epoch [8341/20000], Training Loss: 0.4844\n",
            "Epoch [8342/20000], Training Loss: 0.4713\n",
            "Epoch [8343/20000], Training Loss: 0.5056\n",
            "Epoch [8344/20000], Training Loss: 0.4324\n",
            "Epoch [8345/20000], Training Loss: 0.4644\n",
            "Epoch [8346/20000], Training Loss: 0.4582\n",
            "Epoch [8347/20000], Training Loss: 0.4678\n",
            "Epoch [8348/20000], Training Loss: 0.5045\n",
            "Epoch [8349/20000], Training Loss: 0.5182\n",
            "Epoch [8350/20000], Training Loss: 0.5058\n",
            "Epoch [8351/20000], Training Loss: 0.4732\n",
            "Epoch [8352/20000], Training Loss: 0.4839\n",
            "Epoch [8353/20000], Training Loss: 0.5213\n",
            "Epoch [8354/20000], Training Loss: 0.5241\n",
            "Epoch [8355/20000], Training Loss: 0.4937\n",
            "Epoch [8356/20000], Training Loss: 0.4948\n",
            "Epoch [8357/20000], Training Loss: 0.5070\n",
            "Epoch [8358/20000], Training Loss: 0.4744\n",
            "Epoch [8359/20000], Training Loss: 0.4969\n",
            "Epoch [8360/20000], Training Loss: 0.4734\n",
            "Epoch [8361/20000], Training Loss: 0.5197\n",
            "Epoch [8362/20000], Training Loss: 0.4774\n",
            "Epoch [8363/20000], Training Loss: 0.5092\n",
            "Epoch [8364/20000], Training Loss: 0.4663\n",
            "Epoch [8365/20000], Training Loss: 0.5037\n",
            "Epoch [8366/20000], Training Loss: 0.4784\n",
            "Epoch [8367/20000], Training Loss: 0.4976\n",
            "Epoch [8368/20000], Training Loss: 0.4907\n",
            "Epoch [8369/20000], Training Loss: 0.4496\n",
            "Epoch [8370/20000], Training Loss: 0.4329\n",
            "Epoch [8371/20000], Training Loss: 0.4799\n",
            "Epoch [8372/20000], Training Loss: 0.4638\n",
            "Epoch [8373/20000], Training Loss: 0.4854\n",
            "Epoch [8374/20000], Training Loss: 0.4442\n",
            "Epoch [8375/20000], Training Loss: 0.4978\n",
            "Epoch [8376/20000], Training Loss: 0.4632\n",
            "Epoch [8377/20000], Training Loss: 0.5092\n",
            "Epoch [8378/20000], Training Loss: 0.4542\n",
            "Epoch [8379/20000], Training Loss: 0.4613\n",
            "Epoch [8380/20000], Training Loss: 0.4771\n",
            "Epoch [8381/20000], Training Loss: 0.4816\n",
            "Epoch [8382/20000], Training Loss: 0.4474\n",
            "Epoch [8383/20000], Training Loss: 0.4678\n",
            "Epoch [8384/20000], Training Loss: 0.4857\n",
            "Epoch [8385/20000], Training Loss: 0.5189\n",
            "Epoch [8386/20000], Training Loss: 0.4673\n",
            "Epoch [8387/20000], Training Loss: 0.4812\n",
            "Epoch [8388/20000], Training Loss: 0.4783\n",
            "Epoch [8389/20000], Training Loss: 0.4911\n",
            "Epoch [8390/20000], Training Loss: 0.4445\n",
            "Epoch [8391/20000], Training Loss: 0.4977\n",
            "Epoch [8392/20000], Training Loss: 0.5062\n",
            "Epoch [8393/20000], Training Loss: 0.5173\n",
            "Epoch [8394/20000], Training Loss: 0.4784\n",
            "Epoch [8395/20000], Training Loss: 0.5065\n",
            "Epoch [8396/20000], Training Loss: 0.4784\n",
            "Epoch [8397/20000], Training Loss: 0.5321\n",
            "Epoch [8398/20000], Training Loss: 0.4367\n",
            "Epoch [8399/20000], Training Loss: 0.4739\n",
            "Epoch [8400/20000], Training Loss: 0.4965\n",
            "Epoch [8401/20000], Training Loss: 0.5451\n",
            "Epoch [8402/20000], Training Loss: 0.5114\n",
            "Epoch [8403/20000], Training Loss: 0.4493\n",
            "Epoch [8404/20000], Training Loss: 0.4753\n",
            "Epoch [8405/20000], Training Loss: 0.5112\n",
            "Epoch [8406/20000], Training Loss: 0.4708\n",
            "Epoch [8407/20000], Training Loss: 0.5332\n",
            "Epoch [8408/20000], Training Loss: 0.4509\n",
            "Epoch [8409/20000], Training Loss: 0.4818\n",
            "Epoch [8410/20000], Training Loss: 0.4465\n",
            "Epoch [8411/20000], Training Loss: 0.5166\n",
            "Epoch [8412/20000], Training Loss: 0.4926\n",
            "Epoch [8413/20000], Training Loss: 0.4727\n",
            "Epoch [8414/20000], Training Loss: 0.4901\n",
            "Epoch [8415/20000], Training Loss: 0.4801\n",
            "Epoch [8416/20000], Training Loss: 0.4760\n",
            "Epoch [8417/20000], Training Loss: 0.4641\n",
            "Epoch [8418/20000], Training Loss: 0.4660\n",
            "Epoch [8419/20000], Training Loss: 0.4891\n",
            "Epoch [8420/20000], Training Loss: 0.5269\n",
            "Epoch [8421/20000], Training Loss: 0.4571\n",
            "Epoch [8422/20000], Training Loss: 0.5459\n",
            "Epoch [8423/20000], Training Loss: 0.4544\n",
            "Epoch [8424/20000], Training Loss: 0.4675\n",
            "Epoch [8425/20000], Training Loss: 0.4630\n",
            "Epoch [8426/20000], Training Loss: 0.5063\n",
            "Epoch [8427/20000], Training Loss: 0.5419\n",
            "Epoch [8428/20000], Training Loss: 0.5034\n",
            "Epoch [8429/20000], Training Loss: 0.4558\n",
            "Epoch [8430/20000], Training Loss: 0.4727\n",
            "Epoch [8431/20000], Training Loss: 0.4818\n",
            "Epoch [8432/20000], Training Loss: 0.4479\n",
            "Epoch [8433/20000], Training Loss: 0.4776\n",
            "Epoch [8434/20000], Training Loss: 0.5044\n",
            "Epoch [8435/20000], Training Loss: 0.5467\n",
            "Epoch [8436/20000], Training Loss: 0.4737\n",
            "Epoch [8437/20000], Training Loss: 0.4692\n",
            "Epoch [8438/20000], Training Loss: 0.5355\n",
            "Epoch [8439/20000], Training Loss: 0.4953\n",
            "Epoch [8440/20000], Training Loss: 0.4963\n",
            "Epoch [8441/20000], Training Loss: 0.4592\n",
            "Epoch [8442/20000], Training Loss: 0.5410\n",
            "Epoch [8443/20000], Training Loss: 0.4824\n",
            "Epoch [8444/20000], Training Loss: 0.4774\n",
            "Epoch [8445/20000], Training Loss: 0.4573\n",
            "Epoch [8446/20000], Training Loss: 0.4366\n",
            "Epoch [8447/20000], Training Loss: 0.4728\n",
            "Epoch [8448/20000], Training Loss: 0.4427\n",
            "Epoch [8449/20000], Training Loss: 0.4818\n",
            "Epoch [8450/20000], Training Loss: 0.4374\n",
            "Epoch [8451/20000], Training Loss: 0.4633\n",
            "Epoch [8452/20000], Training Loss: 0.4615\n",
            "Epoch [8453/20000], Training Loss: 0.4747\n",
            "Epoch [8454/20000], Training Loss: 0.4667\n",
            "Epoch [8455/20000], Training Loss: 0.4877\n",
            "Epoch [8456/20000], Training Loss: 0.4960\n",
            "Epoch [8457/20000], Training Loss: 0.4740\n",
            "Epoch [8458/20000], Training Loss: 0.5216\n",
            "Epoch [8459/20000], Training Loss: 0.4689\n",
            "Epoch [8460/20000], Training Loss: 0.5147\n",
            "Epoch [8461/20000], Training Loss: 0.4647\n",
            "Epoch [8462/20000], Training Loss: 0.5105\n",
            "Epoch [8463/20000], Training Loss: 0.4697\n",
            "Epoch [8464/20000], Training Loss: 0.4812\n",
            "Epoch [8465/20000], Training Loss: 0.4911\n",
            "Epoch [8466/20000], Training Loss: 0.5147\n",
            "Epoch [8467/20000], Training Loss: 0.4680\n",
            "Epoch [8468/20000], Training Loss: 0.5088\n",
            "Epoch [8469/20000], Training Loss: 0.5019\n",
            "Epoch [8470/20000], Training Loss: 0.4993\n",
            "Epoch [8471/20000], Training Loss: 0.5024\n",
            "Epoch [8472/20000], Training Loss: 0.4738\n",
            "Epoch [8473/20000], Training Loss: 0.5021\n",
            "Epoch [8474/20000], Training Loss: 0.4394\n",
            "Epoch [8475/20000], Training Loss: 0.4940\n",
            "Epoch [8476/20000], Training Loss: 0.4689\n",
            "Epoch [8477/20000], Training Loss: 0.4972\n",
            "Epoch [8478/20000], Training Loss: 0.4514\n",
            "Epoch [8479/20000], Training Loss: 0.4758\n",
            "Epoch [8480/20000], Training Loss: 0.4947\n",
            "Epoch [8481/20000], Training Loss: 0.5391\n",
            "Epoch [8482/20000], Training Loss: 0.4439\n",
            "Epoch [8483/20000], Training Loss: 0.4802\n",
            "Epoch [8484/20000], Training Loss: 0.5076\n",
            "Epoch [8485/20000], Training Loss: 0.4796\n",
            "Epoch [8486/20000], Training Loss: 0.4512\n",
            "Epoch [8487/20000], Training Loss: 0.4645\n",
            "Epoch [8488/20000], Training Loss: 0.5446\n",
            "Epoch [8489/20000], Training Loss: 0.5019\n",
            "Epoch [8490/20000], Training Loss: 0.4473\n",
            "Epoch [8491/20000], Training Loss: 0.5183\n",
            "Epoch [8492/20000], Training Loss: 0.4994\n",
            "Epoch [8493/20000], Training Loss: 0.4599\n",
            "Epoch [8494/20000], Training Loss: 0.4445\n",
            "Epoch [8495/20000], Training Loss: 0.4920\n",
            "Epoch [8496/20000], Training Loss: 0.4715\n",
            "Epoch [8497/20000], Training Loss: 0.5077\n",
            "Epoch [8498/20000], Training Loss: 0.4691\n",
            "Epoch [8499/20000], Training Loss: 0.5211\n",
            "Epoch [8500/20000], Training Loss: 0.4649\n",
            "Epoch [8501/20000], Training Loss: 0.4597\n",
            "Epoch [8502/20000], Training Loss: 0.4978\n",
            "Epoch [8503/20000], Training Loss: 0.4579\n",
            "Epoch [8504/20000], Training Loss: 0.5334\n",
            "Epoch [8505/20000], Training Loss: 0.5046\n",
            "Epoch [8506/20000], Training Loss: 0.4939\n",
            "Epoch [8507/20000], Training Loss: 0.5039\n",
            "Epoch [8508/20000], Training Loss: 0.4669\n",
            "Epoch [8509/20000], Training Loss: 0.5040\n",
            "Epoch [8510/20000], Training Loss: 0.4769\n",
            "Epoch [8511/20000], Training Loss: 0.4882\n",
            "Epoch [8512/20000], Training Loss: 0.4585\n",
            "Epoch [8513/20000], Training Loss: 0.4815\n",
            "Epoch [8514/20000], Training Loss: 0.4676\n",
            "Epoch [8515/20000], Training Loss: 0.4880\n",
            "Epoch [8516/20000], Training Loss: 0.5176\n",
            "Epoch [8517/20000], Training Loss: 0.5643\n",
            "Epoch [8518/20000], Training Loss: 0.4743\n",
            "Epoch [8519/20000], Training Loss: 0.5317\n",
            "Epoch [8520/20000], Training Loss: 0.4767\n",
            "Epoch [8521/20000], Training Loss: 0.4883\n",
            "Epoch [8522/20000], Training Loss: 0.4714\n",
            "Epoch [8523/20000], Training Loss: 0.5106\n",
            "Epoch [8524/20000], Training Loss: 0.5055\n",
            "Epoch [8525/20000], Training Loss: 0.4859\n",
            "Epoch [8526/20000], Training Loss: 0.4969\n",
            "Epoch [8527/20000], Training Loss: 0.5214\n",
            "Epoch [8528/20000], Training Loss: 0.4688\n",
            "Epoch [8529/20000], Training Loss: 0.4383\n",
            "Epoch [8530/20000], Training Loss: 0.4938\n",
            "Epoch [8531/20000], Training Loss: 0.5035\n",
            "Epoch [8532/20000], Training Loss: 0.4791\n",
            "Epoch [8533/20000], Training Loss: 0.4818\n",
            "Epoch [8534/20000], Training Loss: 0.4871\n",
            "Epoch [8535/20000], Training Loss: 0.4853\n",
            "Epoch [8536/20000], Training Loss: 0.4454\n",
            "Epoch [8537/20000], Training Loss: 0.4909\n",
            "Epoch [8538/20000], Training Loss: 0.4823\n",
            "Epoch [8539/20000], Training Loss: 0.5195\n",
            "Epoch [8540/20000], Training Loss: 0.4634\n",
            "Epoch [8541/20000], Training Loss: 0.4344\n",
            "Epoch [8542/20000], Training Loss: 0.5240\n",
            "Epoch [8543/20000], Training Loss: 0.5068\n",
            "Epoch [8544/20000], Training Loss: 0.5184\n",
            "Epoch [8545/20000], Training Loss: 0.5194\n",
            "Epoch [8546/20000], Training Loss: 0.4808\n",
            "Epoch [8547/20000], Training Loss: 0.4850\n",
            "Epoch [8548/20000], Training Loss: 0.4849\n",
            "Epoch [8549/20000], Training Loss: 0.4354\n",
            "Epoch [8550/20000], Training Loss: 0.4751\n",
            "Epoch [8551/20000], Training Loss: 0.4443\n",
            "Epoch [8552/20000], Training Loss: 0.5060\n",
            "Epoch [8553/20000], Training Loss: 0.4702\n",
            "Epoch [8554/20000], Training Loss: 0.4742\n",
            "Epoch [8555/20000], Training Loss: 0.5176\n",
            "Epoch [8556/20000], Training Loss: 0.5080\n",
            "Epoch [8557/20000], Training Loss: 0.4792\n",
            "Epoch [8558/20000], Training Loss: 0.4748\n",
            "Epoch [8559/20000], Training Loss: 0.4718\n",
            "Epoch [8560/20000], Training Loss: 0.4928\n",
            "Epoch [8561/20000], Training Loss: 0.4465\n",
            "Epoch [8562/20000], Training Loss: 0.5234\n",
            "Epoch [8563/20000], Training Loss: 0.4883\n",
            "Epoch [8564/20000], Training Loss: 0.4907\n",
            "Epoch [8565/20000], Training Loss: 0.5213\n",
            "Epoch [8566/20000], Training Loss: 0.4930\n",
            "Epoch [8567/20000], Training Loss: 0.4668\n",
            "Epoch [8568/20000], Training Loss: 0.5000\n",
            "Epoch [8569/20000], Training Loss: 0.5064\n",
            "Epoch [8570/20000], Training Loss: 0.4868\n",
            "Epoch [8571/20000], Training Loss: 0.4500\n",
            "Epoch [8572/20000], Training Loss: 0.4495\n",
            "Epoch [8573/20000], Training Loss: 0.5431\n",
            "Epoch [8574/20000], Training Loss: 0.4758\n",
            "Epoch [8575/20000], Training Loss: 0.4871\n",
            "Epoch [8576/20000], Training Loss: 0.4970\n",
            "Epoch [8577/20000], Training Loss: 0.5211\n",
            "Epoch [8578/20000], Training Loss: 0.5001\n",
            "Epoch [8579/20000], Training Loss: 0.4635\n",
            "Epoch [8580/20000], Training Loss: 0.5138\n",
            "Epoch [8581/20000], Training Loss: 0.5397\n",
            "Epoch [8582/20000], Training Loss: 0.4677\n",
            "Epoch [8583/20000], Training Loss: 0.4965\n",
            "Epoch [8584/20000], Training Loss: 0.4617\n",
            "Epoch [8585/20000], Training Loss: 0.4706\n",
            "Epoch [8586/20000], Training Loss: 0.4548\n",
            "Epoch [8587/20000], Training Loss: 0.5213\n",
            "Epoch [8588/20000], Training Loss: 0.4942\n",
            "Epoch [8589/20000], Training Loss: 0.5341\n",
            "Epoch [8590/20000], Training Loss: 0.5181\n",
            "Epoch [8591/20000], Training Loss: 0.5092\n",
            "Epoch [8592/20000], Training Loss: 0.4632\n",
            "Epoch [8593/20000], Training Loss: 0.4707\n",
            "Epoch [8594/20000], Training Loss: 0.4561\n",
            "Epoch [8595/20000], Training Loss: 0.4434\n",
            "Epoch [8596/20000], Training Loss: 0.4994\n",
            "Epoch [8597/20000], Training Loss: 0.4539\n",
            "Epoch [8598/20000], Training Loss: 0.4785\n",
            "Epoch [8599/20000], Training Loss: 0.4348\n",
            "Epoch [8600/20000], Training Loss: 0.4910\n",
            "Epoch [8601/20000], Training Loss: 0.4879\n",
            "Epoch [8602/20000], Training Loss: 0.4481\n",
            "Epoch [8603/20000], Training Loss: 0.5070\n",
            "Epoch [8604/20000], Training Loss: 0.5116\n",
            "Epoch [8605/20000], Training Loss: 0.5282\n",
            "Epoch [8606/20000], Training Loss: 0.4900\n",
            "Epoch [8607/20000], Training Loss: 0.5112\n",
            "Epoch [8608/20000], Training Loss: 0.4916\n",
            "Epoch [8609/20000], Training Loss: 0.4971\n",
            "Epoch [8610/20000], Training Loss: 0.4778\n",
            "Epoch [8611/20000], Training Loss: 0.4975\n",
            "Epoch [8612/20000], Training Loss: 0.4909\n",
            "Epoch [8613/20000], Training Loss: 0.4870\n",
            "Epoch [8614/20000], Training Loss: 0.4768\n",
            "Epoch [8615/20000], Training Loss: 0.4818\n",
            "Epoch [8616/20000], Training Loss: 0.4898\n",
            "Epoch [8617/20000], Training Loss: 0.4924\n",
            "Epoch [8618/20000], Training Loss: 0.4817\n",
            "Epoch [8619/20000], Training Loss: 0.5032\n",
            "Epoch [8620/20000], Training Loss: 0.5056\n",
            "Epoch [8621/20000], Training Loss: 0.4759\n",
            "Epoch [8622/20000], Training Loss: 0.4864\n",
            "Epoch [8623/20000], Training Loss: 0.4342\n",
            "Epoch [8624/20000], Training Loss: 0.4601\n",
            "Epoch [8625/20000], Training Loss: 0.5180\n",
            "Epoch [8626/20000], Training Loss: 0.4759\n",
            "Epoch [8627/20000], Training Loss: 0.4262\n",
            "Epoch [8628/20000], Training Loss: 0.4587\n",
            "Epoch [8629/20000], Training Loss: 0.4901\n",
            "Epoch [8630/20000], Training Loss: 0.4384\n",
            "Epoch [8631/20000], Training Loss: 0.5325\n",
            "Epoch [8632/20000], Training Loss: 0.5333\n",
            "Epoch [8633/20000], Training Loss: 0.4967\n",
            "Epoch [8634/20000], Training Loss: 0.4407\n",
            "Epoch [8635/20000], Training Loss: 0.4825\n",
            "Epoch [8636/20000], Training Loss: 0.4848\n",
            "Epoch [8637/20000], Training Loss: 0.5403\n",
            "Epoch [8638/20000], Training Loss: 0.5413\n",
            "Epoch [8639/20000], Training Loss: 0.4493\n",
            "Epoch [8640/20000], Training Loss: 0.4781\n",
            "Epoch [8641/20000], Training Loss: 0.4836\n",
            "Epoch [8642/20000], Training Loss: 0.4954\n",
            "Epoch [8643/20000], Training Loss: 0.5518\n",
            "Epoch [8644/20000], Training Loss: 0.4539\n",
            "Epoch [8645/20000], Training Loss: 0.4609\n",
            "Epoch [8646/20000], Training Loss: 0.4969\n",
            "Epoch [8647/20000], Training Loss: 0.4992\n",
            "Epoch [8648/20000], Training Loss: 0.4861\n",
            "Epoch [8649/20000], Training Loss: 0.5301\n",
            "Epoch [8650/20000], Training Loss: 0.4763\n",
            "Epoch [8651/20000], Training Loss: 0.4616\n",
            "Epoch [8652/20000], Training Loss: 0.4880\n",
            "Epoch [8653/20000], Training Loss: 0.5230\n",
            "Epoch [8654/20000], Training Loss: 0.5098\n",
            "Epoch [8655/20000], Training Loss: 0.5303\n",
            "Epoch [8656/20000], Training Loss: 0.5290\n",
            "Epoch [8657/20000], Training Loss: 0.5051\n",
            "Epoch [8658/20000], Training Loss: 0.4786\n",
            "Epoch [8659/20000], Training Loss: 0.4864\n",
            "Epoch [8660/20000], Training Loss: 0.5179\n",
            "Epoch [8661/20000], Training Loss: 0.4714\n",
            "Epoch [8662/20000], Training Loss: 0.4906\n",
            "Epoch [8663/20000], Training Loss: 0.5325\n",
            "Epoch [8664/20000], Training Loss: 0.5036\n",
            "Epoch [8665/20000], Training Loss: 0.4877\n",
            "Epoch [8666/20000], Training Loss: 0.4519\n",
            "Epoch [8667/20000], Training Loss: 0.4490\n",
            "Epoch [8668/20000], Training Loss: 0.4723\n",
            "Epoch [8669/20000], Training Loss: 0.4252\n",
            "Epoch [8670/20000], Training Loss: 0.5139\n",
            "Epoch [8671/20000], Training Loss: 0.4994\n",
            "Epoch [8672/20000], Training Loss: 0.4890\n",
            "Epoch [8673/20000], Training Loss: 0.4757\n",
            "Epoch [8674/20000], Training Loss: 0.5172\n",
            "Epoch [8675/20000], Training Loss: 0.4507\n",
            "Epoch [8676/20000], Training Loss: 0.4665\n",
            "Epoch [8677/20000], Training Loss: 0.5012\n",
            "Epoch [8678/20000], Training Loss: 0.4650\n",
            "Epoch [8679/20000], Training Loss: 0.4357\n",
            "Epoch [8680/20000], Training Loss: 0.4351\n",
            "Epoch [8681/20000], Training Loss: 0.4815\n",
            "Epoch [8682/20000], Training Loss: 0.4901\n",
            "Epoch [8683/20000], Training Loss: 0.5103\n",
            "Epoch [8684/20000], Training Loss: 0.4775\n",
            "Epoch [8685/20000], Training Loss: 0.5233\n",
            "Epoch [8686/20000], Training Loss: 0.5034\n",
            "Epoch [8687/20000], Training Loss: 0.4558\n",
            "Epoch [8688/20000], Training Loss: 0.4843\n",
            "Epoch [8689/20000], Training Loss: 0.4954\n",
            "Epoch [8690/20000], Training Loss: 0.4667\n",
            "Epoch [8691/20000], Training Loss: 0.5008\n",
            "Epoch [8692/20000], Training Loss: 0.5016\n",
            "Epoch [8693/20000], Training Loss: 0.4548\n",
            "Epoch [8694/20000], Training Loss: 0.4922\n",
            "Epoch [8695/20000], Training Loss: 0.4337\n",
            "Epoch [8696/20000], Training Loss: 0.4991\n",
            "Epoch [8697/20000], Training Loss: 0.5224\n",
            "Epoch [8698/20000], Training Loss: 0.4447\n",
            "Epoch [8699/20000], Training Loss: 0.4651\n",
            "Epoch [8700/20000], Training Loss: 0.4864\n",
            "Epoch [8701/20000], Training Loss: 0.4713\n",
            "Epoch [8702/20000], Training Loss: 0.4992\n",
            "Epoch [8703/20000], Training Loss: 0.4615\n",
            "Epoch [8704/20000], Training Loss: 0.5100\n",
            "Epoch [8705/20000], Training Loss: 0.4830\n",
            "Epoch [8706/20000], Training Loss: 0.4954\n",
            "Epoch [8707/20000], Training Loss: 0.4906\n",
            "Epoch [8708/20000], Training Loss: 0.4495\n",
            "Epoch [8709/20000], Training Loss: 0.4879\n",
            "Epoch [8710/20000], Training Loss: 0.4953\n",
            "Epoch [8711/20000], Training Loss: 0.5005\n",
            "Epoch [8712/20000], Training Loss: 0.4815\n",
            "Epoch [8713/20000], Training Loss: 0.4780\n",
            "Epoch [8714/20000], Training Loss: 0.4727\n",
            "Epoch [8715/20000], Training Loss: 0.5070\n",
            "Epoch [8716/20000], Training Loss: 0.4521\n",
            "Epoch [8717/20000], Training Loss: 0.4719\n",
            "Epoch [8718/20000], Training Loss: 0.4361\n",
            "Epoch [8719/20000], Training Loss: 0.5318\n",
            "Epoch [8720/20000], Training Loss: 0.4464\n",
            "Epoch [8721/20000], Training Loss: 0.4378\n",
            "Epoch [8722/20000], Training Loss: 0.4911\n",
            "Epoch [8723/20000], Training Loss: 0.4667\n",
            "Epoch [8724/20000], Training Loss: 0.4756\n",
            "Epoch [8725/20000], Training Loss: 0.5099\n",
            "Epoch [8726/20000], Training Loss: 0.4919\n",
            "Epoch [8727/20000], Training Loss: 0.4338\n",
            "Epoch [8728/20000], Training Loss: 0.5327\n",
            "Epoch [8729/20000], Training Loss: 0.4520\n",
            "Epoch [8730/20000], Training Loss: 0.4714\n",
            "Epoch [8731/20000], Training Loss: 0.4651\n",
            "Epoch [8732/20000], Training Loss: 0.4734\n",
            "Epoch [8733/20000], Training Loss: 0.4401\n",
            "Epoch [8734/20000], Training Loss: 0.4766\n",
            "Epoch [8735/20000], Training Loss: 0.4558\n",
            "Epoch [8736/20000], Training Loss: 0.4804\n",
            "Epoch [8737/20000], Training Loss: 0.5296\n",
            "Epoch [8738/20000], Training Loss: 0.5257\n",
            "Epoch [8739/20000], Training Loss: 0.4701\n",
            "Epoch [8740/20000], Training Loss: 0.4739\n",
            "Epoch [8741/20000], Training Loss: 0.5030\n",
            "Epoch [8742/20000], Training Loss: 0.5082\n",
            "Epoch [8743/20000], Training Loss: 0.5270\n",
            "Epoch [8744/20000], Training Loss: 0.5004\n",
            "Epoch [8745/20000], Training Loss: 0.4751\n",
            "Epoch [8746/20000], Training Loss: 0.4577\n",
            "Epoch [8747/20000], Training Loss: 0.4892\n",
            "Epoch [8748/20000], Training Loss: 0.5055\n",
            "Epoch [8749/20000], Training Loss: 0.4907\n",
            "Epoch [8750/20000], Training Loss: 0.5171\n",
            "Epoch [8751/20000], Training Loss: 0.4820\n",
            "Epoch [8752/20000], Training Loss: 0.5202\n",
            "Epoch [8753/20000], Training Loss: 0.4726\n",
            "Epoch [8754/20000], Training Loss: 0.4604\n",
            "Epoch [8755/20000], Training Loss: 0.5267\n",
            "Epoch [8756/20000], Training Loss: 0.4717\n",
            "Epoch [8757/20000], Training Loss: 0.4979\n",
            "Epoch [8758/20000], Training Loss: 0.4824\n",
            "Epoch [8759/20000], Training Loss: 0.4619\n",
            "Epoch [8760/20000], Training Loss: 0.4427\n",
            "Epoch [8761/20000], Training Loss: 0.4764\n",
            "Epoch [8762/20000], Training Loss: 0.4812\n",
            "Epoch [8763/20000], Training Loss: 0.4938\n",
            "Epoch [8764/20000], Training Loss: 0.4912\n",
            "Epoch [8765/20000], Training Loss: 0.4970\n",
            "Epoch [8766/20000], Training Loss: 0.4782\n",
            "Epoch [8767/20000], Training Loss: 0.4695\n",
            "Epoch [8768/20000], Training Loss: 0.4869\n",
            "Epoch [8769/20000], Training Loss: 0.5185\n",
            "Epoch [8770/20000], Training Loss: 0.4851\n",
            "Epoch [8771/20000], Training Loss: 0.5007\n",
            "Epoch [8772/20000], Training Loss: 0.4972\n",
            "Epoch [8773/20000], Training Loss: 0.5146\n",
            "Epoch [8774/20000], Training Loss: 0.4793\n",
            "Epoch [8775/20000], Training Loss: 0.5027\n",
            "Epoch [8776/20000], Training Loss: 0.4782\n",
            "Epoch [8777/20000], Training Loss: 0.4634\n",
            "Epoch [8778/20000], Training Loss: 0.4715\n",
            "Epoch [8779/20000], Training Loss: 0.4766\n",
            "Epoch [8780/20000], Training Loss: 0.4353\n",
            "Epoch [8781/20000], Training Loss: 0.4753\n",
            "Epoch [8782/20000], Training Loss: 0.5087\n",
            "Epoch [8783/20000], Training Loss: 0.4866\n",
            "Epoch [8784/20000], Training Loss: 0.4700\n",
            "Epoch [8785/20000], Training Loss: 0.5034\n",
            "Epoch [8786/20000], Training Loss: 0.4587\n",
            "Epoch [8787/20000], Training Loss: 0.4736\n",
            "Epoch [8788/20000], Training Loss: 0.4970\n",
            "Epoch [8789/20000], Training Loss: 0.4852\n",
            "Epoch [8790/20000], Training Loss: 0.4927\n",
            "Epoch [8791/20000], Training Loss: 0.5103\n",
            "Epoch [8792/20000], Training Loss: 0.4930\n",
            "Epoch [8793/20000], Training Loss: 0.4251\n",
            "Epoch [8794/20000], Training Loss: 0.5012\n",
            "Epoch [8795/20000], Training Loss: 0.4913\n",
            "Epoch [8796/20000], Training Loss: 0.4851\n",
            "Epoch [8797/20000], Training Loss: 0.5036\n",
            "Epoch [8798/20000], Training Loss: 0.4922\n",
            "Epoch [8799/20000], Training Loss: 0.4678\n",
            "Epoch [8800/20000], Training Loss: 0.5082\n",
            "Epoch [8801/20000], Training Loss: 0.5142\n",
            "Epoch [8802/20000], Training Loss: 0.4906\n",
            "Epoch [8803/20000], Training Loss: 0.4844\n",
            "Epoch [8804/20000], Training Loss: 0.5466\n",
            "Epoch [8805/20000], Training Loss: 0.5156\n",
            "Epoch [8806/20000], Training Loss: 0.5083\n",
            "Epoch [8807/20000], Training Loss: 0.4648\n",
            "Epoch [8808/20000], Training Loss: 0.5006\n",
            "Epoch [8809/20000], Training Loss: 0.4913\n",
            "Epoch [8810/20000], Training Loss: 0.4954\n",
            "Epoch [8811/20000], Training Loss: 0.5017\n",
            "Epoch [8812/20000], Training Loss: 0.4511\n",
            "Epoch [8813/20000], Training Loss: 0.4758\n",
            "Epoch [8814/20000], Training Loss: 0.4713\n",
            "Epoch [8815/20000], Training Loss: 0.4805\n",
            "Epoch [8816/20000], Training Loss: 0.5172\n",
            "Epoch [8817/20000], Training Loss: 0.5051\n",
            "Epoch [8818/20000], Training Loss: 0.4835\n",
            "Epoch [8819/20000], Training Loss: 0.4812\n",
            "Epoch [8820/20000], Training Loss: 0.4410\n",
            "Epoch [8821/20000], Training Loss: 0.5022\n",
            "Epoch [8822/20000], Training Loss: 0.4949\n",
            "Epoch [8823/20000], Training Loss: 0.4998\n",
            "Epoch [8824/20000], Training Loss: 0.4994\n",
            "Epoch [8825/20000], Training Loss: 0.4925\n",
            "Epoch [8826/20000], Training Loss: 0.5292\n",
            "Epoch [8827/20000], Training Loss: 0.4857\n",
            "Epoch [8828/20000], Training Loss: 0.4633\n",
            "Epoch [8829/20000], Training Loss: 0.5045\n",
            "Epoch [8830/20000], Training Loss: 0.4879\n",
            "Epoch [8831/20000], Training Loss: 0.4462\n",
            "Epoch [8832/20000], Training Loss: 0.5236\n",
            "Epoch [8833/20000], Training Loss: 0.4789\n",
            "Epoch [8834/20000], Training Loss: 0.4712\n",
            "Epoch [8835/20000], Training Loss: 0.5095\n",
            "Epoch [8836/20000], Training Loss: 0.4610\n",
            "Epoch [8837/20000], Training Loss: 0.4759\n",
            "Epoch [8838/20000], Training Loss: 0.4963\n",
            "Epoch [8839/20000], Training Loss: 0.5082\n",
            "Epoch [8840/20000], Training Loss: 0.4973\n",
            "Epoch [8841/20000], Training Loss: 0.5179\n",
            "Epoch [8842/20000], Training Loss: 0.4809\n",
            "Epoch [8843/20000], Training Loss: 0.5136\n",
            "Epoch [8844/20000], Training Loss: 0.4860\n",
            "Epoch [8845/20000], Training Loss: 0.4309\n",
            "Epoch [8846/20000], Training Loss: 0.4771\n",
            "Epoch [8847/20000], Training Loss: 0.5092\n",
            "Epoch [8848/20000], Training Loss: 0.4946\n",
            "Epoch [8849/20000], Training Loss: 0.4558\n",
            "Epoch [8850/20000], Training Loss: 0.4857\n",
            "Epoch [8851/20000], Training Loss: 0.5237\n",
            "Epoch [8852/20000], Training Loss: 0.4536\n",
            "Epoch [8853/20000], Training Loss: 0.5405\n",
            "Epoch [8854/20000], Training Loss: 0.4942\n",
            "Epoch [8855/20000], Training Loss: 0.4791\n",
            "Epoch [8856/20000], Training Loss: 0.5156\n",
            "Epoch [8857/20000], Training Loss: 0.5028\n",
            "Epoch [8858/20000], Training Loss: 0.5453\n",
            "Epoch [8859/20000], Training Loss: 0.5211\n",
            "Epoch [8860/20000], Training Loss: 0.4771\n",
            "Epoch [8861/20000], Training Loss: 0.4725\n",
            "Epoch [8862/20000], Training Loss: 0.4953\n",
            "Epoch [8863/20000], Training Loss: 0.4973\n",
            "Epoch [8864/20000], Training Loss: 0.4971\n",
            "Epoch [8865/20000], Training Loss: 0.5059\n",
            "Epoch [8866/20000], Training Loss: 0.4732\n",
            "Epoch [8867/20000], Training Loss: 0.4449\n",
            "Epoch [8868/20000], Training Loss: 0.4862\n",
            "Epoch [8869/20000], Training Loss: 0.4561\n",
            "Epoch [8870/20000], Training Loss: 0.5054\n",
            "Epoch [8871/20000], Training Loss: 0.4828\n",
            "Epoch [8872/20000], Training Loss: 0.4601\n",
            "Epoch [8873/20000], Training Loss: 0.5242\n",
            "Epoch [8874/20000], Training Loss: 0.4720\n",
            "Epoch [8875/20000], Training Loss: 0.5155\n",
            "Epoch [8876/20000], Training Loss: 0.4667\n",
            "Epoch [8877/20000], Training Loss: 0.5058\n",
            "Epoch [8878/20000], Training Loss: 0.4666\n",
            "Epoch [8879/20000], Training Loss: 0.4867\n",
            "Epoch [8880/20000], Training Loss: 0.4814\n",
            "Epoch [8881/20000], Training Loss: 0.4490\n",
            "Epoch [8882/20000], Training Loss: 0.4764\n",
            "Epoch [8883/20000], Training Loss: 0.5071\n",
            "Epoch [8884/20000], Training Loss: 0.5225\n",
            "Epoch [8885/20000], Training Loss: 0.4982\n",
            "Epoch [8886/20000], Training Loss: 0.4890\n",
            "Epoch [8887/20000], Training Loss: 0.5295\n",
            "Epoch [8888/20000], Training Loss: 0.5103\n",
            "Epoch [8889/20000], Training Loss: 0.4891\n",
            "Epoch [8890/20000], Training Loss: 0.5014\n",
            "Epoch [8891/20000], Training Loss: 0.4654\n",
            "Epoch [8892/20000], Training Loss: 0.5066\n",
            "Epoch [8893/20000], Training Loss: 0.5084\n",
            "Epoch [8894/20000], Training Loss: 0.4739\n",
            "Epoch [8895/20000], Training Loss: 0.5004\n",
            "Epoch [8896/20000], Training Loss: 0.4566\n",
            "Epoch [8897/20000], Training Loss: 0.4899\n",
            "Epoch [8898/20000], Training Loss: 0.5013\n",
            "Epoch [8899/20000], Training Loss: 0.4914\n",
            "Epoch [8900/20000], Training Loss: 0.4383\n",
            "Epoch [8901/20000], Training Loss: 0.4943\n",
            "Epoch [8902/20000], Training Loss: 0.4964\n",
            "Epoch [8903/20000], Training Loss: 0.4660\n",
            "Epoch [8904/20000], Training Loss: 0.5010\n",
            "Epoch [8905/20000], Training Loss: 0.4945\n",
            "Epoch [8906/20000], Training Loss: 0.5010\n",
            "Epoch [8907/20000], Training Loss: 0.5076\n",
            "Epoch [8908/20000], Training Loss: 0.4608\n",
            "Epoch [8909/20000], Training Loss: 0.4814\n",
            "Epoch [8910/20000], Training Loss: 0.5043\n",
            "Epoch [8911/20000], Training Loss: 0.4905\n",
            "Epoch [8912/20000], Training Loss: 0.5300\n",
            "Epoch [8913/20000], Training Loss: 0.4796\n",
            "Epoch [8914/20000], Training Loss: 0.5110\n",
            "Epoch [8915/20000], Training Loss: 0.4539\n",
            "Epoch [8916/20000], Training Loss: 0.4705\n",
            "Epoch [8917/20000], Training Loss: 0.4412\n",
            "Epoch [8918/20000], Training Loss: 0.4831\n",
            "Epoch [8919/20000], Training Loss: 0.4208\n",
            "Epoch [8920/20000], Training Loss: 0.4590\n",
            "Epoch [8921/20000], Training Loss: 0.4895\n",
            "Epoch [8922/20000], Training Loss: 0.4492\n",
            "Epoch [8923/20000], Training Loss: 0.4983\n",
            "Epoch [8924/20000], Training Loss: 0.5171\n",
            "Epoch [8925/20000], Training Loss: 0.4873\n",
            "Epoch [8926/20000], Training Loss: 0.4835\n",
            "Epoch [8927/20000], Training Loss: 0.5012\n",
            "Epoch [8928/20000], Training Loss: 0.4543\n",
            "Epoch [8929/20000], Training Loss: 0.4689\n",
            "Epoch [8930/20000], Training Loss: 0.5083\n",
            "Epoch [8931/20000], Training Loss: 0.4540\n",
            "Epoch [8932/20000], Training Loss: 0.4769\n",
            "Epoch [8933/20000], Training Loss: 0.5308\n",
            "Epoch [8934/20000], Training Loss: 0.4668\n",
            "Epoch [8935/20000], Training Loss: 0.5077\n",
            "Epoch [8936/20000], Training Loss: 0.4687\n",
            "Epoch [8937/20000], Training Loss: 0.4755\n",
            "Epoch [8938/20000], Training Loss: 0.4640\n",
            "Epoch [8939/20000], Training Loss: 0.5318\n",
            "Epoch [8940/20000], Training Loss: 0.5307\n",
            "Epoch [8941/20000], Training Loss: 0.4920\n",
            "Epoch [8942/20000], Training Loss: 0.4731\n",
            "Epoch [8943/20000], Training Loss: 0.4740\n",
            "Epoch [8944/20000], Training Loss: 0.4439\n",
            "Epoch [8945/20000], Training Loss: 0.4728\n",
            "Epoch [8946/20000], Training Loss: 0.4667\n",
            "Epoch [8947/20000], Training Loss: 0.5192\n",
            "Epoch [8948/20000], Training Loss: 0.4991\n",
            "Epoch [8949/20000], Training Loss: 0.5403\n",
            "Epoch [8950/20000], Training Loss: 0.4802\n",
            "Epoch [8951/20000], Training Loss: 0.5336\n",
            "Epoch [8952/20000], Training Loss: 0.4826\n",
            "Epoch [8953/20000], Training Loss: 0.4665\n",
            "Epoch [8954/20000], Training Loss: 0.4499\n",
            "Epoch [8955/20000], Training Loss: 0.4688\n",
            "Epoch [8956/20000], Training Loss: 0.4877\n",
            "Epoch [8957/20000], Training Loss: 0.4944\n",
            "Epoch [8958/20000], Training Loss: 0.5135\n",
            "Epoch [8959/20000], Training Loss: 0.4738\n",
            "Epoch [8960/20000], Training Loss: 0.4542\n",
            "Epoch [8961/20000], Training Loss: 0.5501\n",
            "Epoch [8962/20000], Training Loss: 0.4806\n",
            "Epoch [8963/20000], Training Loss: 0.5236\n",
            "Epoch [8964/20000], Training Loss: 0.5112\n",
            "Epoch [8965/20000], Training Loss: 0.5395\n",
            "Epoch [8966/20000], Training Loss: 0.4683\n",
            "Epoch [8967/20000], Training Loss: 0.4698\n",
            "Epoch [8968/20000], Training Loss: 0.5033\n",
            "Epoch [8969/20000], Training Loss: 0.4608\n",
            "Epoch [8970/20000], Training Loss: 0.4645\n",
            "Epoch [8971/20000], Training Loss: 0.4669\n",
            "Epoch [8972/20000], Training Loss: 0.4578\n",
            "Epoch [8973/20000], Training Loss: 0.5333\n",
            "Epoch [8974/20000], Training Loss: 0.4900\n",
            "Epoch [8975/20000], Training Loss: 0.5014\n",
            "Epoch [8976/20000], Training Loss: 0.5137\n",
            "Epoch [8977/20000], Training Loss: 0.4731\n",
            "Epoch [8978/20000], Training Loss: 0.5186\n",
            "Epoch [8979/20000], Training Loss: 0.4708\n",
            "Epoch [8980/20000], Training Loss: 0.4735\n",
            "Epoch [8981/20000], Training Loss: 0.5085\n",
            "Epoch [8982/20000], Training Loss: 0.5138\n",
            "Epoch [8983/20000], Training Loss: 0.4418\n",
            "Epoch [8984/20000], Training Loss: 0.5056\n",
            "Epoch [8985/20000], Training Loss: 0.4949\n",
            "Epoch [8986/20000], Training Loss: 0.4935\n",
            "Epoch [8987/20000], Training Loss: 0.5027\n",
            "Epoch [8988/20000], Training Loss: 0.5019\n",
            "Epoch [8989/20000], Training Loss: 0.5301\n",
            "Epoch [8990/20000], Training Loss: 0.4587\n",
            "Epoch [8991/20000], Training Loss: 0.4870\n",
            "Epoch [8992/20000], Training Loss: 0.5014\n",
            "Epoch [8993/20000], Training Loss: 0.5499\n",
            "Epoch [8994/20000], Training Loss: 0.5243\n",
            "Epoch [8995/20000], Training Loss: 0.5442\n",
            "Epoch [8996/20000], Training Loss: 0.4797\n",
            "Epoch [8997/20000], Training Loss: 0.4971\n",
            "Epoch [8998/20000], Training Loss: 0.5073\n",
            "Epoch [8999/20000], Training Loss: 0.4854\n",
            "Epoch [9000/20000], Training Loss: 0.4481\n",
            "Epoch [9001/20000], Training Loss: 0.4396\n",
            "Epoch [9002/20000], Training Loss: 0.5292\n",
            "Epoch [9003/20000], Training Loss: 0.4737\n",
            "Epoch [9004/20000], Training Loss: 0.5142\n",
            "Epoch [9005/20000], Training Loss: 0.5091\n",
            "Epoch [9006/20000], Training Loss: 0.4763\n",
            "Epoch [9007/20000], Training Loss: 0.4900\n",
            "Epoch [9008/20000], Training Loss: 0.5336\n",
            "Epoch [9009/20000], Training Loss: 0.4479\n",
            "Epoch [9010/20000], Training Loss: 0.4722\n",
            "Epoch [9011/20000], Training Loss: 0.4590\n",
            "Epoch [9012/20000], Training Loss: 0.4739\n",
            "Epoch [9013/20000], Training Loss: 0.4744\n",
            "Epoch [9014/20000], Training Loss: 0.5092\n",
            "Epoch [9015/20000], Training Loss: 0.4684\n",
            "Epoch [9016/20000], Training Loss: 0.4974\n",
            "Epoch [9017/20000], Training Loss: 0.5082\n",
            "Epoch [9018/20000], Training Loss: 0.4918\n",
            "Epoch [9019/20000], Training Loss: 0.5045\n",
            "Epoch [9020/20000], Training Loss: 0.4815\n",
            "Epoch [9021/20000], Training Loss: 0.5082\n",
            "Epoch [9022/20000], Training Loss: 0.4541\n",
            "Epoch [9023/20000], Training Loss: 0.5074\n",
            "Epoch [9024/20000], Training Loss: 0.4872\n",
            "Epoch [9025/20000], Training Loss: 0.4523\n",
            "Epoch [9026/20000], Training Loss: 0.5165\n",
            "Epoch [9027/20000], Training Loss: 0.4395\n",
            "Epoch [9028/20000], Training Loss: 0.4883\n",
            "Epoch [9029/20000], Training Loss: 0.4596\n",
            "Epoch [9030/20000], Training Loss: 0.5150\n",
            "Epoch [9031/20000], Training Loss: 0.4650\n",
            "Epoch [9032/20000], Training Loss: 0.4664\n",
            "Epoch [9033/20000], Training Loss: 0.4984\n",
            "Epoch [9034/20000], Training Loss: 0.5003\n",
            "Epoch [9035/20000], Training Loss: 0.5075\n",
            "Epoch [9036/20000], Training Loss: 0.4817\n",
            "Epoch [9037/20000], Training Loss: 0.4913\n",
            "Epoch [9038/20000], Training Loss: 0.5174\n",
            "Epoch [9039/20000], Training Loss: 0.5135\n",
            "Epoch [9040/20000], Training Loss: 0.4722\n",
            "Epoch [9041/20000], Training Loss: 0.5031\n",
            "Epoch [9042/20000], Training Loss: 0.5024\n",
            "Epoch [9043/20000], Training Loss: 0.4837\n",
            "Epoch [9044/20000], Training Loss: 0.4862\n",
            "Epoch [9045/20000], Training Loss: 0.4440\n",
            "Epoch [9046/20000], Training Loss: 0.5097\n",
            "Epoch [9047/20000], Training Loss: 0.4715\n",
            "Epoch [9048/20000], Training Loss: 0.5195\n",
            "Epoch [9049/20000], Training Loss: 0.4847\n",
            "Epoch [9050/20000], Training Loss: 0.4953\n",
            "Epoch [9051/20000], Training Loss: 0.4971\n",
            "Epoch [9052/20000], Training Loss: 0.4695\n",
            "Epoch [9053/20000], Training Loss: 0.4989\n",
            "Epoch [9054/20000], Training Loss: 0.4694\n",
            "Epoch [9055/20000], Training Loss: 0.4838\n",
            "Epoch [9056/20000], Training Loss: 0.4997\n",
            "Epoch [9057/20000], Training Loss: 0.5066\n",
            "Epoch [9058/20000], Training Loss: 0.5022\n",
            "Epoch [9059/20000], Training Loss: 0.4517\n",
            "Epoch [9060/20000], Training Loss: 0.4890\n",
            "Epoch [9061/20000], Training Loss: 0.5129\n",
            "Epoch [9062/20000], Training Loss: 0.5025\n",
            "Epoch [9063/20000], Training Loss: 0.4512\n",
            "Epoch [9064/20000], Training Loss: 0.4974\n",
            "Epoch [9065/20000], Training Loss: 0.5226\n",
            "Epoch [9066/20000], Training Loss: 0.4764\n",
            "Epoch [9067/20000], Training Loss: 0.5098\n",
            "Epoch [9068/20000], Training Loss: 0.5004\n",
            "Epoch [9069/20000], Training Loss: 0.5279\n",
            "Epoch [9070/20000], Training Loss: 0.4899\n",
            "Epoch [9071/20000], Training Loss: 0.4726\n",
            "Epoch [9072/20000], Training Loss: 0.5171\n",
            "Epoch [9073/20000], Training Loss: 0.4771\n",
            "Epoch [9074/20000], Training Loss: 0.4993\n",
            "Epoch [9075/20000], Training Loss: 0.5079\n",
            "Epoch [9076/20000], Training Loss: 0.4868\n",
            "Epoch [9077/20000], Training Loss: 0.4725\n",
            "Epoch [9078/20000], Training Loss: 0.4839\n",
            "Epoch [9079/20000], Training Loss: 0.5135\n",
            "Epoch [9080/20000], Training Loss: 0.5134\n",
            "Epoch [9081/20000], Training Loss: 0.4889\n",
            "Epoch [9082/20000], Training Loss: 0.4984\n",
            "Epoch [9083/20000], Training Loss: 0.4971\n",
            "Epoch [9084/20000], Training Loss: 0.5353\n",
            "Epoch [9085/20000], Training Loss: 0.4977\n",
            "Epoch [9086/20000], Training Loss: 0.5040\n",
            "Epoch [9087/20000], Training Loss: 0.4444\n",
            "Epoch [9088/20000], Training Loss: 0.5146\n",
            "Epoch [9089/20000], Training Loss: 0.4853\n",
            "Epoch [9090/20000], Training Loss: 0.4587\n",
            "Epoch [9091/20000], Training Loss: 0.4822\n",
            "Epoch [9092/20000], Training Loss: 0.4999\n",
            "Epoch [9093/20000], Training Loss: 0.5088\n",
            "Epoch [9094/20000], Training Loss: 0.4429\n",
            "Epoch [9095/20000], Training Loss: 0.5041\n",
            "Epoch [9096/20000], Training Loss: 0.4786\n",
            "Epoch [9097/20000], Training Loss: 0.4884\n",
            "Epoch [9098/20000], Training Loss: 0.5103\n",
            "Epoch [9099/20000], Training Loss: 0.5400\n",
            "Epoch [9100/20000], Training Loss: 0.5199\n",
            "Epoch [9101/20000], Training Loss: 0.5241\n",
            "Epoch [9102/20000], Training Loss: 0.4709\n",
            "Epoch [9103/20000], Training Loss: 0.5131\n",
            "Epoch [9104/20000], Training Loss: 0.4690\n",
            "Epoch [9105/20000], Training Loss: 0.4779\n",
            "Epoch [9106/20000], Training Loss: 0.4966\n",
            "Epoch [9107/20000], Training Loss: 0.5059\n",
            "Epoch [9108/20000], Training Loss: 0.4950\n",
            "Epoch [9109/20000], Training Loss: 0.4588\n",
            "Epoch [9110/20000], Training Loss: 0.4471\n",
            "Epoch [9111/20000], Training Loss: 0.4513\n",
            "Epoch [9112/20000], Training Loss: 0.4894\n",
            "Epoch [9113/20000], Training Loss: 0.4619\n",
            "Epoch [9114/20000], Training Loss: 0.4909\n",
            "Epoch [9115/20000], Training Loss: 0.5124\n",
            "Epoch [9116/20000], Training Loss: 0.5277\n",
            "Epoch [9117/20000], Training Loss: 0.5557\n",
            "Epoch [9118/20000], Training Loss: 0.5241\n",
            "Epoch [9119/20000], Training Loss: 0.4520\n",
            "Epoch [9120/20000], Training Loss: 0.5050\n",
            "Epoch [9121/20000], Training Loss: 0.4523\n",
            "Epoch [9122/20000], Training Loss: 0.5348\n",
            "Epoch [9123/20000], Training Loss: 0.5191\n",
            "Epoch [9124/20000], Training Loss: 0.4621\n",
            "Epoch [9125/20000], Training Loss: 0.4799\n",
            "Epoch [9126/20000], Training Loss: 0.4985\n",
            "Epoch [9127/20000], Training Loss: 0.5134\n",
            "Epoch [9128/20000], Training Loss: 0.4803\n",
            "Epoch [9129/20000], Training Loss: 0.4628\n",
            "Epoch [9130/20000], Training Loss: 0.4637\n",
            "Epoch [9131/20000], Training Loss: 0.4540\n",
            "Epoch [9132/20000], Training Loss: 0.4975\n",
            "Epoch [9133/20000], Training Loss: 0.4924\n",
            "Epoch [9134/20000], Training Loss: 0.5182\n",
            "Epoch [9135/20000], Training Loss: 0.4641\n",
            "Epoch [9136/20000], Training Loss: 0.4517\n",
            "Epoch [9137/20000], Training Loss: 0.5543\n",
            "Epoch [9138/20000], Training Loss: 0.4785\n",
            "Epoch [9139/20000], Training Loss: 0.4763\n",
            "Epoch [9140/20000], Training Loss: 0.4940\n",
            "Epoch [9141/20000], Training Loss: 0.4619\n",
            "Epoch [9142/20000], Training Loss: 0.4599\n",
            "Epoch [9143/20000], Training Loss: 0.4372\n",
            "Epoch [9144/20000], Training Loss: 0.5177\n",
            "Epoch [9145/20000], Training Loss: 0.5098\n",
            "Epoch [9146/20000], Training Loss: 0.4803\n",
            "Epoch [9147/20000], Training Loss: 0.5027\n",
            "Epoch [9148/20000], Training Loss: 0.5132\n",
            "Epoch [9149/20000], Training Loss: 0.4737\n",
            "Epoch [9150/20000], Training Loss: 0.5017\n",
            "Epoch [9151/20000], Training Loss: 0.4511\n",
            "Epoch [9152/20000], Training Loss: 0.5026\n",
            "Epoch [9153/20000], Training Loss: 0.4920\n",
            "Epoch [9154/20000], Training Loss: 0.4912\n",
            "Epoch [9155/20000], Training Loss: 0.4567\n",
            "Epoch [9156/20000], Training Loss: 0.5144\n",
            "Epoch [9157/20000], Training Loss: 0.4823\n",
            "Epoch [9158/20000], Training Loss: 0.4620\n",
            "Epoch [9159/20000], Training Loss: 0.4495\n",
            "Epoch [9160/20000], Training Loss: 0.4989\n",
            "Epoch [9161/20000], Training Loss: 0.4754\n",
            "Epoch [9162/20000], Training Loss: 0.4968\n",
            "Epoch [9163/20000], Training Loss: 0.5163\n",
            "Epoch [9164/20000], Training Loss: 0.4400\n",
            "Epoch [9165/20000], Training Loss: 0.4999\n",
            "Epoch [9166/20000], Training Loss: 0.4665\n",
            "Epoch [9167/20000], Training Loss: 0.5055\n",
            "Epoch [9168/20000], Training Loss: 0.4874\n",
            "Epoch [9169/20000], Training Loss: 0.4349\n",
            "Epoch [9170/20000], Training Loss: 0.5071\n",
            "Epoch [9171/20000], Training Loss: 0.5344\n",
            "Epoch [9172/20000], Training Loss: 0.4809\n",
            "Epoch [9173/20000], Training Loss: 0.4877\n",
            "Epoch [9174/20000], Training Loss: 0.5141\n",
            "Epoch [9175/20000], Training Loss: 0.5420\n",
            "Epoch [9176/20000], Training Loss: 0.4648\n",
            "Epoch [9177/20000], Training Loss: 0.4594\n",
            "Epoch [9178/20000], Training Loss: 0.5163\n",
            "Epoch [9179/20000], Training Loss: 0.5003\n",
            "Epoch [9180/20000], Training Loss: 0.4907\n",
            "Epoch [9181/20000], Training Loss: 0.4626\n",
            "Epoch [9182/20000], Training Loss: 0.4907\n",
            "Epoch [9183/20000], Training Loss: 0.5136\n",
            "Epoch [9184/20000], Training Loss: 0.4845\n",
            "Epoch [9185/20000], Training Loss: 0.4772\n",
            "Epoch [9186/20000], Training Loss: 0.4815\n",
            "Epoch [9187/20000], Training Loss: 0.5067\n",
            "Epoch [9188/20000], Training Loss: 0.4983\n",
            "Epoch [9189/20000], Training Loss: 0.4617\n",
            "Epoch [9190/20000], Training Loss: 0.5126\n",
            "Epoch [9191/20000], Training Loss: 0.5272\n",
            "Epoch [9192/20000], Training Loss: 0.4684\n",
            "Epoch [9193/20000], Training Loss: 0.4927\n",
            "Epoch [9194/20000], Training Loss: 0.4522\n",
            "Epoch [9195/20000], Training Loss: 0.5097\n",
            "Epoch [9196/20000], Training Loss: 0.4692\n",
            "Epoch [9197/20000], Training Loss: 0.5093\n",
            "Epoch [9198/20000], Training Loss: 0.4535\n",
            "Epoch [9199/20000], Training Loss: 0.4558\n",
            "Epoch [9200/20000], Training Loss: 0.4967\n",
            "Epoch [9201/20000], Training Loss: 0.4693\n",
            "Epoch [9202/20000], Training Loss: 0.4969\n",
            "Epoch [9203/20000], Training Loss: 0.5127\n",
            "Epoch [9204/20000], Training Loss: 0.4610\n",
            "Epoch [9205/20000], Training Loss: 0.4809\n",
            "Epoch [9206/20000], Training Loss: 0.5178\n",
            "Epoch [9207/20000], Training Loss: 0.5030\n",
            "Epoch [9208/20000], Training Loss: 0.4856\n",
            "Epoch [9209/20000], Training Loss: 0.4838\n",
            "Epoch [9210/20000], Training Loss: 0.5049\n",
            "Epoch [9211/20000], Training Loss: 0.4706\n",
            "Epoch [9212/20000], Training Loss: 0.4529\n",
            "Epoch [9213/20000], Training Loss: 0.5097\n",
            "Epoch [9214/20000], Training Loss: 0.4838\n",
            "Epoch [9215/20000], Training Loss: 0.5045\n",
            "Epoch [9216/20000], Training Loss: 0.4993\n",
            "Epoch [9217/20000], Training Loss: 0.4668\n",
            "Epoch [9218/20000], Training Loss: 0.4639\n",
            "Epoch [9219/20000], Training Loss: 0.4491\n",
            "Epoch [9220/20000], Training Loss: 0.4876\n",
            "Epoch [9221/20000], Training Loss: 0.4725\n",
            "Epoch [9222/20000], Training Loss: 0.4983\n",
            "Epoch [9223/20000], Training Loss: 0.5123\n",
            "Epoch [9224/20000], Training Loss: 0.4570\n",
            "Epoch [9225/20000], Training Loss: 0.4481\n",
            "Epoch [9226/20000], Training Loss: 0.4622\n",
            "Epoch [9227/20000], Training Loss: 0.5180\n",
            "Epoch [9228/20000], Training Loss: 0.5158\n",
            "Epoch [9229/20000], Training Loss: 0.5083\n",
            "Epoch [9230/20000], Training Loss: 0.4825\n",
            "Epoch [9231/20000], Training Loss: 0.5141\n",
            "Epoch [9232/20000], Training Loss: 0.4895\n",
            "Epoch [9233/20000], Training Loss: 0.5064\n",
            "Epoch [9234/20000], Training Loss: 0.4758\n",
            "Epoch [9235/20000], Training Loss: 0.4649\n",
            "Epoch [9236/20000], Training Loss: 0.5175\n",
            "Epoch [9237/20000], Training Loss: 0.4726\n",
            "Epoch [9238/20000], Training Loss: 0.4456\n",
            "Epoch [9239/20000], Training Loss: 0.4463\n",
            "Epoch [9240/20000], Training Loss: 0.4301\n",
            "Epoch [9241/20000], Training Loss: 0.5004\n",
            "Epoch [9242/20000], Training Loss: 0.4800\n",
            "Epoch [9243/20000], Training Loss: 0.4898\n",
            "Epoch [9244/20000], Training Loss: 0.4955\n",
            "Epoch [9245/20000], Training Loss: 0.5013\n",
            "Epoch [9246/20000], Training Loss: 0.4720\n",
            "Epoch [9247/20000], Training Loss: 0.5181\n",
            "Epoch [9248/20000], Training Loss: 0.4979\n",
            "Epoch [9249/20000], Training Loss: 0.5088\n",
            "Epoch [9250/20000], Training Loss: 0.4825\n",
            "Epoch [9251/20000], Training Loss: 0.4866\n",
            "Epoch [9252/20000], Training Loss: 0.4889\n",
            "Epoch [9253/20000], Training Loss: 0.5224\n",
            "Epoch [9254/20000], Training Loss: 0.5146\n",
            "Epoch [9255/20000], Training Loss: 0.5033\n",
            "Epoch [9256/20000], Training Loss: 0.4747\n",
            "Epoch [9257/20000], Training Loss: 0.4488\n",
            "Epoch [9258/20000], Training Loss: 0.4471\n",
            "Epoch [9259/20000], Training Loss: 0.4841\n",
            "Epoch [9260/20000], Training Loss: 0.4921\n",
            "Epoch [9261/20000], Training Loss: 0.5197\n",
            "Epoch [9262/20000], Training Loss: 0.4960\n",
            "Epoch [9263/20000], Training Loss: 0.4580\n",
            "Epoch [9264/20000], Training Loss: 0.5376\n",
            "Epoch [9265/20000], Training Loss: 0.4902\n",
            "Epoch [9266/20000], Training Loss: 0.5122\n",
            "Epoch [9267/20000], Training Loss: 0.5132\n",
            "Epoch [9268/20000], Training Loss: 0.4782\n",
            "Epoch [9269/20000], Training Loss: 0.5031\n",
            "Epoch [9270/20000], Training Loss: 0.4599\n",
            "Epoch [9271/20000], Training Loss: 0.4704\n",
            "Epoch [9272/20000], Training Loss: 0.5302\n",
            "Epoch [9273/20000], Training Loss: 0.5499\n",
            "Epoch [9274/20000], Training Loss: 0.4596\n",
            "Epoch [9275/20000], Training Loss: 0.5017\n",
            "Epoch [9276/20000], Training Loss: 0.4851\n",
            "Epoch [9277/20000], Training Loss: 0.5143\n",
            "Epoch [9278/20000], Training Loss: 0.4838\n",
            "Epoch [9279/20000], Training Loss: 0.4613\n",
            "Epoch [9280/20000], Training Loss: 0.4924\n",
            "Epoch [9281/20000], Training Loss: 0.4847\n",
            "Epoch [9282/20000], Training Loss: 0.5175\n",
            "Epoch [9283/20000], Training Loss: 0.5248\n",
            "Epoch [9284/20000], Training Loss: 0.4767\n",
            "Epoch [9285/20000], Training Loss: 0.5524\n",
            "Epoch [9286/20000], Training Loss: 0.4587\n",
            "Epoch [9287/20000], Training Loss: 0.5327\n",
            "Epoch [9288/20000], Training Loss: 0.5078\n",
            "Epoch [9289/20000], Training Loss: 0.4746\n",
            "Epoch [9290/20000], Training Loss: 0.4861\n",
            "Epoch [9291/20000], Training Loss: 0.5202\n",
            "Epoch [9292/20000], Training Loss: 0.5431\n",
            "Epoch [9293/20000], Training Loss: 0.5149\n",
            "Epoch [9294/20000], Training Loss: 0.4607\n",
            "Epoch [9295/20000], Training Loss: 0.4952\n",
            "Epoch [9296/20000], Training Loss: 0.4685\n",
            "Epoch [9297/20000], Training Loss: 0.4875\n",
            "Epoch [9298/20000], Training Loss: 0.5117\n",
            "Epoch [9299/20000], Training Loss: 0.5459\n",
            "Epoch [9300/20000], Training Loss: 0.5164\n",
            "Epoch [9301/20000], Training Loss: 0.4738\n",
            "Epoch [9302/20000], Training Loss: 0.4841\n",
            "Epoch [9303/20000], Training Loss: 0.5264\n",
            "Epoch [9304/20000], Training Loss: 0.4850\n",
            "Epoch [9305/20000], Training Loss: 0.4994\n",
            "Epoch [9306/20000], Training Loss: 0.4978\n",
            "Epoch [9307/20000], Training Loss: 0.4801\n",
            "Epoch [9308/20000], Training Loss: 0.4511\n",
            "Epoch [9309/20000], Training Loss: 0.4985\n",
            "Epoch [9310/20000], Training Loss: 0.4683\n",
            "Epoch [9311/20000], Training Loss: 0.5214\n",
            "Epoch [9312/20000], Training Loss: 0.4817\n",
            "Epoch [9313/20000], Training Loss: 0.4441\n",
            "Epoch [9314/20000], Training Loss: 0.5182\n",
            "Epoch [9315/20000], Training Loss: 0.5011\n",
            "Epoch [9316/20000], Training Loss: 0.4958\n",
            "Epoch [9317/20000], Training Loss: 0.4563\n",
            "Epoch [9318/20000], Training Loss: 0.4566\n",
            "Epoch [9319/20000], Training Loss: 0.4822\n",
            "Epoch [9320/20000], Training Loss: 0.5234\n",
            "Epoch [9321/20000], Training Loss: 0.5163\n",
            "Epoch [9322/20000], Training Loss: 0.4502\n",
            "Epoch [9323/20000], Training Loss: 0.5612\n",
            "Epoch [9324/20000], Training Loss: 0.5018\n",
            "Epoch [9325/20000], Training Loss: 0.4879\n",
            "Epoch [9326/20000], Training Loss: 0.4604\n",
            "Epoch [9327/20000], Training Loss: 0.4925\n",
            "Epoch [9328/20000], Training Loss: 0.4628\n",
            "Epoch [9329/20000], Training Loss: 0.4877\n",
            "Epoch [9330/20000], Training Loss: 0.4479\n",
            "Epoch [9331/20000], Training Loss: 0.4884\n",
            "Epoch [9332/20000], Training Loss: 0.4925\n",
            "Epoch [9333/20000], Training Loss: 0.4553\n",
            "Epoch [9334/20000], Training Loss: 0.5218\n",
            "Epoch [9335/20000], Training Loss: 0.4701\n",
            "Epoch [9336/20000], Training Loss: 0.4856\n",
            "Epoch [9337/20000], Training Loss: 0.4437\n",
            "Epoch [9338/20000], Training Loss: 0.4901\n",
            "Epoch [9339/20000], Training Loss: 0.4995\n",
            "Epoch [9340/20000], Training Loss: 0.5025\n",
            "Epoch [9341/20000], Training Loss: 0.4715\n",
            "Epoch [9342/20000], Training Loss: 0.5256\n",
            "Epoch [9343/20000], Training Loss: 0.4788\n",
            "Epoch [9344/20000], Training Loss: 0.4715\n",
            "Epoch [9345/20000], Training Loss: 0.5123\n",
            "Epoch [9346/20000], Training Loss: 0.5019\n",
            "Epoch [9347/20000], Training Loss: 0.4521\n",
            "Epoch [9348/20000], Training Loss: 0.4900\n",
            "Epoch [9349/20000], Training Loss: 0.4646\n",
            "Epoch [9350/20000], Training Loss: 0.4763\n",
            "Epoch [9351/20000], Training Loss: 0.4854\n",
            "Epoch [9352/20000], Training Loss: 0.4957\n",
            "Epoch [9353/20000], Training Loss: 0.5027\n",
            "Epoch [9354/20000], Training Loss: 0.4912\n",
            "Epoch [9355/20000], Training Loss: 0.4838\n",
            "Epoch [9356/20000], Training Loss: 0.4608\n",
            "Epoch [9357/20000], Training Loss: 0.4772\n",
            "Epoch [9358/20000], Training Loss: 0.4435\n",
            "Epoch [9359/20000], Training Loss: 0.5038\n",
            "Epoch [9360/20000], Training Loss: 0.5155\n",
            "Epoch [9361/20000], Training Loss: 0.4879\n",
            "Epoch [9362/20000], Training Loss: 0.5018\n",
            "Epoch [9363/20000], Training Loss: 0.4855\n",
            "Epoch [9364/20000], Training Loss: 0.4975\n",
            "Epoch [9365/20000], Training Loss: 0.4537\n",
            "Epoch [9366/20000], Training Loss: 0.5011\n",
            "Epoch [9367/20000], Training Loss: 0.4762\n",
            "Epoch [9368/20000], Training Loss: 0.5161\n",
            "Epoch [9369/20000], Training Loss: 0.4473\n",
            "Epoch [9370/20000], Training Loss: 0.4644\n",
            "Epoch [9371/20000], Training Loss: 0.4341\n",
            "Epoch [9372/20000], Training Loss: 0.4513\n",
            "Epoch [9373/20000], Training Loss: 0.4593\n",
            "Epoch [9374/20000], Training Loss: 0.4873\n",
            "Epoch [9375/20000], Training Loss: 0.4602\n",
            "Epoch [9376/20000], Training Loss: 0.5133\n",
            "Epoch [9377/20000], Training Loss: 0.4671\n",
            "Epoch [9378/20000], Training Loss: 0.4732\n",
            "Epoch [9379/20000], Training Loss: 0.4746\n",
            "Epoch [9380/20000], Training Loss: 0.4918\n",
            "Epoch [9381/20000], Training Loss: 0.4560\n",
            "Epoch [9382/20000], Training Loss: 0.4610\n",
            "Epoch [9383/20000], Training Loss: 0.4929\n",
            "Epoch [9384/20000], Training Loss: 0.4554\n",
            "Epoch [9385/20000], Training Loss: 0.4962\n",
            "Epoch [9386/20000], Training Loss: 0.4511\n",
            "Epoch [9387/20000], Training Loss: 0.5305\n",
            "Epoch [9388/20000], Training Loss: 0.4479\n",
            "Epoch [9389/20000], Training Loss: 0.4611\n",
            "Epoch [9390/20000], Training Loss: 0.4785\n",
            "Epoch [9391/20000], Training Loss: 0.4683\n",
            "Epoch [9392/20000], Training Loss: 0.4548\n",
            "Epoch [9393/20000], Training Loss: 0.4947\n",
            "Epoch [9394/20000], Training Loss: 0.4814\n",
            "Epoch [9395/20000], Training Loss: 0.4835\n",
            "Epoch [9396/20000], Training Loss: 0.4987\n",
            "Epoch [9397/20000], Training Loss: 0.4586\n",
            "Epoch [9398/20000], Training Loss: 0.4933\n",
            "Epoch [9399/20000], Training Loss: 0.4659\n",
            "Epoch [9400/20000], Training Loss: 0.5019\n",
            "Epoch [9401/20000], Training Loss: 0.4825\n",
            "Epoch [9402/20000], Training Loss: 0.4735\n",
            "Epoch [9403/20000], Training Loss: 0.4626\n",
            "Epoch [9404/20000], Training Loss: 0.5151\n",
            "Epoch [9405/20000], Training Loss: 0.4682\n",
            "Epoch [9406/20000], Training Loss: 0.4846\n",
            "Epoch [9407/20000], Training Loss: 0.5438\n",
            "Epoch [9408/20000], Training Loss: 0.4969\n",
            "Epoch [9409/20000], Training Loss: 0.4765\n",
            "Epoch [9410/20000], Training Loss: 0.5054\n",
            "Epoch [9411/20000], Training Loss: 0.5172\n",
            "Epoch [9412/20000], Training Loss: 0.4732\n",
            "Epoch [9413/20000], Training Loss: 0.4874\n",
            "Epoch [9414/20000], Training Loss: 0.4831\n",
            "Epoch [9415/20000], Training Loss: 0.4459\n",
            "Epoch [9416/20000], Training Loss: 0.5226\n",
            "Epoch [9417/20000], Training Loss: 0.5135\n",
            "Epoch [9418/20000], Training Loss: 0.4935\n",
            "Epoch [9419/20000], Training Loss: 0.4514\n",
            "Epoch [9420/20000], Training Loss: 0.4936\n",
            "Epoch [9421/20000], Training Loss: 0.4930\n",
            "Epoch [9422/20000], Training Loss: 0.4392\n",
            "Epoch [9423/20000], Training Loss: 0.5138\n",
            "Epoch [9424/20000], Training Loss: 0.4440\n",
            "Epoch [9425/20000], Training Loss: 0.4969\n",
            "Epoch [9426/20000], Training Loss: 0.4813\n",
            "Epoch [9427/20000], Training Loss: 0.4641\n",
            "Epoch [9428/20000], Training Loss: 0.4647\n",
            "Epoch [9429/20000], Training Loss: 0.4710\n",
            "Epoch [9430/20000], Training Loss: 0.4736\n",
            "Epoch [9431/20000], Training Loss: 0.5045\n",
            "Epoch [9432/20000], Training Loss: 0.5167\n",
            "Epoch [9433/20000], Training Loss: 0.4558\n",
            "Epoch [9434/20000], Training Loss: 0.4328\n",
            "Epoch [9435/20000], Training Loss: 0.4713\n",
            "Epoch [9436/20000], Training Loss: 0.4658\n",
            "Epoch [9437/20000], Training Loss: 0.4682\n",
            "Epoch [9438/20000], Training Loss: 0.5057\n",
            "Epoch [9439/20000], Training Loss: 0.4766\n",
            "Epoch [9440/20000], Training Loss: 0.4605\n",
            "Epoch [9441/20000], Training Loss: 0.4879\n",
            "Epoch [9442/20000], Training Loss: 0.5224\n",
            "Epoch [9443/20000], Training Loss: 0.4797\n",
            "Epoch [9444/20000], Training Loss: 0.5187\n",
            "Epoch [9445/20000], Training Loss: 0.4704\n",
            "Epoch [9446/20000], Training Loss: 0.4794\n",
            "Epoch [9447/20000], Training Loss: 0.4857\n",
            "Epoch [9448/20000], Training Loss: 0.5535\n",
            "Epoch [9449/20000], Training Loss: 0.4744\n",
            "Epoch [9450/20000], Training Loss: 0.4679\n",
            "Epoch [9451/20000], Training Loss: 0.5287\n",
            "Epoch [9452/20000], Training Loss: 0.4804\n",
            "Epoch [9453/20000], Training Loss: 0.4910\n",
            "Epoch [9454/20000], Training Loss: 0.4839\n",
            "Epoch [9455/20000], Training Loss: 0.4850\n",
            "Epoch [9456/20000], Training Loss: 0.4599\n",
            "Epoch [9457/20000], Training Loss: 0.4776\n",
            "Epoch [9458/20000], Training Loss: 0.5573\n",
            "Epoch [9459/20000], Training Loss: 0.4598\n",
            "Epoch [9460/20000], Training Loss: 0.4615\n",
            "Epoch [9461/20000], Training Loss: 0.4624\n",
            "Epoch [9462/20000], Training Loss: 0.5180\n",
            "Epoch [9463/20000], Training Loss: 0.4943\n",
            "Epoch [9464/20000], Training Loss: 0.4853\n",
            "Epoch [9465/20000], Training Loss: 0.5085\n",
            "Epoch [9466/20000], Training Loss: 0.5117\n",
            "Epoch [9467/20000], Training Loss: 0.5183\n",
            "Epoch [9468/20000], Training Loss: 0.5343\n",
            "Epoch [9469/20000], Training Loss: 0.5099\n",
            "Epoch [9470/20000], Training Loss: 0.4766\n",
            "Epoch [9471/20000], Training Loss: 0.4649\n",
            "Epoch [9472/20000], Training Loss: 0.4933\n",
            "Epoch [9473/20000], Training Loss: 0.5157\n",
            "Epoch [9474/20000], Training Loss: 0.4825\n",
            "Epoch [9475/20000], Training Loss: 0.4954\n",
            "Epoch [9476/20000], Training Loss: 0.4719\n",
            "Epoch [9477/20000], Training Loss: 0.4831\n",
            "Epoch [9478/20000], Training Loss: 0.4983\n",
            "Epoch [9479/20000], Training Loss: 0.5031\n",
            "Epoch [9480/20000], Training Loss: 0.4629\n",
            "Epoch [9481/20000], Training Loss: 0.4900\n",
            "Epoch [9482/20000], Training Loss: 0.5212\n",
            "Epoch [9483/20000], Training Loss: 0.5590\n",
            "Epoch [9484/20000], Training Loss: 0.4896\n",
            "Epoch [9485/20000], Training Loss: 0.4894\n",
            "Epoch [9486/20000], Training Loss: 0.4675\n",
            "Epoch [9487/20000], Training Loss: 0.4994\n",
            "Epoch [9488/20000], Training Loss: 0.4822\n",
            "Epoch [9489/20000], Training Loss: 0.4956\n",
            "Epoch [9490/20000], Training Loss: 0.4602\n",
            "Epoch [9491/20000], Training Loss: 0.4721\n",
            "Epoch [9492/20000], Training Loss: 0.4546\n",
            "Epoch [9493/20000], Training Loss: 0.5039\n",
            "Epoch [9494/20000], Training Loss: 0.4717\n",
            "Epoch [9495/20000], Training Loss: 0.4753\n",
            "Epoch [9496/20000], Training Loss: 0.4591\n",
            "Epoch [9497/20000], Training Loss: 0.4799\n",
            "Epoch [9498/20000], Training Loss: 0.5044\n",
            "Epoch [9499/20000], Training Loss: 0.5370\n",
            "Epoch [9500/20000], Training Loss: 0.4745\n",
            "Epoch [9501/20000], Training Loss: 0.5025\n",
            "Epoch [9502/20000], Training Loss: 0.4759\n",
            "Epoch [9503/20000], Training Loss: 0.5331\n",
            "Epoch [9504/20000], Training Loss: 0.4638\n",
            "Epoch [9505/20000], Training Loss: 0.5072\n",
            "Epoch [9506/20000], Training Loss: 0.5157\n",
            "Epoch [9507/20000], Training Loss: 0.5150\n",
            "Epoch [9508/20000], Training Loss: 0.4726\n",
            "Epoch [9509/20000], Training Loss: 0.4555\n",
            "Epoch [9510/20000], Training Loss: 0.5393\n",
            "Epoch [9511/20000], Training Loss: 0.4847\n",
            "Epoch [9512/20000], Training Loss: 0.5085\n",
            "Epoch [9513/20000], Training Loss: 0.5257\n",
            "Epoch [9514/20000], Training Loss: 0.4939\n",
            "Epoch [9515/20000], Training Loss: 0.4941\n",
            "Epoch [9516/20000], Training Loss: 0.5057\n",
            "Epoch [9517/20000], Training Loss: 0.4865\n",
            "Epoch [9518/20000], Training Loss: 0.5209\n",
            "Epoch [9519/20000], Training Loss: 0.5009\n",
            "Epoch [9520/20000], Training Loss: 0.4601\n",
            "Epoch [9521/20000], Training Loss: 0.5229\n",
            "Epoch [9522/20000], Training Loss: 0.4837\n",
            "Epoch [9523/20000], Training Loss: 0.5363\n",
            "Epoch [9524/20000], Training Loss: 0.4900\n",
            "Epoch [9525/20000], Training Loss: 0.5215\n",
            "Epoch [9526/20000], Training Loss: 0.4999\n",
            "Epoch [9527/20000], Training Loss: 0.4330\n",
            "Epoch [9528/20000], Training Loss: 0.5505\n",
            "Epoch [9529/20000], Training Loss: 0.4866\n",
            "Epoch [9530/20000], Training Loss: 0.5325\n",
            "Epoch [9531/20000], Training Loss: 0.4719\n",
            "Epoch [9532/20000], Training Loss: 0.4892\n",
            "Epoch [9533/20000], Training Loss: 0.4770\n",
            "Epoch [9534/20000], Training Loss: 0.4903\n",
            "Epoch [9535/20000], Training Loss: 0.5174\n",
            "Epoch [9536/20000], Training Loss: 0.4861\n",
            "Epoch [9537/20000], Training Loss: 0.4815\n",
            "Epoch [9538/20000], Training Loss: 0.4873\n",
            "Epoch [9539/20000], Training Loss: 0.4644\n",
            "Epoch [9540/20000], Training Loss: 0.5148\n",
            "Epoch [9541/20000], Training Loss: 0.4788\n",
            "Epoch [9542/20000], Training Loss: 0.4708\n",
            "Epoch [9543/20000], Training Loss: 0.4694\n",
            "Epoch [9544/20000], Training Loss: 0.4583\n",
            "Epoch [9545/20000], Training Loss: 0.4703\n",
            "Epoch [9546/20000], Training Loss: 0.4593\n",
            "Epoch [9547/20000], Training Loss: 0.4752\n",
            "Epoch [9548/20000], Training Loss: 0.4815\n",
            "Epoch [9549/20000], Training Loss: 0.4972\n",
            "Epoch [9550/20000], Training Loss: 0.4982\n",
            "Epoch [9551/20000], Training Loss: 0.4808\n",
            "Epoch [9552/20000], Training Loss: 0.4487\n",
            "Epoch [9553/20000], Training Loss: 0.4863\n",
            "Epoch [9554/20000], Training Loss: 0.5141\n",
            "Epoch [9555/20000], Training Loss: 0.4465\n",
            "Epoch [9556/20000], Training Loss: 0.5112\n",
            "Epoch [9557/20000], Training Loss: 0.4861\n",
            "Epoch [9558/20000], Training Loss: 0.5012\n",
            "Epoch [9559/20000], Training Loss: 0.5075\n",
            "Epoch [9560/20000], Training Loss: 0.4722\n",
            "Epoch [9561/20000], Training Loss: 0.4616\n",
            "Epoch [9562/20000], Training Loss: 0.5481\n",
            "Epoch [9563/20000], Training Loss: 0.5095\n",
            "Epoch [9564/20000], Training Loss: 0.5077\n",
            "Epoch [9565/20000], Training Loss: 0.4897\n",
            "Epoch [9566/20000], Training Loss: 0.4665\n",
            "Epoch [9567/20000], Training Loss: 0.5245\n",
            "Epoch [9568/20000], Training Loss: 0.5313\n",
            "Epoch [9569/20000], Training Loss: 0.4636\n",
            "Epoch [9570/20000], Training Loss: 0.4602\n",
            "Epoch [9571/20000], Training Loss: 0.4850\n",
            "Epoch [9572/20000], Training Loss: 0.4995\n",
            "Epoch [9573/20000], Training Loss: 0.5226\n",
            "Epoch [9574/20000], Training Loss: 0.5062\n",
            "Epoch [9575/20000], Training Loss: 0.4612\n",
            "Epoch [9576/20000], Training Loss: 0.4712\n",
            "Epoch [9577/20000], Training Loss: 0.5055\n",
            "Epoch [9578/20000], Training Loss: 0.5157\n",
            "Epoch [9579/20000], Training Loss: 0.4962\n",
            "Epoch [9580/20000], Training Loss: 0.4968\n",
            "Epoch [9581/20000], Training Loss: 0.5157\n",
            "Epoch [9582/20000], Training Loss: 0.4697\n",
            "Epoch [9583/20000], Training Loss: 0.4911\n",
            "Epoch [9584/20000], Training Loss: 0.4506\n",
            "Epoch [9585/20000], Training Loss: 0.5021\n",
            "Epoch [9586/20000], Training Loss: 0.4882\n",
            "Epoch [9587/20000], Training Loss: 0.5095\n",
            "Epoch [9588/20000], Training Loss: 0.4954\n",
            "Epoch [9589/20000], Training Loss: 0.5182\n",
            "Epoch [9590/20000], Training Loss: 0.4474\n",
            "Epoch [9591/20000], Training Loss: 0.5102\n",
            "Epoch [9592/20000], Training Loss: 0.4774\n",
            "Epoch [9593/20000], Training Loss: 0.4948\n",
            "Epoch [9594/20000], Training Loss: 0.5333\n",
            "Epoch [9595/20000], Training Loss: 0.5017\n",
            "Epoch [9596/20000], Training Loss: 0.4523\n",
            "Epoch [9597/20000], Training Loss: 0.4959\n",
            "Epoch [9598/20000], Training Loss: 0.4936\n",
            "Epoch [9599/20000], Training Loss: 0.4922\n",
            "Epoch [9600/20000], Training Loss: 0.4679\n",
            "Epoch [9601/20000], Training Loss: 0.5258\n",
            "Epoch [9602/20000], Training Loss: 0.4687\n",
            "Epoch [9603/20000], Training Loss: 0.4880\n",
            "Epoch [9604/20000], Training Loss: 0.4910\n",
            "Epoch [9605/20000], Training Loss: 0.4710\n",
            "Epoch [9606/20000], Training Loss: 0.5072\n",
            "Epoch [9607/20000], Training Loss: 0.5132\n",
            "Epoch [9608/20000], Training Loss: 0.4795\n",
            "Epoch [9609/20000], Training Loss: 0.4474\n",
            "Epoch [9610/20000], Training Loss: 0.4474\n",
            "Epoch [9611/20000], Training Loss: 0.5056\n",
            "Epoch [9612/20000], Training Loss: 0.5269\n",
            "Epoch [9613/20000], Training Loss: 0.4756\n",
            "Epoch [9614/20000], Training Loss: 0.4699\n",
            "Epoch [9615/20000], Training Loss: 0.4895\n",
            "Epoch [9616/20000], Training Loss: 0.5029\n",
            "Epoch [9617/20000], Training Loss: 0.4733\n",
            "Epoch [9618/20000], Training Loss: 0.4812\n",
            "Epoch [9619/20000], Training Loss: 0.4484\n",
            "Epoch [9620/20000], Training Loss: 0.5209\n",
            "Epoch [9621/20000], Training Loss: 0.4612\n",
            "Epoch [9622/20000], Training Loss: 0.5064\n",
            "Epoch [9623/20000], Training Loss: 0.4943\n",
            "Epoch [9624/20000], Training Loss: 0.5090\n",
            "Epoch [9625/20000], Training Loss: 0.4813\n",
            "Epoch [9626/20000], Training Loss: 0.5111\n",
            "Epoch [9627/20000], Training Loss: 0.4721\n",
            "Epoch [9628/20000], Training Loss: 0.5043\n",
            "Epoch [9629/20000], Training Loss: 0.4925\n",
            "Epoch [9630/20000], Training Loss: 0.4853\n",
            "Epoch [9631/20000], Training Loss: 0.5196\n",
            "Epoch [9632/20000], Training Loss: 0.4803\n",
            "Epoch [9633/20000], Training Loss: 0.4734\n",
            "Epoch [9634/20000], Training Loss: 0.4674\n",
            "Epoch [9635/20000], Training Loss: 0.5473\n",
            "Epoch [9636/20000], Training Loss: 0.5062\n",
            "Epoch [9637/20000], Training Loss: 0.4772\n",
            "Epoch [9638/20000], Training Loss: 0.4625\n",
            "Epoch [9639/20000], Training Loss: 0.4848\n",
            "Epoch [9640/20000], Training Loss: 0.4898\n",
            "Epoch [9641/20000], Training Loss: 0.4408\n",
            "Epoch [9642/20000], Training Loss: 0.5102\n",
            "Epoch [9643/20000], Training Loss: 0.5254\n",
            "Epoch [9644/20000], Training Loss: 0.4595\n",
            "Epoch [9645/20000], Training Loss: 0.4979\n",
            "Epoch [9646/20000], Training Loss: 0.5217\n",
            "Epoch [9647/20000], Training Loss: 0.4833\n",
            "Epoch [9648/20000], Training Loss: 0.5155\n",
            "Epoch [9649/20000], Training Loss: 0.5307\n",
            "Epoch [9650/20000], Training Loss: 0.4812\n",
            "Epoch [9651/20000], Training Loss: 0.4488\n",
            "Epoch [9652/20000], Training Loss: 0.4717\n",
            "Epoch [9653/20000], Training Loss: 0.4966\n",
            "Epoch [9654/20000], Training Loss: 0.4517\n",
            "Epoch [9655/20000], Training Loss: 0.5308\n",
            "Epoch [9656/20000], Training Loss: 0.4704\n",
            "Epoch [9657/20000], Training Loss: 0.4850\n",
            "Epoch [9658/20000], Training Loss: 0.5001\n",
            "Epoch [9659/20000], Training Loss: 0.5083\n",
            "Epoch [9660/20000], Training Loss: 0.5205\n",
            "Epoch [9661/20000], Training Loss: 0.4538\n",
            "Epoch [9662/20000], Training Loss: 0.4520\n",
            "Epoch [9663/20000], Training Loss: 0.5087\n",
            "Epoch [9664/20000], Training Loss: 0.4552\n",
            "Epoch [9665/20000], Training Loss: 0.4796\n",
            "Epoch [9666/20000], Training Loss: 0.4918\n",
            "Epoch [9667/20000], Training Loss: 0.4860\n",
            "Epoch [9668/20000], Training Loss: 0.5139\n",
            "Epoch [9669/20000], Training Loss: 0.4791\n",
            "Epoch [9670/20000], Training Loss: 0.4675\n",
            "Epoch [9671/20000], Training Loss: 0.5298\n",
            "Epoch [9672/20000], Training Loss: 0.4470\n",
            "Epoch [9673/20000], Training Loss: 0.5083\n",
            "Epoch [9674/20000], Training Loss: 0.5141\n",
            "Epoch [9675/20000], Training Loss: 0.4622\n",
            "Epoch [9676/20000], Training Loss: 0.5277\n",
            "Epoch [9677/20000], Training Loss: 0.4812\n",
            "Epoch [9678/20000], Training Loss: 0.5057\n",
            "Epoch [9679/20000], Training Loss: 0.5316\n",
            "Epoch [9680/20000], Training Loss: 0.5012\n",
            "Epoch [9681/20000], Training Loss: 0.4700\n",
            "Epoch [9682/20000], Training Loss: 0.5359\n",
            "Epoch [9683/20000], Training Loss: 0.4791\n",
            "Epoch [9684/20000], Training Loss: 0.4981\n",
            "Epoch [9685/20000], Training Loss: 0.5236\n",
            "Epoch [9686/20000], Training Loss: 0.4646\n",
            "Epoch [9687/20000], Training Loss: 0.4454\n",
            "Epoch [9688/20000], Training Loss: 0.4754\n",
            "Epoch [9689/20000], Training Loss: 0.5001\n",
            "Epoch [9690/20000], Training Loss: 0.4586\n",
            "Epoch [9691/20000], Training Loss: 0.5225\n",
            "Epoch [9692/20000], Training Loss: 0.4888\n",
            "Epoch [9693/20000], Training Loss: 0.4909\n",
            "Epoch [9694/20000], Training Loss: 0.4838\n",
            "Epoch [9695/20000], Training Loss: 0.5141\n",
            "Epoch [9696/20000], Training Loss: 0.4888\n",
            "Epoch [9697/20000], Training Loss: 0.4607\n",
            "Epoch [9698/20000], Training Loss: 0.4702\n",
            "Epoch [9699/20000], Training Loss: 0.4902\n",
            "Epoch [9700/20000], Training Loss: 0.5206\n",
            "Epoch [9701/20000], Training Loss: 0.5509\n",
            "Epoch [9702/20000], Training Loss: 0.4898\n",
            "Epoch [9703/20000], Training Loss: 0.5235\n",
            "Epoch [9704/20000], Training Loss: 0.4974\n",
            "Epoch [9705/20000], Training Loss: 0.4657\n",
            "Epoch [9706/20000], Training Loss: 0.4656\n",
            "Epoch [9707/20000], Training Loss: 0.5058\n",
            "Epoch [9708/20000], Training Loss: 0.4333\n",
            "Epoch [9709/20000], Training Loss: 0.4528\n",
            "Epoch [9710/20000], Training Loss: 0.4789\n",
            "Epoch [9711/20000], Training Loss: 0.5196\n",
            "Epoch [9712/20000], Training Loss: 0.4883\n",
            "Epoch [9713/20000], Training Loss: 0.5178\n",
            "Epoch [9714/20000], Training Loss: 0.4511\n",
            "Epoch [9715/20000], Training Loss: 0.5062\n",
            "Epoch [9716/20000], Training Loss: 0.5176\n",
            "Epoch [9717/20000], Training Loss: 0.4577\n",
            "Epoch [9718/20000], Training Loss: 0.4977\n",
            "Epoch [9719/20000], Training Loss: 0.4886\n",
            "Epoch [9720/20000], Training Loss: 0.4660\n",
            "Epoch [9721/20000], Training Loss: 0.4686\n",
            "Epoch [9722/20000], Training Loss: 0.4864\n",
            "Epoch [9723/20000], Training Loss: 0.5397\n",
            "Epoch [9724/20000], Training Loss: 0.4870\n",
            "Epoch [9725/20000], Training Loss: 0.5196\n",
            "Epoch [9726/20000], Training Loss: 0.5012\n",
            "Epoch [9727/20000], Training Loss: 0.4837\n",
            "Epoch [9728/20000], Training Loss: 0.5248\n",
            "Epoch [9729/20000], Training Loss: 0.4694\n",
            "Epoch [9730/20000], Training Loss: 0.4942\n",
            "Epoch [9731/20000], Training Loss: 0.4400\n",
            "Epoch [9732/20000], Training Loss: 0.4821\n",
            "Epoch [9733/20000], Training Loss: 0.4323\n",
            "Epoch [9734/20000], Training Loss: 0.4571\n",
            "Epoch [9735/20000], Training Loss: 0.4861\n",
            "Epoch [9736/20000], Training Loss: 0.4706\n",
            "Epoch [9737/20000], Training Loss: 0.5338\n",
            "Epoch [9738/20000], Training Loss: 0.4789\n",
            "Epoch [9739/20000], Training Loss: 0.4804\n",
            "Epoch [9740/20000], Training Loss: 0.5240\n",
            "Epoch [9741/20000], Training Loss: 0.5074\n",
            "Epoch [9742/20000], Training Loss: 0.4668\n",
            "Epoch [9743/20000], Training Loss: 0.4744\n",
            "Epoch [9744/20000], Training Loss: 0.5321\n",
            "Epoch [9745/20000], Training Loss: 0.4634\n",
            "Epoch [9746/20000], Training Loss: 0.4397\n",
            "Epoch [9747/20000], Training Loss: 0.5167\n",
            "Epoch [9748/20000], Training Loss: 0.4581\n",
            "Epoch [9749/20000], Training Loss: 0.5215\n",
            "Epoch [9750/20000], Training Loss: 0.4446\n",
            "Epoch [9751/20000], Training Loss: 0.4999\n",
            "Epoch [9752/20000], Training Loss: 0.5185\n",
            "Epoch [9753/20000], Training Loss: 0.5205\n",
            "Epoch [9754/20000], Training Loss: 0.4777\n",
            "Epoch [9755/20000], Training Loss: 0.5196\n",
            "Epoch [9756/20000], Training Loss: 0.4726\n",
            "Epoch [9757/20000], Training Loss: 0.4789\n",
            "Epoch [9758/20000], Training Loss: 0.4676\n",
            "Epoch [9759/20000], Training Loss: 0.4522\n",
            "Epoch [9760/20000], Training Loss: 0.5062\n",
            "Epoch [9761/20000], Training Loss: 0.5278\n",
            "Epoch [9762/20000], Training Loss: 0.4580\n",
            "Epoch [9763/20000], Training Loss: 0.4814\n",
            "Epoch [9764/20000], Training Loss: 0.4894\n",
            "Epoch [9765/20000], Training Loss: 0.4807\n",
            "Epoch [9766/20000], Training Loss: 0.5005\n",
            "Epoch [9767/20000], Training Loss: 0.4960\n",
            "Epoch [9768/20000], Training Loss: 0.4759\n",
            "Epoch [9769/20000], Training Loss: 0.4714\n",
            "Epoch [9770/20000], Training Loss: 0.5020\n",
            "Epoch [9771/20000], Training Loss: 0.4703\n",
            "Epoch [9772/20000], Training Loss: 0.5021\n",
            "Epoch [9773/20000], Training Loss: 0.4577\n",
            "Epoch [9774/20000], Training Loss: 0.5148\n",
            "Epoch [9775/20000], Training Loss: 0.4913\n",
            "Epoch [9776/20000], Training Loss: 0.4776\n",
            "Epoch [9777/20000], Training Loss: 0.4641\n",
            "Epoch [9778/20000], Training Loss: 0.4745\n",
            "Epoch [9779/20000], Training Loss: 0.4900\n",
            "Epoch [9780/20000], Training Loss: 0.5286\n",
            "Epoch [9781/20000], Training Loss: 0.4809\n",
            "Epoch [9782/20000], Training Loss: 0.5245\n",
            "Epoch [9783/20000], Training Loss: 0.5235\n",
            "Epoch [9784/20000], Training Loss: 0.4493\n",
            "Epoch [9785/20000], Training Loss: 0.4833\n",
            "Epoch [9786/20000], Training Loss: 0.5193\n",
            "Epoch [9787/20000], Training Loss: 0.4615\n",
            "Epoch [9788/20000], Training Loss: 0.4540\n",
            "Epoch [9789/20000], Training Loss: 0.4753\n",
            "Epoch [9790/20000], Training Loss: 0.4403\n",
            "Epoch [9791/20000], Training Loss: 0.4928\n",
            "Epoch [9792/20000], Training Loss: 0.4893\n",
            "Epoch [9793/20000], Training Loss: 0.4955\n",
            "Epoch [9794/20000], Training Loss: 0.4965\n",
            "Epoch [9795/20000], Training Loss: 0.5203\n",
            "Epoch [9796/20000], Training Loss: 0.5327\n",
            "Epoch [9797/20000], Training Loss: 0.4547\n",
            "Epoch [9798/20000], Training Loss: 0.4644\n",
            "Epoch [9799/20000], Training Loss: 0.5350\n",
            "Epoch [9800/20000], Training Loss: 0.4888\n",
            "Epoch [9801/20000], Training Loss: 0.4606\n",
            "Epoch [9802/20000], Training Loss: 0.4957\n",
            "Epoch [9803/20000], Training Loss: 0.4659\n",
            "Epoch [9804/20000], Training Loss: 0.4739\n",
            "Epoch [9805/20000], Training Loss: 0.4619\n",
            "Epoch [9806/20000], Training Loss: 0.4598\n",
            "Epoch [9807/20000], Training Loss: 0.4863\n",
            "Epoch [9808/20000], Training Loss: 0.4569\n",
            "Epoch [9809/20000], Training Loss: 0.5335\n",
            "Epoch [9810/20000], Training Loss: 0.4897\n",
            "Epoch [9811/20000], Training Loss: 0.4787\n",
            "Epoch [9812/20000], Training Loss: 0.4566\n",
            "Epoch [9813/20000], Training Loss: 0.4902\n",
            "Epoch [9814/20000], Training Loss: 0.5193\n",
            "Epoch [9815/20000], Training Loss: 0.4484\n",
            "Epoch [9816/20000], Training Loss: 0.4836\n",
            "Epoch [9817/20000], Training Loss: 0.4852\n",
            "Epoch [9818/20000], Training Loss: 0.4574\n",
            "Epoch [9819/20000], Training Loss: 0.4744\n",
            "Epoch [9820/20000], Training Loss: 0.4667\n",
            "Epoch [9821/20000], Training Loss: 0.4934\n",
            "Epoch [9822/20000], Training Loss: 0.4825\n",
            "Epoch [9823/20000], Training Loss: 0.4570\n",
            "Epoch [9824/20000], Training Loss: 0.4830\n",
            "Epoch [9825/20000], Training Loss: 0.5057\n",
            "Epoch [9826/20000], Training Loss: 0.4988\n",
            "Epoch [9827/20000], Training Loss: 0.4919\n",
            "Epoch [9828/20000], Training Loss: 0.4604\n",
            "Epoch [9829/20000], Training Loss: 0.5347\n",
            "Epoch [9830/20000], Training Loss: 0.4695\n",
            "Epoch [9831/20000], Training Loss: 0.4476\n",
            "Epoch [9832/20000], Training Loss: 0.5549\n",
            "Epoch [9833/20000], Training Loss: 0.4785\n",
            "Epoch [9834/20000], Training Loss: 0.4785\n",
            "Epoch [9835/20000], Training Loss: 0.4703\n",
            "Epoch [9836/20000], Training Loss: 0.5024\n",
            "Epoch [9837/20000], Training Loss: 0.5105\n",
            "Epoch [9838/20000], Training Loss: 0.4389\n",
            "Epoch [9839/20000], Training Loss: 0.5053\n",
            "Epoch [9840/20000], Training Loss: 0.4935\n",
            "Epoch [9841/20000], Training Loss: 0.4812\n",
            "Epoch [9842/20000], Training Loss: 0.4639\n",
            "Epoch [9843/20000], Training Loss: 0.5062\n",
            "Epoch [9844/20000], Training Loss: 0.5073\n",
            "Epoch [9845/20000], Training Loss: 0.5031\n",
            "Epoch [9846/20000], Training Loss: 0.4900\n",
            "Epoch [9847/20000], Training Loss: 0.4975\n",
            "Epoch [9848/20000], Training Loss: 0.4964\n",
            "Epoch [9849/20000], Training Loss: 0.5152\n",
            "Epoch [9850/20000], Training Loss: 0.4922\n",
            "Epoch [9851/20000], Training Loss: 0.4893\n",
            "Epoch [9852/20000], Training Loss: 0.4337\n",
            "Epoch [9853/20000], Training Loss: 0.4423\n",
            "Epoch [9854/20000], Training Loss: 0.4863\n",
            "Epoch [9855/20000], Training Loss: 0.5476\n",
            "Epoch [9856/20000], Training Loss: 0.4463\n",
            "Epoch [9857/20000], Training Loss: 0.4604\n",
            "Epoch [9858/20000], Training Loss: 0.4809\n",
            "Epoch [9859/20000], Training Loss: 0.5211\n",
            "Epoch [9860/20000], Training Loss: 0.4547\n",
            "Epoch [9861/20000], Training Loss: 0.5164\n",
            "Epoch [9862/20000], Training Loss: 0.4864\n",
            "Epoch [9863/20000], Training Loss: 0.5159\n",
            "Epoch [9864/20000], Training Loss: 0.5167\n",
            "Epoch [9865/20000], Training Loss: 0.4849\n",
            "Epoch [9866/20000], Training Loss: 0.4993\n",
            "Epoch [9867/20000], Training Loss: 0.4652\n",
            "Epoch [9868/20000], Training Loss: 0.4698\n",
            "Epoch [9869/20000], Training Loss: 0.4932\n",
            "Epoch [9870/20000], Training Loss: 0.4900\n",
            "Epoch [9871/20000], Training Loss: 0.4823\n",
            "Epoch [9872/20000], Training Loss: 0.4953\n",
            "Epoch [9873/20000], Training Loss: 0.4745\n",
            "Epoch [9874/20000], Training Loss: 0.5222\n",
            "Epoch [9875/20000], Training Loss: 0.4776\n",
            "Epoch [9876/20000], Training Loss: 0.4675\n",
            "Epoch [9877/20000], Training Loss: 0.4840\n",
            "Epoch [9878/20000], Training Loss: 0.4823\n",
            "Epoch [9879/20000], Training Loss: 0.5042\n",
            "Epoch [9880/20000], Training Loss: 0.4788\n",
            "Epoch [9881/20000], Training Loss: 0.4620\n",
            "Epoch [9882/20000], Training Loss: 0.4756\n",
            "Epoch [9883/20000], Training Loss: 0.5179\n",
            "Epoch [9884/20000], Training Loss: 0.5398\n",
            "Epoch [9885/20000], Training Loss: 0.5103\n",
            "Epoch [9886/20000], Training Loss: 0.4905\n",
            "Epoch [9887/20000], Training Loss: 0.4578\n",
            "Epoch [9888/20000], Training Loss: 0.4572\n",
            "Epoch [9889/20000], Training Loss: 0.4912\n",
            "Epoch [9890/20000], Training Loss: 0.5077\n",
            "Epoch [9891/20000], Training Loss: 0.5396\n",
            "Epoch [9892/20000], Training Loss: 0.4967\n",
            "Epoch [9893/20000], Training Loss: 0.5059\n",
            "Epoch [9894/20000], Training Loss: 0.5188\n",
            "Epoch [9895/20000], Training Loss: 0.5122\n",
            "Epoch [9896/20000], Training Loss: 0.4573\n",
            "Epoch [9897/20000], Training Loss: 0.4553\n",
            "Epoch [9898/20000], Training Loss: 0.4820\n",
            "Epoch [9899/20000], Training Loss: 0.4853\n",
            "Epoch [9900/20000], Training Loss: 0.4786\n",
            "Epoch [9901/20000], Training Loss: 0.4861\n",
            "Epoch [9902/20000], Training Loss: 0.4654\n",
            "Epoch [9903/20000], Training Loss: 0.5167\n",
            "Epoch [9904/20000], Training Loss: 0.4989\n",
            "Epoch [9905/20000], Training Loss: 0.5496\n",
            "Epoch [9906/20000], Training Loss: 0.4847\n",
            "Epoch [9907/20000], Training Loss: 0.4782\n",
            "Epoch [9908/20000], Training Loss: 0.5015\n",
            "Epoch [9909/20000], Training Loss: 0.5081\n",
            "Epoch [9910/20000], Training Loss: 0.4369\n",
            "Epoch [9911/20000], Training Loss: 0.4811\n",
            "Epoch [9912/20000], Training Loss: 0.4771\n",
            "Epoch [9913/20000], Training Loss: 0.5440\n",
            "Epoch [9914/20000], Training Loss: 0.5099\n",
            "Epoch [9915/20000], Training Loss: 0.4706\n",
            "Epoch [9916/20000], Training Loss: 0.4552\n",
            "Epoch [9917/20000], Training Loss: 0.5068\n",
            "Epoch [9918/20000], Training Loss: 0.5021\n",
            "Epoch [9919/20000], Training Loss: 0.4840\n",
            "Epoch [9920/20000], Training Loss: 0.5371\n",
            "Epoch [9921/20000], Training Loss: 0.4596\n",
            "Epoch [9922/20000], Training Loss: 0.4524\n",
            "Epoch [9923/20000], Training Loss: 0.5135\n",
            "Epoch [9924/20000], Training Loss: 0.4836\n",
            "Epoch [9925/20000], Training Loss: 0.4936\n",
            "Epoch [9926/20000], Training Loss: 0.5013\n",
            "Epoch [9927/20000], Training Loss: 0.4917\n",
            "Epoch [9928/20000], Training Loss: 0.5044\n",
            "Epoch [9929/20000], Training Loss: 0.5045\n",
            "Epoch [9930/20000], Training Loss: 0.4585\n",
            "Epoch [9931/20000], Training Loss: 0.4729\n",
            "Epoch [9932/20000], Training Loss: 0.5132\n",
            "Epoch [9933/20000], Training Loss: 0.4612\n",
            "Epoch [9934/20000], Training Loss: 0.4784\n",
            "Epoch [9935/20000], Training Loss: 0.4480\n",
            "Epoch [9936/20000], Training Loss: 0.5094\n",
            "Epoch [9937/20000], Training Loss: 0.4572\n",
            "Epoch [9938/20000], Training Loss: 0.5036\n",
            "Epoch [9939/20000], Training Loss: 0.4776\n",
            "Epoch [9940/20000], Training Loss: 0.4933\n",
            "Epoch [9941/20000], Training Loss: 0.5157\n",
            "Epoch [9942/20000], Training Loss: 0.4929\n",
            "Epoch [9943/20000], Training Loss: 0.4846\n",
            "Epoch [9944/20000], Training Loss: 0.5003\n",
            "Epoch [9945/20000], Training Loss: 0.4755\n",
            "Epoch [9946/20000], Training Loss: 0.4834\n",
            "Epoch [9947/20000], Training Loss: 0.4629\n",
            "Epoch [9948/20000], Training Loss: 0.4615\n",
            "Epoch [9949/20000], Training Loss: 0.4949\n",
            "Epoch [9950/20000], Training Loss: 0.4733\n",
            "Epoch [9951/20000], Training Loss: 0.4720\n",
            "Epoch [9952/20000], Training Loss: 0.5419\n",
            "Epoch [9953/20000], Training Loss: 0.4721\n",
            "Epoch [9954/20000], Training Loss: 0.4569\n",
            "Epoch [9955/20000], Training Loss: 0.4963\n",
            "Epoch [9956/20000], Training Loss: 0.4912\n",
            "Epoch [9957/20000], Training Loss: 0.4766\n",
            "Epoch [9958/20000], Training Loss: 0.4408\n",
            "Epoch [9959/20000], Training Loss: 0.4443\n",
            "Epoch [9960/20000], Training Loss: 0.4920\n",
            "Epoch [9961/20000], Training Loss: 0.4482\n",
            "Epoch [9962/20000], Training Loss: 0.5036\n",
            "Epoch [9963/20000], Training Loss: 0.5227\n",
            "Epoch [9964/20000], Training Loss: 0.5045\n",
            "Epoch [9965/20000], Training Loss: 0.5431\n",
            "Epoch [9966/20000], Training Loss: 0.4629\n",
            "Epoch [9967/20000], Training Loss: 0.4582\n",
            "Epoch [9968/20000], Training Loss: 0.4851\n",
            "Epoch [9969/20000], Training Loss: 0.4694\n",
            "Epoch [9970/20000], Training Loss: 0.4863\n",
            "Epoch [9971/20000], Training Loss: 0.5234\n",
            "Epoch [9972/20000], Training Loss: 0.4558\n",
            "Epoch [9973/20000], Training Loss: 0.4849\n",
            "Epoch [9974/20000], Training Loss: 0.5016\n",
            "Epoch [9975/20000], Training Loss: 0.4476\n",
            "Epoch [9976/20000], Training Loss: 0.4861\n",
            "Epoch [9977/20000], Training Loss: 0.4452\n",
            "Epoch [9978/20000], Training Loss: 0.4666\n",
            "Epoch [9979/20000], Training Loss: 0.4762\n",
            "Epoch [9980/20000], Training Loss: 0.4755\n",
            "Epoch [9981/20000], Training Loss: 0.4882\n",
            "Epoch [9982/20000], Training Loss: 0.4815\n",
            "Epoch [9983/20000], Training Loss: 0.5021\n",
            "Epoch [9984/20000], Training Loss: 0.5102\n",
            "Epoch [9985/20000], Training Loss: 0.4789\n",
            "Epoch [9986/20000], Training Loss: 0.5209\n",
            "Epoch [9987/20000], Training Loss: 0.5047\n",
            "Epoch [9988/20000], Training Loss: 0.5279\n",
            "Epoch [9989/20000], Training Loss: 0.4686\n",
            "Epoch [9990/20000], Training Loss: 0.4814\n",
            "Epoch [9991/20000], Training Loss: 0.4931\n",
            "Epoch [9992/20000], Training Loss: 0.4851\n",
            "Epoch [9993/20000], Training Loss: 0.4909\n",
            "Epoch [9994/20000], Training Loss: 0.5184\n",
            "Epoch [9995/20000], Training Loss: 0.4809\n",
            "Epoch [9996/20000], Training Loss: 0.4927\n",
            "Epoch [9997/20000], Training Loss: 0.5068\n",
            "Epoch [9998/20000], Training Loss: 0.5127\n",
            "Epoch [9999/20000], Training Loss: 0.4803\n",
            "Epoch [10000/20000], Training Loss: 0.4653\n",
            "Epoch [10001/20000], Training Loss: 0.4692\n",
            "Epoch [10002/20000], Training Loss: 0.5033\n",
            "Epoch [10003/20000], Training Loss: 0.4692\n",
            "Epoch [10004/20000], Training Loss: 0.4700\n",
            "Epoch [10005/20000], Training Loss: 0.4783\n",
            "Epoch [10006/20000], Training Loss: 0.4865\n",
            "Epoch [10007/20000], Training Loss: 0.5108\n",
            "Epoch [10008/20000], Training Loss: 0.4953\n",
            "Epoch [10009/20000], Training Loss: 0.4816\n",
            "Epoch [10010/20000], Training Loss: 0.4756\n",
            "Epoch [10011/20000], Training Loss: 0.4644\n",
            "Epoch [10012/20000], Training Loss: 0.5057\n",
            "Epoch [10013/20000], Training Loss: 0.5111\n",
            "Epoch [10014/20000], Training Loss: 0.4863\n",
            "Epoch [10015/20000], Training Loss: 0.4860\n",
            "Epoch [10016/20000], Training Loss: 0.5304\n",
            "Epoch [10017/20000], Training Loss: 0.4977\n",
            "Epoch [10018/20000], Training Loss: 0.4480\n",
            "Epoch [10019/20000], Training Loss: 0.4665\n",
            "Epoch [10020/20000], Training Loss: 0.5106\n",
            "Epoch [10021/20000], Training Loss: 0.5100\n",
            "Epoch [10022/20000], Training Loss: 0.5324\n",
            "Epoch [10023/20000], Training Loss: 0.4914\n",
            "Epoch [10024/20000], Training Loss: 0.5290\n",
            "Epoch [10025/20000], Training Loss: 0.4853\n",
            "Epoch [10026/20000], Training Loss: 0.4483\n",
            "Epoch [10027/20000], Training Loss: 0.4476\n",
            "Epoch [10028/20000], Training Loss: 0.4562\n",
            "Epoch [10029/20000], Training Loss: 0.4689\n",
            "Epoch [10030/20000], Training Loss: 0.5382\n",
            "Epoch [10031/20000], Training Loss: 0.5071\n",
            "Epoch [10032/20000], Training Loss: 0.4954\n",
            "Epoch [10033/20000], Training Loss: 0.4953\n",
            "Epoch [10034/20000], Training Loss: 0.5452\n",
            "Epoch [10035/20000], Training Loss: 0.4998\n",
            "Epoch [10036/20000], Training Loss: 0.4764\n",
            "Epoch [10037/20000], Training Loss: 0.5054\n",
            "Epoch [10038/20000], Training Loss: 0.5181\n",
            "Epoch [10039/20000], Training Loss: 0.5130\n",
            "Epoch [10040/20000], Training Loss: 0.4952\n",
            "Epoch [10041/20000], Training Loss: 0.4685\n",
            "Epoch [10042/20000], Training Loss: 0.5057\n",
            "Epoch [10043/20000], Training Loss: 0.4516\n",
            "Epoch [10044/20000], Training Loss: 0.4508\n",
            "Epoch [10045/20000], Training Loss: 0.4863\n",
            "Epoch [10046/20000], Training Loss: 0.4703\n",
            "Epoch [10047/20000], Training Loss: 0.5067\n",
            "Epoch [10048/20000], Training Loss: 0.5334\n",
            "Epoch [10049/20000], Training Loss: 0.5066\n",
            "Epoch [10050/20000], Training Loss: 0.4904\n",
            "Epoch [10051/20000], Training Loss: 0.4777\n",
            "Epoch [10052/20000], Training Loss: 0.4386\n",
            "Epoch [10053/20000], Training Loss: 0.4913\n",
            "Epoch [10054/20000], Training Loss: 0.4772\n",
            "Epoch [10055/20000], Training Loss: 0.4830\n",
            "Epoch [10056/20000], Training Loss: 0.4842\n",
            "Epoch [10057/20000], Training Loss: 0.5224\n",
            "Epoch [10058/20000], Training Loss: 0.4593\n",
            "Epoch [10059/20000], Training Loss: 0.5233\n",
            "Epoch [10060/20000], Training Loss: 0.5286\n",
            "Epoch [10061/20000], Training Loss: 0.4405\n",
            "Epoch [10062/20000], Training Loss: 0.4942\n",
            "Epoch [10063/20000], Training Loss: 0.5476\n",
            "Epoch [10064/20000], Training Loss: 0.5357\n",
            "Epoch [10065/20000], Training Loss: 0.4522\n",
            "Epoch [10066/20000], Training Loss: 0.4636\n",
            "Epoch [10067/20000], Training Loss: 0.4902\n",
            "Epoch [10068/20000], Training Loss: 0.4893\n",
            "Epoch [10069/20000], Training Loss: 0.4911\n",
            "Epoch [10070/20000], Training Loss: 0.4751\n",
            "Epoch [10071/20000], Training Loss: 0.4926\n",
            "Epoch [10072/20000], Training Loss: 0.4893\n",
            "Epoch [10073/20000], Training Loss: 0.4675\n",
            "Epoch [10074/20000], Training Loss: 0.4956\n",
            "Epoch [10075/20000], Training Loss: 0.4794\n",
            "Epoch [10076/20000], Training Loss: 0.5173\n",
            "Epoch [10077/20000], Training Loss: 0.5378\n",
            "Epoch [10078/20000], Training Loss: 0.4505\n",
            "Epoch [10079/20000], Training Loss: 0.4726\n",
            "Epoch [10080/20000], Training Loss: 0.4578\n",
            "Epoch [10081/20000], Training Loss: 0.4406\n",
            "Epoch [10082/20000], Training Loss: 0.4879\n",
            "Epoch [10083/20000], Training Loss: 0.5162\n",
            "Epoch [10084/20000], Training Loss: 0.4893\n",
            "Epoch [10085/20000], Training Loss: 0.4977\n",
            "Epoch [10086/20000], Training Loss: 0.4585\n",
            "Epoch [10087/20000], Training Loss: 0.4701\n",
            "Epoch [10088/20000], Training Loss: 0.4664\n",
            "Epoch [10089/20000], Training Loss: 0.4545\n",
            "Epoch [10090/20000], Training Loss: 0.4646\n",
            "Epoch [10091/20000], Training Loss: 0.4439\n",
            "Epoch [10092/20000], Training Loss: 0.5240\n",
            "Epoch [10093/20000], Training Loss: 0.4503\n",
            "Epoch [10094/20000], Training Loss: 0.4929\n",
            "Epoch [10095/20000], Training Loss: 0.5225\n",
            "Epoch [10096/20000], Training Loss: 0.4779\n",
            "Epoch [10097/20000], Training Loss: 0.5057\n",
            "Epoch [10098/20000], Training Loss: 0.4467\n",
            "Epoch [10099/20000], Training Loss: 0.4606\n",
            "Epoch [10100/20000], Training Loss: 0.4965\n",
            "Epoch [10101/20000], Training Loss: 0.4725\n",
            "Epoch [10102/20000], Training Loss: 0.5421\n",
            "Epoch [10103/20000], Training Loss: 0.4821\n",
            "Epoch [10104/20000], Training Loss: 0.5042\n",
            "Epoch [10105/20000], Training Loss: 0.4953\n",
            "Epoch [10106/20000], Training Loss: 0.4884\n",
            "Epoch [10107/20000], Training Loss: 0.4644\n",
            "Epoch [10108/20000], Training Loss: 0.5224\n",
            "Epoch [10109/20000], Training Loss: 0.5141\n",
            "Epoch [10110/20000], Training Loss: 0.4536\n",
            "Epoch [10111/20000], Training Loss: 0.5158\n",
            "Epoch [10112/20000], Training Loss: 0.4754\n",
            "Epoch [10113/20000], Training Loss: 0.5529\n",
            "Epoch [10114/20000], Training Loss: 0.4770\n",
            "Epoch [10115/20000], Training Loss: 0.4985\n",
            "Epoch [10116/20000], Training Loss: 0.5028\n",
            "Epoch [10117/20000], Training Loss: 0.5020\n",
            "Epoch [10118/20000], Training Loss: 0.4839\n",
            "Epoch [10119/20000], Training Loss: 0.5123\n",
            "Epoch [10120/20000], Training Loss: 0.5228\n",
            "Epoch [10121/20000], Training Loss: 0.4795\n",
            "Epoch [10122/20000], Training Loss: 0.4873\n",
            "Epoch [10123/20000], Training Loss: 0.4661\n",
            "Epoch [10124/20000], Training Loss: 0.5268\n",
            "Epoch [10125/20000], Training Loss: 0.4865\n",
            "Epoch [10126/20000], Training Loss: 0.4827\n",
            "Epoch [10127/20000], Training Loss: 0.4642\n",
            "Epoch [10128/20000], Training Loss: 0.4656\n",
            "Epoch [10129/20000], Training Loss: 0.5232\n",
            "Epoch [10130/20000], Training Loss: 0.4576\n",
            "Epoch [10131/20000], Training Loss: 0.4677\n",
            "Epoch [10132/20000], Training Loss: 0.4808\n",
            "Epoch [10133/20000], Training Loss: 0.4868\n",
            "Epoch [10134/20000], Training Loss: 0.4944\n",
            "Epoch [10135/20000], Training Loss: 0.5221\n",
            "Epoch [10136/20000], Training Loss: 0.4578\n",
            "Epoch [10137/20000], Training Loss: 0.5172\n",
            "Epoch [10138/20000], Training Loss: 0.5169\n",
            "Epoch [10139/20000], Training Loss: 0.5182\n",
            "Epoch [10140/20000], Training Loss: 0.4743\n",
            "Epoch [10141/20000], Training Loss: 0.4424\n",
            "Epoch [10142/20000], Training Loss: 0.4484\n",
            "Epoch [10143/20000], Training Loss: 0.4885\n",
            "Epoch [10144/20000], Training Loss: 0.5095\n",
            "Epoch [10145/20000], Training Loss: 0.5516\n",
            "Epoch [10146/20000], Training Loss: 0.4369\n",
            "Epoch [10147/20000], Training Loss: 0.5013\n",
            "Epoch [10148/20000], Training Loss: 0.5237\n",
            "Epoch [10149/20000], Training Loss: 0.4841\n",
            "Epoch [10150/20000], Training Loss: 0.5150\n",
            "Epoch [10151/20000], Training Loss: 0.4748\n",
            "Epoch [10152/20000], Training Loss: 0.4578\n",
            "Epoch [10153/20000], Training Loss: 0.5020\n",
            "Epoch [10154/20000], Training Loss: 0.4470\n",
            "Epoch [10155/20000], Training Loss: 0.5357\n",
            "Epoch [10156/20000], Training Loss: 0.5149\n",
            "Epoch [10157/20000], Training Loss: 0.4882\n",
            "Epoch [10158/20000], Training Loss: 0.5059\n",
            "Epoch [10159/20000], Training Loss: 0.5143\n",
            "Epoch [10160/20000], Training Loss: 0.4752\n",
            "Epoch [10161/20000], Training Loss: 0.5118\n",
            "Epoch [10162/20000], Training Loss: 0.5152\n",
            "Epoch [10163/20000], Training Loss: 0.4665\n",
            "Epoch [10164/20000], Training Loss: 0.4379\n",
            "Epoch [10165/20000], Training Loss: 0.5022\n",
            "Epoch [10166/20000], Training Loss: 0.4822\n",
            "Epoch [10167/20000], Training Loss: 0.4731\n",
            "Epoch [10168/20000], Training Loss: 0.4754\n",
            "Epoch [10169/20000], Training Loss: 0.5453\n",
            "Epoch [10170/20000], Training Loss: 0.4287\n",
            "Epoch [10171/20000], Training Loss: 0.5020\n",
            "Epoch [10172/20000], Training Loss: 0.4833\n",
            "Epoch [10173/20000], Training Loss: 0.4911\n",
            "Epoch [10174/20000], Training Loss: 0.5272\n",
            "Epoch [10175/20000], Training Loss: 0.4985\n",
            "Epoch [10176/20000], Training Loss: 0.4982\n",
            "Epoch [10177/20000], Training Loss: 0.4472\n",
            "Epoch [10178/20000], Training Loss: 0.5033\n",
            "Epoch [10179/20000], Training Loss: 0.5468\n",
            "Epoch [10180/20000], Training Loss: 0.5122\n",
            "Epoch [10181/20000], Training Loss: 0.5303\n",
            "Epoch [10182/20000], Training Loss: 0.5177\n",
            "Epoch [10183/20000], Training Loss: 0.4705\n",
            "Epoch [10184/20000], Training Loss: 0.4968\n",
            "Epoch [10185/20000], Training Loss: 0.4825\n",
            "Epoch [10186/20000], Training Loss: 0.4868\n",
            "Epoch [10187/20000], Training Loss: 0.5157\n",
            "Epoch [10188/20000], Training Loss: 0.5061\n",
            "Epoch [10189/20000], Training Loss: 0.5155\n",
            "Epoch [10190/20000], Training Loss: 0.4824\n",
            "Epoch [10191/20000], Training Loss: 0.4624\n",
            "Epoch [10192/20000], Training Loss: 0.4876\n",
            "Epoch [10193/20000], Training Loss: 0.5284\n",
            "Epoch [10194/20000], Training Loss: 0.4698\n",
            "Epoch [10195/20000], Training Loss: 0.4812\n",
            "Epoch [10196/20000], Training Loss: 0.4901\n",
            "Epoch [10197/20000], Training Loss: 0.4806\n",
            "Epoch [10198/20000], Training Loss: 0.4864\n",
            "Epoch [10199/20000], Training Loss: 0.5242\n",
            "Epoch [10200/20000], Training Loss: 0.5002\n",
            "Epoch [10201/20000], Training Loss: 0.4535\n",
            "Epoch [10202/20000], Training Loss: 0.4823\n",
            "Epoch [10203/20000], Training Loss: 0.4931\n",
            "Epoch [10204/20000], Training Loss: 0.5042\n",
            "Epoch [10205/20000], Training Loss: 0.5153\n",
            "Epoch [10206/20000], Training Loss: 0.4889\n",
            "Epoch [10207/20000], Training Loss: 0.4994\n",
            "Epoch [10208/20000], Training Loss: 0.5113\n",
            "Epoch [10209/20000], Training Loss: 0.4511\n",
            "Epoch [10210/20000], Training Loss: 0.4933\n",
            "Epoch [10211/20000], Training Loss: 0.4823\n",
            "Epoch [10212/20000], Training Loss: 0.4938\n",
            "Epoch [10213/20000], Training Loss: 0.5360\n",
            "Epoch [10214/20000], Training Loss: 0.4872\n",
            "Epoch [10215/20000], Training Loss: 0.5363\n",
            "Epoch [10216/20000], Training Loss: 0.4455\n",
            "Epoch [10217/20000], Training Loss: 0.4709\n",
            "Epoch [10218/20000], Training Loss: 0.4883\n",
            "Epoch [10219/20000], Training Loss: 0.4637\n",
            "Epoch [10220/20000], Training Loss: 0.4618\n",
            "Epoch [10221/20000], Training Loss: 0.4920\n",
            "Epoch [10222/20000], Training Loss: 0.4683\n",
            "Epoch [10223/20000], Training Loss: 0.4952\n",
            "Epoch [10224/20000], Training Loss: 0.4754\n",
            "Epoch [10225/20000], Training Loss: 0.4559\n",
            "Epoch [10226/20000], Training Loss: 0.4644\n",
            "Epoch [10227/20000], Training Loss: 0.5021\n",
            "Epoch [10228/20000], Training Loss: 0.5024\n",
            "Epoch [10229/20000], Training Loss: 0.5193\n",
            "Epoch [10230/20000], Training Loss: 0.5010\n",
            "Epoch [10231/20000], Training Loss: 0.4514\n",
            "Epoch [10232/20000], Training Loss: 0.4863\n",
            "Epoch [10233/20000], Training Loss: 0.5094\n",
            "Epoch [10234/20000], Training Loss: 0.4671\n",
            "Epoch [10235/20000], Training Loss: 0.5155\n",
            "Epoch [10236/20000], Training Loss: 0.4745\n",
            "Epoch [10237/20000], Training Loss: 0.4602\n",
            "Epoch [10238/20000], Training Loss: 0.4881\n",
            "Epoch [10239/20000], Training Loss: 0.4582\n",
            "Epoch [10240/20000], Training Loss: 0.5084\n",
            "Epoch [10241/20000], Training Loss: 0.4678\n",
            "Epoch [10242/20000], Training Loss: 0.4533\n",
            "Epoch [10243/20000], Training Loss: 0.4525\n",
            "Epoch [10244/20000], Training Loss: 0.4769\n",
            "Epoch [10245/20000], Training Loss: 0.5086\n",
            "Epoch [10246/20000], Training Loss: 0.4764\n",
            "Epoch [10247/20000], Training Loss: 0.4832\n",
            "Epoch [10248/20000], Training Loss: 0.4816\n",
            "Epoch [10249/20000], Training Loss: 0.4729\n",
            "Epoch [10250/20000], Training Loss: 0.5191\n",
            "Epoch [10251/20000], Training Loss: 0.4351\n",
            "Epoch [10252/20000], Training Loss: 0.4569\n",
            "Epoch [10253/20000], Training Loss: 0.5013\n",
            "Epoch [10254/20000], Training Loss: 0.4681\n",
            "Epoch [10255/20000], Training Loss: 0.4590\n",
            "Epoch [10256/20000], Training Loss: 0.4628\n",
            "Epoch [10257/20000], Training Loss: 0.5136\n",
            "Epoch [10258/20000], Training Loss: 0.4659\n",
            "Epoch [10259/20000], Training Loss: 0.4580\n",
            "Epoch [10260/20000], Training Loss: 0.5056\n",
            "Epoch [10261/20000], Training Loss: 0.4601\n",
            "Epoch [10262/20000], Training Loss: 0.4690\n",
            "Epoch [10263/20000], Training Loss: 0.5176\n",
            "Epoch [10264/20000], Training Loss: 0.5038\n",
            "Epoch [10265/20000], Training Loss: 0.4923\n",
            "Epoch [10266/20000], Training Loss: 0.4664\n",
            "Epoch [10267/20000], Training Loss: 0.4755\n",
            "Epoch [10268/20000], Training Loss: 0.5031\n",
            "Epoch [10269/20000], Training Loss: 0.5023\n",
            "Epoch [10270/20000], Training Loss: 0.4557\n",
            "Epoch [10271/20000], Training Loss: 0.4846\n",
            "Epoch [10272/20000], Training Loss: 0.5081\n",
            "Epoch [10273/20000], Training Loss: 0.4570\n",
            "Epoch [10274/20000], Training Loss: 0.5160\n",
            "Epoch [10275/20000], Training Loss: 0.5046\n",
            "Epoch [10276/20000], Training Loss: 0.5106\n",
            "Epoch [10277/20000], Training Loss: 0.4560\n",
            "Epoch [10278/20000], Training Loss: 0.4543\n",
            "Epoch [10279/20000], Training Loss: 0.4889\n",
            "Epoch [10280/20000], Training Loss: 0.4424\n",
            "Epoch [10281/20000], Training Loss: 0.4467\n",
            "Epoch [10282/20000], Training Loss: 0.5238\n",
            "Epoch [10283/20000], Training Loss: 0.4943\n",
            "Epoch [10284/20000], Training Loss: 0.5106\n",
            "Epoch [10285/20000], Training Loss: 0.4689\n",
            "Epoch [10286/20000], Training Loss: 0.4827\n",
            "Epoch [10287/20000], Training Loss: 0.4700\n",
            "Epoch [10288/20000], Training Loss: 0.4507\n",
            "Epoch [10289/20000], Training Loss: 0.4519\n",
            "Epoch [10290/20000], Training Loss: 0.4596\n",
            "Epoch [10291/20000], Training Loss: 0.4978\n",
            "Epoch [10292/20000], Training Loss: 0.5037\n",
            "Epoch [10293/20000], Training Loss: 0.5483\n",
            "Epoch [10294/20000], Training Loss: 0.5196\n",
            "Epoch [10295/20000], Training Loss: 0.5033\n",
            "Epoch [10296/20000], Training Loss: 0.4685\n",
            "Epoch [10297/20000], Training Loss: 0.5144\n",
            "Epoch [10298/20000], Training Loss: 0.4633\n",
            "Epoch [10299/20000], Training Loss: 0.5117\n",
            "Epoch [10300/20000], Training Loss: 0.4582\n",
            "Epoch [10301/20000], Training Loss: 0.4363\n",
            "Epoch [10302/20000], Training Loss: 0.5061\n",
            "Epoch [10303/20000], Training Loss: 0.4963\n",
            "Epoch [10304/20000], Training Loss: 0.5121\n",
            "Epoch [10305/20000], Training Loss: 0.4562\n",
            "Epoch [10306/20000], Training Loss: 0.5191\n",
            "Epoch [10307/20000], Training Loss: 0.5173\n",
            "Epoch [10308/20000], Training Loss: 0.5286\n",
            "Epoch [10309/20000], Training Loss: 0.5103\n",
            "Epoch [10310/20000], Training Loss: 0.4795\n",
            "Epoch [10311/20000], Training Loss: 0.4897\n",
            "Epoch [10312/20000], Training Loss: 0.4983\n",
            "Epoch [10313/20000], Training Loss: 0.4844\n",
            "Epoch [10314/20000], Training Loss: 0.5015\n",
            "Epoch [10315/20000], Training Loss: 0.5383\n",
            "Epoch [10316/20000], Training Loss: 0.5238\n",
            "Epoch [10317/20000], Training Loss: 0.4714\n",
            "Epoch [10318/20000], Training Loss: 0.4932\n",
            "Epoch [10319/20000], Training Loss: 0.4530\n",
            "Epoch [10320/20000], Training Loss: 0.5033\n",
            "Epoch [10321/20000], Training Loss: 0.4378\n",
            "Epoch [10322/20000], Training Loss: 0.4928\n",
            "Epoch [10323/20000], Training Loss: 0.5126\n",
            "Epoch [10324/20000], Training Loss: 0.5066\n",
            "Epoch [10325/20000], Training Loss: 0.4781\n",
            "Epoch [10326/20000], Training Loss: 0.5403\n",
            "Epoch [10327/20000], Training Loss: 0.4922\n",
            "Epoch [10328/20000], Training Loss: 0.5024\n",
            "Epoch [10329/20000], Training Loss: 0.4997\n",
            "Epoch [10330/20000], Training Loss: 0.4331\n",
            "Epoch [10331/20000], Training Loss: 0.5538\n",
            "Epoch [10332/20000], Training Loss: 0.4617\n",
            "Epoch [10333/20000], Training Loss: 0.5173\n",
            "Epoch [10334/20000], Training Loss: 0.5068\n",
            "Epoch [10335/20000], Training Loss: 0.4680\n",
            "Epoch [10336/20000], Training Loss: 0.4621\n",
            "Epoch [10337/20000], Training Loss: 0.4878\n",
            "Epoch [10338/20000], Training Loss: 0.4803\n",
            "Epoch [10339/20000], Training Loss: 0.4785\n",
            "Epoch [10340/20000], Training Loss: 0.5315\n",
            "Epoch [10341/20000], Training Loss: 0.4764\n",
            "Epoch [10342/20000], Training Loss: 0.4634\n",
            "Epoch [10343/20000], Training Loss: 0.4571\n",
            "Epoch [10344/20000], Training Loss: 0.4666\n",
            "Epoch [10345/20000], Training Loss: 0.4607\n",
            "Epoch [10346/20000], Training Loss: 0.4980\n",
            "Epoch [10347/20000], Training Loss: 0.5283\n",
            "Epoch [10348/20000], Training Loss: 0.4719\n",
            "Epoch [10349/20000], Training Loss: 0.5029\n",
            "Epoch [10350/20000], Training Loss: 0.4883\n",
            "Epoch [10351/20000], Training Loss: 0.4859\n",
            "Epoch [10352/20000], Training Loss: 0.4616\n",
            "Epoch [10353/20000], Training Loss: 0.4975\n",
            "Epoch [10354/20000], Training Loss: 0.4770\n",
            "Epoch [10355/20000], Training Loss: 0.4572\n",
            "Epoch [10356/20000], Training Loss: 0.5186\n",
            "Epoch [10357/20000], Training Loss: 0.5005\n",
            "Epoch [10358/20000], Training Loss: 0.5461\n",
            "Epoch [10359/20000], Training Loss: 0.5217\n",
            "Epoch [10360/20000], Training Loss: 0.4735\n",
            "Epoch [10361/20000], Training Loss: 0.4744\n",
            "Epoch [10362/20000], Training Loss: 0.4745\n",
            "Epoch [10363/20000], Training Loss: 0.4737\n",
            "Epoch [10364/20000], Training Loss: 0.5525\n",
            "Epoch [10365/20000], Training Loss: 0.4958\n",
            "Epoch [10366/20000], Training Loss: 0.4722\n",
            "Epoch [10367/20000], Training Loss: 0.5034\n",
            "Epoch [10368/20000], Training Loss: 0.4628\n",
            "Epoch [10369/20000], Training Loss: 0.4906\n",
            "Epoch [10370/20000], Training Loss: 0.5173\n",
            "Epoch [10371/20000], Training Loss: 0.4498\n",
            "Epoch [10372/20000], Training Loss: 0.4922\n",
            "Epoch [10373/20000], Training Loss: 0.4811\n",
            "Epoch [10374/20000], Training Loss: 0.4722\n",
            "Epoch [10375/20000], Training Loss: 0.5130\n",
            "Epoch [10376/20000], Training Loss: 0.4911\n",
            "Epoch [10377/20000], Training Loss: 0.4580\n",
            "Epoch [10378/20000], Training Loss: 0.5080\n",
            "Epoch [10379/20000], Training Loss: 0.4814\n",
            "Epoch [10380/20000], Training Loss: 0.4657\n",
            "Epoch [10381/20000], Training Loss: 0.4440\n",
            "Epoch [10382/20000], Training Loss: 0.4677\n",
            "Epoch [10383/20000], Training Loss: 0.4716\n",
            "Epoch [10384/20000], Training Loss: 0.4813\n",
            "Epoch [10385/20000], Training Loss: 0.4888\n",
            "Epoch [10386/20000], Training Loss: 0.4860\n",
            "Epoch [10387/20000], Training Loss: 0.5021\n",
            "Epoch [10388/20000], Training Loss: 0.5285\n",
            "Epoch [10389/20000], Training Loss: 0.4858\n",
            "Epoch [10390/20000], Training Loss: 0.4705\n",
            "Epoch [10391/20000], Training Loss: 0.4863\n",
            "Epoch [10392/20000], Training Loss: 0.4491\n",
            "Epoch [10393/20000], Training Loss: 0.5119\n",
            "Epoch [10394/20000], Training Loss: 0.5014\n",
            "Epoch [10395/20000], Training Loss: 0.4800\n",
            "Epoch [10396/20000], Training Loss: 0.4489\n",
            "Epoch [10397/20000], Training Loss: 0.4987\n",
            "Epoch [10398/20000], Training Loss: 0.4953\n",
            "Epoch [10399/20000], Training Loss: 0.5092\n",
            "Epoch [10400/20000], Training Loss: 0.4571\n",
            "Epoch [10401/20000], Training Loss: 0.4640\n",
            "Epoch [10402/20000], Training Loss: 0.4916\n",
            "Epoch [10403/20000], Training Loss: 0.4769\n",
            "Epoch [10404/20000], Training Loss: 0.4858\n",
            "Epoch [10405/20000], Training Loss: 0.4719\n",
            "Epoch [10406/20000], Training Loss: 0.4810\n",
            "Epoch [10407/20000], Training Loss: 0.4448\n",
            "Epoch [10408/20000], Training Loss: 0.5158\n",
            "Epoch [10409/20000], Training Loss: 0.5111\n",
            "Epoch [10410/20000], Training Loss: 0.4713\n",
            "Epoch [10411/20000], Training Loss: 0.4790\n",
            "Epoch [10412/20000], Training Loss: 0.5021\n",
            "Epoch [10413/20000], Training Loss: 0.4760\n",
            "Epoch [10414/20000], Training Loss: 0.4754\n",
            "Epoch [10415/20000], Training Loss: 0.4746\n",
            "Epoch [10416/20000], Training Loss: 0.4746\n",
            "Epoch [10417/20000], Training Loss: 0.4891\n",
            "Epoch [10418/20000], Training Loss: 0.4674\n",
            "Epoch [10419/20000], Training Loss: 0.4769\n",
            "Epoch [10420/20000], Training Loss: 0.4851\n",
            "Epoch [10421/20000], Training Loss: 0.5088\n",
            "Epoch [10422/20000], Training Loss: 0.4840\n",
            "Epoch [10423/20000], Training Loss: 0.5022\n",
            "Epoch [10424/20000], Training Loss: 0.4919\n",
            "Epoch [10425/20000], Training Loss: 0.5163\n",
            "Epoch [10426/20000], Training Loss: 0.5000\n",
            "Epoch [10427/20000], Training Loss: 0.4878\n",
            "Epoch [10428/20000], Training Loss: 0.4963\n",
            "Epoch [10429/20000], Training Loss: 0.5027\n",
            "Epoch [10430/20000], Training Loss: 0.4864\n",
            "Epoch [10431/20000], Training Loss: 0.4887\n",
            "Epoch [10432/20000], Training Loss: 0.4853\n",
            "Epoch [10433/20000], Training Loss: 0.5487\n",
            "Epoch [10434/20000], Training Loss: 0.5253\n",
            "Epoch [10435/20000], Training Loss: 0.4925\n",
            "Epoch [10436/20000], Training Loss: 0.4706\n",
            "Epoch [10437/20000], Training Loss: 0.4559\n",
            "Epoch [10438/20000], Training Loss: 0.5228\n",
            "Epoch [10439/20000], Training Loss: 0.4964\n",
            "Epoch [10440/20000], Training Loss: 0.4742\n",
            "Epoch [10441/20000], Training Loss: 0.4970\n",
            "Epoch [10442/20000], Training Loss: 0.4827\n",
            "Epoch [10443/20000], Training Loss: 0.4530\n",
            "Epoch [10444/20000], Training Loss: 0.4783\n",
            "Epoch [10445/20000], Training Loss: 0.5099\n",
            "Epoch [10446/20000], Training Loss: 0.4345\n",
            "Epoch [10447/20000], Training Loss: 0.4668\n",
            "Epoch [10448/20000], Training Loss: 0.4595\n",
            "Epoch [10449/20000], Training Loss: 0.4626\n",
            "Epoch [10450/20000], Training Loss: 0.4708\n",
            "Epoch [10451/20000], Training Loss: 0.5117\n",
            "Epoch [10452/20000], Training Loss: 0.4741\n",
            "Epoch [10453/20000], Training Loss: 0.4824\n",
            "Epoch [10454/20000], Training Loss: 0.5080\n",
            "Epoch [10455/20000], Training Loss: 0.4275\n",
            "Epoch [10456/20000], Training Loss: 0.4945\n",
            "Epoch [10457/20000], Training Loss: 0.4707\n",
            "Epoch [10458/20000], Training Loss: 0.4417\n",
            "Epoch [10459/20000], Training Loss: 0.4642\n",
            "Epoch [10460/20000], Training Loss: 0.5333\n",
            "Epoch [10461/20000], Training Loss: 0.4620\n",
            "Epoch [10462/20000], Training Loss: 0.5396\n",
            "Epoch [10463/20000], Training Loss: 0.5065\n",
            "Epoch [10464/20000], Training Loss: 0.5014\n",
            "Epoch [10465/20000], Training Loss: 0.4550\n",
            "Epoch [10466/20000], Training Loss: 0.4523\n",
            "Epoch [10467/20000], Training Loss: 0.5226\n",
            "Epoch [10468/20000], Training Loss: 0.4676\n",
            "Epoch [10469/20000], Training Loss: 0.5056\n",
            "Epoch [10470/20000], Training Loss: 0.4941\n",
            "Epoch [10471/20000], Training Loss: 0.4871\n",
            "Epoch [10472/20000], Training Loss: 0.5185\n",
            "Epoch [10473/20000], Training Loss: 0.4997\n",
            "Epoch [10474/20000], Training Loss: 0.5133\n",
            "Epoch [10475/20000], Training Loss: 0.4934\n",
            "Epoch [10476/20000], Training Loss: 0.5050\n",
            "Epoch [10477/20000], Training Loss: 0.4957\n",
            "Epoch [10478/20000], Training Loss: 0.4957\n",
            "Epoch [10479/20000], Training Loss: 0.4754\n",
            "Epoch [10480/20000], Training Loss: 0.4454\n",
            "Epoch [10481/20000], Training Loss: 0.4880\n",
            "Epoch [10482/20000], Training Loss: 0.4886\n",
            "Epoch [10483/20000], Training Loss: 0.5020\n",
            "Epoch [10484/20000], Training Loss: 0.4711\n",
            "Epoch [10485/20000], Training Loss: 0.4692\n",
            "Epoch [10486/20000], Training Loss: 0.5195\n",
            "Epoch [10487/20000], Training Loss: 0.5244\n",
            "Epoch [10488/20000], Training Loss: 0.5197\n",
            "Epoch [10489/20000], Training Loss: 0.4830\n",
            "Epoch [10490/20000], Training Loss: 0.4650\n",
            "Epoch [10491/20000], Training Loss: 0.4809\n",
            "Epoch [10492/20000], Training Loss: 0.5293\n",
            "Epoch [10493/20000], Training Loss: 0.4748\n",
            "Epoch [10494/20000], Training Loss: 0.4864\n",
            "Epoch [10495/20000], Training Loss: 0.4629\n",
            "Epoch [10496/20000], Training Loss: 0.4790\n",
            "Epoch [10497/20000], Training Loss: 0.4786\n",
            "Epoch [10498/20000], Training Loss: 0.4842\n",
            "Epoch [10499/20000], Training Loss: 0.5173\n",
            "Epoch [10500/20000], Training Loss: 0.5019\n",
            "Epoch [10501/20000], Training Loss: 0.5048\n",
            "Epoch [10502/20000], Training Loss: 0.4718\n",
            "Epoch [10503/20000], Training Loss: 0.4972\n",
            "Epoch [10504/20000], Training Loss: 0.4453\n",
            "Epoch [10505/20000], Training Loss: 0.5003\n",
            "Epoch [10506/20000], Training Loss: 0.4936\n",
            "Epoch [10507/20000], Training Loss: 0.5250\n",
            "Epoch [10508/20000], Training Loss: 0.4869\n",
            "Epoch [10509/20000], Training Loss: 0.4565\n",
            "Epoch [10510/20000], Training Loss: 0.4976\n",
            "Epoch [10511/20000], Training Loss: 0.5107\n",
            "Epoch [10512/20000], Training Loss: 0.4852\n",
            "Epoch [10513/20000], Training Loss: 0.4473\n",
            "Epoch [10514/20000], Training Loss: 0.5143\n",
            "Epoch [10515/20000], Training Loss: 0.4705\n",
            "Epoch [10516/20000], Training Loss: 0.5098\n",
            "Epoch [10517/20000], Training Loss: 0.4884\n",
            "Epoch [10518/20000], Training Loss: 0.4961\n",
            "Epoch [10519/20000], Training Loss: 0.4895\n",
            "Epoch [10520/20000], Training Loss: 0.4891\n",
            "Epoch [10521/20000], Training Loss: 0.4827\n",
            "Epoch [10522/20000], Training Loss: 0.4688\n",
            "Epoch [10523/20000], Training Loss: 0.4829\n",
            "Epoch [10524/20000], Training Loss: 0.4777\n",
            "Epoch [10525/20000], Training Loss: 0.5163\n",
            "Epoch [10526/20000], Training Loss: 0.5089\n",
            "Epoch [10527/20000], Training Loss: 0.4683\n",
            "Epoch [10528/20000], Training Loss: 0.4709\n",
            "Epoch [10529/20000], Training Loss: 0.5251\n",
            "Epoch [10530/20000], Training Loss: 0.4674\n",
            "Epoch [10531/20000], Training Loss: 0.4740\n",
            "Epoch [10532/20000], Training Loss: 0.4599\n",
            "Epoch [10533/20000], Training Loss: 0.4726\n",
            "Epoch [10534/20000], Training Loss: 0.5180\n",
            "Epoch [10535/20000], Training Loss: 0.4776\n",
            "Epoch [10536/20000], Training Loss: 0.4709\n",
            "Epoch [10537/20000], Training Loss: 0.5092\n",
            "Epoch [10538/20000], Training Loss: 0.4379\n",
            "Epoch [10539/20000], Training Loss: 0.4551\n",
            "Epoch [10540/20000], Training Loss: 0.4506\n",
            "Epoch [10541/20000], Training Loss: 0.4688\n",
            "Epoch [10542/20000], Training Loss: 0.4869\n",
            "Epoch [10543/20000], Training Loss: 0.4894\n",
            "Epoch [10544/20000], Training Loss: 0.4719\n",
            "Epoch [10545/20000], Training Loss: 0.5472\n",
            "Epoch [10546/20000], Training Loss: 0.5210\n",
            "Epoch [10547/20000], Training Loss: 0.4685\n",
            "Epoch [10548/20000], Training Loss: 0.4422\n",
            "Epoch [10549/20000], Training Loss: 0.4950\n",
            "Epoch [10550/20000], Training Loss: 0.4692\n",
            "Epoch [10551/20000], Training Loss: 0.4956\n",
            "Epoch [10552/20000], Training Loss: 0.5059\n",
            "Epoch [10553/20000], Training Loss: 0.4505\n",
            "Epoch [10554/20000], Training Loss: 0.4968\n",
            "Epoch [10555/20000], Training Loss: 0.4998\n",
            "Epoch [10556/20000], Training Loss: 0.5020\n",
            "Epoch [10557/20000], Training Loss: 0.4306\n",
            "Epoch [10558/20000], Training Loss: 0.4585\n",
            "Epoch [10559/20000], Training Loss: 0.5013\n",
            "Epoch [10560/20000], Training Loss: 0.5053\n",
            "Epoch [10561/20000], Training Loss: 0.4535\n",
            "Epoch [10562/20000], Training Loss: 0.4627\n",
            "Epoch [10563/20000], Training Loss: 0.4875\n",
            "Epoch [10564/20000], Training Loss: 0.4603\n",
            "Epoch [10565/20000], Training Loss: 0.4922\n",
            "Epoch [10566/20000], Training Loss: 0.5316\n",
            "Epoch [10567/20000], Training Loss: 0.4731\n",
            "Epoch [10568/20000], Training Loss: 0.4560\n",
            "Epoch [10569/20000], Training Loss: 0.4970\n",
            "Epoch [10570/20000], Training Loss: 0.5087\n",
            "Epoch [10571/20000], Training Loss: 0.5257\n",
            "Epoch [10572/20000], Training Loss: 0.4544\n",
            "Epoch [10573/20000], Training Loss: 0.4876\n",
            "Epoch [10574/20000], Training Loss: 0.5153\n",
            "Epoch [10575/20000], Training Loss: 0.4707\n",
            "Epoch [10576/20000], Training Loss: 0.4250\n",
            "Epoch [10577/20000], Training Loss: 0.4527\n",
            "Epoch [10578/20000], Training Loss: 0.4785\n",
            "Epoch [10579/20000], Training Loss: 0.4762\n",
            "Epoch [10580/20000], Training Loss: 0.4602\n",
            "Epoch [10581/20000], Training Loss: 0.4699\n",
            "Epoch [10582/20000], Training Loss: 0.4608\n",
            "Epoch [10583/20000], Training Loss: 0.4756\n",
            "Epoch [10584/20000], Training Loss: 0.4633\n",
            "Epoch [10585/20000], Training Loss: 0.4448\n",
            "Epoch [10586/20000], Training Loss: 0.4519\n",
            "Epoch [10587/20000], Training Loss: 0.4546\n",
            "Epoch [10588/20000], Training Loss: 0.4850\n",
            "Epoch [10589/20000], Training Loss: 0.5107\n",
            "Epoch [10590/20000], Training Loss: 0.5127\n",
            "Epoch [10591/20000], Training Loss: 0.4956\n",
            "Epoch [10592/20000], Training Loss: 0.4733\n",
            "Epoch [10593/20000], Training Loss: 0.4599\n",
            "Epoch [10594/20000], Training Loss: 0.5147\n",
            "Epoch [10595/20000], Training Loss: 0.4837\n",
            "Epoch [10596/20000], Training Loss: 0.4643\n",
            "Epoch [10597/20000], Training Loss: 0.5195\n",
            "Epoch [10598/20000], Training Loss: 0.4650\n",
            "Epoch [10599/20000], Training Loss: 0.4876\n",
            "Epoch [10600/20000], Training Loss: 0.5056\n",
            "Epoch [10601/20000], Training Loss: 0.4848\n",
            "Epoch [10602/20000], Training Loss: 0.5019\n",
            "Epoch [10603/20000], Training Loss: 0.4763\n",
            "Epoch [10604/20000], Training Loss: 0.4696\n",
            "Epoch [10605/20000], Training Loss: 0.4973\n",
            "Epoch [10606/20000], Training Loss: 0.5359\n",
            "Epoch [10607/20000], Training Loss: 0.5174\n",
            "Epoch [10608/20000], Training Loss: 0.5290\n",
            "Epoch [10609/20000], Training Loss: 0.4802\n",
            "Epoch [10610/20000], Training Loss: 0.4309\n",
            "Epoch [10611/20000], Training Loss: 0.4444\n",
            "Epoch [10612/20000], Training Loss: 0.4727\n",
            "Epoch [10613/20000], Training Loss: 0.4482\n",
            "Epoch [10614/20000], Training Loss: 0.4598\n",
            "Epoch [10615/20000], Training Loss: 0.4793\n",
            "Epoch [10616/20000], Training Loss: 0.4529\n",
            "Epoch [10617/20000], Training Loss: 0.4717\n",
            "Epoch [10618/20000], Training Loss: 0.4667\n",
            "Epoch [10619/20000], Training Loss: 0.4890\n",
            "Epoch [10620/20000], Training Loss: 0.4406\n",
            "Epoch [10621/20000], Training Loss: 0.4693\n",
            "Epoch [10622/20000], Training Loss: 0.5081\n",
            "Epoch [10623/20000], Training Loss: 0.4773\n",
            "Epoch [10624/20000], Training Loss: 0.5444\n",
            "Epoch [10625/20000], Training Loss: 0.4742\n",
            "Epoch [10626/20000], Training Loss: 0.5271\n",
            "Epoch [10627/20000], Training Loss: 0.5189\n",
            "Epoch [10628/20000], Training Loss: 0.4550\n",
            "Epoch [10629/20000], Training Loss: 0.4688\n",
            "Epoch [10630/20000], Training Loss: 0.5172\n",
            "Epoch [10631/20000], Training Loss: 0.5063\n",
            "Epoch [10632/20000], Training Loss: 0.4706\n",
            "Epoch [10633/20000], Training Loss: 0.4854\n",
            "Epoch [10634/20000], Training Loss: 0.5031\n",
            "Epoch [10635/20000], Training Loss: 0.4358\n",
            "Epoch [10636/20000], Training Loss: 0.5000\n",
            "Epoch [10637/20000], Training Loss: 0.4903\n",
            "Epoch [10638/20000], Training Loss: 0.4977\n",
            "Epoch [10639/20000], Training Loss: 0.4776\n",
            "Epoch [10640/20000], Training Loss: 0.4724\n",
            "Epoch [10641/20000], Training Loss: 0.4747\n",
            "Epoch [10642/20000], Training Loss: 0.4360\n",
            "Epoch [10643/20000], Training Loss: 0.5014\n",
            "Epoch [10644/20000], Training Loss: 0.4563\n",
            "Epoch [10645/20000], Training Loss: 0.5311\n",
            "Epoch [10646/20000], Training Loss: 0.4580\n",
            "Epoch [10647/20000], Training Loss: 0.5143\n",
            "Epoch [10648/20000], Training Loss: 0.5379\n",
            "Epoch [10649/20000], Training Loss: 0.5320\n",
            "Epoch [10650/20000], Training Loss: 0.5148\n",
            "Epoch [10651/20000], Training Loss: 0.4382\n",
            "Epoch [10652/20000], Training Loss: 0.5052\n",
            "Epoch [10653/20000], Training Loss: 0.4991\n",
            "Epoch [10654/20000], Training Loss: 0.4657\n",
            "Epoch [10655/20000], Training Loss: 0.4847\n",
            "Epoch [10656/20000], Training Loss: 0.5036\n",
            "Epoch [10657/20000], Training Loss: 0.4985\n",
            "Epoch [10658/20000], Training Loss: 0.4943\n",
            "Epoch [10659/20000], Training Loss: 0.4799\n",
            "Epoch [10660/20000], Training Loss: 0.5017\n",
            "Epoch [10661/20000], Training Loss: 0.5260\n",
            "Epoch [10662/20000], Training Loss: 0.4824\n",
            "Epoch [10663/20000], Training Loss: 0.4762\n",
            "Epoch [10664/20000], Training Loss: 0.4915\n",
            "Epoch [10665/20000], Training Loss: 0.5134\n",
            "Epoch [10666/20000], Training Loss: 0.4663\n",
            "Epoch [10667/20000], Training Loss: 0.5345\n",
            "Epoch [10668/20000], Training Loss: 0.4647\n",
            "Epoch [10669/20000], Training Loss: 0.4532\n",
            "Epoch [10670/20000], Training Loss: 0.4812\n",
            "Epoch [10671/20000], Training Loss: 0.5108\n",
            "Epoch [10672/20000], Training Loss: 0.4487\n",
            "Epoch [10673/20000], Training Loss: 0.5051\n",
            "Epoch [10674/20000], Training Loss: 0.5110\n",
            "Epoch [10675/20000], Training Loss: 0.5147\n",
            "Epoch [10676/20000], Training Loss: 0.4997\n",
            "Epoch [10677/20000], Training Loss: 0.5000\n",
            "Epoch [10678/20000], Training Loss: 0.4845\n",
            "Epoch [10679/20000], Training Loss: 0.4443\n",
            "Epoch [10680/20000], Training Loss: 0.4461\n",
            "Epoch [10681/20000], Training Loss: 0.5444\n",
            "Epoch [10682/20000], Training Loss: 0.4404\n",
            "Epoch [10683/20000], Training Loss: 0.4809\n",
            "Epoch [10684/20000], Training Loss: 0.4657\n",
            "Epoch [10685/20000], Training Loss: 0.4787\n",
            "Epoch [10686/20000], Training Loss: 0.4934\n",
            "Epoch [10687/20000], Training Loss: 0.4956\n",
            "Epoch [10688/20000], Training Loss: 0.4848\n",
            "Epoch [10689/20000], Training Loss: 0.5148\n",
            "Epoch [10690/20000], Training Loss: 0.5023\n",
            "Epoch [10691/20000], Training Loss: 0.4822\n",
            "Epoch [10692/20000], Training Loss: 0.5516\n",
            "Epoch [10693/20000], Training Loss: 0.4845\n",
            "Epoch [10694/20000], Training Loss: 0.4502\n",
            "Epoch [10695/20000], Training Loss: 0.5118\n",
            "Epoch [10696/20000], Training Loss: 0.4433\n",
            "Epoch [10697/20000], Training Loss: 0.5386\n",
            "Epoch [10698/20000], Training Loss: 0.5113\n",
            "Epoch [10699/20000], Training Loss: 0.4683\n",
            "Epoch [10700/20000], Training Loss: 0.5232\n",
            "Epoch [10701/20000], Training Loss: 0.4726\n",
            "Epoch [10702/20000], Training Loss: 0.5159\n",
            "Epoch [10703/20000], Training Loss: 0.4374\n",
            "Epoch [10704/20000], Training Loss: 0.4811\n",
            "Epoch [10705/20000], Training Loss: 0.4627\n",
            "Epoch [10706/20000], Training Loss: 0.4564\n",
            "Epoch [10707/20000], Training Loss: 0.4351\n",
            "Epoch [10708/20000], Training Loss: 0.5015\n",
            "Epoch [10709/20000], Training Loss: 0.4885\n",
            "Epoch [10710/20000], Training Loss: 0.4653\n",
            "Epoch [10711/20000], Training Loss: 0.5103\n",
            "Epoch [10712/20000], Training Loss: 0.4618\n",
            "Epoch [10713/20000], Training Loss: 0.4866\n",
            "Epoch [10714/20000], Training Loss: 0.4891\n",
            "Epoch [10715/20000], Training Loss: 0.5257\n",
            "Epoch [10716/20000], Training Loss: 0.4751\n",
            "Epoch [10717/20000], Training Loss: 0.5130\n",
            "Epoch [10718/20000], Training Loss: 0.5133\n",
            "Epoch [10719/20000], Training Loss: 0.5123\n",
            "Epoch [10720/20000], Training Loss: 0.5014\n",
            "Epoch [10721/20000], Training Loss: 0.4511\n",
            "Epoch [10722/20000], Training Loss: 0.5053\n",
            "Epoch [10723/20000], Training Loss: 0.4759\n",
            "Epoch [10724/20000], Training Loss: 0.4673\n",
            "Epoch [10725/20000], Training Loss: 0.5138\n",
            "Epoch [10726/20000], Training Loss: 0.5479\n",
            "Epoch [10727/20000], Training Loss: 0.4753\n",
            "Epoch [10728/20000], Training Loss: 0.5141\n",
            "Epoch [10729/20000], Training Loss: 0.5191\n",
            "Epoch [10730/20000], Training Loss: 0.4925\n",
            "Epoch [10731/20000], Training Loss: 0.4961\n",
            "Epoch [10732/20000], Training Loss: 0.5515\n",
            "Epoch [10733/20000], Training Loss: 0.5290\n",
            "Epoch [10734/20000], Training Loss: 0.4840\n",
            "Epoch [10735/20000], Training Loss: 0.5170\n",
            "Epoch [10736/20000], Training Loss: 0.4768\n",
            "Epoch [10737/20000], Training Loss: 0.4983\n",
            "Epoch [10738/20000], Training Loss: 0.4698\n",
            "Epoch [10739/20000], Training Loss: 0.4967\n",
            "Epoch [10740/20000], Training Loss: 0.4977\n",
            "Epoch [10741/20000], Training Loss: 0.4542\n",
            "Epoch [10742/20000], Training Loss: 0.5070\n",
            "Epoch [10743/20000], Training Loss: 0.4771\n",
            "Epoch [10744/20000], Training Loss: 0.4604\n",
            "Epoch [10745/20000], Training Loss: 0.5073\n",
            "Epoch [10746/20000], Training Loss: 0.5035\n",
            "Epoch [10747/20000], Training Loss: 0.4876\n",
            "Epoch [10748/20000], Training Loss: 0.4943\n",
            "Epoch [10749/20000], Training Loss: 0.5336\n",
            "Epoch [10750/20000], Training Loss: 0.5491\n",
            "Epoch [10751/20000], Training Loss: 0.4955\n",
            "Epoch [10752/20000], Training Loss: 0.4978\n",
            "Epoch [10753/20000], Training Loss: 0.4821\n",
            "Epoch [10754/20000], Training Loss: 0.4786\n",
            "Epoch [10755/20000], Training Loss: 0.5118\n",
            "Epoch [10756/20000], Training Loss: 0.5049\n",
            "Epoch [10757/20000], Training Loss: 0.5114\n",
            "Epoch [10758/20000], Training Loss: 0.4957\n",
            "Epoch [10759/20000], Training Loss: 0.4943\n",
            "Epoch [10760/20000], Training Loss: 0.4885\n",
            "Epoch [10761/20000], Training Loss: 0.4802\n",
            "Epoch [10762/20000], Training Loss: 0.4831\n",
            "Epoch [10763/20000], Training Loss: 0.5208\n",
            "Epoch [10764/20000], Training Loss: 0.4526\n",
            "Epoch [10765/20000], Training Loss: 0.4520\n",
            "Epoch [10766/20000], Training Loss: 0.4770\n",
            "Epoch [10767/20000], Training Loss: 0.5073\n",
            "Epoch [10768/20000], Training Loss: 0.4619\n",
            "Epoch [10769/20000], Training Loss: 0.4822\n",
            "Epoch [10770/20000], Training Loss: 0.4770\n",
            "Epoch [10771/20000], Training Loss: 0.5231\n",
            "Epoch [10772/20000], Training Loss: 0.4769\n",
            "Epoch [10773/20000], Training Loss: 0.4939\n",
            "Epoch [10774/20000], Training Loss: 0.5074\n",
            "Epoch [10775/20000], Training Loss: 0.5018\n",
            "Epoch [10776/20000], Training Loss: 0.4577\n",
            "Epoch [10777/20000], Training Loss: 0.4812\n",
            "Epoch [10778/20000], Training Loss: 0.5208\n",
            "Epoch [10779/20000], Training Loss: 0.4790\n",
            "Epoch [10780/20000], Training Loss: 0.4836\n",
            "Epoch [10781/20000], Training Loss: 0.4394\n",
            "Epoch [10782/20000], Training Loss: 0.5026\n",
            "Epoch [10783/20000], Training Loss: 0.5344\n",
            "Epoch [10784/20000], Training Loss: 0.5130\n",
            "Epoch [10785/20000], Training Loss: 0.4673\n",
            "Epoch [10786/20000], Training Loss: 0.4590\n",
            "Epoch [10787/20000], Training Loss: 0.5422\n",
            "Epoch [10788/20000], Training Loss: 0.5557\n",
            "Epoch [10789/20000], Training Loss: 0.5137\n",
            "Epoch [10790/20000], Training Loss: 0.5217\n",
            "Epoch [10791/20000], Training Loss: 0.5083\n",
            "Epoch [10792/20000], Training Loss: 0.5100\n",
            "Epoch [10793/20000], Training Loss: 0.4884\n",
            "Epoch [10794/20000], Training Loss: 0.4503\n",
            "Epoch [10795/20000], Training Loss: 0.4762\n",
            "Epoch [10796/20000], Training Loss: 0.4966\n",
            "Epoch [10797/20000], Training Loss: 0.4778\n",
            "Epoch [10798/20000], Training Loss: 0.5291\n",
            "Epoch [10799/20000], Training Loss: 0.4988\n",
            "Epoch [10800/20000], Training Loss: 0.5147\n",
            "Epoch [10801/20000], Training Loss: 0.4860\n",
            "Epoch [10802/20000], Training Loss: 0.4976\n",
            "Epoch [10803/20000], Training Loss: 0.5225\n",
            "Epoch [10804/20000], Training Loss: 0.5012\n",
            "Epoch [10805/20000], Training Loss: 0.4666\n",
            "Epoch [10806/20000], Training Loss: 0.5126\n",
            "Epoch [10807/20000], Training Loss: 0.4925\n",
            "Epoch [10808/20000], Training Loss: 0.5110\n",
            "Epoch [10809/20000], Training Loss: 0.4291\n",
            "Epoch [10810/20000], Training Loss: 0.5548\n",
            "Epoch [10811/20000], Training Loss: 0.5325\n",
            "Epoch [10812/20000], Training Loss: 0.4634\n",
            "Epoch [10813/20000], Training Loss: 0.5407\n",
            "Epoch [10814/20000], Training Loss: 0.5140\n",
            "Epoch [10815/20000], Training Loss: 0.5064\n",
            "Epoch [10816/20000], Training Loss: 0.4446\n",
            "Epoch [10817/20000], Training Loss: 0.5084\n",
            "Epoch [10818/20000], Training Loss: 0.4662\n",
            "Epoch [10819/20000], Training Loss: 0.5207\n",
            "Epoch [10820/20000], Training Loss: 0.4876\n",
            "Epoch [10821/20000], Training Loss: 0.4809\n",
            "Epoch [10822/20000], Training Loss: 0.5149\n",
            "Epoch [10823/20000], Training Loss: 0.5096\n",
            "Epoch [10824/20000], Training Loss: 0.4491\n",
            "Epoch [10825/20000], Training Loss: 0.4630\n",
            "Epoch [10826/20000], Training Loss: 0.5031\n",
            "Epoch [10827/20000], Training Loss: 0.4450\n",
            "Epoch [10828/20000], Training Loss: 0.4950\n",
            "Epoch [10829/20000], Training Loss: 0.5247\n",
            "Epoch [10830/20000], Training Loss: 0.4813\n",
            "Epoch [10831/20000], Training Loss: 0.4789\n",
            "Epoch [10832/20000], Training Loss: 0.4619\n",
            "Epoch [10833/20000], Training Loss: 0.4942\n",
            "Epoch [10834/20000], Training Loss: 0.5612\n",
            "Epoch [10835/20000], Training Loss: 0.4938\n",
            "Epoch [10836/20000], Training Loss: 0.5016\n",
            "Epoch [10837/20000], Training Loss: 0.4740\n",
            "Epoch [10838/20000], Training Loss: 0.4880\n",
            "Epoch [10839/20000], Training Loss: 0.4616\n",
            "Epoch [10840/20000], Training Loss: 0.4994\n",
            "Epoch [10841/20000], Training Loss: 0.5613\n",
            "Epoch [10842/20000], Training Loss: 0.5155\n",
            "Epoch [10843/20000], Training Loss: 0.4389\n",
            "Epoch [10844/20000], Training Loss: 0.4891\n",
            "Epoch [10845/20000], Training Loss: 0.4554\n",
            "Epoch [10846/20000], Training Loss: 0.5103\n",
            "Epoch [10847/20000], Training Loss: 0.4730\n",
            "Epoch [10848/20000], Training Loss: 0.4479\n",
            "Epoch [10849/20000], Training Loss: 0.5184\n",
            "Epoch [10850/20000], Training Loss: 0.5108\n",
            "Epoch [10851/20000], Training Loss: 0.4726\n",
            "Epoch [10852/20000], Training Loss: 0.5311\n",
            "Epoch [10853/20000], Training Loss: 0.5113\n",
            "Epoch [10854/20000], Training Loss: 0.4714\n",
            "Epoch [10855/20000], Training Loss: 0.4756\n",
            "Epoch [10856/20000], Training Loss: 0.4746\n",
            "Epoch [10857/20000], Training Loss: 0.4698\n",
            "Epoch [10858/20000], Training Loss: 0.4623\n",
            "Epoch [10859/20000], Training Loss: 0.4776\n",
            "Epoch [10860/20000], Training Loss: 0.4712\n",
            "Epoch [10861/20000], Training Loss: 0.5028\n",
            "Epoch [10862/20000], Training Loss: 0.4737\n",
            "Epoch [10863/20000], Training Loss: 0.4906\n",
            "Epoch [10864/20000], Training Loss: 0.4776\n",
            "Epoch [10865/20000], Training Loss: 0.4980\n",
            "Epoch [10866/20000], Training Loss: 0.5007\n",
            "Epoch [10867/20000], Training Loss: 0.4703\n",
            "Epoch [10868/20000], Training Loss: 0.4953\n",
            "Epoch [10869/20000], Training Loss: 0.4572\n",
            "Epoch [10870/20000], Training Loss: 0.4981\n",
            "Epoch [10871/20000], Training Loss: 0.5019\n",
            "Epoch [10872/20000], Training Loss: 0.4872\n",
            "Epoch [10873/20000], Training Loss: 0.4782\n",
            "Epoch [10874/20000], Training Loss: 0.4724\n",
            "Epoch [10875/20000], Training Loss: 0.4699\n",
            "Epoch [10876/20000], Training Loss: 0.4717\n",
            "Epoch [10877/20000], Training Loss: 0.4511\n",
            "Epoch [10878/20000], Training Loss: 0.4588\n",
            "Epoch [10879/20000], Training Loss: 0.4575\n",
            "Epoch [10880/20000], Training Loss: 0.4471\n",
            "Epoch [10881/20000], Training Loss: 0.4710\n",
            "Epoch [10882/20000], Training Loss: 0.4771\n",
            "Epoch [10883/20000], Training Loss: 0.4721\n",
            "Epoch [10884/20000], Training Loss: 0.4518\n",
            "Epoch [10885/20000], Training Loss: 0.4915\n",
            "Epoch [10886/20000], Training Loss: 0.4660\n",
            "Epoch [10887/20000], Training Loss: 0.5036\n",
            "Epoch [10888/20000], Training Loss: 0.5352\n",
            "Epoch [10889/20000], Training Loss: 0.4907\n",
            "Epoch [10890/20000], Training Loss: 0.4433\n",
            "Epoch [10891/20000], Training Loss: 0.4500\n",
            "Epoch [10892/20000], Training Loss: 0.4967\n",
            "Epoch [10893/20000], Training Loss: 0.5173\n",
            "Epoch [10894/20000], Training Loss: 0.4772\n",
            "Epoch [10895/20000], Training Loss: 0.4699\n",
            "Epoch [10896/20000], Training Loss: 0.4821\n",
            "Epoch [10897/20000], Training Loss: 0.4803\n",
            "Epoch [10898/20000], Training Loss: 0.4851\n",
            "Epoch [10899/20000], Training Loss: 0.4719\n",
            "Epoch [10900/20000], Training Loss: 0.4690\n",
            "Epoch [10901/20000], Training Loss: 0.5167\n",
            "Epoch [10902/20000], Training Loss: 0.4848\n",
            "Epoch [10903/20000], Training Loss: 0.5094\n",
            "Epoch [10904/20000], Training Loss: 0.4632\n",
            "Epoch [10905/20000], Training Loss: 0.4405\n",
            "Epoch [10906/20000], Training Loss: 0.4990\n",
            "Epoch [10907/20000], Training Loss: 0.4904\n",
            "Epoch [10908/20000], Training Loss: 0.4768\n",
            "Epoch [10909/20000], Training Loss: 0.4732\n",
            "Epoch [10910/20000], Training Loss: 0.4708\n",
            "Epoch [10911/20000], Training Loss: 0.4783\n",
            "Epoch [10912/20000], Training Loss: 0.5663\n",
            "Epoch [10913/20000], Training Loss: 0.4639\n",
            "Epoch [10914/20000], Training Loss: 0.5048\n",
            "Epoch [10915/20000], Training Loss: 0.4554\n",
            "Epoch [10916/20000], Training Loss: 0.4550\n",
            "Epoch [10917/20000], Training Loss: 0.4749\n",
            "Epoch [10918/20000], Training Loss: 0.4782\n",
            "Epoch [10919/20000], Training Loss: 0.4658\n",
            "Epoch [10920/20000], Training Loss: 0.4955\n",
            "Epoch [10921/20000], Training Loss: 0.5157\n",
            "Epoch [10922/20000], Training Loss: 0.4571\n",
            "Epoch [10923/20000], Training Loss: 0.4991\n",
            "Epoch [10924/20000], Training Loss: 0.4613\n",
            "Epoch [10925/20000], Training Loss: 0.5023\n",
            "Epoch [10926/20000], Training Loss: 0.4566\n",
            "Epoch [10927/20000], Training Loss: 0.4810\n",
            "Epoch [10928/20000], Training Loss: 0.4663\n",
            "Epoch [10929/20000], Training Loss: 0.4784\n",
            "Epoch [10930/20000], Training Loss: 0.4621\n",
            "Epoch [10931/20000], Training Loss: 0.5290\n",
            "Epoch [10932/20000], Training Loss: 0.4490\n",
            "Epoch [10933/20000], Training Loss: 0.4633\n",
            "Epoch [10934/20000], Training Loss: 0.4856\n",
            "Epoch [10935/20000], Training Loss: 0.5529\n",
            "Epoch [10936/20000], Training Loss: 0.4775\n",
            "Epoch [10937/20000], Training Loss: 0.4881\n",
            "Epoch [10938/20000], Training Loss: 0.4657\n",
            "Epoch [10939/20000], Training Loss: 0.5077\n",
            "Epoch [10940/20000], Training Loss: 0.5118\n",
            "Epoch [10941/20000], Training Loss: 0.5204\n",
            "Epoch [10942/20000], Training Loss: 0.5468\n",
            "Epoch [10943/20000], Training Loss: 0.5369\n",
            "Epoch [10944/20000], Training Loss: 0.5124\n",
            "Epoch [10945/20000], Training Loss: 0.4992\n",
            "Epoch [10946/20000], Training Loss: 0.4359\n",
            "Epoch [10947/20000], Training Loss: 0.4824\n",
            "Epoch [10948/20000], Training Loss: 0.4560\n",
            "Epoch [10949/20000], Training Loss: 0.4492\n",
            "Epoch [10950/20000], Training Loss: 0.4955\n",
            "Epoch [10951/20000], Training Loss: 0.4688\n",
            "Epoch [10952/20000], Training Loss: 0.4581\n",
            "Epoch [10953/20000], Training Loss: 0.4941\n",
            "Epoch [10954/20000], Training Loss: 0.5178\n",
            "Epoch [10955/20000], Training Loss: 0.5249\n",
            "Epoch [10956/20000], Training Loss: 0.4864\n",
            "Epoch [10957/20000], Training Loss: 0.4342\n",
            "Epoch [10958/20000], Training Loss: 0.5098\n",
            "Epoch [10959/20000], Training Loss: 0.4943\n",
            "Epoch [10960/20000], Training Loss: 0.4319\n",
            "Epoch [10961/20000], Training Loss: 0.4677\n",
            "Epoch [10962/20000], Training Loss: 0.4771\n",
            "Epoch [10963/20000], Training Loss: 0.4673\n",
            "Epoch [10964/20000], Training Loss: 0.5398\n",
            "Epoch [10965/20000], Training Loss: 0.4575\n",
            "Epoch [10966/20000], Training Loss: 0.4785\n",
            "Epoch [10967/20000], Training Loss: 0.4851\n",
            "Epoch [10968/20000], Training Loss: 0.5034\n",
            "Epoch [10969/20000], Training Loss: 0.4737\n",
            "Epoch [10970/20000], Training Loss: 0.4796\n",
            "Epoch [10971/20000], Training Loss: 0.4618\n",
            "Epoch [10972/20000], Training Loss: 0.5305\n",
            "Epoch [10973/20000], Training Loss: 0.4893\n",
            "Epoch [10974/20000], Training Loss: 0.4349\n",
            "Epoch [10975/20000], Training Loss: 0.4920\n",
            "Epoch [10976/20000], Training Loss: 0.5000\n",
            "Epoch [10977/20000], Training Loss: 0.4863\n",
            "Epoch [10978/20000], Training Loss: 0.5030\n",
            "Epoch [10979/20000], Training Loss: 0.5192\n",
            "Epoch [10980/20000], Training Loss: 0.4994\n",
            "Epoch [10981/20000], Training Loss: 0.5241\n",
            "Epoch [10982/20000], Training Loss: 0.4705\n",
            "Epoch [10983/20000], Training Loss: 0.4815\n",
            "Epoch [10984/20000], Training Loss: 0.4586\n",
            "Epoch [10985/20000], Training Loss: 0.4584\n",
            "Epoch [10986/20000], Training Loss: 0.4645\n",
            "Epoch [10987/20000], Training Loss: 0.5088\n",
            "Epoch [10988/20000], Training Loss: 0.5187\n",
            "Epoch [10989/20000], Training Loss: 0.5195\n",
            "Epoch [10990/20000], Training Loss: 0.5030\n",
            "Epoch [10991/20000], Training Loss: 0.4904\n",
            "Epoch [10992/20000], Training Loss: 0.5422\n",
            "Epoch [10993/20000], Training Loss: 0.5181\n",
            "Epoch [10994/20000], Training Loss: 0.4681\n",
            "Epoch [10995/20000], Training Loss: 0.4733\n",
            "Epoch [10996/20000], Training Loss: 0.5335\n",
            "Epoch [10997/20000], Training Loss: 0.5179\n",
            "Epoch [10998/20000], Training Loss: 0.4971\n",
            "Epoch [10999/20000], Training Loss: 0.5106\n",
            "Epoch [11000/20000], Training Loss: 0.4722\n",
            "Epoch [11001/20000], Training Loss: 0.4814\n",
            "Epoch [11002/20000], Training Loss: 0.4571\n",
            "Epoch [11003/20000], Training Loss: 0.4498\n",
            "Epoch [11004/20000], Training Loss: 0.4496\n",
            "Epoch [11005/20000], Training Loss: 0.4997\n",
            "Epoch [11006/20000], Training Loss: 0.4688\n",
            "Epoch [11007/20000], Training Loss: 0.4856\n",
            "Epoch [11008/20000], Training Loss: 0.4258\n",
            "Epoch [11009/20000], Training Loss: 0.4670\n",
            "Epoch [11010/20000], Training Loss: 0.4928\n",
            "Epoch [11011/20000], Training Loss: 0.4506\n",
            "Epoch [11012/20000], Training Loss: 0.4839\n",
            "Epoch [11013/20000], Training Loss: 0.4524\n",
            "Epoch [11014/20000], Training Loss: 0.4936\n",
            "Epoch [11015/20000], Training Loss: 0.5528\n",
            "Epoch [11016/20000], Training Loss: 0.4993\n",
            "Epoch [11017/20000], Training Loss: 0.5044\n",
            "Epoch [11018/20000], Training Loss: 0.4594\n",
            "Epoch [11019/20000], Training Loss: 0.4983\n",
            "Epoch [11020/20000], Training Loss: 0.5067\n",
            "Epoch [11021/20000], Training Loss: 0.5128\n",
            "Epoch [11022/20000], Training Loss: 0.5181\n",
            "Epoch [11023/20000], Training Loss: 0.5043\n",
            "Epoch [11024/20000], Training Loss: 0.4923\n",
            "Epoch [11025/20000], Training Loss: 0.4874\n",
            "Epoch [11026/20000], Training Loss: 0.4818\n",
            "Epoch [11027/20000], Training Loss: 0.4792\n",
            "Epoch [11028/20000], Training Loss: 0.5077\n",
            "Epoch [11029/20000], Training Loss: 0.4912\n",
            "Epoch [11030/20000], Training Loss: 0.4962\n",
            "Epoch [11031/20000], Training Loss: 0.4963\n",
            "Epoch [11032/20000], Training Loss: 0.4864\n",
            "Epoch [11033/20000], Training Loss: 0.5036\n",
            "Epoch [11034/20000], Training Loss: 0.4932\n",
            "Epoch [11035/20000], Training Loss: 0.4970\n",
            "Epoch [11036/20000], Training Loss: 0.5209\n",
            "Epoch [11037/20000], Training Loss: 0.4729\n",
            "Epoch [11038/20000], Training Loss: 0.5290\n",
            "Epoch [11039/20000], Training Loss: 0.4882\n",
            "Epoch [11040/20000], Training Loss: 0.4967\n",
            "Epoch [11041/20000], Training Loss: 0.5250\n",
            "Epoch [11042/20000], Training Loss: 0.4868\n",
            "Epoch [11043/20000], Training Loss: 0.5100\n",
            "Epoch [11044/20000], Training Loss: 0.4749\n",
            "Epoch [11045/20000], Training Loss: 0.4819\n",
            "Epoch [11046/20000], Training Loss: 0.4718\n",
            "Epoch [11047/20000], Training Loss: 0.4579\n",
            "Epoch [11048/20000], Training Loss: 0.4630\n",
            "Epoch [11049/20000], Training Loss: 0.4853\n",
            "Epoch [11050/20000], Training Loss: 0.5403\n",
            "Epoch [11051/20000], Training Loss: 0.4733\n",
            "Epoch [11052/20000], Training Loss: 0.5018\n",
            "Epoch [11053/20000], Training Loss: 0.5109\n",
            "Epoch [11054/20000], Training Loss: 0.4937\n",
            "Epoch [11055/20000], Training Loss: 0.4851\n",
            "Epoch [11056/20000], Training Loss: 0.4853\n",
            "Epoch [11057/20000], Training Loss: 0.4486\n",
            "Epoch [11058/20000], Training Loss: 0.4795\n",
            "Epoch [11059/20000], Training Loss: 0.4803\n",
            "Epoch [11060/20000], Training Loss: 0.4869\n",
            "Epoch [11061/20000], Training Loss: 0.4800\n",
            "Epoch [11062/20000], Training Loss: 0.4685\n",
            "Epoch [11063/20000], Training Loss: 0.5239\n",
            "Epoch [11064/20000], Training Loss: 0.4784\n",
            "Epoch [11065/20000], Training Loss: 0.4646\n",
            "Epoch [11066/20000], Training Loss: 0.5112\n",
            "Epoch [11067/20000], Training Loss: 0.5030\n",
            "Epoch [11068/20000], Training Loss: 0.5219\n",
            "Epoch [11069/20000], Training Loss: 0.5324\n",
            "Epoch [11070/20000], Training Loss: 0.5252\n",
            "Epoch [11071/20000], Training Loss: 0.4755\n",
            "Epoch [11072/20000], Training Loss: 0.4960\n",
            "Epoch [11073/20000], Training Loss: 0.4552\n",
            "Epoch [11074/20000], Training Loss: 0.4441\n",
            "Epoch [11075/20000], Training Loss: 0.4721\n",
            "Epoch [11076/20000], Training Loss: 0.4740\n",
            "Epoch [11077/20000], Training Loss: 0.5006\n",
            "Epoch [11078/20000], Training Loss: 0.5031\n",
            "Epoch [11079/20000], Training Loss: 0.4804\n",
            "Epoch [11080/20000], Training Loss: 0.4911\n",
            "Epoch [11081/20000], Training Loss: 0.4958\n",
            "Epoch [11082/20000], Training Loss: 0.4736\n",
            "Epoch [11083/20000], Training Loss: 0.4658\n",
            "Epoch [11084/20000], Training Loss: 0.5041\n",
            "Epoch [11085/20000], Training Loss: 0.5060\n",
            "Epoch [11086/20000], Training Loss: 0.4830\n",
            "Epoch [11087/20000], Training Loss: 0.4879\n",
            "Epoch [11088/20000], Training Loss: 0.4658\n",
            "Epoch [11089/20000], Training Loss: 0.4704\n",
            "Epoch [11090/20000], Training Loss: 0.4672\n",
            "Epoch [11091/20000], Training Loss: 0.4562\n",
            "Epoch [11092/20000], Training Loss: 0.4948\n",
            "Epoch [11093/20000], Training Loss: 0.4975\n",
            "Epoch [11094/20000], Training Loss: 0.4650\n",
            "Epoch [11095/20000], Training Loss: 0.4647\n",
            "Epoch [11096/20000], Training Loss: 0.4645\n",
            "Epoch [11097/20000], Training Loss: 0.4817\n",
            "Epoch [11098/20000], Training Loss: 0.4970\n",
            "Epoch [11099/20000], Training Loss: 0.5138\n",
            "Epoch [11100/20000], Training Loss: 0.5294\n",
            "Epoch [11101/20000], Training Loss: 0.4802\n",
            "Epoch [11102/20000], Training Loss: 0.5098\n",
            "Epoch [11103/20000], Training Loss: 0.4859\n",
            "Epoch [11104/20000], Training Loss: 0.4937\n",
            "Epoch [11105/20000], Training Loss: 0.4470\n",
            "Epoch [11106/20000], Training Loss: 0.4796\n",
            "Epoch [11107/20000], Training Loss: 0.4993\n",
            "Epoch [11108/20000], Training Loss: 0.4865\n",
            "Epoch [11109/20000], Training Loss: 0.4709\n",
            "Epoch [11110/20000], Training Loss: 0.4721\n",
            "Epoch [11111/20000], Training Loss: 0.4587\n",
            "Epoch [11112/20000], Training Loss: 0.4766\n",
            "Epoch [11113/20000], Training Loss: 0.4967\n",
            "Epoch [11114/20000], Training Loss: 0.5461\n",
            "Epoch [11115/20000], Training Loss: 0.5007\n",
            "Epoch [11116/20000], Training Loss: 0.4550\n",
            "Epoch [11117/20000], Training Loss: 0.4870\n",
            "Epoch [11118/20000], Training Loss: 0.5192\n",
            "Epoch [11119/20000], Training Loss: 0.4952\n",
            "Epoch [11120/20000], Training Loss: 0.4955\n",
            "Epoch [11121/20000], Training Loss: 0.4786\n",
            "Epoch [11122/20000], Training Loss: 0.5277\n",
            "Epoch [11123/20000], Training Loss: 0.4793\n",
            "Epoch [11124/20000], Training Loss: 0.4855\n",
            "Epoch [11125/20000], Training Loss: 0.4848\n",
            "Epoch [11126/20000], Training Loss: 0.4751\n",
            "Epoch [11127/20000], Training Loss: 0.4974\n",
            "Epoch [11128/20000], Training Loss: 0.4993\n",
            "Epoch [11129/20000], Training Loss: 0.4727\n",
            "Epoch [11130/20000], Training Loss: 0.5081\n",
            "Epoch [11131/20000], Training Loss: 0.4721\n",
            "Epoch [11132/20000], Training Loss: 0.5310\n",
            "Epoch [11133/20000], Training Loss: 0.5119\n",
            "Epoch [11134/20000], Training Loss: 0.4955\n",
            "Epoch [11135/20000], Training Loss: 0.4969\n",
            "Epoch [11136/20000], Training Loss: 0.4839\n",
            "Epoch [11137/20000], Training Loss: 0.4694\n",
            "Epoch [11138/20000], Training Loss: 0.4975\n",
            "Epoch [11139/20000], Training Loss: 0.4729\n",
            "Epoch [11140/20000], Training Loss: 0.5501\n",
            "Epoch [11141/20000], Training Loss: 0.4992\n",
            "Epoch [11142/20000], Training Loss: 0.5131\n",
            "Epoch [11143/20000], Training Loss: 0.5245\n",
            "Epoch [11144/20000], Training Loss: 0.5026\n",
            "Epoch [11145/20000], Training Loss: 0.5047\n",
            "Epoch [11146/20000], Training Loss: 0.5225\n",
            "Epoch [11147/20000], Training Loss: 0.4987\n",
            "Epoch [11148/20000], Training Loss: 0.5022\n",
            "Epoch [11149/20000], Training Loss: 0.5263\n",
            "Epoch [11150/20000], Training Loss: 0.5268\n",
            "Epoch [11151/20000], Training Loss: 0.4584\n",
            "Epoch [11152/20000], Training Loss: 0.4947\n",
            "Epoch [11153/20000], Training Loss: 0.4539\n",
            "Epoch [11154/20000], Training Loss: 0.4835\n",
            "Epoch [11155/20000], Training Loss: 0.4943\n",
            "Epoch [11156/20000], Training Loss: 0.4383\n",
            "Epoch [11157/20000], Training Loss: 0.4801\n",
            "Epoch [11158/20000], Training Loss: 0.5037\n",
            "Epoch [11159/20000], Training Loss: 0.4775\n",
            "Epoch [11160/20000], Training Loss: 0.4941\n",
            "Epoch [11161/20000], Training Loss: 0.4229\n",
            "Epoch [11162/20000], Training Loss: 0.4773\n",
            "Epoch [11163/20000], Training Loss: 0.4683\n",
            "Epoch [11164/20000], Training Loss: 0.5086\n",
            "Epoch [11165/20000], Training Loss: 0.5051\n",
            "Epoch [11166/20000], Training Loss: 0.4590\n",
            "Epoch [11167/20000], Training Loss: 0.4937\n",
            "Epoch [11168/20000], Training Loss: 0.5190\n",
            "Epoch [11169/20000], Training Loss: 0.4523\n",
            "Epoch [11170/20000], Training Loss: 0.5306\n",
            "Epoch [11171/20000], Training Loss: 0.4287\n",
            "Epoch [11172/20000], Training Loss: 0.4964\n",
            "Epoch [11173/20000], Training Loss: 0.4999\n",
            "Epoch [11174/20000], Training Loss: 0.4419\n",
            "Epoch [11175/20000], Training Loss: 0.4671\n",
            "Epoch [11176/20000], Training Loss: 0.5151\n",
            "Epoch [11177/20000], Training Loss: 0.5403\n",
            "Epoch [11178/20000], Training Loss: 0.4548\n",
            "Epoch [11179/20000], Training Loss: 0.4480\n",
            "Epoch [11180/20000], Training Loss: 0.4880\n",
            "Epoch [11181/20000], Training Loss: 0.4528\n",
            "Epoch [11182/20000], Training Loss: 0.4690\n",
            "Epoch [11183/20000], Training Loss: 0.4496\n",
            "Epoch [11184/20000], Training Loss: 0.4831\n",
            "Epoch [11185/20000], Training Loss: 0.4432\n",
            "Epoch [11186/20000], Training Loss: 0.5237\n",
            "Epoch [11187/20000], Training Loss: 0.4804\n",
            "Epoch [11188/20000], Training Loss: 0.4931\n",
            "Epoch [11189/20000], Training Loss: 0.4550\n",
            "Epoch [11190/20000], Training Loss: 0.5294\n",
            "Epoch [11191/20000], Training Loss: 0.5434\n",
            "Epoch [11192/20000], Training Loss: 0.4854\n",
            "Epoch [11193/20000], Training Loss: 0.5018\n",
            "Epoch [11194/20000], Training Loss: 0.4703\n",
            "Epoch [11195/20000], Training Loss: 0.4836\n",
            "Epoch [11196/20000], Training Loss: 0.4520\n",
            "Epoch [11197/20000], Training Loss: 0.5278\n",
            "Epoch [11198/20000], Training Loss: 0.5021\n",
            "Epoch [11199/20000], Training Loss: 0.4937\n",
            "Epoch [11200/20000], Training Loss: 0.4950\n",
            "Epoch [11201/20000], Training Loss: 0.5084\n",
            "Epoch [11202/20000], Training Loss: 0.4591\n",
            "Epoch [11203/20000], Training Loss: 0.4832\n",
            "Epoch [11204/20000], Training Loss: 0.4562\n",
            "Epoch [11205/20000], Training Loss: 0.5107\n",
            "Epoch [11206/20000], Training Loss: 0.5088\n",
            "Epoch [11207/20000], Training Loss: 0.4841\n",
            "Epoch [11208/20000], Training Loss: 0.4500\n",
            "Epoch [11209/20000], Training Loss: 0.4784\n",
            "Epoch [11210/20000], Training Loss: 0.5041\n",
            "Epoch [11211/20000], Training Loss: 0.5288\n",
            "Epoch [11212/20000], Training Loss: 0.5457\n",
            "Epoch [11213/20000], Training Loss: 0.4672\n",
            "Epoch [11214/20000], Training Loss: 0.4746\n",
            "Epoch [11215/20000], Training Loss: 0.5172\n",
            "Epoch [11216/20000], Training Loss: 0.5069\n",
            "Epoch [11217/20000], Training Loss: 0.5421\n",
            "Epoch [11218/20000], Training Loss: 0.5116\n",
            "Epoch [11219/20000], Training Loss: 0.4933\n",
            "Epoch [11220/20000], Training Loss: 0.4891\n",
            "Epoch [11221/20000], Training Loss: 0.4688\n",
            "Epoch [11222/20000], Training Loss: 0.4874\n",
            "Epoch [11223/20000], Training Loss: 0.5010\n",
            "Epoch [11224/20000], Training Loss: 0.5083\n",
            "Epoch [11225/20000], Training Loss: 0.5343\n",
            "Epoch [11226/20000], Training Loss: 0.5104\n",
            "Epoch [11227/20000], Training Loss: 0.5052\n",
            "Epoch [11228/20000], Training Loss: 0.4620\n",
            "Epoch [11229/20000], Training Loss: 0.4609\n",
            "Epoch [11230/20000], Training Loss: 0.4957\n",
            "Epoch [11231/20000], Training Loss: 0.4801\n",
            "Epoch [11232/20000], Training Loss: 0.4394\n",
            "Epoch [11233/20000], Training Loss: 0.4884\n",
            "Epoch [11234/20000], Training Loss: 0.5014\n",
            "Epoch [11235/20000], Training Loss: 0.4783\n",
            "Epoch [11236/20000], Training Loss: 0.4554\n",
            "Epoch [11237/20000], Training Loss: 0.5285\n",
            "Epoch [11238/20000], Training Loss: 0.4604\n",
            "Epoch [11239/20000], Training Loss: 0.4642\n",
            "Epoch [11240/20000], Training Loss: 0.4683\n",
            "Epoch [11241/20000], Training Loss: 0.5237\n",
            "Epoch [11242/20000], Training Loss: 0.4730\n",
            "Epoch [11243/20000], Training Loss: 0.5085\n",
            "Epoch [11244/20000], Training Loss: 0.4857\n",
            "Epoch [11245/20000], Training Loss: 0.4558\n",
            "Epoch [11246/20000], Training Loss: 0.5115\n",
            "Epoch [11247/20000], Training Loss: 0.4845\n",
            "Epoch [11248/20000], Training Loss: 0.4703\n",
            "Epoch [11249/20000], Training Loss: 0.5025\n",
            "Epoch [11250/20000], Training Loss: 0.4476\n",
            "Epoch [11251/20000], Training Loss: 0.4576\n",
            "Epoch [11252/20000], Training Loss: 0.4659\n",
            "Epoch [11253/20000], Training Loss: 0.4851\n",
            "Epoch [11254/20000], Training Loss: 0.4764\n",
            "Epoch [11255/20000], Training Loss: 0.4415\n",
            "Epoch [11256/20000], Training Loss: 0.4580\n",
            "Epoch [11257/20000], Training Loss: 0.5135\n",
            "Epoch [11258/20000], Training Loss: 0.4513\n",
            "Epoch [11259/20000], Training Loss: 0.5156\n",
            "Epoch [11260/20000], Training Loss: 0.4508\n",
            "Epoch [11261/20000], Training Loss: 0.5059\n",
            "Epoch [11262/20000], Training Loss: 0.4520\n",
            "Epoch [11263/20000], Training Loss: 0.4684\n",
            "Epoch [11264/20000], Training Loss: 0.5186\n",
            "Epoch [11265/20000], Training Loss: 0.4796\n",
            "Epoch [11266/20000], Training Loss: 0.4677\n",
            "Epoch [11267/20000], Training Loss: 0.4702\n",
            "Epoch [11268/20000], Training Loss: 0.5002\n",
            "Epoch [11269/20000], Training Loss: 0.4740\n",
            "Epoch [11270/20000], Training Loss: 0.4753\n",
            "Epoch [11271/20000], Training Loss: 0.4377\n",
            "Epoch [11272/20000], Training Loss: 0.4945\n",
            "Epoch [11273/20000], Training Loss: 0.5195\n",
            "Epoch [11274/20000], Training Loss: 0.4895\n",
            "Epoch [11275/20000], Training Loss: 0.5060\n",
            "Epoch [11276/20000], Training Loss: 0.4570\n",
            "Epoch [11277/20000], Training Loss: 0.4602\n",
            "Epoch [11278/20000], Training Loss: 0.4836\n",
            "Epoch [11279/20000], Training Loss: 0.4997\n",
            "Epoch [11280/20000], Training Loss: 0.4980\n",
            "Epoch [11281/20000], Training Loss: 0.4699\n",
            "Epoch [11282/20000], Training Loss: 0.4893\n",
            "Epoch [11283/20000], Training Loss: 0.4859\n",
            "Epoch [11284/20000], Training Loss: 0.4755\n",
            "Epoch [11285/20000], Training Loss: 0.4870\n",
            "Epoch [11286/20000], Training Loss: 0.4697\n",
            "Epoch [11287/20000], Training Loss: 0.4537\n",
            "Epoch [11288/20000], Training Loss: 0.4870\n",
            "Epoch [11289/20000], Training Loss: 0.4616\n",
            "Epoch [11290/20000], Training Loss: 0.5122\n",
            "Epoch [11291/20000], Training Loss: 0.4685\n",
            "Epoch [11292/20000], Training Loss: 0.4767\n",
            "Epoch [11293/20000], Training Loss: 0.5202\n",
            "Epoch [11294/20000], Training Loss: 0.4488\n",
            "Epoch [11295/20000], Training Loss: 0.5438\n",
            "Epoch [11296/20000], Training Loss: 0.4991\n",
            "Epoch [11297/20000], Training Loss: 0.4415\n",
            "Epoch [11298/20000], Training Loss: 0.4581\n",
            "Epoch [11299/20000], Training Loss: 0.4628\n",
            "Epoch [11300/20000], Training Loss: 0.5311\n",
            "Epoch [11301/20000], Training Loss: 0.4681\n",
            "Epoch [11302/20000], Training Loss: 0.4947\n",
            "Epoch [11303/20000], Training Loss: 0.5254\n",
            "Epoch [11304/20000], Training Loss: 0.4953\n",
            "Epoch [11305/20000], Training Loss: 0.4956\n",
            "Epoch [11306/20000], Training Loss: 0.4991\n",
            "Epoch [11307/20000], Training Loss: 0.4676\n",
            "Epoch [11308/20000], Training Loss: 0.4979\n",
            "Epoch [11309/20000], Training Loss: 0.5292\n",
            "Epoch [11310/20000], Training Loss: 0.5049\n",
            "Epoch [11311/20000], Training Loss: 0.4781\n",
            "Epoch [11312/20000], Training Loss: 0.4874\n",
            "Epoch [11313/20000], Training Loss: 0.4679\n",
            "Epoch [11314/20000], Training Loss: 0.4806\n",
            "Epoch [11315/20000], Training Loss: 0.4916\n",
            "Epoch [11316/20000], Training Loss: 0.4837\n",
            "Epoch [11317/20000], Training Loss: 0.4464\n",
            "Epoch [11318/20000], Training Loss: 0.5252\n",
            "Epoch [11319/20000], Training Loss: 0.5567\n",
            "Epoch [11320/20000], Training Loss: 0.5104\n",
            "Epoch [11321/20000], Training Loss: 0.5011\n",
            "Epoch [11322/20000], Training Loss: 0.4914\n",
            "Epoch [11323/20000], Training Loss: 0.4705\n",
            "Epoch [11324/20000], Training Loss: 0.4630\n",
            "Epoch [11325/20000], Training Loss: 0.5033\n",
            "Epoch [11326/20000], Training Loss: 0.4611\n",
            "Epoch [11327/20000], Training Loss: 0.5159\n",
            "Epoch [11328/20000], Training Loss: 0.5405\n",
            "Epoch [11329/20000], Training Loss: 0.4627\n",
            "Epoch [11330/20000], Training Loss: 0.4719\n",
            "Epoch [11331/20000], Training Loss: 0.5081\n",
            "Epoch [11332/20000], Training Loss: 0.5234\n",
            "Epoch [11333/20000], Training Loss: 0.4660\n",
            "Epoch [11334/20000], Training Loss: 0.4808\n",
            "Epoch [11335/20000], Training Loss: 0.4907\n",
            "Epoch [11336/20000], Training Loss: 0.4546\n",
            "Epoch [11337/20000], Training Loss: 0.5040\n",
            "Epoch [11338/20000], Training Loss: 0.4601\n",
            "Epoch [11339/20000], Training Loss: 0.4566\n",
            "Epoch [11340/20000], Training Loss: 0.4589\n",
            "Epoch [11341/20000], Training Loss: 0.4967\n",
            "Epoch [11342/20000], Training Loss: 0.4862\n",
            "Epoch [11343/20000], Training Loss: 0.5223\n",
            "Epoch [11344/20000], Training Loss: 0.5090\n",
            "Epoch [11345/20000], Training Loss: 0.4509\n",
            "Epoch [11346/20000], Training Loss: 0.4427\n",
            "Epoch [11347/20000], Training Loss: 0.4981\n",
            "Epoch [11348/20000], Training Loss: 0.5114\n",
            "Epoch [11349/20000], Training Loss: 0.5282\n",
            "Epoch [11350/20000], Training Loss: 0.4680\n",
            "Epoch [11351/20000], Training Loss: 0.4370\n",
            "Epoch [11352/20000], Training Loss: 0.4754\n",
            "Epoch [11353/20000], Training Loss: 0.4730\n",
            "Epoch [11354/20000], Training Loss: 0.4668\n",
            "Epoch [11355/20000], Training Loss: 0.4891\n",
            "Epoch [11356/20000], Training Loss: 0.4758\n",
            "Epoch [11357/20000], Training Loss: 0.4516\n",
            "Epoch [11358/20000], Training Loss: 0.4874\n",
            "Epoch [11359/20000], Training Loss: 0.4845\n",
            "Epoch [11360/20000], Training Loss: 0.4788\n",
            "Epoch [11361/20000], Training Loss: 0.4938\n",
            "Epoch [11362/20000], Training Loss: 0.5139\n",
            "Epoch [11363/20000], Training Loss: 0.4577\n",
            "Epoch [11364/20000], Training Loss: 0.4713\n",
            "Epoch [11365/20000], Training Loss: 0.5067\n",
            "Epoch [11366/20000], Training Loss: 0.4936\n",
            "Epoch [11367/20000], Training Loss: 0.4828\n",
            "Epoch [11368/20000], Training Loss: 0.5162\n",
            "Epoch [11369/20000], Training Loss: 0.5202\n",
            "Epoch [11370/20000], Training Loss: 0.4640\n",
            "Epoch [11371/20000], Training Loss: 0.4709\n",
            "Epoch [11372/20000], Training Loss: 0.4584\n",
            "Epoch [11373/20000], Training Loss: 0.5291\n",
            "Epoch [11374/20000], Training Loss: 0.4840\n",
            "Epoch [11375/20000], Training Loss: 0.5048\n",
            "Epoch [11376/20000], Training Loss: 0.4654\n",
            "Epoch [11377/20000], Training Loss: 0.4990\n",
            "Epoch [11378/20000], Training Loss: 0.4813\n",
            "Epoch [11379/20000], Training Loss: 0.4432\n",
            "Epoch [11380/20000], Training Loss: 0.4591\n",
            "Epoch [11381/20000], Training Loss: 0.5110\n",
            "Epoch [11382/20000], Training Loss: 0.4643\n",
            "Epoch [11383/20000], Training Loss: 0.4830\n",
            "Epoch [11384/20000], Training Loss: 0.4687\n",
            "Epoch [11385/20000], Training Loss: 0.4786\n",
            "Epoch [11386/20000], Training Loss: 0.4964\n",
            "Epoch [11387/20000], Training Loss: 0.4841\n",
            "Epoch [11388/20000], Training Loss: 0.4652\n",
            "Epoch [11389/20000], Training Loss: 0.5257\n",
            "Epoch [11390/20000], Training Loss: 0.4836\n",
            "Epoch [11391/20000], Training Loss: 0.4715\n",
            "Epoch [11392/20000], Training Loss: 0.4429\n",
            "Epoch [11393/20000], Training Loss: 0.4893\n",
            "Epoch [11394/20000], Training Loss: 0.4761\n",
            "Epoch [11395/20000], Training Loss: 0.4845\n",
            "Epoch [11396/20000], Training Loss: 0.4636\n",
            "Epoch [11397/20000], Training Loss: 0.4732\n",
            "Epoch [11398/20000], Training Loss: 0.5320\n",
            "Epoch [11399/20000], Training Loss: 0.4424\n",
            "Epoch [11400/20000], Training Loss: 0.4722\n",
            "Epoch [11401/20000], Training Loss: 0.4627\n",
            "Epoch [11402/20000], Training Loss: 0.4807\n",
            "Epoch [11403/20000], Training Loss: 0.4672\n",
            "Epoch [11404/20000], Training Loss: 0.5363\n",
            "Epoch [11405/20000], Training Loss: 0.4950\n",
            "Epoch [11406/20000], Training Loss: 0.4780\n",
            "Epoch [11407/20000], Training Loss: 0.4870\n",
            "Epoch [11408/20000], Training Loss: 0.4451\n",
            "Epoch [11409/20000], Training Loss: 0.4809\n",
            "Epoch [11410/20000], Training Loss: 0.4586\n",
            "Epoch [11411/20000], Training Loss: 0.4599\n",
            "Epoch [11412/20000], Training Loss: 0.4596\n",
            "Epoch [11413/20000], Training Loss: 0.4525\n",
            "Epoch [11414/20000], Training Loss: 0.5156\n",
            "Epoch [11415/20000], Training Loss: 0.5011\n",
            "Epoch [11416/20000], Training Loss: 0.4635\n",
            "Epoch [11417/20000], Training Loss: 0.4996\n",
            "Epoch [11418/20000], Training Loss: 0.4766\n",
            "Epoch [11419/20000], Training Loss: 0.4752\n",
            "Epoch [11420/20000], Training Loss: 0.4675\n",
            "Epoch [11421/20000], Training Loss: 0.4665\n",
            "Epoch [11422/20000], Training Loss: 0.5038\n",
            "Epoch [11423/20000], Training Loss: 0.4892\n",
            "Epoch [11424/20000], Training Loss: 0.4882\n",
            "Epoch [11425/20000], Training Loss: 0.4751\n",
            "Epoch [11426/20000], Training Loss: 0.4415\n",
            "Epoch [11427/20000], Training Loss: 0.4744\n",
            "Epoch [11428/20000], Training Loss: 0.4520\n",
            "Epoch [11429/20000], Training Loss: 0.5064\n",
            "Epoch [11430/20000], Training Loss: 0.4928\n",
            "Epoch [11431/20000], Training Loss: 0.4925\n",
            "Epoch [11432/20000], Training Loss: 0.5203\n",
            "Epoch [11433/20000], Training Loss: 0.4867\n",
            "Epoch [11434/20000], Training Loss: 0.4821\n",
            "Epoch [11435/20000], Training Loss: 0.5287\n",
            "Epoch [11436/20000], Training Loss: 0.5104\n",
            "Epoch [11437/20000], Training Loss: 0.4719\n",
            "Epoch [11438/20000], Training Loss: 0.4871\n",
            "Epoch [11439/20000], Training Loss: 0.4691\n",
            "Epoch [11440/20000], Training Loss: 0.5406\n",
            "Epoch [11441/20000], Training Loss: 0.4750\n",
            "Epoch [11442/20000], Training Loss: 0.4549\n",
            "Epoch [11443/20000], Training Loss: 0.4709\n",
            "Epoch [11444/20000], Training Loss: 0.4636\n",
            "Epoch [11445/20000], Training Loss: 0.4917\n",
            "Epoch [11446/20000], Training Loss: 0.4798\n",
            "Epoch [11447/20000], Training Loss: 0.4632\n",
            "Epoch [11448/20000], Training Loss: 0.4807\n",
            "Epoch [11449/20000], Training Loss: 0.5261\n",
            "Epoch [11450/20000], Training Loss: 0.4894\n",
            "Epoch [11451/20000], Training Loss: 0.5061\n",
            "Epoch [11452/20000], Training Loss: 0.4868\n",
            "Epoch [11453/20000], Training Loss: 0.5103\n",
            "Epoch [11454/20000], Training Loss: 0.4622\n",
            "Epoch [11455/20000], Training Loss: 0.5385\n",
            "Epoch [11456/20000], Training Loss: 0.4667\n",
            "Epoch [11457/20000], Training Loss: 0.4652\n",
            "Epoch [11458/20000], Training Loss: 0.5083\n",
            "Epoch [11459/20000], Training Loss: 0.5145\n",
            "Epoch [11460/20000], Training Loss: 0.4688\n",
            "Epoch [11461/20000], Training Loss: 0.4917\n",
            "Epoch [11462/20000], Training Loss: 0.4514\n",
            "Epoch [11463/20000], Training Loss: 0.5120\n",
            "Epoch [11464/20000], Training Loss: 0.4975\n",
            "Epoch [11465/20000], Training Loss: 0.4612\n",
            "Epoch [11466/20000], Training Loss: 0.4771\n",
            "Epoch [11467/20000], Training Loss: 0.5510\n",
            "Epoch [11468/20000], Training Loss: 0.4502\n",
            "Epoch [11469/20000], Training Loss: 0.5059\n",
            "Epoch [11470/20000], Training Loss: 0.5166\n",
            "Epoch [11471/20000], Training Loss: 0.4268\n",
            "Epoch [11472/20000], Training Loss: 0.4586\n",
            "Epoch [11473/20000], Training Loss: 0.4760\n",
            "Epoch [11474/20000], Training Loss: 0.5032\n",
            "Epoch [11475/20000], Training Loss: 0.4557\n",
            "Epoch [11476/20000], Training Loss: 0.4957\n",
            "Epoch [11477/20000], Training Loss: 0.5190\n",
            "Epoch [11478/20000], Training Loss: 0.4634\n",
            "Epoch [11479/20000], Training Loss: 0.5311\n",
            "Epoch [11480/20000], Training Loss: 0.5076\n",
            "Epoch [11481/20000], Training Loss: 0.4752\n",
            "Epoch [11482/20000], Training Loss: 0.4567\n",
            "Epoch [11483/20000], Training Loss: 0.5021\n",
            "Epoch [11484/20000], Training Loss: 0.4921\n",
            "Epoch [11485/20000], Training Loss: 0.4983\n",
            "Epoch [11486/20000], Training Loss: 0.4606\n",
            "Epoch [11487/20000], Training Loss: 0.5065\n",
            "Epoch [11488/20000], Training Loss: 0.4842\n",
            "Epoch [11489/20000], Training Loss: 0.4852\n",
            "Epoch [11490/20000], Training Loss: 0.4459\n",
            "Epoch [11491/20000], Training Loss: 0.4936\n",
            "Epoch [11492/20000], Training Loss: 0.5192\n",
            "Epoch [11493/20000], Training Loss: 0.4746\n",
            "Epoch [11494/20000], Training Loss: 0.4973\n",
            "Epoch [11495/20000], Training Loss: 0.5069\n",
            "Epoch [11496/20000], Training Loss: 0.4542\n",
            "Epoch [11497/20000], Training Loss: 0.4924\n",
            "Epoch [11498/20000], Training Loss: 0.5004\n",
            "Epoch [11499/20000], Training Loss: 0.5190\n",
            "Epoch [11500/20000], Training Loss: 0.4948\n",
            "Epoch [11501/20000], Training Loss: 0.5411\n",
            "Epoch [11502/20000], Training Loss: 0.4590\n",
            "Epoch [11503/20000], Training Loss: 0.4507\n",
            "Epoch [11504/20000], Training Loss: 0.4774\n",
            "Epoch [11505/20000], Training Loss: 0.4924\n",
            "Epoch [11506/20000], Training Loss: 0.4793\n",
            "Epoch [11507/20000], Training Loss: 0.4975\n",
            "Epoch [11508/20000], Training Loss: 0.4509\n",
            "Epoch [11509/20000], Training Loss: 0.4926\n",
            "Epoch [11510/20000], Training Loss: 0.4780\n",
            "Epoch [11511/20000], Training Loss: 0.5060\n",
            "Epoch [11512/20000], Training Loss: 0.4948\n",
            "Epoch [11513/20000], Training Loss: 0.4449\n",
            "Epoch [11514/20000], Training Loss: 0.4541\n",
            "Epoch [11515/20000], Training Loss: 0.5133\n",
            "Epoch [11516/20000], Training Loss: 0.4871\n",
            "Epoch [11517/20000], Training Loss: 0.4763\n",
            "Epoch [11518/20000], Training Loss: 0.4463\n",
            "Epoch [11519/20000], Training Loss: 0.4665\n",
            "Epoch [11520/20000], Training Loss: 0.4974\n",
            "Epoch [11521/20000], Training Loss: 0.4711\n",
            "Epoch [11522/20000], Training Loss: 0.5110\n",
            "Epoch [11523/20000], Training Loss: 0.4808\n",
            "Epoch [11524/20000], Training Loss: 0.5230\n",
            "Epoch [11525/20000], Training Loss: 0.4671\n",
            "Epoch [11526/20000], Training Loss: 0.5212\n",
            "Epoch [11527/20000], Training Loss: 0.4699\n",
            "Epoch [11528/20000], Training Loss: 0.4939\n",
            "Epoch [11529/20000], Training Loss: 0.5043\n",
            "Epoch [11530/20000], Training Loss: 0.5203\n",
            "Epoch [11531/20000], Training Loss: 0.4841\n",
            "Epoch [11532/20000], Training Loss: 0.4816\n",
            "Epoch [11533/20000], Training Loss: 0.4934\n",
            "Epoch [11534/20000], Training Loss: 0.4404\n",
            "Epoch [11535/20000], Training Loss: 0.4631\n",
            "Epoch [11536/20000], Training Loss: 0.4623\n",
            "Epoch [11537/20000], Training Loss: 0.5248\n",
            "Epoch [11538/20000], Training Loss: 0.4875\n",
            "Epoch [11539/20000], Training Loss: 0.4647\n",
            "Epoch [11540/20000], Training Loss: 0.5009\n",
            "Epoch [11541/20000], Training Loss: 0.5051\n",
            "Epoch [11542/20000], Training Loss: 0.4548\n",
            "Epoch [11543/20000], Training Loss: 0.4469\n",
            "Epoch [11544/20000], Training Loss: 0.4842\n",
            "Epoch [11545/20000], Training Loss: 0.5377\n",
            "Epoch [11546/20000], Training Loss: 0.4914\n",
            "Epoch [11547/20000], Training Loss: 0.5136\n",
            "Epoch [11548/20000], Training Loss: 0.4977\n",
            "Epoch [11549/20000], Training Loss: 0.4912\n",
            "Epoch [11550/20000], Training Loss: 0.4768\n",
            "Epoch [11551/20000], Training Loss: 0.4693\n",
            "Epoch [11552/20000], Training Loss: 0.4849\n",
            "Epoch [11553/20000], Training Loss: 0.4990\n",
            "Epoch [11554/20000], Training Loss: 0.4811\n",
            "Epoch [11555/20000], Training Loss: 0.4935\n",
            "Epoch [11556/20000], Training Loss: 0.4464\n",
            "Epoch [11557/20000], Training Loss: 0.4400\n",
            "Epoch [11558/20000], Training Loss: 0.4391\n",
            "Epoch [11559/20000], Training Loss: 0.4865\n",
            "Epoch [11560/20000], Training Loss: 0.4537\n",
            "Epoch [11561/20000], Training Loss: 0.5234\n",
            "Epoch [11562/20000], Training Loss: 0.4624\n",
            "Epoch [11563/20000], Training Loss: 0.4634\n",
            "Epoch [11564/20000], Training Loss: 0.4834\n",
            "Epoch [11565/20000], Training Loss: 0.5203\n",
            "Epoch [11566/20000], Training Loss: 0.5075\n",
            "Epoch [11567/20000], Training Loss: 0.5334\n",
            "Epoch [11568/20000], Training Loss: 0.4907\n",
            "Epoch [11569/20000], Training Loss: 0.4606\n",
            "Epoch [11570/20000], Training Loss: 0.5184\n",
            "Epoch [11571/20000], Training Loss: 0.4895\n",
            "Epoch [11572/20000], Training Loss: 0.4490\n",
            "Epoch [11573/20000], Training Loss: 0.5229\n",
            "Epoch [11574/20000], Training Loss: 0.4393\n",
            "Epoch [11575/20000], Training Loss: 0.5179\n",
            "Epoch [11576/20000], Training Loss: 0.4723\n",
            "Epoch [11577/20000], Training Loss: 0.4676\n",
            "Epoch [11578/20000], Training Loss: 0.4505\n",
            "Epoch [11579/20000], Training Loss: 0.4946\n",
            "Epoch [11580/20000], Training Loss: 0.4542\n",
            "Epoch [11581/20000], Training Loss: 0.5192\n",
            "Epoch [11582/20000], Training Loss: 0.4884\n",
            "Epoch [11583/20000], Training Loss: 0.4519\n",
            "Epoch [11584/20000], Training Loss: 0.4994\n",
            "Epoch [11585/20000], Training Loss: 0.4571\n",
            "Epoch [11586/20000], Training Loss: 0.4667\n",
            "Epoch [11587/20000], Training Loss: 0.4759\n",
            "Epoch [11588/20000], Training Loss: 0.4828\n",
            "Epoch [11589/20000], Training Loss: 0.4725\n",
            "Epoch [11590/20000], Training Loss: 0.5095\n",
            "Epoch [11591/20000], Training Loss: 0.4899\n",
            "Epoch [11592/20000], Training Loss: 0.5047\n",
            "Epoch [11593/20000], Training Loss: 0.4793\n",
            "Epoch [11594/20000], Training Loss: 0.4650\n",
            "Epoch [11595/20000], Training Loss: 0.5194\n",
            "Epoch [11596/20000], Training Loss: 0.4863\n",
            "Epoch [11597/20000], Training Loss: 0.4780\n",
            "Epoch [11598/20000], Training Loss: 0.4860\n",
            "Epoch [11599/20000], Training Loss: 0.4811\n",
            "Epoch [11600/20000], Training Loss: 0.4462\n",
            "Epoch [11601/20000], Training Loss: 0.4837\n",
            "Epoch [11602/20000], Training Loss: 0.4714\n",
            "Epoch [11603/20000], Training Loss: 0.5470\n",
            "Epoch [11604/20000], Training Loss: 0.4932\n",
            "Epoch [11605/20000], Training Loss: 0.4739\n",
            "Epoch [11606/20000], Training Loss: 0.4709\n",
            "Epoch [11607/20000], Training Loss: 0.4975\n",
            "Epoch [11608/20000], Training Loss: 0.4430\n",
            "Epoch [11609/20000], Training Loss: 0.5304\n",
            "Epoch [11610/20000], Training Loss: 0.5131\n",
            "Epoch [11611/20000], Training Loss: 0.4847\n",
            "Epoch [11612/20000], Training Loss: 0.5270\n",
            "Epoch [11613/20000], Training Loss: 0.4730\n",
            "Epoch [11614/20000], Training Loss: 0.4665\n",
            "Epoch [11615/20000], Training Loss: 0.5543\n",
            "Epoch [11616/20000], Training Loss: 0.5240\n",
            "Epoch [11617/20000], Training Loss: 0.4846\n",
            "Epoch [11618/20000], Training Loss: 0.4944\n",
            "Epoch [11619/20000], Training Loss: 0.5356\n",
            "Epoch [11620/20000], Training Loss: 0.5074\n",
            "Epoch [11621/20000], Training Loss: 0.5141\n",
            "Epoch [11622/20000], Training Loss: 0.4850\n",
            "Epoch [11623/20000], Training Loss: 0.5264\n",
            "Epoch [11624/20000], Training Loss: 0.5063\n",
            "Epoch [11625/20000], Training Loss: 0.5372\n",
            "Epoch [11626/20000], Training Loss: 0.4839\n",
            "Epoch [11627/20000], Training Loss: 0.4853\n",
            "Epoch [11628/20000], Training Loss: 0.4554\n",
            "Epoch [11629/20000], Training Loss: 0.4873\n",
            "Epoch [11630/20000], Training Loss: 0.4614\n",
            "Epoch [11631/20000], Training Loss: 0.5543\n",
            "Epoch [11632/20000], Training Loss: 0.4737\n",
            "Epoch [11633/20000], Training Loss: 0.5104\n",
            "Epoch [11634/20000], Training Loss: 0.4663\n",
            "Epoch [11635/20000], Training Loss: 0.4612\n",
            "Epoch [11636/20000], Training Loss: 0.4669\n",
            "Epoch [11637/20000], Training Loss: 0.4781\n",
            "Epoch [11638/20000], Training Loss: 0.4368\n",
            "Epoch [11639/20000], Training Loss: 0.4549\n",
            "Epoch [11640/20000], Training Loss: 0.4612\n",
            "Epoch [11641/20000], Training Loss: 0.5017\n",
            "Epoch [11642/20000], Training Loss: 0.4804\n",
            "Epoch [11643/20000], Training Loss: 0.4736\n",
            "Epoch [11644/20000], Training Loss: 0.5158\n",
            "Epoch [11645/20000], Training Loss: 0.4711\n",
            "Epoch [11646/20000], Training Loss: 0.4793\n",
            "Epoch [11647/20000], Training Loss: 0.4506\n",
            "Epoch [11648/20000], Training Loss: 0.5110\n",
            "Epoch [11649/20000], Training Loss: 0.4791\n",
            "Epoch [11650/20000], Training Loss: 0.5141\n",
            "Epoch [11651/20000], Training Loss: 0.4694\n",
            "Epoch [11652/20000], Training Loss: 0.4806\n",
            "Epoch [11653/20000], Training Loss: 0.4919\n",
            "Epoch [11654/20000], Training Loss: 0.4478\n",
            "Epoch [11655/20000], Training Loss: 0.4835\n",
            "Epoch [11656/20000], Training Loss: 0.4683\n",
            "Epoch [11657/20000], Training Loss: 0.5194\n",
            "Epoch [11658/20000], Training Loss: 0.5247\n",
            "Epoch [11659/20000], Training Loss: 0.5029\n",
            "Epoch [11660/20000], Training Loss: 0.4752\n",
            "Epoch [11661/20000], Training Loss: 0.5114\n",
            "Epoch [11662/20000], Training Loss: 0.4783\n",
            "Epoch [11663/20000], Training Loss: 0.5154\n",
            "Epoch [11664/20000], Training Loss: 0.4796\n",
            "Epoch [11665/20000], Training Loss: 0.4702\n",
            "Epoch [11666/20000], Training Loss: 0.4754\n",
            "Epoch [11667/20000], Training Loss: 0.5108\n",
            "Epoch [11668/20000], Training Loss: 0.4730\n",
            "Epoch [11669/20000], Training Loss: 0.4537\n",
            "Epoch [11670/20000], Training Loss: 0.5404\n",
            "Epoch [11671/20000], Training Loss: 0.4786\n",
            "Epoch [11672/20000], Training Loss: 0.4554\n",
            "Epoch [11673/20000], Training Loss: 0.4965\n",
            "Epoch [11674/20000], Training Loss: 0.5129\n",
            "Epoch [11675/20000], Training Loss: 0.4910\n",
            "Epoch [11676/20000], Training Loss: 0.5239\n",
            "Epoch [11677/20000], Training Loss: 0.4913\n",
            "Epoch [11678/20000], Training Loss: 0.5158\n",
            "Epoch [11679/20000], Training Loss: 0.5404\n",
            "Epoch [11680/20000], Training Loss: 0.4835\n",
            "Epoch [11681/20000], Training Loss: 0.4774\n",
            "Epoch [11682/20000], Training Loss: 0.4880\n",
            "Epoch [11683/20000], Training Loss: 0.4929\n",
            "Epoch [11684/20000], Training Loss: 0.4700\n",
            "Epoch [11685/20000], Training Loss: 0.5034\n",
            "Epoch [11686/20000], Training Loss: 0.5225\n",
            "Epoch [11687/20000], Training Loss: 0.4804\n",
            "Epoch [11688/20000], Training Loss: 0.5132\n",
            "Epoch [11689/20000], Training Loss: 0.5150\n",
            "Epoch [11690/20000], Training Loss: 0.5253\n",
            "Epoch [11691/20000], Training Loss: 0.4790\n",
            "Epoch [11692/20000], Training Loss: 0.4840\n",
            "Epoch [11693/20000], Training Loss: 0.5115\n",
            "Epoch [11694/20000], Training Loss: 0.4996\n",
            "Epoch [11695/20000], Training Loss: 0.5090\n",
            "Epoch [11696/20000], Training Loss: 0.4864\n",
            "Epoch [11697/20000], Training Loss: 0.4790\n",
            "Epoch [11698/20000], Training Loss: 0.4643\n",
            "Epoch [11699/20000], Training Loss: 0.5176\n",
            "Epoch [11700/20000], Training Loss: 0.5226\n",
            "Epoch [11701/20000], Training Loss: 0.5037\n",
            "Epoch [11702/20000], Training Loss: 0.4771\n",
            "Epoch [11703/20000], Training Loss: 0.4937\n",
            "Epoch [11704/20000], Training Loss: 0.4841\n",
            "Epoch [11705/20000], Training Loss: 0.5004\n",
            "Epoch [11706/20000], Training Loss: 0.4861\n",
            "Epoch [11707/20000], Training Loss: 0.4558\n",
            "Epoch [11708/20000], Training Loss: 0.4982\n",
            "Epoch [11709/20000], Training Loss: 0.4783\n",
            "Epoch [11710/20000], Training Loss: 0.5201\n",
            "Epoch [11711/20000], Training Loss: 0.5212\n",
            "Epoch [11712/20000], Training Loss: 0.4975\n",
            "Epoch [11713/20000], Training Loss: 0.5469\n",
            "Epoch [11714/20000], Training Loss: 0.4694\n",
            "Epoch [11715/20000], Training Loss: 0.5327\n",
            "Epoch [11716/20000], Training Loss: 0.4808\n",
            "Epoch [11717/20000], Training Loss: 0.4899\n",
            "Epoch [11718/20000], Training Loss: 0.4956\n",
            "Epoch [11719/20000], Training Loss: 0.4889\n",
            "Epoch [11720/20000], Training Loss: 0.4970\n",
            "Epoch [11721/20000], Training Loss: 0.4451\n",
            "Epoch [11722/20000], Training Loss: 0.5235\n",
            "Epoch [11723/20000], Training Loss: 0.5061\n",
            "Epoch [11724/20000], Training Loss: 0.4972\n",
            "Epoch [11725/20000], Training Loss: 0.4598\n",
            "Epoch [11726/20000], Training Loss: 0.4646\n",
            "Epoch [11727/20000], Training Loss: 0.4503\n",
            "Epoch [11728/20000], Training Loss: 0.5043\n",
            "Epoch [11729/20000], Training Loss: 0.4776\n",
            "Epoch [11730/20000], Training Loss: 0.4473\n",
            "Epoch [11731/20000], Training Loss: 0.4842\n",
            "Epoch [11732/20000], Training Loss: 0.4669\n",
            "Epoch [11733/20000], Training Loss: 0.4774\n",
            "Epoch [11734/20000], Training Loss: 0.4793\n",
            "Epoch [11735/20000], Training Loss: 0.4219\n",
            "Epoch [11736/20000], Training Loss: 0.4738\n",
            "Epoch [11737/20000], Training Loss: 0.5300\n",
            "Epoch [11738/20000], Training Loss: 0.5301\n",
            "Epoch [11739/20000], Training Loss: 0.5196\n",
            "Epoch [11740/20000], Training Loss: 0.4551\n",
            "Epoch [11741/20000], Training Loss: 0.5330\n",
            "Epoch [11742/20000], Training Loss: 0.4739\n",
            "Epoch [11743/20000], Training Loss: 0.4467\n",
            "Epoch [11744/20000], Training Loss: 0.4935\n",
            "Epoch [11745/20000], Training Loss: 0.5332\n",
            "Epoch [11746/20000], Training Loss: 0.5032\n",
            "Epoch [11747/20000], Training Loss: 0.5333\n",
            "Epoch [11748/20000], Training Loss: 0.4772\n",
            "Epoch [11749/20000], Training Loss: 0.5071\n",
            "Epoch [11750/20000], Training Loss: 0.4930\n",
            "Epoch [11751/20000], Training Loss: 0.5169\n",
            "Epoch [11752/20000], Training Loss: 0.5180\n",
            "Epoch [11753/20000], Training Loss: 0.4938\n",
            "Epoch [11754/20000], Training Loss: 0.4778\n",
            "Epoch [11755/20000], Training Loss: 0.5156\n",
            "Epoch [11756/20000], Training Loss: 0.4982\n",
            "Epoch [11757/20000], Training Loss: 0.4889\n",
            "Epoch [11758/20000], Training Loss: 0.4650\n",
            "Epoch [11759/20000], Training Loss: 0.5395\n",
            "Epoch [11760/20000], Training Loss: 0.4771\n",
            "Epoch [11761/20000], Training Loss: 0.5196\n",
            "Epoch [11762/20000], Training Loss: 0.4535\n",
            "Epoch [11763/20000], Training Loss: 0.5008\n",
            "Epoch [11764/20000], Training Loss: 0.5057\n",
            "Epoch [11765/20000], Training Loss: 0.4646\n",
            "Epoch [11766/20000], Training Loss: 0.4808\n",
            "Epoch [11767/20000], Training Loss: 0.4582\n",
            "Epoch [11768/20000], Training Loss: 0.5177\n",
            "Epoch [11769/20000], Training Loss: 0.4891\n",
            "Epoch [11770/20000], Training Loss: 0.4774\n",
            "Epoch [11771/20000], Training Loss: 0.5253\n",
            "Epoch [11772/20000], Training Loss: 0.4973\n",
            "Epoch [11773/20000], Training Loss: 0.4999\n",
            "Epoch [11774/20000], Training Loss: 0.4498\n",
            "Epoch [11775/20000], Training Loss: 0.5125\n",
            "Epoch [11776/20000], Training Loss: 0.5089\n",
            "Epoch [11777/20000], Training Loss: 0.4497\n",
            "Epoch [11778/20000], Training Loss: 0.4580\n",
            "Epoch [11779/20000], Training Loss: 0.5104\n",
            "Epoch [11780/20000], Training Loss: 0.4803\n",
            "Epoch [11781/20000], Training Loss: 0.5242\n",
            "Epoch [11782/20000], Training Loss: 0.5142\n",
            "Epoch [11783/20000], Training Loss: 0.4679\n",
            "Epoch [11784/20000], Training Loss: 0.4977\n",
            "Epoch [11785/20000], Training Loss: 0.4912\n",
            "Epoch [11786/20000], Training Loss: 0.5026\n",
            "Epoch [11787/20000], Training Loss: 0.4726\n",
            "Epoch [11788/20000], Training Loss: 0.5024\n",
            "Epoch [11789/20000], Training Loss: 0.5318\n",
            "Epoch [11790/20000], Training Loss: 0.5069\n",
            "Epoch [11791/20000], Training Loss: 0.5232\n",
            "Epoch [11792/20000], Training Loss: 0.5272\n",
            "Epoch [11793/20000], Training Loss: 0.4944\n",
            "Epoch [11794/20000], Training Loss: 0.4703\n",
            "Epoch [11795/20000], Training Loss: 0.4955\n",
            "Epoch [11796/20000], Training Loss: 0.4453\n",
            "Epoch [11797/20000], Training Loss: 0.4762\n",
            "Epoch [11798/20000], Training Loss: 0.5300\n",
            "Epoch [11799/20000], Training Loss: 0.4703\n",
            "Epoch [11800/20000], Training Loss: 0.4849\n",
            "Epoch [11801/20000], Training Loss: 0.4909\n",
            "Epoch [11802/20000], Training Loss: 0.4663\n",
            "Epoch [11803/20000], Training Loss: 0.5165\n",
            "Epoch [11804/20000], Training Loss: 0.4714\n",
            "Epoch [11805/20000], Training Loss: 0.4612\n",
            "Epoch [11806/20000], Training Loss: 0.5126\n",
            "Epoch [11807/20000], Training Loss: 0.4808\n",
            "Epoch [11808/20000], Training Loss: 0.5056\n",
            "Epoch [11809/20000], Training Loss: 0.5211\n",
            "Epoch [11810/20000], Training Loss: 0.4770\n",
            "Epoch [11811/20000], Training Loss: 0.5157\n",
            "Epoch [11812/20000], Training Loss: 0.4752\n",
            "Epoch [11813/20000], Training Loss: 0.4593\n",
            "Epoch [11814/20000], Training Loss: 0.4871\n",
            "Epoch [11815/20000], Training Loss: 0.5008\n",
            "Epoch [11816/20000], Training Loss: 0.5122\n",
            "Epoch [11817/20000], Training Loss: 0.4882\n",
            "Epoch [11818/20000], Training Loss: 0.4698\n",
            "Epoch [11819/20000], Training Loss: 0.4548\n",
            "Epoch [11820/20000], Training Loss: 0.5124\n",
            "Epoch [11821/20000], Training Loss: 0.4927\n",
            "Epoch [11822/20000], Training Loss: 0.4718\n",
            "Epoch [11823/20000], Training Loss: 0.5297\n",
            "Epoch [11824/20000], Training Loss: 0.5110\n",
            "Epoch [11825/20000], Training Loss: 0.4606\n",
            "Epoch [11826/20000], Training Loss: 0.4539\n",
            "Epoch [11827/20000], Training Loss: 0.4846\n",
            "Epoch [11828/20000], Training Loss: 0.4801\n",
            "Epoch [11829/20000], Training Loss: 0.4987\n",
            "Epoch [11830/20000], Training Loss: 0.4967\n",
            "Epoch [11831/20000], Training Loss: 0.4560\n",
            "Epoch [11832/20000], Training Loss: 0.4646\n",
            "Epoch [11833/20000], Training Loss: 0.5062\n",
            "Epoch [11834/20000], Training Loss: 0.4724\n",
            "Epoch [11835/20000], Training Loss: 0.5090\n",
            "Epoch [11836/20000], Training Loss: 0.4828\n",
            "Epoch [11837/20000], Training Loss: 0.4545\n",
            "Epoch [11838/20000], Training Loss: 0.4997\n",
            "Epoch [11839/20000], Training Loss: 0.4676\n",
            "Epoch [11840/20000], Training Loss: 0.4759\n",
            "Epoch [11841/20000], Training Loss: 0.4836\n",
            "Epoch [11842/20000], Training Loss: 0.5489\n",
            "Epoch [11843/20000], Training Loss: 0.4983\n",
            "Epoch [11844/20000], Training Loss: 0.4442\n",
            "Epoch [11845/20000], Training Loss: 0.5071\n",
            "Epoch [11846/20000], Training Loss: 0.5020\n",
            "Epoch [11847/20000], Training Loss: 0.5167\n",
            "Epoch [11848/20000], Training Loss: 0.4606\n",
            "Epoch [11849/20000], Training Loss: 0.5125\n",
            "Epoch [11850/20000], Training Loss: 0.4772\n",
            "Epoch [11851/20000], Training Loss: 0.5056\n",
            "Epoch [11852/20000], Training Loss: 0.5083\n",
            "Epoch [11853/20000], Training Loss: 0.5052\n",
            "Epoch [11854/20000], Training Loss: 0.4480\n",
            "Epoch [11855/20000], Training Loss: 0.4535\n",
            "Epoch [11856/20000], Training Loss: 0.4705\n",
            "Epoch [11857/20000], Training Loss: 0.4926\n",
            "Epoch [11858/20000], Training Loss: 0.4643\n",
            "Epoch [11859/20000], Training Loss: 0.4812\n",
            "Epoch [11860/20000], Training Loss: 0.4946\n",
            "Epoch [11861/20000], Training Loss: 0.4704\n",
            "Epoch [11862/20000], Training Loss: 0.4969\n",
            "Epoch [11863/20000], Training Loss: 0.5184\n",
            "Epoch [11864/20000], Training Loss: 0.4738\n",
            "Epoch [11865/20000], Training Loss: 0.4892\n",
            "Epoch [11866/20000], Training Loss: 0.4646\n",
            "Epoch [11867/20000], Training Loss: 0.5034\n",
            "Epoch [11868/20000], Training Loss: 0.4708\n",
            "Epoch [11869/20000], Training Loss: 0.4890\n",
            "Epoch [11870/20000], Training Loss: 0.5055\n",
            "Epoch [11871/20000], Training Loss: 0.5348\n",
            "Epoch [11872/20000], Training Loss: 0.4605\n",
            "Epoch [11873/20000], Training Loss: 0.5319\n",
            "Epoch [11874/20000], Training Loss: 0.5198\n",
            "Epoch [11875/20000], Training Loss: 0.4447\n",
            "Epoch [11876/20000], Training Loss: 0.4777\n",
            "Epoch [11877/20000], Training Loss: 0.4824\n",
            "Epoch [11878/20000], Training Loss: 0.4916\n",
            "Epoch [11879/20000], Training Loss: 0.4728\n",
            "Epoch [11880/20000], Training Loss: 0.4815\n",
            "Epoch [11881/20000], Training Loss: 0.4516\n",
            "Epoch [11882/20000], Training Loss: 0.5317\n",
            "Epoch [11883/20000], Training Loss: 0.4554\n",
            "Epoch [11884/20000], Training Loss: 0.5066\n",
            "Epoch [11885/20000], Training Loss: 0.4990\n",
            "Epoch [11886/20000], Training Loss: 0.4617\n",
            "Epoch [11887/20000], Training Loss: 0.4612\n",
            "Epoch [11888/20000], Training Loss: 0.5008\n",
            "Epoch [11889/20000], Training Loss: 0.4767\n",
            "Epoch [11890/20000], Training Loss: 0.5384\n",
            "Epoch [11891/20000], Training Loss: 0.4488\n",
            "Epoch [11892/20000], Training Loss: 0.5204\n",
            "Epoch [11893/20000], Training Loss: 0.5131\n",
            "Epoch [11894/20000], Training Loss: 0.4950\n",
            "Epoch [11895/20000], Training Loss: 0.5455\n",
            "Epoch [11896/20000], Training Loss: 0.4585\n",
            "Epoch [11897/20000], Training Loss: 0.5316\n",
            "Epoch [11898/20000], Training Loss: 0.4948\n",
            "Epoch [11899/20000], Training Loss: 0.4985\n",
            "Epoch [11900/20000], Training Loss: 0.4696\n",
            "Epoch [11901/20000], Training Loss: 0.4424\n",
            "Epoch [11902/20000], Training Loss: 0.4664\n",
            "Epoch [11903/20000], Training Loss: 0.5028\n",
            "Epoch [11904/20000], Training Loss: 0.4710\n",
            "Epoch [11905/20000], Training Loss: 0.4939\n",
            "Epoch [11906/20000], Training Loss: 0.5145\n",
            "Epoch [11907/20000], Training Loss: 0.5134\n",
            "Epoch [11908/20000], Training Loss: 0.5085\n",
            "Epoch [11909/20000], Training Loss: 0.4616\n",
            "Epoch [11910/20000], Training Loss: 0.5000\n",
            "Epoch [11911/20000], Training Loss: 0.4714\n",
            "Epoch [11912/20000], Training Loss: 0.4634\n",
            "Epoch [11913/20000], Training Loss: 0.4609\n",
            "Epoch [11914/20000], Training Loss: 0.4990\n",
            "Epoch [11915/20000], Training Loss: 0.4620\n",
            "Epoch [11916/20000], Training Loss: 0.5233\n",
            "Epoch [11917/20000], Training Loss: 0.4494\n",
            "Epoch [11918/20000], Training Loss: 0.4414\n",
            "Epoch [11919/20000], Training Loss: 0.4612\n",
            "Epoch [11920/20000], Training Loss: 0.4941\n",
            "Epoch [11921/20000], Training Loss: 0.4700\n",
            "Epoch [11922/20000], Training Loss: 0.4572\n",
            "Epoch [11923/20000], Training Loss: 0.4766\n",
            "Epoch [11924/20000], Training Loss: 0.4506\n",
            "Epoch [11925/20000], Training Loss: 0.5344\n",
            "Epoch [11926/20000], Training Loss: 0.5065\n",
            "Epoch [11927/20000], Training Loss: 0.4305\n",
            "Epoch [11928/20000], Training Loss: 0.5286\n",
            "Epoch [11929/20000], Training Loss: 0.4606\n",
            "Epoch [11930/20000], Training Loss: 0.4825\n",
            "Epoch [11931/20000], Training Loss: 0.4718\n",
            "Epoch [11932/20000], Training Loss: 0.4959\n",
            "Epoch [11933/20000], Training Loss: 0.4676\n",
            "Epoch [11934/20000], Training Loss: 0.5105\n",
            "Epoch [11935/20000], Training Loss: 0.4881\n",
            "Epoch [11936/20000], Training Loss: 0.4591\n",
            "Epoch [11937/20000], Training Loss: 0.4597\n",
            "Epoch [11938/20000], Training Loss: 0.4813\n",
            "Epoch [11939/20000], Training Loss: 0.4825\n",
            "Epoch [11940/20000], Training Loss: 0.4666\n",
            "Epoch [11941/20000], Training Loss: 0.5234\n",
            "Epoch [11942/20000], Training Loss: 0.4898\n",
            "Epoch [11943/20000], Training Loss: 0.5023\n",
            "Epoch [11944/20000], Training Loss: 0.4998\n",
            "Epoch [11945/20000], Training Loss: 0.4886\n",
            "Epoch [11946/20000], Training Loss: 0.5215\n",
            "Epoch [11947/20000], Training Loss: 0.4497\n",
            "Epoch [11948/20000], Training Loss: 0.4761\n",
            "Epoch [11949/20000], Training Loss: 0.4577\n",
            "Epoch [11950/20000], Training Loss: 0.5232\n",
            "Epoch [11951/20000], Training Loss: 0.5194\n",
            "Epoch [11952/20000], Training Loss: 0.5080\n",
            "Epoch [11953/20000], Training Loss: 0.5010\n",
            "Epoch [11954/20000], Training Loss: 0.5032\n",
            "Epoch [11955/20000], Training Loss: 0.4877\n",
            "Epoch [11956/20000], Training Loss: 0.4529\n",
            "Epoch [11957/20000], Training Loss: 0.4633\n",
            "Epoch [11958/20000], Training Loss: 0.4546\n",
            "Epoch [11959/20000], Training Loss: 0.4322\n",
            "Epoch [11960/20000], Training Loss: 0.4947\n",
            "Epoch [11961/20000], Training Loss: 0.4352\n",
            "Epoch [11962/20000], Training Loss: 0.4783\n",
            "Epoch [11963/20000], Training Loss: 0.4712\n",
            "Epoch [11964/20000], Training Loss: 0.4714\n",
            "Epoch [11965/20000], Training Loss: 0.4626\n",
            "Epoch [11966/20000], Training Loss: 0.4870\n",
            "Epoch [11967/20000], Training Loss: 0.4979\n",
            "Epoch [11968/20000], Training Loss: 0.5064\n",
            "Epoch [11969/20000], Training Loss: 0.4991\n",
            "Epoch [11970/20000], Training Loss: 0.4961\n",
            "Epoch [11971/20000], Training Loss: 0.5238\n",
            "Epoch [11972/20000], Training Loss: 0.4586\n",
            "Epoch [11973/20000], Training Loss: 0.4930\n",
            "Epoch [11974/20000], Training Loss: 0.5015\n",
            "Epoch [11975/20000], Training Loss: 0.4534\n",
            "Epoch [11976/20000], Training Loss: 0.5259\n",
            "Epoch [11977/20000], Training Loss: 0.4947\n",
            "Epoch [11978/20000], Training Loss: 0.4968\n",
            "Epoch [11979/20000], Training Loss: 0.4570\n",
            "Epoch [11980/20000], Training Loss: 0.4695\n",
            "Epoch [11981/20000], Training Loss: 0.4944\n",
            "Epoch [11982/20000], Training Loss: 0.5204\n",
            "Epoch [11983/20000], Training Loss: 0.4761\n",
            "Epoch [11984/20000], Training Loss: 0.4902\n",
            "Epoch [11985/20000], Training Loss: 0.4310\n",
            "Epoch [11986/20000], Training Loss: 0.4475\n",
            "Epoch [11987/20000], Training Loss: 0.4810\n",
            "Epoch [11988/20000], Training Loss: 0.4809\n",
            "Epoch [11989/20000], Training Loss: 0.4351\n",
            "Epoch [11990/20000], Training Loss: 0.4675\n",
            "Epoch [11991/20000], Training Loss: 0.4522\n",
            "Epoch [11992/20000], Training Loss: 0.5277\n",
            "Epoch [11993/20000], Training Loss: 0.5272\n",
            "Epoch [11994/20000], Training Loss: 0.5082\n",
            "Epoch [11995/20000], Training Loss: 0.5046\n",
            "Epoch [11996/20000], Training Loss: 0.5071\n",
            "Epoch [11997/20000], Training Loss: 0.4995\n",
            "Epoch [11998/20000], Training Loss: 0.4649\n",
            "Epoch [11999/20000], Training Loss: 0.4894\n",
            "Epoch [12000/20000], Training Loss: 0.5075\n",
            "Epoch [12001/20000], Training Loss: 0.5105\n",
            "Epoch [12002/20000], Training Loss: 0.4690\n",
            "Epoch [12003/20000], Training Loss: 0.4776\n",
            "Epoch [12004/20000], Training Loss: 0.4855\n",
            "Epoch [12005/20000], Training Loss: 0.4995\n",
            "Epoch [12006/20000], Training Loss: 0.5278\n",
            "Epoch [12007/20000], Training Loss: 0.5057\n",
            "Epoch [12008/20000], Training Loss: 0.5095\n",
            "Epoch [12009/20000], Training Loss: 0.4776\n",
            "Epoch [12010/20000], Training Loss: 0.5150\n",
            "Epoch [12011/20000], Training Loss: 0.4838\n",
            "Epoch [12012/20000], Training Loss: 0.5098\n",
            "Epoch [12013/20000], Training Loss: 0.4669\n",
            "Epoch [12014/20000], Training Loss: 0.4561\n",
            "Epoch [12015/20000], Training Loss: 0.4618\n",
            "Epoch [12016/20000], Training Loss: 0.5097\n",
            "Epoch [12017/20000], Training Loss: 0.4960\n",
            "Epoch [12018/20000], Training Loss: 0.4238\n",
            "Epoch [12019/20000], Training Loss: 0.4846\n",
            "Epoch [12020/20000], Training Loss: 0.4709\n",
            "Epoch [12021/20000], Training Loss: 0.5054\n",
            "Epoch [12022/20000], Training Loss: 0.4446\n",
            "Epoch [12023/20000], Training Loss: 0.4960\n",
            "Epoch [12024/20000], Training Loss: 0.5002\n",
            "Epoch [12025/20000], Training Loss: 0.4852\n",
            "Epoch [12026/20000], Training Loss: 0.5228\n",
            "Epoch [12027/20000], Training Loss: 0.4599\n",
            "Epoch [12028/20000], Training Loss: 0.4494\n",
            "Epoch [12029/20000], Training Loss: 0.4652\n",
            "Epoch [12030/20000], Training Loss: 0.4736\n",
            "Epoch [12031/20000], Training Loss: 0.4968\n",
            "Epoch [12032/20000], Training Loss: 0.5396\n",
            "Epoch [12033/20000], Training Loss: 0.5133\n",
            "Epoch [12034/20000], Training Loss: 0.4832\n",
            "Epoch [12035/20000], Training Loss: 0.4766\n",
            "Epoch [12036/20000], Training Loss: 0.4691\n",
            "Epoch [12037/20000], Training Loss: 0.4802\n",
            "Epoch [12038/20000], Training Loss: 0.5202\n",
            "Epoch [12039/20000], Training Loss: 0.5187\n",
            "Epoch [12040/20000], Training Loss: 0.4709\n",
            "Epoch [12041/20000], Training Loss: 0.5125\n",
            "Epoch [12042/20000], Training Loss: 0.4684\n",
            "Epoch [12043/20000], Training Loss: 0.4902\n",
            "Epoch [12044/20000], Training Loss: 0.4667\n",
            "Epoch [12045/20000], Training Loss: 0.5108\n",
            "Epoch [12046/20000], Training Loss: 0.5050\n",
            "Epoch [12047/20000], Training Loss: 0.5084\n",
            "Epoch [12048/20000], Training Loss: 0.4688\n",
            "Epoch [12049/20000], Training Loss: 0.5264\n",
            "Epoch [12050/20000], Training Loss: 0.4707\n",
            "Epoch [12051/20000], Training Loss: 0.4645\n",
            "Epoch [12052/20000], Training Loss: 0.4920\n",
            "Epoch [12053/20000], Training Loss: 0.4821\n",
            "Epoch [12054/20000], Training Loss: 0.4787\n",
            "Epoch [12055/20000], Training Loss: 0.5085\n",
            "Epoch [12056/20000], Training Loss: 0.4690\n",
            "Epoch [12057/20000], Training Loss: 0.5087\n",
            "Epoch [12058/20000], Training Loss: 0.4561\n",
            "Epoch [12059/20000], Training Loss: 0.4612\n",
            "Epoch [12060/20000], Training Loss: 0.4751\n",
            "Epoch [12061/20000], Training Loss: 0.5050\n",
            "Epoch [12062/20000], Training Loss: 0.4898\n",
            "Epoch [12063/20000], Training Loss: 0.5069\n",
            "Epoch [12064/20000], Training Loss: 0.4685\n",
            "Epoch [12065/20000], Training Loss: 0.4906\n",
            "Epoch [12066/20000], Training Loss: 0.5354\n",
            "Epoch [12067/20000], Training Loss: 0.5148\n",
            "Epoch [12068/20000], Training Loss: 0.4876\n",
            "Epoch [12069/20000], Training Loss: 0.4772\n",
            "Epoch [12070/20000], Training Loss: 0.5017\n",
            "Epoch [12071/20000], Training Loss: 0.4636\n",
            "Epoch [12072/20000], Training Loss: 0.5082\n",
            "Epoch [12073/20000], Training Loss: 0.4513\n",
            "Epoch [12074/20000], Training Loss: 0.4850\n",
            "Epoch [12075/20000], Training Loss: 0.5201\n",
            "Epoch [12076/20000], Training Loss: 0.5431\n",
            "Epoch [12077/20000], Training Loss: 0.4749\n",
            "Epoch [12078/20000], Training Loss: 0.4840\n",
            "Epoch [12079/20000], Training Loss: 0.4400\n",
            "Epoch [12080/20000], Training Loss: 0.4660\n",
            "Epoch [12081/20000], Training Loss: 0.5066\n",
            "Epoch [12082/20000], Training Loss: 0.5034\n",
            "Epoch [12083/20000], Training Loss: 0.5104\n",
            "Epoch [12084/20000], Training Loss: 0.4819\n",
            "Epoch [12085/20000], Training Loss: 0.4685\n",
            "Epoch [12086/20000], Training Loss: 0.5022\n",
            "Epoch [12087/20000], Training Loss: 0.4681\n",
            "Epoch [12088/20000], Training Loss: 0.4898\n",
            "Epoch [12089/20000], Training Loss: 0.4626\n",
            "Epoch [12090/20000], Training Loss: 0.4762\n",
            "Epoch [12091/20000], Training Loss: 0.4479\n",
            "Epoch [12092/20000], Training Loss: 0.5227\n",
            "Epoch [12093/20000], Training Loss: 0.4656\n",
            "Epoch [12094/20000], Training Loss: 0.5292\n",
            "Epoch [12095/20000], Training Loss: 0.4408\n",
            "Epoch [12096/20000], Training Loss: 0.4745\n",
            "Epoch [12097/20000], Training Loss: 0.4723\n",
            "Epoch [12098/20000], Training Loss: 0.4662\n",
            "Epoch [12099/20000], Training Loss: 0.5260\n",
            "Epoch [12100/20000], Training Loss: 0.4655\n",
            "Epoch [12101/20000], Training Loss: 0.5006\n",
            "Epoch [12102/20000], Training Loss: 0.5463\n",
            "Epoch [12103/20000], Training Loss: 0.5112\n",
            "Epoch [12104/20000], Training Loss: 0.4641\n",
            "Epoch [12105/20000], Training Loss: 0.5007\n",
            "Epoch [12106/20000], Training Loss: 0.5029\n",
            "Epoch [12107/20000], Training Loss: 0.4630\n",
            "Epoch [12108/20000], Training Loss: 0.4687\n",
            "Epoch [12109/20000], Training Loss: 0.4879\n",
            "Epoch [12110/20000], Training Loss: 0.4820\n",
            "Epoch [12111/20000], Training Loss: 0.4637\n",
            "Epoch [12112/20000], Training Loss: 0.4707\n",
            "Epoch [12113/20000], Training Loss: 0.4940\n",
            "Epoch [12114/20000], Training Loss: 0.4888\n",
            "Epoch [12115/20000], Training Loss: 0.4471\n",
            "Epoch [12116/20000], Training Loss: 0.5120\n",
            "Epoch [12117/20000], Training Loss: 0.4758\n",
            "Epoch [12118/20000], Training Loss: 0.4795\n",
            "Epoch [12119/20000], Training Loss: 0.5028\n",
            "Epoch [12120/20000], Training Loss: 0.4747\n",
            "Epoch [12121/20000], Training Loss: 0.4889\n",
            "Epoch [12122/20000], Training Loss: 0.4450\n",
            "Epoch [12123/20000], Training Loss: 0.4722\n",
            "Epoch [12124/20000], Training Loss: 0.4862\n",
            "Epoch [12125/20000], Training Loss: 0.5031\n",
            "Epoch [12126/20000], Training Loss: 0.5151\n",
            "Epoch [12127/20000], Training Loss: 0.4716\n",
            "Epoch [12128/20000], Training Loss: 0.4547\n",
            "Epoch [12129/20000], Training Loss: 0.5272\n",
            "Epoch [12130/20000], Training Loss: 0.4730\n",
            "Epoch [12131/20000], Training Loss: 0.4865\n",
            "Epoch [12132/20000], Training Loss: 0.4661\n",
            "Epoch [12133/20000], Training Loss: 0.5305\n",
            "Epoch [12134/20000], Training Loss: 0.5242\n",
            "Epoch [12135/20000], Training Loss: 0.4846\n",
            "Epoch [12136/20000], Training Loss: 0.5534\n",
            "Epoch [12137/20000], Training Loss: 0.4770\n",
            "Epoch [12138/20000], Training Loss: 0.5040\n",
            "Epoch [12139/20000], Training Loss: 0.4500\n",
            "Epoch [12140/20000], Training Loss: 0.5268\n",
            "Epoch [12141/20000], Training Loss: 0.4529\n",
            "Epoch [12142/20000], Training Loss: 0.5271\n",
            "Epoch [12143/20000], Training Loss: 0.4396\n",
            "Epoch [12144/20000], Training Loss: 0.4798\n",
            "Epoch [12145/20000], Training Loss: 0.4562\n",
            "Epoch [12146/20000], Training Loss: 0.4702\n",
            "Epoch [12147/20000], Training Loss: 0.4948\n",
            "Epoch [12148/20000], Training Loss: 0.4770\n",
            "Epoch [12149/20000], Training Loss: 0.5077\n",
            "Epoch [12150/20000], Training Loss: 0.5433\n",
            "Epoch [12151/20000], Training Loss: 0.4475\n",
            "Epoch [12152/20000], Training Loss: 0.5096\n",
            "Epoch [12153/20000], Training Loss: 0.4715\n",
            "Epoch [12154/20000], Training Loss: 0.4469\n",
            "Epoch [12155/20000], Training Loss: 0.4700\n",
            "Epoch [12156/20000], Training Loss: 0.4437\n",
            "Epoch [12157/20000], Training Loss: 0.5003\n",
            "Epoch [12158/20000], Training Loss: 0.5159\n",
            "Epoch [12159/20000], Training Loss: 0.4459\n",
            "Epoch [12160/20000], Training Loss: 0.4344\n",
            "Epoch [12161/20000], Training Loss: 0.4666\n",
            "Epoch [12162/20000], Training Loss: 0.4839\n",
            "Epoch [12163/20000], Training Loss: 0.4835\n",
            "Epoch [12164/20000], Training Loss: 0.4482\n",
            "Epoch [12165/20000], Training Loss: 0.4664\n",
            "Epoch [12166/20000], Training Loss: 0.4649\n",
            "Epoch [12167/20000], Training Loss: 0.5407\n",
            "Epoch [12168/20000], Training Loss: 0.4482\n",
            "Epoch [12169/20000], Training Loss: 0.4941\n",
            "Epoch [12170/20000], Training Loss: 0.4876\n",
            "Epoch [12171/20000], Training Loss: 0.4588\n",
            "Epoch [12172/20000], Training Loss: 0.4954\n",
            "Epoch [12173/20000], Training Loss: 0.4635\n",
            "Epoch [12174/20000], Training Loss: 0.4824\n",
            "Epoch [12175/20000], Training Loss: 0.4553\n",
            "Epoch [12176/20000], Training Loss: 0.5242\n",
            "Epoch [12177/20000], Training Loss: 0.5032\n",
            "Epoch [12178/20000], Training Loss: 0.4864\n",
            "Epoch [12179/20000], Training Loss: 0.4691\n",
            "Epoch [12180/20000], Training Loss: 0.4748\n",
            "Epoch [12181/20000], Training Loss: 0.4859\n",
            "Epoch [12182/20000], Training Loss: 0.4694\n",
            "Epoch [12183/20000], Training Loss: 0.4861\n",
            "Epoch [12184/20000], Training Loss: 0.4948\n",
            "Epoch [12185/20000], Training Loss: 0.4336\n",
            "Epoch [12186/20000], Training Loss: 0.4531\n",
            "Epoch [12187/20000], Training Loss: 0.4750\n",
            "Epoch [12188/20000], Training Loss: 0.5096\n",
            "Epoch [12189/20000], Training Loss: 0.4693\n",
            "Epoch [12190/20000], Training Loss: 0.4608\n",
            "Epoch [12191/20000], Training Loss: 0.5023\n",
            "Epoch [12192/20000], Training Loss: 0.5469\n",
            "Epoch [12193/20000], Training Loss: 0.4464\n",
            "Epoch [12194/20000], Training Loss: 0.5158\n",
            "Epoch [12195/20000], Training Loss: 0.4941\n",
            "Epoch [12196/20000], Training Loss: 0.5010\n",
            "Epoch [12197/20000], Training Loss: 0.4903\n",
            "Epoch [12198/20000], Training Loss: 0.4871\n",
            "Epoch [12199/20000], Training Loss: 0.4666\n",
            "Epoch [12200/20000], Training Loss: 0.4945\n",
            "Epoch [12201/20000], Training Loss: 0.4835\n",
            "Epoch [12202/20000], Training Loss: 0.5029\n",
            "Epoch [12203/20000], Training Loss: 0.4735\n",
            "Epoch [12204/20000], Training Loss: 0.4421\n",
            "Epoch [12205/20000], Training Loss: 0.4988\n",
            "Epoch [12206/20000], Training Loss: 0.4780\n",
            "Epoch [12207/20000], Training Loss: 0.5353\n",
            "Epoch [12208/20000], Training Loss: 0.4629\n",
            "Epoch [12209/20000], Training Loss: 0.4984\n",
            "Epoch [12210/20000], Training Loss: 0.4973\n",
            "Epoch [12211/20000], Training Loss: 0.4596\n",
            "Epoch [12212/20000], Training Loss: 0.5374\n",
            "Epoch [12213/20000], Training Loss: 0.4414\n",
            "Epoch [12214/20000], Training Loss: 0.4787\n",
            "Epoch [12215/20000], Training Loss: 0.4805\n",
            "Epoch [12216/20000], Training Loss: 0.5002\n",
            "Epoch [12217/20000], Training Loss: 0.5029\n",
            "Epoch [12218/20000], Training Loss: 0.4847\n",
            "Epoch [12219/20000], Training Loss: 0.5004\n",
            "Epoch [12220/20000], Training Loss: 0.5054\n",
            "Epoch [12221/20000], Training Loss: 0.4641\n",
            "Epoch [12222/20000], Training Loss: 0.5165\n",
            "Epoch [12223/20000], Training Loss: 0.4685\n",
            "Epoch [12224/20000], Training Loss: 0.4960\n",
            "Epoch [12225/20000], Training Loss: 0.4972\n",
            "Epoch [12226/20000], Training Loss: 0.4489\n",
            "Epoch [12227/20000], Training Loss: 0.4978\n",
            "Epoch [12228/20000], Training Loss: 0.4937\n",
            "Epoch [12229/20000], Training Loss: 0.4893\n",
            "Epoch [12230/20000], Training Loss: 0.5025\n",
            "Epoch [12231/20000], Training Loss: 0.4819\n",
            "Epoch [12232/20000], Training Loss: 0.5030\n",
            "Epoch [12233/20000], Training Loss: 0.5085\n",
            "Epoch [12234/20000], Training Loss: 0.5155\n",
            "Epoch [12235/20000], Training Loss: 0.4864\n",
            "Epoch [12236/20000], Training Loss: 0.4629\n",
            "Epoch [12237/20000], Training Loss: 0.4216\n",
            "Epoch [12238/20000], Training Loss: 0.4804\n",
            "Epoch [12239/20000], Training Loss: 0.4419\n",
            "Epoch [12240/20000], Training Loss: 0.4477\n",
            "Epoch [12241/20000], Training Loss: 0.4667\n",
            "Epoch [12242/20000], Training Loss: 0.4806\n",
            "Epoch [12243/20000], Training Loss: 0.4670\n",
            "Epoch [12244/20000], Training Loss: 0.4869\n",
            "Epoch [12245/20000], Training Loss: 0.5108\n",
            "Epoch [12246/20000], Training Loss: 0.4775\n",
            "Epoch [12247/20000], Training Loss: 0.5290\n",
            "Epoch [12248/20000], Training Loss: 0.5240\n",
            "Epoch [12249/20000], Training Loss: 0.4994\n",
            "Epoch [12250/20000], Training Loss: 0.4447\n",
            "Epoch [12251/20000], Training Loss: 0.4766\n",
            "Epoch [12252/20000], Training Loss: 0.5039\n",
            "Epoch [12253/20000], Training Loss: 0.5373\n",
            "Epoch [12254/20000], Training Loss: 0.4670\n",
            "Epoch [12255/20000], Training Loss: 0.4858\n",
            "Epoch [12256/20000], Training Loss: 0.4706\n",
            "Epoch [12257/20000], Training Loss: 0.4339\n",
            "Epoch [12258/20000], Training Loss: 0.5055\n",
            "Epoch [12259/20000], Training Loss: 0.4654\n",
            "Epoch [12260/20000], Training Loss: 0.4563\n",
            "Epoch [12261/20000], Training Loss: 0.4717\n",
            "Epoch [12262/20000], Training Loss: 0.4781\n",
            "Epoch [12263/20000], Training Loss: 0.4662\n",
            "Epoch [12264/20000], Training Loss: 0.4559\n",
            "Epoch [12265/20000], Training Loss: 0.5157\n",
            "Epoch [12266/20000], Training Loss: 0.4704\n",
            "Epoch [12267/20000], Training Loss: 0.4311\n",
            "Epoch [12268/20000], Training Loss: 0.4678\n",
            "Epoch [12269/20000], Training Loss: 0.4794\n",
            "Epoch [12270/20000], Training Loss: 0.4385\n",
            "Epoch [12271/20000], Training Loss: 0.4915\n",
            "Epoch [12272/20000], Training Loss: 0.5049\n",
            "Epoch [12273/20000], Training Loss: 0.4801\n",
            "Epoch [12274/20000], Training Loss: 0.5095\n",
            "Epoch [12275/20000], Training Loss: 0.4845\n",
            "Epoch [12276/20000], Training Loss: 0.5440\n",
            "Epoch [12277/20000], Training Loss: 0.4827\n",
            "Epoch [12278/20000], Training Loss: 0.4675\n",
            "Epoch [12279/20000], Training Loss: 0.5276\n",
            "Epoch [12280/20000], Training Loss: 0.4658\n",
            "Epoch [12281/20000], Training Loss: 0.4452\n",
            "Epoch [12282/20000], Training Loss: 0.4777\n",
            "Epoch [12283/20000], Training Loss: 0.4672\n",
            "Epoch [12284/20000], Training Loss: 0.5035\n",
            "Epoch [12285/20000], Training Loss: 0.4830\n",
            "Epoch [12286/20000], Training Loss: 0.5214\n",
            "Epoch [12287/20000], Training Loss: 0.4719\n",
            "Epoch [12288/20000], Training Loss: 0.4712\n",
            "Epoch [12289/20000], Training Loss: 0.4936\n",
            "Epoch [12290/20000], Training Loss: 0.4682\n",
            "Epoch [12291/20000], Training Loss: 0.5122\n",
            "Epoch [12292/20000], Training Loss: 0.4631\n",
            "Epoch [12293/20000], Training Loss: 0.4580\n",
            "Epoch [12294/20000], Training Loss: 0.4424\n",
            "Epoch [12295/20000], Training Loss: 0.4627\n",
            "Epoch [12296/20000], Training Loss: 0.4531\n",
            "Epoch [12297/20000], Training Loss: 0.5046\n",
            "Epoch [12298/20000], Training Loss: 0.4841\n",
            "Epoch [12299/20000], Training Loss: 0.4769\n",
            "Epoch [12300/20000], Training Loss: 0.4488\n",
            "Epoch [12301/20000], Training Loss: 0.4520\n",
            "Epoch [12302/20000], Training Loss: 0.5093\n",
            "Epoch [12303/20000], Training Loss: 0.4805\n",
            "Epoch [12304/20000], Training Loss: 0.4799\n",
            "Epoch [12305/20000], Training Loss: 0.4860\n",
            "Epoch [12306/20000], Training Loss: 0.4579\n",
            "Epoch [12307/20000], Training Loss: 0.4966\n",
            "Epoch [12308/20000], Training Loss: 0.4446\n",
            "Epoch [12309/20000], Training Loss: 0.4547\n",
            "Epoch [12310/20000], Training Loss: 0.4446\n",
            "Epoch [12311/20000], Training Loss: 0.4832\n",
            "Epoch [12312/20000], Training Loss: 0.5166\n",
            "Epoch [12313/20000], Training Loss: 0.4919\n",
            "Epoch [12314/20000], Training Loss: 0.4968\n",
            "Epoch [12315/20000], Training Loss: 0.4956\n",
            "Epoch [12316/20000], Training Loss: 0.5103\n",
            "Epoch [12317/20000], Training Loss: 0.5021\n",
            "Epoch [12318/20000], Training Loss: 0.4625\n",
            "Epoch [12319/20000], Training Loss: 0.5044\n",
            "Epoch [12320/20000], Training Loss: 0.5057\n",
            "Epoch [12321/20000], Training Loss: 0.5056\n",
            "Epoch [12322/20000], Training Loss: 0.4767\n",
            "Epoch [12323/20000], Training Loss: 0.4820\n",
            "Epoch [12324/20000], Training Loss: 0.4977\n",
            "Epoch [12325/20000], Training Loss: 0.4693\n",
            "Epoch [12326/20000], Training Loss: 0.5030\n",
            "Epoch [12327/20000], Training Loss: 0.5005\n",
            "Epoch [12328/20000], Training Loss: 0.4786\n",
            "Epoch [12329/20000], Training Loss: 0.5255\n",
            "Epoch [12330/20000], Training Loss: 0.4801\n",
            "Epoch [12331/20000], Training Loss: 0.4501\n",
            "Epoch [12332/20000], Training Loss: 0.4779\n",
            "Epoch [12333/20000], Training Loss: 0.5153\n",
            "Epoch [12334/20000], Training Loss: 0.4746\n",
            "Epoch [12335/20000], Training Loss: 0.4363\n",
            "Epoch [12336/20000], Training Loss: 0.5002\n",
            "Epoch [12337/20000], Training Loss: 0.5047\n",
            "Epoch [12338/20000], Training Loss: 0.4787\n",
            "Epoch [12339/20000], Training Loss: 0.4883\n",
            "Epoch [12340/20000], Training Loss: 0.4936\n",
            "Epoch [12341/20000], Training Loss: 0.4427\n",
            "Epoch [12342/20000], Training Loss: 0.5109\n",
            "Epoch [12343/20000], Training Loss: 0.4960\n",
            "Epoch [12344/20000], Training Loss: 0.4967\n",
            "Epoch [12345/20000], Training Loss: 0.4548\n",
            "Epoch [12346/20000], Training Loss: 0.5379\n",
            "Epoch [12347/20000], Training Loss: 0.4781\n",
            "Epoch [12348/20000], Training Loss: 0.4481\n",
            "Epoch [12349/20000], Training Loss: 0.5090\n",
            "Epoch [12350/20000], Training Loss: 0.5163\n",
            "Epoch [12351/20000], Training Loss: 0.4957\n",
            "Epoch [12352/20000], Training Loss: 0.4961\n",
            "Epoch [12353/20000], Training Loss: 0.4434\n",
            "Epoch [12354/20000], Training Loss: 0.4527\n",
            "Epoch [12355/20000], Training Loss: 0.4684\n",
            "Epoch [12356/20000], Training Loss: 0.4758\n",
            "Epoch [12357/20000], Training Loss: 0.4512\n",
            "Epoch [12358/20000], Training Loss: 0.4777\n",
            "Epoch [12359/20000], Training Loss: 0.4983\n",
            "Epoch [12360/20000], Training Loss: 0.5120\n",
            "Epoch [12361/20000], Training Loss: 0.4727\n",
            "Epoch [12362/20000], Training Loss: 0.4784\n",
            "Epoch [12363/20000], Training Loss: 0.4912\n",
            "Epoch [12364/20000], Training Loss: 0.5087\n",
            "Epoch [12365/20000], Training Loss: 0.4596\n",
            "Epoch [12366/20000], Training Loss: 0.4662\n",
            "Epoch [12367/20000], Training Loss: 0.4838\n",
            "Epoch [12368/20000], Training Loss: 0.4649\n",
            "Epoch [12369/20000], Training Loss: 0.5144\n",
            "Epoch [12370/20000], Training Loss: 0.5232\n",
            "Epoch [12371/20000], Training Loss: 0.5177\n",
            "Epoch [12372/20000], Training Loss: 0.4344\n",
            "Epoch [12373/20000], Training Loss: 0.4873\n",
            "Epoch [12374/20000], Training Loss: 0.4820\n",
            "Epoch [12375/20000], Training Loss: 0.4636\n",
            "Epoch [12376/20000], Training Loss: 0.4824\n",
            "Epoch [12377/20000], Training Loss: 0.4698\n",
            "Epoch [12378/20000], Training Loss: 0.4670\n",
            "Epoch [12379/20000], Training Loss: 0.5198\n",
            "Epoch [12380/20000], Training Loss: 0.5007\n",
            "Epoch [12381/20000], Training Loss: 0.5058\n",
            "Epoch [12382/20000], Training Loss: 0.5070\n",
            "Epoch [12383/20000], Training Loss: 0.4582\n",
            "Epoch [12384/20000], Training Loss: 0.4751\n",
            "Epoch [12385/20000], Training Loss: 0.4496\n",
            "Epoch [12386/20000], Training Loss: 0.5213\n",
            "Epoch [12387/20000], Training Loss: 0.4947\n",
            "Epoch [12388/20000], Training Loss: 0.4473\n",
            "Epoch [12389/20000], Training Loss: 0.5161\n",
            "Epoch [12390/20000], Training Loss: 0.5242\n",
            "Epoch [12391/20000], Training Loss: 0.5085\n",
            "Epoch [12392/20000], Training Loss: 0.5196\n",
            "Epoch [12393/20000], Training Loss: 0.4719\n",
            "Epoch [12394/20000], Training Loss: 0.4810\n",
            "Epoch [12395/20000], Training Loss: 0.4727\n",
            "Epoch [12396/20000], Training Loss: 0.5583\n",
            "Epoch [12397/20000], Training Loss: 0.4671\n",
            "Epoch [12398/20000], Training Loss: 0.4752\n",
            "Epoch [12399/20000], Training Loss: 0.4686\n",
            "Epoch [12400/20000], Training Loss: 0.5247\n",
            "Epoch [12401/20000], Training Loss: 0.4772\n",
            "Epoch [12402/20000], Training Loss: 0.4846\n",
            "Epoch [12403/20000], Training Loss: 0.4728\n",
            "Epoch [12404/20000], Training Loss: 0.5019\n",
            "Epoch [12405/20000], Training Loss: 0.4852\n",
            "Epoch [12406/20000], Training Loss: 0.5359\n",
            "Epoch [12407/20000], Training Loss: 0.4618\n",
            "Epoch [12408/20000], Training Loss: 0.4373\n",
            "Epoch [12409/20000], Training Loss: 0.4720\n",
            "Epoch [12410/20000], Training Loss: 0.5101\n",
            "Epoch [12411/20000], Training Loss: 0.4886\n",
            "Epoch [12412/20000], Training Loss: 0.4447\n",
            "Epoch [12413/20000], Training Loss: 0.4896\n",
            "Epoch [12414/20000], Training Loss: 0.4993\n",
            "Epoch [12415/20000], Training Loss: 0.4742\n",
            "Epoch [12416/20000], Training Loss: 0.4350\n",
            "Epoch [12417/20000], Training Loss: 0.4920\n",
            "Epoch [12418/20000], Training Loss: 0.4565\n",
            "Epoch [12419/20000], Training Loss: 0.4579\n",
            "Epoch [12420/20000], Training Loss: 0.4866\n",
            "Epoch [12421/20000], Training Loss: 0.5444\n",
            "Epoch [12422/20000], Training Loss: 0.4910\n",
            "Epoch [12423/20000], Training Loss: 0.4981\n",
            "Epoch [12424/20000], Training Loss: 0.5238\n",
            "Epoch [12425/20000], Training Loss: 0.5210\n",
            "Epoch [12426/20000], Training Loss: 0.4683\n",
            "Epoch [12427/20000], Training Loss: 0.4713\n",
            "Epoch [12428/20000], Training Loss: 0.5227\n",
            "Epoch [12429/20000], Training Loss: 0.5221\n",
            "Epoch [12430/20000], Training Loss: 0.4512\n",
            "Epoch [12431/20000], Training Loss: 0.5137\n",
            "Epoch [12432/20000], Training Loss: 0.4889\n",
            "Epoch [12433/20000], Training Loss: 0.4598\n",
            "Epoch [12434/20000], Training Loss: 0.4749\n",
            "Epoch [12435/20000], Training Loss: 0.4697\n",
            "Epoch [12436/20000], Training Loss: 0.4440\n",
            "Epoch [12437/20000], Training Loss: 0.5238\n",
            "Epoch [12438/20000], Training Loss: 0.4996\n",
            "Epoch [12439/20000], Training Loss: 0.4961\n",
            "Epoch [12440/20000], Training Loss: 0.4724\n",
            "Epoch [12441/20000], Training Loss: 0.4787\n",
            "Epoch [12442/20000], Training Loss: 0.5143\n",
            "Epoch [12443/20000], Training Loss: 0.4892\n",
            "Epoch [12444/20000], Training Loss: 0.4716\n",
            "Epoch [12445/20000], Training Loss: 0.4621\n",
            "Epoch [12446/20000], Training Loss: 0.5069\n",
            "Epoch [12447/20000], Training Loss: 0.4593\n",
            "Epoch [12448/20000], Training Loss: 0.5019\n",
            "Epoch [12449/20000], Training Loss: 0.4784\n",
            "Epoch [12450/20000], Training Loss: 0.5253\n",
            "Epoch [12451/20000], Training Loss: 0.4961\n",
            "Epoch [12452/20000], Training Loss: 0.4781\n",
            "Epoch [12453/20000], Training Loss: 0.4623\n",
            "Epoch [12454/20000], Training Loss: 0.5134\n",
            "Epoch [12455/20000], Training Loss: 0.4635\n",
            "Epoch [12456/20000], Training Loss: 0.4881\n",
            "Epoch [12457/20000], Training Loss: 0.4424\n",
            "Epoch [12458/20000], Training Loss: 0.4872\n",
            "Epoch [12459/20000], Training Loss: 0.4582\n",
            "Epoch [12460/20000], Training Loss: 0.4716\n",
            "Epoch [12461/20000], Training Loss: 0.5002\n",
            "Epoch [12462/20000], Training Loss: 0.4663\n",
            "Epoch [12463/20000], Training Loss: 0.4339\n",
            "Epoch [12464/20000], Training Loss: 0.5228\n",
            "Epoch [12465/20000], Training Loss: 0.4483\n",
            "Epoch [12466/20000], Training Loss: 0.4542\n",
            "Epoch [12467/20000], Training Loss: 0.4931\n",
            "Epoch [12468/20000], Training Loss: 0.5584\n",
            "Epoch [12469/20000], Training Loss: 0.4480\n",
            "Epoch [12470/20000], Training Loss: 0.5409\n",
            "Epoch [12471/20000], Training Loss: 0.5161\n",
            "Epoch [12472/20000], Training Loss: 0.5125\n",
            "Epoch [12473/20000], Training Loss: 0.4843\n",
            "Epoch [12474/20000], Training Loss: 0.5313\n",
            "Epoch [12475/20000], Training Loss: 0.4551\n",
            "Epoch [12476/20000], Training Loss: 0.4800\n",
            "Epoch [12477/20000], Training Loss: 0.5085\n",
            "Epoch [12478/20000], Training Loss: 0.5038\n",
            "Epoch [12479/20000], Training Loss: 0.5037\n",
            "Epoch [12480/20000], Training Loss: 0.5188\n",
            "Epoch [12481/20000], Training Loss: 0.4943\n",
            "Epoch [12482/20000], Training Loss: 0.5452\n",
            "Epoch [12483/20000], Training Loss: 0.4927\n",
            "Epoch [12484/20000], Training Loss: 0.4583\n",
            "Epoch [12485/20000], Training Loss: 0.5100\n",
            "Epoch [12486/20000], Training Loss: 0.4916\n",
            "Epoch [12487/20000], Training Loss: 0.4942\n",
            "Epoch [12488/20000], Training Loss: 0.4987\n",
            "Epoch [12489/20000], Training Loss: 0.5052\n",
            "Epoch [12490/20000], Training Loss: 0.5089\n",
            "Epoch [12491/20000], Training Loss: 0.4907\n",
            "Epoch [12492/20000], Training Loss: 0.4680\n",
            "Epoch [12493/20000], Training Loss: 0.5141\n",
            "Epoch [12494/20000], Training Loss: 0.4907\n",
            "Epoch [12495/20000], Training Loss: 0.4836\n",
            "Epoch [12496/20000], Training Loss: 0.5215\n",
            "Epoch [12497/20000], Training Loss: 0.4659\n",
            "Epoch [12498/20000], Training Loss: 0.4736\n",
            "Epoch [12499/20000], Training Loss: 0.5108\n",
            "Epoch [12500/20000], Training Loss: 0.4836\n",
            "Epoch [12501/20000], Training Loss: 0.5124\n",
            "Epoch [12502/20000], Training Loss: 0.5089\n",
            "Epoch [12503/20000], Training Loss: 0.5483\n",
            "Epoch [12504/20000], Training Loss: 0.4752\n",
            "Epoch [12505/20000], Training Loss: 0.5353\n",
            "Epoch [12506/20000], Training Loss: 0.5021\n",
            "Epoch [12507/20000], Training Loss: 0.4293\n",
            "Epoch [12508/20000], Training Loss: 0.4635\n",
            "Epoch [12509/20000], Training Loss: 0.4980\n",
            "Epoch [12510/20000], Training Loss: 0.4988\n",
            "Epoch [12511/20000], Training Loss: 0.4726\n",
            "Epoch [12512/20000], Training Loss: 0.5195\n",
            "Epoch [12513/20000], Training Loss: 0.4789\n",
            "Epoch [12514/20000], Training Loss: 0.4995\n",
            "Epoch [12515/20000], Training Loss: 0.4821\n",
            "Epoch [12516/20000], Training Loss: 0.4401\n",
            "Epoch [12517/20000], Training Loss: 0.4555\n",
            "Epoch [12518/20000], Training Loss: 0.4914\n",
            "Epoch [12519/20000], Training Loss: 0.4360\n",
            "Epoch [12520/20000], Training Loss: 0.4680\n",
            "Epoch [12521/20000], Training Loss: 0.4616\n",
            "Epoch [12522/20000], Training Loss: 0.4765\n",
            "Epoch [12523/20000], Training Loss: 0.5175\n",
            "Epoch [12524/20000], Training Loss: 0.5165\n",
            "Epoch [12525/20000], Training Loss: 0.4848\n",
            "Epoch [12526/20000], Training Loss: 0.4668\n",
            "Epoch [12527/20000], Training Loss: 0.4622\n",
            "Epoch [12528/20000], Training Loss: 0.4756\n",
            "Epoch [12529/20000], Training Loss: 0.4671\n",
            "Epoch [12530/20000], Training Loss: 0.4619\n",
            "Epoch [12531/20000], Training Loss: 0.4861\n",
            "Epoch [12532/20000], Training Loss: 0.4874\n",
            "Epoch [12533/20000], Training Loss: 0.4757\n",
            "Epoch [12534/20000], Training Loss: 0.5263\n",
            "Epoch [12535/20000], Training Loss: 0.5115\n",
            "Epoch [12536/20000], Training Loss: 0.4530\n",
            "Epoch [12537/20000], Training Loss: 0.4809\n",
            "Epoch [12538/20000], Training Loss: 0.4972\n",
            "Epoch [12539/20000], Training Loss: 0.5379\n",
            "Epoch [12540/20000], Training Loss: 0.4839\n",
            "Epoch [12541/20000], Training Loss: 0.4711\n",
            "Epoch [12542/20000], Training Loss: 0.5032\n",
            "Epoch [12543/20000], Training Loss: 0.4710\n",
            "Epoch [12544/20000], Training Loss: 0.4919\n",
            "Epoch [12545/20000], Training Loss: 0.4940\n",
            "Epoch [12546/20000], Training Loss: 0.4824\n",
            "Epoch [12547/20000], Training Loss: 0.4682\n",
            "Epoch [12548/20000], Training Loss: 0.4964\n",
            "Epoch [12549/20000], Training Loss: 0.4963\n",
            "Epoch [12550/20000], Training Loss: 0.4975\n",
            "Epoch [12551/20000], Training Loss: 0.4802\n",
            "Epoch [12552/20000], Training Loss: 0.4752\n",
            "Epoch [12553/20000], Training Loss: 0.4658\n",
            "Epoch [12554/20000], Training Loss: 0.5077\n",
            "Epoch [12555/20000], Training Loss: 0.4668\n",
            "Epoch [12556/20000], Training Loss: 0.4654\n",
            "Epoch [12557/20000], Training Loss: 0.4986\n",
            "Epoch [12558/20000], Training Loss: 0.4732\n",
            "Epoch [12559/20000], Training Loss: 0.4291\n",
            "Epoch [12560/20000], Training Loss: 0.4795\n",
            "Epoch [12561/20000], Training Loss: 0.5159\n",
            "Epoch [12562/20000], Training Loss: 0.4675\n",
            "Epoch [12563/20000], Training Loss: 0.4704\n",
            "Epoch [12564/20000], Training Loss: 0.5002\n",
            "Epoch [12565/20000], Training Loss: 0.5324\n",
            "Epoch [12566/20000], Training Loss: 0.5215\n",
            "Epoch [12567/20000], Training Loss: 0.5421\n",
            "Epoch [12568/20000], Training Loss: 0.5032\n",
            "Epoch [12569/20000], Training Loss: 0.4935\n",
            "Epoch [12570/20000], Training Loss: 0.4793\n",
            "Epoch [12571/20000], Training Loss: 0.5258\n",
            "Epoch [12572/20000], Training Loss: 0.4580\n",
            "Epoch [12573/20000], Training Loss: 0.4779\n",
            "Epoch [12574/20000], Training Loss: 0.5056\n",
            "Epoch [12575/20000], Training Loss: 0.4997\n",
            "Epoch [12576/20000], Training Loss: 0.5314\n",
            "Epoch [12577/20000], Training Loss: 0.4743\n",
            "Epoch [12578/20000], Training Loss: 0.4434\n",
            "Epoch [12579/20000], Training Loss: 0.4621\n",
            "Epoch [12580/20000], Training Loss: 0.5163\n",
            "Epoch [12581/20000], Training Loss: 0.4372\n",
            "Epoch [12582/20000], Training Loss: 0.4472\n",
            "Epoch [12583/20000], Training Loss: 0.4594\n",
            "Epoch [12584/20000], Training Loss: 0.5135\n",
            "Epoch [12585/20000], Training Loss: 0.5049\n",
            "Epoch [12586/20000], Training Loss: 0.4716\n",
            "Epoch [12587/20000], Training Loss: 0.4907\n",
            "Epoch [12588/20000], Training Loss: 0.5014\n",
            "Epoch [12589/20000], Training Loss: 0.4790\n",
            "Epoch [12590/20000], Training Loss: 0.4752\n",
            "Epoch [12591/20000], Training Loss: 0.5248\n",
            "Epoch [12592/20000], Training Loss: 0.4795\n",
            "Epoch [12593/20000], Training Loss: 0.4731\n",
            "Epoch [12594/20000], Training Loss: 0.4713\n",
            "Epoch [12595/20000], Training Loss: 0.4521\n",
            "Epoch [12596/20000], Training Loss: 0.4893\n",
            "Epoch [12597/20000], Training Loss: 0.4464\n",
            "Epoch [12598/20000], Training Loss: 0.4301\n",
            "Epoch [12599/20000], Training Loss: 0.5203\n",
            "Epoch [12600/20000], Training Loss: 0.4424\n",
            "Epoch [12601/20000], Training Loss: 0.5415\n",
            "Epoch [12602/20000], Training Loss: 0.5027\n",
            "Epoch [12603/20000], Training Loss: 0.5077\n",
            "Epoch [12604/20000], Training Loss: 0.5082\n",
            "Epoch [12605/20000], Training Loss: 0.4670\n",
            "Epoch [12606/20000], Training Loss: 0.4306\n",
            "Epoch [12607/20000], Training Loss: 0.4678\n",
            "Epoch [12608/20000], Training Loss: 0.4858\n",
            "Epoch [12609/20000], Training Loss: 0.4822\n",
            "Epoch [12610/20000], Training Loss: 0.5153\n",
            "Epoch [12611/20000], Training Loss: 0.4859\n",
            "Epoch [12612/20000], Training Loss: 0.4783\n",
            "Epoch [12613/20000], Training Loss: 0.5032\n",
            "Epoch [12614/20000], Training Loss: 0.4771\n",
            "Epoch [12615/20000], Training Loss: 0.5120\n",
            "Epoch [12616/20000], Training Loss: 0.4814\n",
            "Epoch [12617/20000], Training Loss: 0.4874\n",
            "Epoch [12618/20000], Training Loss: 0.5338\n",
            "Epoch [12619/20000], Training Loss: 0.4746\n",
            "Epoch [12620/20000], Training Loss: 0.4803\n",
            "Epoch [12621/20000], Training Loss: 0.4580\n",
            "Epoch [12622/20000], Training Loss: 0.4452\n",
            "Epoch [12623/20000], Training Loss: 0.4980\n",
            "Epoch [12624/20000], Training Loss: 0.4792\n",
            "Epoch [12625/20000], Training Loss: 0.5228\n",
            "Epoch [12626/20000], Training Loss: 0.4899\n",
            "Epoch [12627/20000], Training Loss: 0.4785\n",
            "Epoch [12628/20000], Training Loss: 0.4972\n",
            "Epoch [12629/20000], Training Loss: 0.4630\n",
            "Epoch [12630/20000], Training Loss: 0.4759\n",
            "Epoch [12631/20000], Training Loss: 0.4693\n",
            "Epoch [12632/20000], Training Loss: 0.5293\n",
            "Epoch [12633/20000], Training Loss: 0.4629\n",
            "Epoch [12634/20000], Training Loss: 0.4449\n",
            "Epoch [12635/20000], Training Loss: 0.5336\n",
            "Epoch [12636/20000], Training Loss: 0.4741\n",
            "Epoch [12637/20000], Training Loss: 0.4870\n",
            "Epoch [12638/20000], Training Loss: 0.4934\n",
            "Epoch [12639/20000], Training Loss: 0.4839\n",
            "Epoch [12640/20000], Training Loss: 0.4747\n",
            "Epoch [12641/20000], Training Loss: 0.4906\n",
            "Epoch [12642/20000], Training Loss: 0.4784\n",
            "Epoch [12643/20000], Training Loss: 0.5162\n",
            "Epoch [12644/20000], Training Loss: 0.4796\n",
            "Epoch [12645/20000], Training Loss: 0.4567\n",
            "Epoch [12646/20000], Training Loss: 0.4699\n",
            "Epoch [12647/20000], Training Loss: 0.5221\n",
            "Epoch [12648/20000], Training Loss: 0.5076\n",
            "Epoch [12649/20000], Training Loss: 0.4980\n",
            "Epoch [12650/20000], Training Loss: 0.4903\n",
            "Epoch [12651/20000], Training Loss: 0.4776\n",
            "Epoch [12652/20000], Training Loss: 0.4944\n",
            "Epoch [12653/20000], Training Loss: 0.4713\n",
            "Epoch [12654/20000], Training Loss: 0.4976\n",
            "Epoch [12655/20000], Training Loss: 0.4324\n",
            "Epoch [12656/20000], Training Loss: 0.4861\n",
            "Epoch [12657/20000], Training Loss: 0.4897\n",
            "Epoch [12658/20000], Training Loss: 0.4541\n",
            "Epoch [12659/20000], Training Loss: 0.4942\n",
            "Epoch [12660/20000], Training Loss: 0.4964\n",
            "Epoch [12661/20000], Training Loss: 0.4998\n",
            "Epoch [12662/20000], Training Loss: 0.4795\n",
            "Epoch [12663/20000], Training Loss: 0.4908\n",
            "Epoch [12664/20000], Training Loss: 0.4756\n",
            "Epoch [12665/20000], Training Loss: 0.5062\n",
            "Epoch [12666/20000], Training Loss: 0.4521\n",
            "Epoch [12667/20000], Training Loss: 0.5039\n",
            "Epoch [12668/20000], Training Loss: 0.5393\n",
            "Epoch [12669/20000], Training Loss: 0.4861\n",
            "Epoch [12670/20000], Training Loss: 0.5141\n",
            "Epoch [12671/20000], Training Loss: 0.4798\n",
            "Epoch [12672/20000], Training Loss: 0.4971\n",
            "Epoch [12673/20000], Training Loss: 0.5051\n",
            "Epoch [12674/20000], Training Loss: 0.5086\n",
            "Epoch [12675/20000], Training Loss: 0.5056\n",
            "Epoch [12676/20000], Training Loss: 0.5068\n",
            "Epoch [12677/20000], Training Loss: 0.4739\n",
            "Epoch [12678/20000], Training Loss: 0.4869\n",
            "Epoch [12679/20000], Training Loss: 0.5068\n",
            "Epoch [12680/20000], Training Loss: 0.5065\n",
            "Epoch [12681/20000], Training Loss: 0.4500\n",
            "Epoch [12682/20000], Training Loss: 0.4549\n",
            "Epoch [12683/20000], Training Loss: 0.5155\n",
            "Epoch [12684/20000], Training Loss: 0.4942\n",
            "Epoch [12685/20000], Training Loss: 0.4895\n",
            "Epoch [12686/20000], Training Loss: 0.5138\n",
            "Epoch [12687/20000], Training Loss: 0.5178\n",
            "Epoch [12688/20000], Training Loss: 0.4873\n",
            "Epoch [12689/20000], Training Loss: 0.5088\n",
            "Epoch [12690/20000], Training Loss: 0.4443\n",
            "Epoch [12691/20000], Training Loss: 0.4568\n",
            "Epoch [12692/20000], Training Loss: 0.5331\n",
            "Epoch [12693/20000], Training Loss: 0.4681\n",
            "Epoch [12694/20000], Training Loss: 0.4860\n",
            "Epoch [12695/20000], Training Loss: 0.5002\n",
            "Epoch [12696/20000], Training Loss: 0.4785\n",
            "Epoch [12697/20000], Training Loss: 0.4984\n",
            "Epoch [12698/20000], Training Loss: 0.5280\n",
            "Epoch [12699/20000], Training Loss: 0.4772\n",
            "Epoch [12700/20000], Training Loss: 0.4747\n",
            "Epoch [12701/20000], Training Loss: 0.4884\n",
            "Epoch [12702/20000], Training Loss: 0.5489\n",
            "Epoch [12703/20000], Training Loss: 0.4982\n",
            "Epoch [12704/20000], Training Loss: 0.5047\n",
            "Epoch [12705/20000], Training Loss: 0.4422\n",
            "Epoch [12706/20000], Training Loss: 0.4656\n",
            "Epoch [12707/20000], Training Loss: 0.5370\n",
            "Epoch [12708/20000], Training Loss: 0.4415\n",
            "Epoch [12709/20000], Training Loss: 0.4858\n",
            "Epoch [12710/20000], Training Loss: 0.4639\n",
            "Epoch [12711/20000], Training Loss: 0.4351\n",
            "Epoch [12712/20000], Training Loss: 0.5105\n",
            "Epoch [12713/20000], Training Loss: 0.4665\n",
            "Epoch [12714/20000], Training Loss: 0.5227\n",
            "Epoch [12715/20000], Training Loss: 0.4471\n",
            "Epoch [12716/20000], Training Loss: 0.4460\n",
            "Epoch [12717/20000], Training Loss: 0.5064\n",
            "Epoch [12718/20000], Training Loss: 0.5099\n",
            "Epoch [12719/20000], Training Loss: 0.5114\n",
            "Epoch [12720/20000], Training Loss: 0.5191\n",
            "Epoch [12721/20000], Training Loss: 0.5358\n",
            "Epoch [12722/20000], Training Loss: 0.5088\n",
            "Epoch [12723/20000], Training Loss: 0.5238\n",
            "Epoch [12724/20000], Training Loss: 0.4779\n",
            "Epoch [12725/20000], Training Loss: 0.4706\n",
            "Epoch [12726/20000], Training Loss: 0.4861\n",
            "Epoch [12727/20000], Training Loss: 0.4690\n",
            "Epoch [12728/20000], Training Loss: 0.4871\n",
            "Epoch [12729/20000], Training Loss: 0.5059\n",
            "Epoch [12730/20000], Training Loss: 0.4780\n",
            "Epoch [12731/20000], Training Loss: 0.5368\n",
            "Epoch [12732/20000], Training Loss: 0.4855\n",
            "Epoch [12733/20000], Training Loss: 0.5349\n",
            "Epoch [12734/20000], Training Loss: 0.5269\n",
            "Epoch [12735/20000], Training Loss: 0.4813\n",
            "Epoch [12736/20000], Training Loss: 0.5201\n",
            "Epoch [12737/20000], Training Loss: 0.4852\n",
            "Epoch [12738/20000], Training Loss: 0.5039\n",
            "Epoch [12739/20000], Training Loss: 0.4870\n",
            "Epoch [12740/20000], Training Loss: 0.4904\n",
            "Epoch [12741/20000], Training Loss: 0.5255\n",
            "Epoch [12742/20000], Training Loss: 0.5023\n",
            "Epoch [12743/20000], Training Loss: 0.4626\n",
            "Epoch [12744/20000], Training Loss: 0.4821\n",
            "Epoch [12745/20000], Training Loss: 0.5067\n",
            "Epoch [12746/20000], Training Loss: 0.4671\n",
            "Epoch [12747/20000], Training Loss: 0.4913\n",
            "Epoch [12748/20000], Training Loss: 0.4764\n",
            "Epoch [12749/20000], Training Loss: 0.4892\n",
            "Epoch [12750/20000], Training Loss: 0.5155\n",
            "Epoch [12751/20000], Training Loss: 0.4462\n",
            "Epoch [12752/20000], Training Loss: 0.4841\n",
            "Epoch [12753/20000], Training Loss: 0.5096\n",
            "Epoch [12754/20000], Training Loss: 0.4531\n",
            "Epoch [12755/20000], Training Loss: 0.5102\n",
            "Epoch [12756/20000], Training Loss: 0.4816\n",
            "Epoch [12757/20000], Training Loss: 0.4875\n",
            "Epoch [12758/20000], Training Loss: 0.4525\n",
            "Epoch [12759/20000], Training Loss: 0.4894\n",
            "Epoch [12760/20000], Training Loss: 0.4285\n",
            "Epoch [12761/20000], Training Loss: 0.4759\n",
            "Epoch [12762/20000], Training Loss: 0.4820\n",
            "Epoch [12763/20000], Training Loss: 0.4664\n",
            "Epoch [12764/20000], Training Loss: 0.5210\n",
            "Epoch [12765/20000], Training Loss: 0.4546\n",
            "Epoch [12766/20000], Training Loss: 0.4323\n",
            "Epoch [12767/20000], Training Loss: 0.5111\n",
            "Epoch [12768/20000], Training Loss: 0.4885\n",
            "Epoch [12769/20000], Training Loss: 0.5410\n",
            "Epoch [12770/20000], Training Loss: 0.5159\n",
            "Epoch [12771/20000], Training Loss: 0.4839\n",
            "Epoch [12772/20000], Training Loss: 0.5023\n",
            "Epoch [12773/20000], Training Loss: 0.4834\n",
            "Epoch [12774/20000], Training Loss: 0.4636\n",
            "Epoch [12775/20000], Training Loss: 0.4934\n",
            "Epoch [12776/20000], Training Loss: 0.5159\n",
            "Epoch [12777/20000], Training Loss: 0.5222\n",
            "Epoch [12778/20000], Training Loss: 0.4887\n",
            "Epoch [12779/20000], Training Loss: 0.4959\n",
            "Epoch [12780/20000], Training Loss: 0.4643\n",
            "Epoch [12781/20000], Training Loss: 0.4728\n",
            "Epoch [12782/20000], Training Loss: 0.4847\n",
            "Epoch [12783/20000], Training Loss: 0.4736\n",
            "Epoch [12784/20000], Training Loss: 0.4762\n",
            "Epoch [12785/20000], Training Loss: 0.4473\n",
            "Epoch [12786/20000], Training Loss: 0.4768\n",
            "Epoch [12787/20000], Training Loss: 0.5222\n",
            "Epoch [12788/20000], Training Loss: 0.5484\n",
            "Epoch [12789/20000], Training Loss: 0.5219\n",
            "Epoch [12790/20000], Training Loss: 0.4544\n",
            "Epoch [12791/20000], Training Loss: 0.5013\n",
            "Epoch [12792/20000], Training Loss: 0.5247\n",
            "Epoch [12793/20000], Training Loss: 0.4629\n",
            "Epoch [12794/20000], Training Loss: 0.4963\n",
            "Epoch [12795/20000], Training Loss: 0.4846\n",
            "Epoch [12796/20000], Training Loss: 0.4986\n",
            "Epoch [12797/20000], Training Loss: 0.4432\n",
            "Epoch [12798/20000], Training Loss: 0.5045\n",
            "Epoch [12799/20000], Training Loss: 0.4751\n",
            "Epoch [12800/20000], Training Loss: 0.5077\n",
            "Epoch [12801/20000], Training Loss: 0.4895\n",
            "Epoch [12802/20000], Training Loss: 0.4858\n",
            "Epoch [12803/20000], Training Loss: 0.4699\n",
            "Epoch [12804/20000], Training Loss: 0.4602\n",
            "Epoch [12805/20000], Training Loss: 0.5056\n",
            "Epoch [12806/20000], Training Loss: 0.4730\n",
            "Epoch [12807/20000], Training Loss: 0.4998\n",
            "Epoch [12808/20000], Training Loss: 0.5278\n",
            "Epoch [12809/20000], Training Loss: 0.5216\n",
            "Epoch [12810/20000], Training Loss: 0.4855\n",
            "Epoch [12811/20000], Training Loss: 0.4902\n",
            "Epoch [12812/20000], Training Loss: 0.5032\n",
            "Epoch [12813/20000], Training Loss: 0.4941\n",
            "Epoch [12814/20000], Training Loss: 0.5649\n",
            "Epoch [12815/20000], Training Loss: 0.5261\n",
            "Epoch [12816/20000], Training Loss: 0.4659\n",
            "Epoch [12817/20000], Training Loss: 0.4598\n",
            "Epoch [12818/20000], Training Loss: 0.4746\n",
            "Epoch [12819/20000], Training Loss: 0.4835\n",
            "Epoch [12820/20000], Training Loss: 0.5183\n",
            "Epoch [12821/20000], Training Loss: 0.4822\n",
            "Epoch [12822/20000], Training Loss: 0.4643\n",
            "Epoch [12823/20000], Training Loss: 0.4546\n",
            "Epoch [12824/20000], Training Loss: 0.4735\n",
            "Epoch [12825/20000], Training Loss: 0.4986\n",
            "Epoch [12826/20000], Training Loss: 0.5028\n",
            "Epoch [12827/20000], Training Loss: 0.5271\n",
            "Epoch [12828/20000], Training Loss: 0.4556\n",
            "Epoch [12829/20000], Training Loss: 0.4981\n",
            "Epoch [12830/20000], Training Loss: 0.4576\n",
            "Epoch [12831/20000], Training Loss: 0.4840\n",
            "Epoch [12832/20000], Training Loss: 0.4939\n",
            "Epoch [12833/20000], Training Loss: 0.4791\n",
            "Epoch [12834/20000], Training Loss: 0.5182\n",
            "Epoch [12835/20000], Training Loss: 0.4523\n",
            "Epoch [12836/20000], Training Loss: 0.5005\n",
            "Epoch [12837/20000], Training Loss: 0.4534\n",
            "Epoch [12838/20000], Training Loss: 0.4901\n",
            "Epoch [12839/20000], Training Loss: 0.5028\n",
            "Epoch [12840/20000], Training Loss: 0.5014\n",
            "Epoch [12841/20000], Training Loss: 0.4749\n",
            "Epoch [12842/20000], Training Loss: 0.4876\n",
            "Epoch [12843/20000], Training Loss: 0.4726\n",
            "Epoch [12844/20000], Training Loss: 0.4989\n",
            "Epoch [12845/20000], Training Loss: 0.4906\n",
            "Epoch [12846/20000], Training Loss: 0.4785\n",
            "Epoch [12847/20000], Training Loss: 0.5066\n",
            "Epoch [12848/20000], Training Loss: 0.5272\n",
            "Epoch [12849/20000], Training Loss: 0.4511\n",
            "Epoch [12850/20000], Training Loss: 0.4770\n",
            "Epoch [12851/20000], Training Loss: 0.4819\n",
            "Epoch [12852/20000], Training Loss: 0.4405\n",
            "Epoch [12853/20000], Training Loss: 0.5091\n",
            "Epoch [12854/20000], Training Loss: 0.5081\n",
            "Epoch [12855/20000], Training Loss: 0.5112\n",
            "Epoch [12856/20000], Training Loss: 0.4663\n",
            "Epoch [12857/20000], Training Loss: 0.4836\n",
            "Epoch [12858/20000], Training Loss: 0.5263\n",
            "Epoch [12859/20000], Training Loss: 0.4571\n",
            "Epoch [12860/20000], Training Loss: 0.5081\n",
            "Epoch [12861/20000], Training Loss: 0.5043\n",
            "Epoch [12862/20000], Training Loss: 0.4508\n",
            "Epoch [12863/20000], Training Loss: 0.5180\n",
            "Epoch [12864/20000], Training Loss: 0.5035\n",
            "Epoch [12865/20000], Training Loss: 0.5166\n",
            "Epoch [12866/20000], Training Loss: 0.4408\n",
            "Epoch [12867/20000], Training Loss: 0.5021\n",
            "Epoch [12868/20000], Training Loss: 0.4416\n",
            "Epoch [12869/20000], Training Loss: 0.4521\n",
            "Epoch [12870/20000], Training Loss: 0.4994\n",
            "Epoch [12871/20000], Training Loss: 0.4713\n",
            "Epoch [12872/20000], Training Loss: 0.4851\n",
            "Epoch [12873/20000], Training Loss: 0.4720\n",
            "Epoch [12874/20000], Training Loss: 0.5113\n",
            "Epoch [12875/20000], Training Loss: 0.4518\n",
            "Epoch [12876/20000], Training Loss: 0.4684\n",
            "Epoch [12877/20000], Training Loss: 0.4867\n",
            "Epoch [12878/20000], Training Loss: 0.5261\n",
            "Epoch [12879/20000], Training Loss: 0.5059\n",
            "Epoch [12880/20000], Training Loss: 0.5258\n",
            "Epoch [12881/20000], Training Loss: 0.4922\n",
            "Epoch [12882/20000], Training Loss: 0.4703\n",
            "Epoch [12883/20000], Training Loss: 0.4847\n",
            "Epoch [12884/20000], Training Loss: 0.5279\n",
            "Epoch [12885/20000], Training Loss: 0.4972\n",
            "Epoch [12886/20000], Training Loss: 0.4588\n",
            "Epoch [12887/20000], Training Loss: 0.4785\n",
            "Epoch [12888/20000], Training Loss: 0.5014\n",
            "Epoch [12889/20000], Training Loss: 0.4830\n",
            "Epoch [12890/20000], Training Loss: 0.5001\n",
            "Epoch [12891/20000], Training Loss: 0.4851\n",
            "Epoch [12892/20000], Training Loss: 0.5150\n",
            "Epoch [12893/20000], Training Loss: 0.4856\n",
            "Epoch [12894/20000], Training Loss: 0.5023\n",
            "Epoch [12895/20000], Training Loss: 0.5082\n",
            "Epoch [12896/20000], Training Loss: 0.5105\n",
            "Epoch [12897/20000], Training Loss: 0.4666\n",
            "Epoch [12898/20000], Training Loss: 0.4397\n",
            "Epoch [12899/20000], Training Loss: 0.4532\n",
            "Epoch [12900/20000], Training Loss: 0.4527\n",
            "Epoch [12901/20000], Training Loss: 0.4973\n",
            "Epoch [12902/20000], Training Loss: 0.5200\n",
            "Epoch [12903/20000], Training Loss: 0.5043\n",
            "Epoch [12904/20000], Training Loss: 0.5430\n",
            "Epoch [12905/20000], Training Loss: 0.4556\n",
            "Epoch [12906/20000], Training Loss: 0.4479\n",
            "Epoch [12907/20000], Training Loss: 0.5082\n",
            "Epoch [12908/20000], Training Loss: 0.5065\n",
            "Epoch [12909/20000], Training Loss: 0.5271\n",
            "Epoch [12910/20000], Training Loss: 0.4927\n",
            "Epoch [12911/20000], Training Loss: 0.4531\n",
            "Epoch [12912/20000], Training Loss: 0.4573\n",
            "Epoch [12913/20000], Training Loss: 0.4987\n",
            "Epoch [12914/20000], Training Loss: 0.4971\n",
            "Epoch [12915/20000], Training Loss: 0.5141\n",
            "Epoch [12916/20000], Training Loss: 0.5279\n",
            "Epoch [12917/20000], Training Loss: 0.5329\n",
            "Epoch [12918/20000], Training Loss: 0.4825\n",
            "Epoch [12919/20000], Training Loss: 0.4865\n",
            "Epoch [12920/20000], Training Loss: 0.4706\n",
            "Epoch [12921/20000], Training Loss: 0.4777\n",
            "Epoch [12922/20000], Training Loss: 0.4409\n",
            "Epoch [12923/20000], Training Loss: 0.4944\n",
            "Epoch [12924/20000], Training Loss: 0.4621\n",
            "Epoch [12925/20000], Training Loss: 0.4970\n",
            "Epoch [12926/20000], Training Loss: 0.4550\n",
            "Epoch [12927/20000], Training Loss: 0.5445\n",
            "Epoch [12928/20000], Training Loss: 0.5218\n",
            "Epoch [12929/20000], Training Loss: 0.4934\n",
            "Epoch [12930/20000], Training Loss: 0.4713\n",
            "Epoch [12931/20000], Training Loss: 0.4991\n",
            "Epoch [12932/20000], Training Loss: 0.4793\n",
            "Epoch [12933/20000], Training Loss: 0.5035\n",
            "Epoch [12934/20000], Training Loss: 0.5399\n",
            "Epoch [12935/20000], Training Loss: 0.5104\n",
            "Epoch [12936/20000], Training Loss: 0.4900\n",
            "Epoch [12937/20000], Training Loss: 0.4587\n",
            "Epoch [12938/20000], Training Loss: 0.4595\n",
            "Epoch [12939/20000], Training Loss: 0.4820\n",
            "Epoch [12940/20000], Training Loss: 0.4649\n",
            "Epoch [12941/20000], Training Loss: 0.5199\n",
            "Epoch [12942/20000], Training Loss: 0.4789\n",
            "Epoch [12943/20000], Training Loss: 0.4927\n",
            "Epoch [12944/20000], Training Loss: 0.4928\n",
            "Epoch [12945/20000], Training Loss: 0.4758\n",
            "Epoch [12946/20000], Training Loss: 0.4785\n",
            "Epoch [12947/20000], Training Loss: 0.4894\n",
            "Epoch [12948/20000], Training Loss: 0.5482\n",
            "Epoch [12949/20000], Training Loss: 0.5151\n",
            "Epoch [12950/20000], Training Loss: 0.4416\n",
            "Epoch [12951/20000], Training Loss: 0.4656\n",
            "Epoch [12952/20000], Training Loss: 0.5122\n",
            "Epoch [12953/20000], Training Loss: 0.4822\n",
            "Epoch [12954/20000], Training Loss: 0.4824\n",
            "Epoch [12955/20000], Training Loss: 0.4733\n",
            "Epoch [12956/20000], Training Loss: 0.5033\n",
            "Epoch [12957/20000], Training Loss: 0.4811\n",
            "Epoch [12958/20000], Training Loss: 0.4713\n",
            "Epoch [12959/20000], Training Loss: 0.5158\n",
            "Epoch [12960/20000], Training Loss: 0.4644\n",
            "Epoch [12961/20000], Training Loss: 0.4643\n",
            "Epoch [12962/20000], Training Loss: 0.4531\n",
            "Epoch [12963/20000], Training Loss: 0.4880\n",
            "Epoch [12964/20000], Training Loss: 0.5179\n",
            "Epoch [12965/20000], Training Loss: 0.5357\n",
            "Epoch [12966/20000], Training Loss: 0.4550\n",
            "Epoch [12967/20000], Training Loss: 0.4865\n",
            "Epoch [12968/20000], Training Loss: 0.4850\n",
            "Epoch [12969/20000], Training Loss: 0.4602\n",
            "Epoch [12970/20000], Training Loss: 0.5179\n",
            "Epoch [12971/20000], Training Loss: 0.4508\n",
            "Epoch [12972/20000], Training Loss: 0.5259\n",
            "Epoch [12973/20000], Training Loss: 0.5039\n",
            "Epoch [12974/20000], Training Loss: 0.5213\n",
            "Epoch [12975/20000], Training Loss: 0.5223\n",
            "Epoch [12976/20000], Training Loss: 0.4480\n",
            "Epoch [12977/20000], Training Loss: 0.4818\n",
            "Epoch [12978/20000], Training Loss: 0.4534\n",
            "Epoch [12979/20000], Training Loss: 0.5159\n",
            "Epoch [12980/20000], Training Loss: 0.5242\n",
            "Epoch [12981/20000], Training Loss: 0.4992\n",
            "Epoch [12982/20000], Training Loss: 0.4680\n",
            "Epoch [12983/20000], Training Loss: 0.5290\n",
            "Epoch [12984/20000], Training Loss: 0.4826\n",
            "Epoch [12985/20000], Training Loss: 0.4745\n",
            "Epoch [12986/20000], Training Loss: 0.5345\n",
            "Epoch [12987/20000], Training Loss: 0.5036\n",
            "Epoch [12988/20000], Training Loss: 0.5105\n",
            "Epoch [12989/20000], Training Loss: 0.5231\n",
            "Epoch [12990/20000], Training Loss: 0.5379\n",
            "Epoch [12991/20000], Training Loss: 0.4732\n",
            "Epoch [12992/20000], Training Loss: 0.4742\n",
            "Epoch [12993/20000], Training Loss: 0.4898\n",
            "Epoch [12994/20000], Training Loss: 0.5496\n",
            "Epoch [12995/20000], Training Loss: 0.4727\n",
            "Epoch [12996/20000], Training Loss: 0.4687\n",
            "Epoch [12997/20000], Training Loss: 0.5216\n",
            "Epoch [12998/20000], Training Loss: 0.4986\n",
            "Epoch [12999/20000], Training Loss: 0.4525\n",
            "Epoch [13000/20000], Training Loss: 0.4979\n",
            "Epoch [13001/20000], Training Loss: 0.5267\n",
            "Epoch [13002/20000], Training Loss: 0.4502\n",
            "Epoch [13003/20000], Training Loss: 0.4818\n",
            "Epoch [13004/20000], Training Loss: 0.4936\n",
            "Epoch [13005/20000], Training Loss: 0.4271\n",
            "Epoch [13006/20000], Training Loss: 0.4530\n",
            "Epoch [13007/20000], Training Loss: 0.4803\n",
            "Epoch [13008/20000], Training Loss: 0.5001\n",
            "Epoch [13009/20000], Training Loss: 0.4995\n",
            "Epoch [13010/20000], Training Loss: 0.4589\n",
            "Epoch [13011/20000], Training Loss: 0.4561\n",
            "Epoch [13012/20000], Training Loss: 0.5115\n",
            "Epoch [13013/20000], Training Loss: 0.4521\n",
            "Epoch [13014/20000], Training Loss: 0.4893\n",
            "Epoch [13015/20000], Training Loss: 0.4702\n",
            "Epoch [13016/20000], Training Loss: 0.4941\n",
            "Epoch [13017/20000], Training Loss: 0.4947\n",
            "Epoch [13018/20000], Training Loss: 0.5066\n",
            "Epoch [13019/20000], Training Loss: 0.4468\n",
            "Epoch [13020/20000], Training Loss: 0.4830\n",
            "Epoch [13021/20000], Training Loss: 0.4629\n",
            "Epoch [13022/20000], Training Loss: 0.4719\n",
            "Epoch [13023/20000], Training Loss: 0.5023\n",
            "Epoch [13024/20000], Training Loss: 0.5053\n",
            "Epoch [13025/20000], Training Loss: 0.5047\n",
            "Epoch [13026/20000], Training Loss: 0.4876\n",
            "Epoch [13027/20000], Training Loss: 0.4658\n",
            "Epoch [13028/20000], Training Loss: 0.5413\n",
            "Epoch [13029/20000], Training Loss: 0.4596\n",
            "Epoch [13030/20000], Training Loss: 0.4890\n",
            "Epoch [13031/20000], Training Loss: 0.4809\n",
            "Epoch [13032/20000], Training Loss: 0.4669\n",
            "Epoch [13033/20000], Training Loss: 0.4743\n",
            "Epoch [13034/20000], Training Loss: 0.5003\n",
            "Epoch [13035/20000], Training Loss: 0.4967\n",
            "Epoch [13036/20000], Training Loss: 0.5007\n",
            "Epoch [13037/20000], Training Loss: 0.4878\n",
            "Epoch [13038/20000], Training Loss: 0.5158\n",
            "Epoch [13039/20000], Training Loss: 0.4960\n",
            "Epoch [13040/20000], Training Loss: 0.4479\n",
            "Epoch [13041/20000], Training Loss: 0.5112\n",
            "Epoch [13042/20000], Training Loss: 0.4893\n",
            "Epoch [13043/20000], Training Loss: 0.5004\n",
            "Epoch [13044/20000], Training Loss: 0.4859\n",
            "Epoch [13045/20000], Training Loss: 0.4858\n",
            "Epoch [13046/20000], Training Loss: 0.4852\n",
            "Epoch [13047/20000], Training Loss: 0.4639\n",
            "Epoch [13048/20000], Training Loss: 0.4647\n",
            "Epoch [13049/20000], Training Loss: 0.5050\n",
            "Epoch [13050/20000], Training Loss: 0.4991\n",
            "Epoch [13051/20000], Training Loss: 0.4576\n",
            "Epoch [13052/20000], Training Loss: 0.5527\n",
            "Epoch [13053/20000], Training Loss: 0.5136\n",
            "Epoch [13054/20000], Training Loss: 0.4824\n",
            "Epoch [13055/20000], Training Loss: 0.4429\n",
            "Epoch [13056/20000], Training Loss: 0.4902\n",
            "Epoch [13057/20000], Training Loss: 0.5367\n",
            "Epoch [13058/20000], Training Loss: 0.5028\n",
            "Epoch [13059/20000], Training Loss: 0.4315\n",
            "Epoch [13060/20000], Training Loss: 0.4982\n",
            "Epoch [13061/20000], Training Loss: 0.4473\n",
            "Epoch [13062/20000], Training Loss: 0.4883\n",
            "Epoch [13063/20000], Training Loss: 0.4528\n",
            "Epoch [13064/20000], Training Loss: 0.4610\n",
            "Epoch [13065/20000], Training Loss: 0.4722\n",
            "Epoch [13066/20000], Training Loss: 0.5024\n",
            "Epoch [13067/20000], Training Loss: 0.4899\n",
            "Epoch [13068/20000], Training Loss: 0.4466\n",
            "Epoch [13069/20000], Training Loss: 0.5059\n",
            "Epoch [13070/20000], Training Loss: 0.5017\n",
            "Epoch [13071/20000], Training Loss: 0.5082\n",
            "Epoch [13072/20000], Training Loss: 0.4470\n",
            "Epoch [13073/20000], Training Loss: 0.4955\n",
            "Epoch [13074/20000], Training Loss: 0.4688\n",
            "Epoch [13075/20000], Training Loss: 0.4717\n",
            "Epoch [13076/20000], Training Loss: 0.4688\n",
            "Epoch [13077/20000], Training Loss: 0.5137\n",
            "Epoch [13078/20000], Training Loss: 0.4839\n",
            "Epoch [13079/20000], Training Loss: 0.4820\n",
            "Epoch [13080/20000], Training Loss: 0.5207\n",
            "Epoch [13081/20000], Training Loss: 0.4718\n",
            "Epoch [13082/20000], Training Loss: 0.4490\n",
            "Epoch [13083/20000], Training Loss: 0.5013\n",
            "Epoch [13084/20000], Training Loss: 0.5482\n",
            "Epoch [13085/20000], Training Loss: 0.5448\n",
            "Epoch [13086/20000], Training Loss: 0.5504\n",
            "Epoch [13087/20000], Training Loss: 0.4664\n",
            "Epoch [13088/20000], Training Loss: 0.4334\n",
            "Epoch [13089/20000], Training Loss: 0.4739\n",
            "Epoch [13090/20000], Training Loss: 0.4713\n",
            "Epoch [13091/20000], Training Loss: 0.4975\n",
            "Epoch [13092/20000], Training Loss: 0.5096\n",
            "Epoch [13093/20000], Training Loss: 0.5070\n",
            "Epoch [13094/20000], Training Loss: 0.4786\n",
            "Epoch [13095/20000], Training Loss: 0.4750\n",
            "Epoch [13096/20000], Training Loss: 0.4736\n",
            "Epoch [13097/20000], Training Loss: 0.4693\n",
            "Epoch [13098/20000], Training Loss: 0.4921\n",
            "Epoch [13099/20000], Training Loss: 0.5024\n",
            "Epoch [13100/20000], Training Loss: 0.5262\n",
            "Epoch [13101/20000], Training Loss: 0.4971\n",
            "Epoch [13102/20000], Training Loss: 0.4983\n",
            "Epoch [13103/20000], Training Loss: 0.5279\n",
            "Epoch [13104/20000], Training Loss: 0.4877\n",
            "Epoch [13105/20000], Training Loss: 0.4471\n",
            "Epoch [13106/20000], Training Loss: 0.5026\n",
            "Epoch [13107/20000], Training Loss: 0.4638\n",
            "Epoch [13108/20000], Training Loss: 0.5199\n",
            "Epoch [13109/20000], Training Loss: 0.4885\n",
            "Epoch [13110/20000], Training Loss: 0.4944\n",
            "Epoch [13111/20000], Training Loss: 0.4911\n",
            "Epoch [13112/20000], Training Loss: 0.4552\n",
            "Epoch [13113/20000], Training Loss: 0.4775\n",
            "Epoch [13114/20000], Training Loss: 0.4716\n",
            "Epoch [13115/20000], Training Loss: 0.4649\n",
            "Epoch [13116/20000], Training Loss: 0.4692\n",
            "Epoch [13117/20000], Training Loss: 0.4809\n",
            "Epoch [13118/20000], Training Loss: 0.4736\n",
            "Epoch [13119/20000], Training Loss: 0.4962\n",
            "Epoch [13120/20000], Training Loss: 0.4863\n",
            "Epoch [13121/20000], Training Loss: 0.4857\n",
            "Epoch [13122/20000], Training Loss: 0.5117\n",
            "Epoch [13123/20000], Training Loss: 0.5298\n",
            "Epoch [13124/20000], Training Loss: 0.4667\n",
            "Epoch [13125/20000], Training Loss: 0.5342\n",
            "Epoch [13126/20000], Training Loss: 0.5338\n",
            "Epoch [13127/20000], Training Loss: 0.5252\n",
            "Epoch [13128/20000], Training Loss: 0.5201\n",
            "Epoch [13129/20000], Training Loss: 0.4804\n",
            "Epoch [13130/20000], Training Loss: 0.4900\n",
            "Epoch [13131/20000], Training Loss: 0.4694\n",
            "Epoch [13132/20000], Training Loss: 0.5083\n",
            "Epoch [13133/20000], Training Loss: 0.4998\n",
            "Epoch [13134/20000], Training Loss: 0.4863\n",
            "Epoch [13135/20000], Training Loss: 0.4687\n",
            "Epoch [13136/20000], Training Loss: 0.5229\n",
            "Epoch [13137/20000], Training Loss: 0.5179\n",
            "Epoch [13138/20000], Training Loss: 0.4988\n",
            "Epoch [13139/20000], Training Loss: 0.4847\n",
            "Epoch [13140/20000], Training Loss: 0.4744\n",
            "Epoch [13141/20000], Training Loss: 0.5005\n",
            "Epoch [13142/20000], Training Loss: 0.4934\n",
            "Epoch [13143/20000], Training Loss: 0.5130\n",
            "Epoch [13144/20000], Training Loss: 0.4932\n",
            "Epoch [13145/20000], Training Loss: 0.4791\n",
            "Epoch [13146/20000], Training Loss: 0.5056\n",
            "Epoch [13147/20000], Training Loss: 0.4887\n",
            "Epoch [13148/20000], Training Loss: 0.5008\n",
            "Epoch [13149/20000], Training Loss: 0.4819\n",
            "Epoch [13150/20000], Training Loss: 0.4235\n",
            "Epoch [13151/20000], Training Loss: 0.4816\n",
            "Epoch [13152/20000], Training Loss: 0.4679\n",
            "Epoch [13153/20000], Training Loss: 0.5400\n",
            "Epoch [13154/20000], Training Loss: 0.4640\n",
            "Epoch [13155/20000], Training Loss: 0.4871\n",
            "Epoch [13156/20000], Training Loss: 0.4811\n",
            "Epoch [13157/20000], Training Loss: 0.4907\n",
            "Epoch [13158/20000], Training Loss: 0.4351\n",
            "Epoch [13159/20000], Training Loss: 0.5282\n",
            "Epoch [13160/20000], Training Loss: 0.4870\n",
            "Epoch [13161/20000], Training Loss: 0.5070\n",
            "Epoch [13162/20000], Training Loss: 0.4492\n",
            "Epoch [13163/20000], Training Loss: 0.5025\n",
            "Epoch [13164/20000], Training Loss: 0.4957\n",
            "Epoch [13165/20000], Training Loss: 0.5225\n",
            "Epoch [13166/20000], Training Loss: 0.4857\n",
            "Epoch [13167/20000], Training Loss: 0.4606\n",
            "Epoch [13168/20000], Training Loss: 0.5076\n",
            "Epoch [13169/20000], Training Loss: 0.4948\n",
            "Epoch [13170/20000], Training Loss: 0.4615\n",
            "Epoch [13171/20000], Training Loss: 0.4929\n",
            "Epoch [13172/20000], Training Loss: 0.4922\n",
            "Epoch [13173/20000], Training Loss: 0.5006\n",
            "Epoch [13174/20000], Training Loss: 0.4917\n",
            "Epoch [13175/20000], Training Loss: 0.5096\n",
            "Epoch [13176/20000], Training Loss: 0.4893\n",
            "Epoch [13177/20000], Training Loss: 0.4488\n",
            "Epoch [13178/20000], Training Loss: 0.4721\n",
            "Epoch [13179/20000], Training Loss: 0.5017\n",
            "Epoch [13180/20000], Training Loss: 0.5080\n",
            "Epoch [13181/20000], Training Loss: 0.4610\n",
            "Epoch [13182/20000], Training Loss: 0.4552\n",
            "Epoch [13183/20000], Training Loss: 0.4891\n",
            "Epoch [13184/20000], Training Loss: 0.5468\n",
            "Epoch [13185/20000], Training Loss: 0.4717\n",
            "Epoch [13186/20000], Training Loss: 0.4990\n",
            "Epoch [13187/20000], Training Loss: 0.4681\n",
            "Epoch [13188/20000], Training Loss: 0.5060\n",
            "Epoch [13189/20000], Training Loss: 0.4594\n",
            "Epoch [13190/20000], Training Loss: 0.5004\n",
            "Epoch [13191/20000], Training Loss: 0.4679\n",
            "Epoch [13192/20000], Training Loss: 0.4871\n",
            "Epoch [13193/20000], Training Loss: 0.5143\n",
            "Epoch [13194/20000], Training Loss: 0.5150\n",
            "Epoch [13195/20000], Training Loss: 0.5304\n",
            "Epoch [13196/20000], Training Loss: 0.5449\n",
            "Epoch [13197/20000], Training Loss: 0.4785\n",
            "Epoch [13198/20000], Training Loss: 0.4720\n",
            "Epoch [13199/20000], Training Loss: 0.4901\n",
            "Epoch [13200/20000], Training Loss: 0.4780\n",
            "Epoch [13201/20000], Training Loss: 0.4938\n",
            "Epoch [13202/20000], Training Loss: 0.4888\n",
            "Epoch [13203/20000], Training Loss: 0.5581\n",
            "Epoch [13204/20000], Training Loss: 0.4675\n",
            "Epoch [13205/20000], Training Loss: 0.4839\n",
            "Epoch [13206/20000], Training Loss: 0.4708\n",
            "Epoch [13207/20000], Training Loss: 0.4834\n",
            "Epoch [13208/20000], Training Loss: 0.5144\n",
            "Epoch [13209/20000], Training Loss: 0.4822\n",
            "Epoch [13210/20000], Training Loss: 0.4544\n",
            "Epoch [13211/20000], Training Loss: 0.4433\n",
            "Epoch [13212/20000], Training Loss: 0.4982\n",
            "Epoch [13213/20000], Training Loss: 0.4530\n",
            "Epoch [13214/20000], Training Loss: 0.5078\n",
            "Epoch [13215/20000], Training Loss: 0.4750\n",
            "Epoch [13216/20000], Training Loss: 0.4924\n",
            "Epoch [13217/20000], Training Loss: 0.4659\n",
            "Epoch [13218/20000], Training Loss: 0.4394\n",
            "Epoch [13219/20000], Training Loss: 0.4665\n",
            "Epoch [13220/20000], Training Loss: 0.5370\n",
            "Epoch [13221/20000], Training Loss: 0.4987\n",
            "Epoch [13222/20000], Training Loss: 0.4672\n",
            "Epoch [13223/20000], Training Loss: 0.4891\n",
            "Epoch [13224/20000], Training Loss: 0.5410\n",
            "Epoch [13225/20000], Training Loss: 0.4760\n",
            "Epoch [13226/20000], Training Loss: 0.4997\n",
            "Epoch [13227/20000], Training Loss: 0.5167\n",
            "Epoch [13228/20000], Training Loss: 0.4589\n",
            "Epoch [13229/20000], Training Loss: 0.4874\n",
            "Epoch [13230/20000], Training Loss: 0.4752\n",
            "Epoch [13231/20000], Training Loss: 0.4874\n",
            "Epoch [13232/20000], Training Loss: 0.4945\n",
            "Epoch [13233/20000], Training Loss: 0.5018\n",
            "Epoch [13234/20000], Training Loss: 0.4980\n",
            "Epoch [13235/20000], Training Loss: 0.4832\n",
            "Epoch [13236/20000], Training Loss: 0.4962\n",
            "Epoch [13237/20000], Training Loss: 0.4858\n",
            "Epoch [13238/20000], Training Loss: 0.5226\n",
            "Epoch [13239/20000], Training Loss: 0.4675\n",
            "Epoch [13240/20000], Training Loss: 0.4773\n",
            "Epoch [13241/20000], Training Loss: 0.5235\n",
            "Epoch [13242/20000], Training Loss: 0.4918\n",
            "Epoch [13243/20000], Training Loss: 0.4855\n",
            "Epoch [13244/20000], Training Loss: 0.4939\n",
            "Epoch [13245/20000], Training Loss: 0.4990\n",
            "Epoch [13246/20000], Training Loss: 0.5061\n",
            "Epoch [13247/20000], Training Loss: 0.5000\n",
            "Epoch [13248/20000], Training Loss: 0.4702\n",
            "Epoch [13249/20000], Training Loss: 0.4636\n",
            "Epoch [13250/20000], Training Loss: 0.4717\n",
            "Epoch [13251/20000], Training Loss: 0.4606\n",
            "Epoch [13252/20000], Training Loss: 0.4511\n",
            "Epoch [13253/20000], Training Loss: 0.5048\n",
            "Epoch [13254/20000], Training Loss: 0.4987\n",
            "Epoch [13255/20000], Training Loss: 0.5205\n",
            "Epoch [13256/20000], Training Loss: 0.4655\n",
            "Epoch [13257/20000], Training Loss: 0.4774\n",
            "Epoch [13258/20000], Training Loss: 0.4774\n",
            "Epoch [13259/20000], Training Loss: 0.5397\n",
            "Epoch [13260/20000], Training Loss: 0.4807\n",
            "Epoch [13261/20000], Training Loss: 0.4570\n",
            "Epoch [13262/20000], Training Loss: 0.4649\n",
            "Epoch [13263/20000], Training Loss: 0.4584\n",
            "Epoch [13264/20000], Training Loss: 0.5404\n",
            "Epoch [13265/20000], Training Loss: 0.5281\n",
            "Epoch [13266/20000], Training Loss: 0.5000\n",
            "Epoch [13267/20000], Training Loss: 0.5050\n",
            "Epoch [13268/20000], Training Loss: 0.4555\n",
            "Epoch [13269/20000], Training Loss: 0.5214\n",
            "Epoch [13270/20000], Training Loss: 0.4546\n",
            "Epoch [13271/20000], Training Loss: 0.4548\n",
            "Epoch [13272/20000], Training Loss: 0.4876\n",
            "Epoch [13273/20000], Training Loss: 0.4644\n",
            "Epoch [13274/20000], Training Loss: 0.5193\n",
            "Epoch [13275/20000], Training Loss: 0.4726\n",
            "Epoch [13276/20000], Training Loss: 0.4923\n",
            "Epoch [13277/20000], Training Loss: 0.4750\n",
            "Epoch [13278/20000], Training Loss: 0.4662\n",
            "Epoch [13279/20000], Training Loss: 0.4585\n",
            "Epoch [13280/20000], Training Loss: 0.4781\n",
            "Epoch [13281/20000], Training Loss: 0.5076\n",
            "Epoch [13282/20000], Training Loss: 0.5122\n",
            "Epoch [13283/20000], Training Loss: 0.4686\n",
            "Epoch [13284/20000], Training Loss: 0.4662\n",
            "Epoch [13285/20000], Training Loss: 0.4662\n",
            "Epoch [13286/20000], Training Loss: 0.4784\n",
            "Epoch [13287/20000], Training Loss: 0.4672\n",
            "Epoch [13288/20000], Training Loss: 0.4933\n",
            "Epoch [13289/20000], Training Loss: 0.4528\n",
            "Epoch [13290/20000], Training Loss: 0.5026\n",
            "Epoch [13291/20000], Training Loss: 0.5242\n",
            "Epoch [13292/20000], Training Loss: 0.5198\n",
            "Epoch [13293/20000], Training Loss: 0.4666\n",
            "Epoch [13294/20000], Training Loss: 0.4904\n",
            "Epoch [13295/20000], Training Loss: 0.4449\n",
            "Epoch [13296/20000], Training Loss: 0.4416\n",
            "Epoch [13297/20000], Training Loss: 0.4681\n",
            "Epoch [13298/20000], Training Loss: 0.4572\n",
            "Epoch [13299/20000], Training Loss: 0.4851\n",
            "Epoch [13300/20000], Training Loss: 0.5141\n",
            "Epoch [13301/20000], Training Loss: 0.4567\n",
            "Epoch [13302/20000], Training Loss: 0.4717\n",
            "Epoch [13303/20000], Training Loss: 0.4800\n",
            "Epoch [13304/20000], Training Loss: 0.4731\n",
            "Epoch [13305/20000], Training Loss: 0.5021\n",
            "Epoch [13306/20000], Training Loss: 0.5211\n",
            "Epoch [13307/20000], Training Loss: 0.4873\n",
            "Epoch [13308/20000], Training Loss: 0.4899\n",
            "Epoch [13309/20000], Training Loss: 0.5232\n",
            "Epoch [13310/20000], Training Loss: 0.5009\n",
            "Epoch [13311/20000], Training Loss: 0.4951\n",
            "Epoch [13312/20000], Training Loss: 0.4629\n",
            "Epoch [13313/20000], Training Loss: 0.4805\n",
            "Epoch [13314/20000], Training Loss: 0.4582\n",
            "Epoch [13315/20000], Training Loss: 0.5075\n",
            "Epoch [13316/20000], Training Loss: 0.5209\n",
            "Epoch [13317/20000], Training Loss: 0.4686\n",
            "Epoch [13318/20000], Training Loss: 0.4838\n",
            "Epoch [13319/20000], Training Loss: 0.5002\n",
            "Epoch [13320/20000], Training Loss: 0.4929\n",
            "Epoch [13321/20000], Training Loss: 0.5283\n",
            "Epoch [13322/20000], Training Loss: 0.4890\n",
            "Epoch [13323/20000], Training Loss: 0.5165\n",
            "Epoch [13324/20000], Training Loss: 0.5299\n",
            "Epoch [13325/20000], Training Loss: 0.4362\n",
            "Epoch [13326/20000], Training Loss: 0.4944\n",
            "Epoch [13327/20000], Training Loss: 0.4374\n",
            "Epoch [13328/20000], Training Loss: 0.5095\n",
            "Epoch [13329/20000], Training Loss: 0.4890\n",
            "Epoch [13330/20000], Training Loss: 0.4527\n",
            "Epoch [13331/20000], Training Loss: 0.4762\n",
            "Epoch [13332/20000], Training Loss: 0.4833\n",
            "Epoch [13333/20000], Training Loss: 0.5156\n",
            "Epoch [13334/20000], Training Loss: 0.4619\n",
            "Epoch [13335/20000], Training Loss: 0.4860\n",
            "Epoch [13336/20000], Training Loss: 0.4843\n",
            "Epoch [13337/20000], Training Loss: 0.4495\n",
            "Epoch [13338/20000], Training Loss: 0.4656\n",
            "Epoch [13339/20000], Training Loss: 0.5061\n",
            "Epoch [13340/20000], Training Loss: 0.4952\n",
            "Epoch [13341/20000], Training Loss: 0.4952\n",
            "Epoch [13342/20000], Training Loss: 0.5118\n",
            "Epoch [13343/20000], Training Loss: 0.4685\n",
            "Epoch [13344/20000], Training Loss: 0.4569\n",
            "Epoch [13345/20000], Training Loss: 0.4905\n",
            "Epoch [13346/20000], Training Loss: 0.5223\n",
            "Epoch [13347/20000], Training Loss: 0.5131\n",
            "Epoch [13348/20000], Training Loss: 0.4663\n",
            "Epoch [13349/20000], Training Loss: 0.4603\n",
            "Epoch [13350/20000], Training Loss: 0.5052\n",
            "Epoch [13351/20000], Training Loss: 0.5416\n",
            "Epoch [13352/20000], Training Loss: 0.4641\n",
            "Epoch [13353/20000], Training Loss: 0.4561\n",
            "Epoch [13354/20000], Training Loss: 0.5029\n",
            "Epoch [13355/20000], Training Loss: 0.4696\n",
            "Epoch [13356/20000], Training Loss: 0.4485\n",
            "Epoch [13357/20000], Training Loss: 0.5319\n",
            "Epoch [13358/20000], Training Loss: 0.5255\n",
            "Epoch [13359/20000], Training Loss: 0.5294\n",
            "Epoch [13360/20000], Training Loss: 0.4675\n",
            "Epoch [13361/20000], Training Loss: 0.4997\n",
            "Epoch [13362/20000], Training Loss: 0.4742\n",
            "Epoch [13363/20000], Training Loss: 0.4692\n",
            "Epoch [13364/20000], Training Loss: 0.4628\n",
            "Epoch [13365/20000], Training Loss: 0.4997\n",
            "Epoch [13366/20000], Training Loss: 0.4385\n",
            "Epoch [13367/20000], Training Loss: 0.4801\n",
            "Epoch [13368/20000], Training Loss: 0.5201\n",
            "Epoch [13369/20000], Training Loss: 0.5268\n",
            "Epoch [13370/20000], Training Loss: 0.4676\n",
            "Epoch [13371/20000], Training Loss: 0.5314\n",
            "Epoch [13372/20000], Training Loss: 0.4753\n",
            "Epoch [13373/20000], Training Loss: 0.4610\n",
            "Epoch [13374/20000], Training Loss: 0.5283\n",
            "Epoch [13375/20000], Training Loss: 0.4985\n",
            "Epoch [13376/20000], Training Loss: 0.5209\n",
            "Epoch [13377/20000], Training Loss: 0.4766\n",
            "Epoch [13378/20000], Training Loss: 0.4916\n",
            "Epoch [13379/20000], Training Loss: 0.4713\n",
            "Epoch [13380/20000], Training Loss: 0.4742\n",
            "Epoch [13381/20000], Training Loss: 0.4816\n",
            "Epoch [13382/20000], Training Loss: 0.5187\n",
            "Epoch [13383/20000], Training Loss: 0.4716\n",
            "Epoch [13384/20000], Training Loss: 0.4579\n",
            "Epoch [13385/20000], Training Loss: 0.4501\n",
            "Epoch [13386/20000], Training Loss: 0.5081\n",
            "Epoch [13387/20000], Training Loss: 0.5315\n",
            "Epoch [13388/20000], Training Loss: 0.4753\n",
            "Epoch [13389/20000], Training Loss: 0.4676\n",
            "Epoch [13390/20000], Training Loss: 0.4458\n",
            "Epoch [13391/20000], Training Loss: 0.5014\n",
            "Epoch [13392/20000], Training Loss: 0.4581\n",
            "Epoch [13393/20000], Training Loss: 0.4989\n",
            "Epoch [13394/20000], Training Loss: 0.4785\n",
            "Epoch [13395/20000], Training Loss: 0.4959\n",
            "Epoch [13396/20000], Training Loss: 0.4791\n",
            "Epoch [13397/20000], Training Loss: 0.4703\n",
            "Epoch [13398/20000], Training Loss: 0.4743\n",
            "Epoch [13399/20000], Training Loss: 0.4730\n",
            "Epoch [13400/20000], Training Loss: 0.4533\n",
            "Epoch [13401/20000], Training Loss: 0.4905\n",
            "Epoch [13402/20000], Training Loss: 0.4642\n",
            "Epoch [13403/20000], Training Loss: 0.4861\n",
            "Epoch [13404/20000], Training Loss: 0.5099\n",
            "Epoch [13405/20000], Training Loss: 0.4941\n",
            "Epoch [13406/20000], Training Loss: 0.4429\n",
            "Epoch [13407/20000], Training Loss: 0.4847\n",
            "Epoch [13408/20000], Training Loss: 0.5091\n",
            "Epoch [13409/20000], Training Loss: 0.4951\n",
            "Epoch [13410/20000], Training Loss: 0.4696\n",
            "Epoch [13411/20000], Training Loss: 0.4969\n",
            "Epoch [13412/20000], Training Loss: 0.5374\n",
            "Epoch [13413/20000], Training Loss: 0.4616\n",
            "Epoch [13414/20000], Training Loss: 0.4828\n",
            "Epoch [13415/20000], Training Loss: 0.4993\n",
            "Epoch [13416/20000], Training Loss: 0.5027\n",
            "Epoch [13417/20000], Training Loss: 0.4835\n",
            "Epoch [13418/20000], Training Loss: 0.4419\n",
            "Epoch [13419/20000], Training Loss: 0.4929\n",
            "Epoch [13420/20000], Training Loss: 0.4596\n",
            "Epoch [13421/20000], Training Loss: 0.5357\n",
            "Epoch [13422/20000], Training Loss: 0.4772\n",
            "Epoch [13423/20000], Training Loss: 0.4458\n",
            "Epoch [13424/20000], Training Loss: 0.4529\n",
            "Epoch [13425/20000], Training Loss: 0.4893\n",
            "Epoch [13426/20000], Training Loss: 0.4832\n",
            "Epoch [13427/20000], Training Loss: 0.4417\n",
            "Epoch [13428/20000], Training Loss: 0.4509\n",
            "Epoch [13429/20000], Training Loss: 0.4433\n",
            "Epoch [13430/20000], Training Loss: 0.4463\n",
            "Epoch [13431/20000], Training Loss: 0.4854\n",
            "Epoch [13432/20000], Training Loss: 0.4786\n",
            "Epoch [13433/20000], Training Loss: 0.4691\n",
            "Epoch [13434/20000], Training Loss: 0.4840\n",
            "Epoch [13435/20000], Training Loss: 0.4599\n",
            "Epoch [13436/20000], Training Loss: 0.4473\n",
            "Epoch [13437/20000], Training Loss: 0.4808\n",
            "Epoch [13438/20000], Training Loss: 0.4849\n",
            "Epoch [13439/20000], Training Loss: 0.4384\n",
            "Epoch [13440/20000], Training Loss: 0.4997\n",
            "Epoch [13441/20000], Training Loss: 0.4734\n",
            "Epoch [13442/20000], Training Loss: 0.4692\n",
            "Epoch [13443/20000], Training Loss: 0.4387\n",
            "Epoch [13444/20000], Training Loss: 0.4804\n",
            "Epoch [13445/20000], Training Loss: 0.4847\n",
            "Epoch [13446/20000], Training Loss: 0.5076\n",
            "Epoch [13447/20000], Training Loss: 0.4848\n",
            "Epoch [13448/20000], Training Loss: 0.4974\n",
            "Epoch [13449/20000], Training Loss: 0.5049\n",
            "Epoch [13450/20000], Training Loss: 0.4834\n",
            "Epoch [13451/20000], Training Loss: 0.5154\n",
            "Epoch [13452/20000], Training Loss: 0.5085\n",
            "Epoch [13453/20000], Training Loss: 0.5424\n",
            "Epoch [13454/20000], Training Loss: 0.4883\n",
            "Epoch [13455/20000], Training Loss: 0.4905\n",
            "Epoch [13456/20000], Training Loss: 0.4411\n",
            "Epoch [13457/20000], Training Loss: 0.4537\n",
            "Epoch [13458/20000], Training Loss: 0.4616\n",
            "Epoch [13459/20000], Training Loss: 0.4918\n",
            "Epoch [13460/20000], Training Loss: 0.4986\n",
            "Epoch [13461/20000], Training Loss: 0.4635\n",
            "Epoch [13462/20000], Training Loss: 0.4422\n",
            "Epoch [13463/20000], Training Loss: 0.4621\n",
            "Epoch [13464/20000], Training Loss: 0.4956\n",
            "Epoch [13465/20000], Training Loss: 0.4690\n",
            "Epoch [13466/20000], Training Loss: 0.5018\n",
            "Epoch [13467/20000], Training Loss: 0.4847\n",
            "Epoch [13468/20000], Training Loss: 0.4500\n",
            "Epoch [13469/20000], Training Loss: 0.4636\n",
            "Epoch [13470/20000], Training Loss: 0.4996\n",
            "Epoch [13471/20000], Training Loss: 0.4691\n",
            "Epoch [13472/20000], Training Loss: 0.5159\n",
            "Epoch [13473/20000], Training Loss: 0.4852\n",
            "Epoch [13474/20000], Training Loss: 0.4850\n",
            "Epoch [13475/20000], Training Loss: 0.5045\n",
            "Epoch [13476/20000], Training Loss: 0.4486\n",
            "Epoch [13477/20000], Training Loss: 0.4466\n",
            "Epoch [13478/20000], Training Loss: 0.4924\n",
            "Epoch [13479/20000], Training Loss: 0.5017\n",
            "Epoch [13480/20000], Training Loss: 0.4881\n",
            "Epoch [13481/20000], Training Loss: 0.4633\n",
            "Epoch [13482/20000], Training Loss: 0.5195\n",
            "Epoch [13483/20000], Training Loss: 0.4772\n",
            "Epoch [13484/20000], Training Loss: 0.4789\n",
            "Epoch [13485/20000], Training Loss: 0.4920\n",
            "Epoch [13486/20000], Training Loss: 0.4794\n",
            "Epoch [13487/20000], Training Loss: 0.5015\n",
            "Epoch [13488/20000], Training Loss: 0.5184\n",
            "Epoch [13489/20000], Training Loss: 0.5272\n",
            "Epoch [13490/20000], Training Loss: 0.4344\n",
            "Epoch [13491/20000], Training Loss: 0.4682\n",
            "Epoch [13492/20000], Training Loss: 0.5136\n",
            "Epoch [13493/20000], Training Loss: 0.4857\n",
            "Epoch [13494/20000], Training Loss: 0.4902\n",
            "Epoch [13495/20000], Training Loss: 0.4824\n",
            "Epoch [13496/20000], Training Loss: 0.4670\n",
            "Epoch [13497/20000], Training Loss: 0.4688\n",
            "Epoch [13498/20000], Training Loss: 0.5227\n",
            "Epoch [13499/20000], Training Loss: 0.4779\n",
            "Epoch [13500/20000], Training Loss: 0.5050\n",
            "Epoch [13501/20000], Training Loss: 0.5157\n",
            "Epoch [13502/20000], Training Loss: 0.4691\n",
            "Epoch [13503/20000], Training Loss: 0.4555\n",
            "Epoch [13504/20000], Training Loss: 0.4550\n",
            "Epoch [13505/20000], Training Loss: 0.5070\n",
            "Epoch [13506/20000], Training Loss: 0.4874\n",
            "Epoch [13507/20000], Training Loss: 0.4778\n",
            "Epoch [13508/20000], Training Loss: 0.4585\n",
            "Epoch [13509/20000], Training Loss: 0.5006\n",
            "Epoch [13510/20000], Training Loss: 0.4585\n",
            "Epoch [13511/20000], Training Loss: 0.4708\n",
            "Epoch [13512/20000], Training Loss: 0.4689\n",
            "Epoch [13513/20000], Training Loss: 0.4539\n",
            "Epoch [13514/20000], Training Loss: 0.4806\n",
            "Epoch [13515/20000], Training Loss: 0.4826\n",
            "Epoch [13516/20000], Training Loss: 0.4342\n",
            "Epoch [13517/20000], Training Loss: 0.4643\n",
            "Epoch [13518/20000], Training Loss: 0.4525\n",
            "Epoch [13519/20000], Training Loss: 0.5084\n",
            "Epoch [13520/20000], Training Loss: 0.4936\n",
            "Epoch [13521/20000], Training Loss: 0.4933\n",
            "Epoch [13522/20000], Training Loss: 0.5024\n",
            "Epoch [13523/20000], Training Loss: 0.4528\n",
            "Epoch [13524/20000], Training Loss: 0.4787\n",
            "Epoch [13525/20000], Training Loss: 0.5225\n",
            "Epoch [13526/20000], Training Loss: 0.4687\n",
            "Epoch [13527/20000], Training Loss: 0.4798\n",
            "Epoch [13528/20000], Training Loss: 0.4910\n",
            "Epoch [13529/20000], Training Loss: 0.4744\n",
            "Epoch [13530/20000], Training Loss: 0.4926\n",
            "Epoch [13531/20000], Training Loss: 0.4714\n",
            "Epoch [13532/20000], Training Loss: 0.4735\n",
            "Epoch [13533/20000], Training Loss: 0.4710\n",
            "Epoch [13534/20000], Training Loss: 0.4716\n",
            "Epoch [13535/20000], Training Loss: 0.4908\n",
            "Epoch [13536/20000], Training Loss: 0.5489\n",
            "Epoch [13537/20000], Training Loss: 0.4849\n",
            "Epoch [13538/20000], Training Loss: 0.4818\n",
            "Epoch [13539/20000], Training Loss: 0.4746\n",
            "Epoch [13540/20000], Training Loss: 0.4936\n",
            "Epoch [13541/20000], Training Loss: 0.4741\n",
            "Epoch [13542/20000], Training Loss: 0.5078\n",
            "Epoch [13543/20000], Training Loss: 0.4803\n",
            "Epoch [13544/20000], Training Loss: 0.4512\n",
            "Epoch [13545/20000], Training Loss: 0.4942\n",
            "Epoch [13546/20000], Training Loss: 0.4829\n",
            "Epoch [13547/20000], Training Loss: 0.5052\n",
            "Epoch [13548/20000], Training Loss: 0.4840\n",
            "Epoch [13549/20000], Training Loss: 0.4679\n",
            "Epoch [13550/20000], Training Loss: 0.4915\n",
            "Epoch [13551/20000], Training Loss: 0.4256\n",
            "Epoch [13552/20000], Training Loss: 0.4712\n",
            "Epoch [13553/20000], Training Loss: 0.5550\n",
            "Epoch [13554/20000], Training Loss: 0.4825\n",
            "Epoch [13555/20000], Training Loss: 0.4757\n",
            "Epoch [13556/20000], Training Loss: 0.5171\n",
            "Epoch [13557/20000], Training Loss: 0.4923\n",
            "Epoch [13558/20000], Training Loss: 0.5006\n",
            "Epoch [13559/20000], Training Loss: 0.4767\n",
            "Epoch [13560/20000], Training Loss: 0.4696\n",
            "Epoch [13561/20000], Training Loss: 0.5155\n",
            "Epoch [13562/20000], Training Loss: 0.4954\n",
            "Epoch [13563/20000], Training Loss: 0.5057\n",
            "Epoch [13564/20000], Training Loss: 0.4552\n",
            "Epoch [13565/20000], Training Loss: 0.4865\n",
            "Epoch [13566/20000], Training Loss: 0.4865\n",
            "Epoch [13567/20000], Training Loss: 0.4814\n",
            "Epoch [13568/20000], Training Loss: 0.4614\n",
            "Epoch [13569/20000], Training Loss: 0.4853\n",
            "Epoch [13570/20000], Training Loss: 0.4753\n",
            "Epoch [13571/20000], Training Loss: 0.5198\n",
            "Epoch [13572/20000], Training Loss: 0.5475\n",
            "Epoch [13573/20000], Training Loss: 0.4530\n",
            "Epoch [13574/20000], Training Loss: 0.5060\n",
            "Epoch [13575/20000], Training Loss: 0.4895\n",
            "Epoch [13576/20000], Training Loss: 0.4907\n",
            "Epoch [13577/20000], Training Loss: 0.4519\n",
            "Epoch [13578/20000], Training Loss: 0.5003\n",
            "Epoch [13579/20000], Training Loss: 0.4332\n",
            "Epoch [13580/20000], Training Loss: 0.4805\n",
            "Epoch [13581/20000], Training Loss: 0.4708\n",
            "Epoch [13582/20000], Training Loss: 0.5085\n",
            "Epoch [13583/20000], Training Loss: 0.4906\n",
            "Epoch [13584/20000], Training Loss: 0.4962\n",
            "Epoch [13585/20000], Training Loss: 0.4992\n",
            "Epoch [13586/20000], Training Loss: 0.5170\n",
            "Epoch [13587/20000], Training Loss: 0.4717\n",
            "Epoch [13588/20000], Training Loss: 0.4877\n",
            "Epoch [13589/20000], Training Loss: 0.4750\n",
            "Epoch [13590/20000], Training Loss: 0.4729\n",
            "Epoch [13591/20000], Training Loss: 0.4838\n",
            "Epoch [13592/20000], Training Loss: 0.5403\n",
            "Epoch [13593/20000], Training Loss: 0.4787\n",
            "Epoch [13594/20000], Training Loss: 0.4926\n",
            "Epoch [13595/20000], Training Loss: 0.4466\n",
            "Epoch [13596/20000], Training Loss: 0.4914\n",
            "Epoch [13597/20000], Training Loss: 0.4756\n",
            "Epoch [13598/20000], Training Loss: 0.4315\n",
            "Epoch [13599/20000], Training Loss: 0.4726\n",
            "Epoch [13600/20000], Training Loss: 0.5223\n",
            "Epoch [13601/20000], Training Loss: 0.4897\n",
            "Epoch [13602/20000], Training Loss: 0.4906\n",
            "Epoch [13603/20000], Training Loss: 0.5014\n",
            "Epoch [13604/20000], Training Loss: 0.4528\n",
            "Epoch [13605/20000], Training Loss: 0.4996\n",
            "Epoch [13606/20000], Training Loss: 0.4710\n",
            "Epoch [13607/20000], Training Loss: 0.4821\n",
            "Epoch [13608/20000], Training Loss: 0.4779\n",
            "Epoch [13609/20000], Training Loss: 0.4669\n",
            "Epoch [13610/20000], Training Loss: 0.5055\n",
            "Epoch [13611/20000], Training Loss: 0.5179\n",
            "Epoch [13612/20000], Training Loss: 0.4487\n",
            "Epoch [13613/20000], Training Loss: 0.4793\n",
            "Epoch [13614/20000], Training Loss: 0.4602\n",
            "Epoch [13615/20000], Training Loss: 0.5351\n",
            "Epoch [13616/20000], Training Loss: 0.4844\n",
            "Epoch [13617/20000], Training Loss: 0.4649\n",
            "Epoch [13618/20000], Training Loss: 0.4895\n",
            "Epoch [13619/20000], Training Loss: 0.4662\n",
            "Epoch [13620/20000], Training Loss: 0.4880\n",
            "Epoch [13621/20000], Training Loss: 0.5067\n",
            "Epoch [13622/20000], Training Loss: 0.4743\n",
            "Epoch [13623/20000], Training Loss: 0.4705\n",
            "Epoch [13624/20000], Training Loss: 0.4671\n",
            "Epoch [13625/20000], Training Loss: 0.4687\n",
            "Epoch [13626/20000], Training Loss: 0.5008\n",
            "Epoch [13627/20000], Training Loss: 0.4990\n",
            "Epoch [13628/20000], Training Loss: 0.4727\n",
            "Epoch [13629/20000], Training Loss: 0.4548\n",
            "Epoch [13630/20000], Training Loss: 0.5317\n",
            "Epoch [13631/20000], Training Loss: 0.4860\n",
            "Epoch [13632/20000], Training Loss: 0.4850\n",
            "Epoch [13633/20000], Training Loss: 0.5011\n",
            "Epoch [13634/20000], Training Loss: 0.4665\n",
            "Epoch [13635/20000], Training Loss: 0.5222\n",
            "Epoch [13636/20000], Training Loss: 0.5179\n",
            "Epoch [13637/20000], Training Loss: 0.4966\n",
            "Epoch [13638/20000], Training Loss: 0.5387\n",
            "Epoch [13639/20000], Training Loss: 0.4582\n",
            "Epoch [13640/20000], Training Loss: 0.4860\n",
            "Epoch [13641/20000], Training Loss: 0.4940\n",
            "Epoch [13642/20000], Training Loss: 0.4786\n",
            "Epoch [13643/20000], Training Loss: 0.4762\n",
            "Epoch [13644/20000], Training Loss: 0.4679\n",
            "Epoch [13645/20000], Training Loss: 0.5052\n",
            "Epoch [13646/20000], Training Loss: 0.5422\n",
            "Epoch [13647/20000], Training Loss: 0.4800\n",
            "Epoch [13648/20000], Training Loss: 0.4711\n",
            "Epoch [13649/20000], Training Loss: 0.4591\n",
            "Epoch [13650/20000], Training Loss: 0.4875\n",
            "Epoch [13651/20000], Training Loss: 0.4907\n",
            "Epoch [13652/20000], Training Loss: 0.4903\n",
            "Epoch [13653/20000], Training Loss: 0.5266\n",
            "Epoch [13654/20000], Training Loss: 0.5083\n",
            "Epoch [13655/20000], Training Loss: 0.5203\n",
            "Epoch [13656/20000], Training Loss: 0.5170\n",
            "Epoch [13657/20000], Training Loss: 0.4987\n",
            "Epoch [13658/20000], Training Loss: 0.4667\n",
            "Epoch [13659/20000], Training Loss: 0.5395\n",
            "Epoch [13660/20000], Training Loss: 0.5125\n",
            "Epoch [13661/20000], Training Loss: 0.4959\n",
            "Epoch [13662/20000], Training Loss: 0.5393\n",
            "Epoch [13663/20000], Training Loss: 0.5022\n",
            "Epoch [13664/20000], Training Loss: 0.4959\n",
            "Epoch [13665/20000], Training Loss: 0.4989\n",
            "Epoch [13666/20000], Training Loss: 0.4748\n",
            "Epoch [13667/20000], Training Loss: 0.5326\n",
            "Epoch [13668/20000], Training Loss: 0.4929\n",
            "Epoch [13669/20000], Training Loss: 0.4737\n",
            "Epoch [13670/20000], Training Loss: 0.4876\n",
            "Epoch [13671/20000], Training Loss: 0.4353\n",
            "Epoch [13672/20000], Training Loss: 0.4323\n",
            "Epoch [13673/20000], Training Loss: 0.4889\n",
            "Epoch [13674/20000], Training Loss: 0.4599\n",
            "Epoch [13675/20000], Training Loss: 0.5103\n",
            "Epoch [13676/20000], Training Loss: 0.4763\n",
            "Epoch [13677/20000], Training Loss: 0.4265\n",
            "Epoch [13678/20000], Training Loss: 0.5215\n",
            "Epoch [13679/20000], Training Loss: 0.4802\n",
            "Epoch [13680/20000], Training Loss: 0.5014\n",
            "Epoch [13681/20000], Training Loss: 0.5034\n",
            "Epoch [13682/20000], Training Loss: 0.4898\n",
            "Epoch [13683/20000], Training Loss: 0.4898\n",
            "Epoch [13684/20000], Training Loss: 0.5062\n",
            "Epoch [13685/20000], Training Loss: 0.4710\n",
            "Epoch [13686/20000], Training Loss: 0.4775\n",
            "Epoch [13687/20000], Training Loss: 0.4926\n",
            "Epoch [13688/20000], Training Loss: 0.4718\n",
            "Epoch [13689/20000], Training Loss: 0.4779\n",
            "Epoch [13690/20000], Training Loss: 0.4740\n",
            "Epoch [13691/20000], Training Loss: 0.5314\n",
            "Epoch [13692/20000], Training Loss: 0.4591\n",
            "Epoch [13693/20000], Training Loss: 0.5032\n",
            "Epoch [13694/20000], Training Loss: 0.4837\n",
            "Epoch [13695/20000], Training Loss: 0.4392\n",
            "Epoch [13696/20000], Training Loss: 0.4661\n",
            "Epoch [13697/20000], Training Loss: 0.4683\n",
            "Epoch [13698/20000], Training Loss: 0.4910\n",
            "Epoch [13699/20000], Training Loss: 0.4962\n",
            "Epoch [13700/20000], Training Loss: 0.4843\n",
            "Epoch [13701/20000], Training Loss: 0.4902\n",
            "Epoch [13702/20000], Training Loss: 0.4711\n",
            "Epoch [13703/20000], Training Loss: 0.4710\n",
            "Epoch [13704/20000], Training Loss: 0.4525\n",
            "Epoch [13705/20000], Training Loss: 0.5320\n",
            "Epoch [13706/20000], Training Loss: 0.4484\n",
            "Epoch [13707/20000], Training Loss: 0.5098\n",
            "Epoch [13708/20000], Training Loss: 0.4746\n",
            "Epoch [13709/20000], Training Loss: 0.5257\n",
            "Epoch [13710/20000], Training Loss: 0.4737\n",
            "Epoch [13711/20000], Training Loss: 0.4857\n",
            "Epoch [13712/20000], Training Loss: 0.4916\n",
            "Epoch [13713/20000], Training Loss: 0.4857\n",
            "Epoch [13714/20000], Training Loss: 0.4767\n",
            "Epoch [13715/20000], Training Loss: 0.4798\n",
            "Epoch [13716/20000], Training Loss: 0.4828\n",
            "Epoch [13717/20000], Training Loss: 0.4850\n",
            "Epoch [13718/20000], Training Loss: 0.4669\n",
            "Epoch [13719/20000], Training Loss: 0.4996\n",
            "Epoch [13720/20000], Training Loss: 0.4988\n",
            "Epoch [13721/20000], Training Loss: 0.5342\n",
            "Epoch [13722/20000], Training Loss: 0.4835\n",
            "Epoch [13723/20000], Training Loss: 0.5017\n",
            "Epoch [13724/20000], Training Loss: 0.4883\n",
            "Epoch [13725/20000], Training Loss: 0.5098\n",
            "Epoch [13726/20000], Training Loss: 0.4704\n",
            "Epoch [13727/20000], Training Loss: 0.4816\n",
            "Epoch [13728/20000], Training Loss: 0.4628\n",
            "Epoch [13729/20000], Training Loss: 0.4964\n",
            "Epoch [13730/20000], Training Loss: 0.4905\n",
            "Epoch [13731/20000], Training Loss: 0.5178\n",
            "Epoch [13732/20000], Training Loss: 0.4862\n",
            "Epoch [13733/20000], Training Loss: 0.5176\n",
            "Epoch [13734/20000], Training Loss: 0.5224\n",
            "Epoch [13735/20000], Training Loss: 0.4893\n",
            "Epoch [13736/20000], Training Loss: 0.4963\n",
            "Epoch [13737/20000], Training Loss: 0.5286\n",
            "Epoch [13738/20000], Training Loss: 0.4913\n",
            "Epoch [13739/20000], Training Loss: 0.4969\n",
            "Epoch [13740/20000], Training Loss: 0.5440\n",
            "Epoch [13741/20000], Training Loss: 0.5057\n",
            "Epoch [13742/20000], Training Loss: 0.4589\n",
            "Epoch [13743/20000], Training Loss: 0.5337\n",
            "Epoch [13744/20000], Training Loss: 0.4601\n",
            "Epoch [13745/20000], Training Loss: 0.4740\n",
            "Epoch [13746/20000], Training Loss: 0.4415\n",
            "Epoch [13747/20000], Training Loss: 0.4814\n",
            "Epoch [13748/20000], Training Loss: 0.5061\n",
            "Epoch [13749/20000], Training Loss: 0.4784\n",
            "Epoch [13750/20000], Training Loss: 0.4350\n",
            "Epoch [13751/20000], Training Loss: 0.4541\n",
            "Epoch [13752/20000], Training Loss: 0.4604\n",
            "Epoch [13753/20000], Training Loss: 0.4443\n",
            "Epoch [13754/20000], Training Loss: 0.4812\n",
            "Epoch [13755/20000], Training Loss: 0.5281\n",
            "Epoch [13756/20000], Training Loss: 0.4667\n",
            "Epoch [13757/20000], Training Loss: 0.4755\n",
            "Epoch [13758/20000], Training Loss: 0.5074\n",
            "Epoch [13759/20000], Training Loss: 0.4537\n",
            "Epoch [13760/20000], Training Loss: 0.4961\n",
            "Epoch [13761/20000], Training Loss: 0.4893\n",
            "Epoch [13762/20000], Training Loss: 0.4509\n",
            "Epoch [13763/20000], Training Loss: 0.5266\n",
            "Epoch [13764/20000], Training Loss: 0.4646\n",
            "Epoch [13765/20000], Training Loss: 0.5230\n",
            "Epoch [13766/20000], Training Loss: 0.4702\n",
            "Epoch [13767/20000], Training Loss: 0.4796\n",
            "Epoch [13768/20000], Training Loss: 0.4985\n",
            "Epoch [13769/20000], Training Loss: 0.4922\n",
            "Epoch [13770/20000], Training Loss: 0.5317\n",
            "Epoch [13771/20000], Training Loss: 0.4811\n",
            "Epoch [13772/20000], Training Loss: 0.4469\n",
            "Epoch [13773/20000], Training Loss: 0.5155\n",
            "Epoch [13774/20000], Training Loss: 0.5315\n",
            "Epoch [13775/20000], Training Loss: 0.5092\n",
            "Epoch [13776/20000], Training Loss: 0.4522\n",
            "Epoch [13777/20000], Training Loss: 0.4612\n",
            "Epoch [13778/20000], Training Loss: 0.4375\n",
            "Epoch [13779/20000], Training Loss: 0.4663\n",
            "Epoch [13780/20000], Training Loss: 0.4372\n",
            "Epoch [13781/20000], Training Loss: 0.5233\n",
            "Epoch [13782/20000], Training Loss: 0.4911\n",
            "Epoch [13783/20000], Training Loss: 0.4597\n",
            "Epoch [13784/20000], Training Loss: 0.4958\n",
            "Epoch [13785/20000], Training Loss: 0.4411\n",
            "Epoch [13786/20000], Training Loss: 0.5025\n",
            "Epoch [13787/20000], Training Loss: 0.4729\n",
            "Epoch [13788/20000], Training Loss: 0.5007\n",
            "Epoch [13789/20000], Training Loss: 0.4765\n",
            "Epoch [13790/20000], Training Loss: 0.4753\n",
            "Epoch [13791/20000], Training Loss: 0.4554\n",
            "Epoch [13792/20000], Training Loss: 0.5174\n",
            "Epoch [13793/20000], Training Loss: 0.4826\n",
            "Epoch [13794/20000], Training Loss: 0.4607\n",
            "Epoch [13795/20000], Training Loss: 0.4836\n",
            "Epoch [13796/20000], Training Loss: 0.4727\n",
            "Epoch [13797/20000], Training Loss: 0.4641\n",
            "Epoch [13798/20000], Training Loss: 0.5252\n",
            "Epoch [13799/20000], Training Loss: 0.4956\n",
            "Epoch [13800/20000], Training Loss: 0.4500\n",
            "Epoch [13801/20000], Training Loss: 0.4550\n",
            "Epoch [13802/20000], Training Loss: 0.5069\n",
            "Epoch [13803/20000], Training Loss: 0.5297\n",
            "Epoch [13804/20000], Training Loss: 0.4785\n",
            "Epoch [13805/20000], Training Loss: 0.4823\n",
            "Epoch [13806/20000], Training Loss: 0.4916\n",
            "Epoch [13807/20000], Training Loss: 0.5300\n",
            "Epoch [13808/20000], Training Loss: 0.5329\n",
            "Epoch [13809/20000], Training Loss: 0.4697\n",
            "Epoch [13810/20000], Training Loss: 0.4654\n",
            "Epoch [13811/20000], Training Loss: 0.4843\n",
            "Epoch [13812/20000], Training Loss: 0.5485\n",
            "Epoch [13813/20000], Training Loss: 0.4735\n",
            "Epoch [13814/20000], Training Loss: 0.4748\n",
            "Epoch [13815/20000], Training Loss: 0.4898\n",
            "Epoch [13816/20000], Training Loss: 0.4642\n",
            "Epoch [13817/20000], Training Loss: 0.4789\n",
            "Epoch [13818/20000], Training Loss: 0.5240\n",
            "Epoch [13819/20000], Training Loss: 0.4992\n",
            "Epoch [13820/20000], Training Loss: 0.4738\n",
            "Epoch [13821/20000], Training Loss: 0.5277\n",
            "Epoch [13822/20000], Training Loss: 0.4614\n",
            "Epoch [13823/20000], Training Loss: 0.4311\n",
            "Epoch [13824/20000], Training Loss: 0.5080\n",
            "Epoch [13825/20000], Training Loss: 0.4862\n",
            "Epoch [13826/20000], Training Loss: 0.5188\n",
            "Epoch [13827/20000], Training Loss: 0.4549\n",
            "Epoch [13828/20000], Training Loss: 0.5359\n",
            "Epoch [13829/20000], Training Loss: 0.4717\n",
            "Epoch [13830/20000], Training Loss: 0.4775\n",
            "Epoch [13831/20000], Training Loss: 0.4852\n",
            "Epoch [13832/20000], Training Loss: 0.5348\n",
            "Epoch [13833/20000], Training Loss: 0.4632\n",
            "Epoch [13834/20000], Training Loss: 0.5249\n",
            "Epoch [13835/20000], Training Loss: 0.4659\n",
            "Epoch [13836/20000], Training Loss: 0.4889\n",
            "Epoch [13837/20000], Training Loss: 0.4855\n",
            "Epoch [13838/20000], Training Loss: 0.4602\n",
            "Epoch [13839/20000], Training Loss: 0.4620\n",
            "Epoch [13840/20000], Training Loss: 0.4778\n",
            "Epoch [13841/20000], Training Loss: 0.4837\n",
            "Epoch [13842/20000], Training Loss: 0.4602\n",
            "Epoch [13843/20000], Training Loss: 0.5274\n",
            "Epoch [13844/20000], Training Loss: 0.4780\n",
            "Epoch [13845/20000], Training Loss: 0.4796\n",
            "Epoch [13846/20000], Training Loss: 0.5191\n",
            "Epoch [13847/20000], Training Loss: 0.4994\n",
            "Epoch [13848/20000], Training Loss: 0.4768\n",
            "Epoch [13849/20000], Training Loss: 0.4766\n",
            "Epoch [13850/20000], Training Loss: 0.5297\n",
            "Epoch [13851/20000], Training Loss: 0.4784\n",
            "Epoch [13852/20000], Training Loss: 0.4719\n",
            "Epoch [13853/20000], Training Loss: 0.4737\n",
            "Epoch [13854/20000], Training Loss: 0.4710\n",
            "Epoch [13855/20000], Training Loss: 0.4553\n",
            "Epoch [13856/20000], Training Loss: 0.5143\n",
            "Epoch [13857/20000], Training Loss: 0.5065\n",
            "Epoch [13858/20000], Training Loss: 0.4690\n",
            "Epoch [13859/20000], Training Loss: 0.5164\n",
            "Epoch [13860/20000], Training Loss: 0.4699\n",
            "Epoch [13861/20000], Training Loss: 0.4752\n",
            "Epoch [13862/20000], Training Loss: 0.4760\n",
            "Epoch [13863/20000], Training Loss: 0.4692\n",
            "Epoch [13864/20000], Training Loss: 0.5131\n",
            "Epoch [13865/20000], Training Loss: 0.4714\n",
            "Epoch [13866/20000], Training Loss: 0.4785\n",
            "Epoch [13867/20000], Training Loss: 0.4677\n",
            "Epoch [13868/20000], Training Loss: 0.5013\n",
            "Epoch [13869/20000], Training Loss: 0.4663\n",
            "Epoch [13870/20000], Training Loss: 0.5155\n",
            "Epoch [13871/20000], Training Loss: 0.5028\n",
            "Epoch [13872/20000], Training Loss: 0.4740\n",
            "Epoch [13873/20000], Training Loss: 0.4835\n",
            "Epoch [13874/20000], Training Loss: 0.5236\n",
            "Epoch [13875/20000], Training Loss: 0.5044\n",
            "Epoch [13876/20000], Training Loss: 0.4454\n",
            "Epoch [13877/20000], Training Loss: 0.4748\n",
            "Epoch [13878/20000], Training Loss: 0.4815\n",
            "Epoch [13879/20000], Training Loss: 0.4774\n",
            "Epoch [13880/20000], Training Loss: 0.4487\n",
            "Epoch [13881/20000], Training Loss: 0.5305\n",
            "Epoch [13882/20000], Training Loss: 0.4889\n",
            "Epoch [13883/20000], Training Loss: 0.4495\n",
            "Epoch [13884/20000], Training Loss: 0.4727\n",
            "Epoch [13885/20000], Training Loss: 0.4986\n",
            "Epoch [13886/20000], Training Loss: 0.4633\n",
            "Epoch [13887/20000], Training Loss: 0.4749\n",
            "Epoch [13888/20000], Training Loss: 0.5383\n",
            "Epoch [13889/20000], Training Loss: 0.4643\n",
            "Epoch [13890/20000], Training Loss: 0.4965\n",
            "Epoch [13891/20000], Training Loss: 0.4746\n",
            "Epoch [13892/20000], Training Loss: 0.4707\n",
            "Epoch [13893/20000], Training Loss: 0.4547\n",
            "Epoch [13894/20000], Training Loss: 0.4921\n",
            "Epoch [13895/20000], Training Loss: 0.4510\n",
            "Epoch [13896/20000], Training Loss: 0.4694\n",
            "Epoch [13897/20000], Training Loss: 0.4976\n",
            "Epoch [13898/20000], Training Loss: 0.4745\n",
            "Epoch [13899/20000], Training Loss: 0.4765\n",
            "Epoch [13900/20000], Training Loss: 0.5313\n",
            "Epoch [13901/20000], Training Loss: 0.4970\n",
            "Epoch [13902/20000], Training Loss: 0.4794\n",
            "Epoch [13903/20000], Training Loss: 0.4788\n",
            "Epoch [13904/20000], Training Loss: 0.4534\n",
            "Epoch [13905/20000], Training Loss: 0.5291\n",
            "Epoch [13906/20000], Training Loss: 0.4703\n",
            "Epoch [13907/20000], Training Loss: 0.4718\n",
            "Epoch [13908/20000], Training Loss: 0.4870\n",
            "Epoch [13909/20000], Training Loss: 0.5164\n",
            "Epoch [13910/20000], Training Loss: 0.4533\n",
            "Epoch [13911/20000], Training Loss: 0.4795\n",
            "Epoch [13912/20000], Training Loss: 0.5238\n",
            "Epoch [13913/20000], Training Loss: 0.5111\n",
            "Epoch [13914/20000], Training Loss: 0.4780\n",
            "Epoch [13915/20000], Training Loss: 0.4726\n",
            "Epoch [13916/20000], Training Loss: 0.5258\n",
            "Epoch [13917/20000], Training Loss: 0.5050\n",
            "Epoch [13918/20000], Training Loss: 0.5210\n",
            "Epoch [13919/20000], Training Loss: 0.5004\n",
            "Epoch [13920/20000], Training Loss: 0.4595\n",
            "Epoch [13921/20000], Training Loss: 0.5061\n",
            "Epoch [13922/20000], Training Loss: 0.4663\n",
            "Epoch [13923/20000], Training Loss: 0.5766\n",
            "Epoch [13924/20000], Training Loss: 0.4959\n",
            "Epoch [13925/20000], Training Loss: 0.4217\n",
            "Epoch [13926/20000], Training Loss: 0.4807\n",
            "Epoch [13927/20000], Training Loss: 0.4476\n",
            "Epoch [13928/20000], Training Loss: 0.5019\n",
            "Epoch [13929/20000], Training Loss: 0.4932\n",
            "Epoch [13930/20000], Training Loss: 0.4846\n",
            "Epoch [13931/20000], Training Loss: 0.4778\n",
            "Epoch [13932/20000], Training Loss: 0.4874\n",
            "Epoch [13933/20000], Training Loss: 0.4652\n",
            "Epoch [13934/20000], Training Loss: 0.4650\n",
            "Epoch [13935/20000], Training Loss: 0.4691\n",
            "Epoch [13936/20000], Training Loss: 0.4928\n",
            "Epoch [13937/20000], Training Loss: 0.4858\n",
            "Epoch [13938/20000], Training Loss: 0.5048\n",
            "Epoch [13939/20000], Training Loss: 0.5203\n",
            "Epoch [13940/20000], Training Loss: 0.4675\n",
            "Epoch [13941/20000], Training Loss: 0.5356\n",
            "Epoch [13942/20000], Training Loss: 0.5152\n",
            "Epoch [13943/20000], Training Loss: 0.4560\n",
            "Epoch [13944/20000], Training Loss: 0.4672\n",
            "Epoch [13945/20000], Training Loss: 0.4744\n",
            "Epoch [13946/20000], Training Loss: 0.4828\n",
            "Epoch [13947/20000], Training Loss: 0.5237\n",
            "Epoch [13948/20000], Training Loss: 0.5042\n",
            "Epoch [13949/20000], Training Loss: 0.4916\n",
            "Epoch [13950/20000], Training Loss: 0.5024\n",
            "Epoch [13951/20000], Training Loss: 0.4912\n",
            "Epoch [13952/20000], Training Loss: 0.4832\n",
            "Epoch [13953/20000], Training Loss: 0.5042\n",
            "Epoch [13954/20000], Training Loss: 0.5106\n",
            "Epoch [13955/20000], Training Loss: 0.4848\n",
            "Epoch [13956/20000], Training Loss: 0.4724\n",
            "Epoch [13957/20000], Training Loss: 0.4923\n",
            "Epoch [13958/20000], Training Loss: 0.5090\n",
            "Epoch [13959/20000], Training Loss: 0.4614\n",
            "Epoch [13960/20000], Training Loss: 0.4586\n",
            "Epoch [13961/20000], Training Loss: 0.4942\n",
            "Epoch [13962/20000], Training Loss: 0.4612\n",
            "Epoch [13963/20000], Training Loss: 0.4795\n",
            "Epoch [13964/20000], Training Loss: 0.4926\n",
            "Epoch [13965/20000], Training Loss: 0.5320\n",
            "Epoch [13966/20000], Training Loss: 0.5090\n",
            "Epoch [13967/20000], Training Loss: 0.5048\n",
            "Epoch [13968/20000], Training Loss: 0.4582\n",
            "Epoch [13969/20000], Training Loss: 0.5082\n",
            "Epoch [13970/20000], Training Loss: 0.4600\n",
            "Epoch [13971/20000], Training Loss: 0.4600\n",
            "Epoch [13972/20000], Training Loss: 0.4986\n",
            "Epoch [13973/20000], Training Loss: 0.5034\n",
            "Epoch [13974/20000], Training Loss: 0.4339\n",
            "Epoch [13975/20000], Training Loss: 0.5010\n",
            "Epoch [13976/20000], Training Loss: 0.5261\n",
            "Epoch [13977/20000], Training Loss: 0.5309\n",
            "Epoch [13978/20000], Training Loss: 0.4655\n",
            "Epoch [13979/20000], Training Loss: 0.4805\n",
            "Epoch [13980/20000], Training Loss: 0.4883\n",
            "Epoch [13981/20000], Training Loss: 0.4680\n",
            "Epoch [13982/20000], Training Loss: 0.5317\n",
            "Epoch [13983/20000], Training Loss: 0.4686\n",
            "Epoch [13984/20000], Training Loss: 0.4916\n",
            "Epoch [13985/20000], Training Loss: 0.4960\n",
            "Epoch [13986/20000], Training Loss: 0.4995\n",
            "Epoch [13987/20000], Training Loss: 0.5269\n",
            "Epoch [13988/20000], Training Loss: 0.4555\n",
            "Epoch [13989/20000], Training Loss: 0.5092\n",
            "Epoch [13990/20000], Training Loss: 0.4720\n",
            "Epoch [13991/20000], Training Loss: 0.4440\n",
            "Epoch [13992/20000], Training Loss: 0.4944\n",
            "Epoch [13993/20000], Training Loss: 0.4266\n",
            "Epoch [13994/20000], Training Loss: 0.4677\n",
            "Epoch [13995/20000], Training Loss: 0.5298\n",
            "Epoch [13996/20000], Training Loss: 0.5035\n",
            "Epoch [13997/20000], Training Loss: 0.4626\n",
            "Epoch [13998/20000], Training Loss: 0.4986\n",
            "Epoch [13999/20000], Training Loss: 0.5130\n",
            "Epoch [14000/20000], Training Loss: 0.4899\n",
            "Epoch [14001/20000], Training Loss: 0.5090\n",
            "Epoch [14002/20000], Training Loss: 0.4713\n",
            "Epoch [14003/20000], Training Loss: 0.4634\n",
            "Epoch [14004/20000], Training Loss: 0.5082\n",
            "Epoch [14005/20000], Training Loss: 0.4821\n",
            "Epoch [14006/20000], Training Loss: 0.4988\n",
            "Epoch [14007/20000], Training Loss: 0.5357\n",
            "Epoch [14008/20000], Training Loss: 0.5001\n",
            "Epoch [14009/20000], Training Loss: 0.4716\n",
            "Epoch [14010/20000], Training Loss: 0.4614\n",
            "Epoch [14011/20000], Training Loss: 0.5007\n",
            "Epoch [14012/20000], Training Loss: 0.5036\n",
            "Epoch [14013/20000], Training Loss: 0.4998\n",
            "Epoch [14014/20000], Training Loss: 0.4600\n",
            "Epoch [14015/20000], Training Loss: 0.4599\n",
            "Epoch [14016/20000], Training Loss: 0.4744\n",
            "Epoch [14017/20000], Training Loss: 0.4770\n",
            "Epoch [14018/20000], Training Loss: 0.4971\n",
            "Epoch [14019/20000], Training Loss: 0.5172\n",
            "Epoch [14020/20000], Training Loss: 0.5444\n",
            "Epoch [14021/20000], Training Loss: 0.4933\n",
            "Epoch [14022/20000], Training Loss: 0.4788\n",
            "Epoch [14023/20000], Training Loss: 0.4448\n",
            "Epoch [14024/20000], Training Loss: 0.5243\n",
            "Epoch [14025/20000], Training Loss: 0.5035\n",
            "Epoch [14026/20000], Training Loss: 0.5062\n",
            "Epoch [14027/20000], Training Loss: 0.4968\n",
            "Epoch [14028/20000], Training Loss: 0.4635\n",
            "Epoch [14029/20000], Training Loss: 0.4583\n",
            "Epoch [14030/20000], Training Loss: 0.4984\n",
            "Epoch [14031/20000], Training Loss: 0.5160\n",
            "Epoch [14032/20000], Training Loss: 0.4291\n",
            "Epoch [14033/20000], Training Loss: 0.4851\n",
            "Epoch [14034/20000], Training Loss: 0.5172\n",
            "Epoch [14035/20000], Training Loss: 0.4556\n",
            "Epoch [14036/20000], Training Loss: 0.4768\n",
            "Epoch [14037/20000], Training Loss: 0.5298\n",
            "Epoch [14038/20000], Training Loss: 0.5279\n",
            "Epoch [14039/20000], Training Loss: 0.4895\n",
            "Epoch [14040/20000], Training Loss: 0.4868\n",
            "Epoch [14041/20000], Training Loss: 0.4801\n",
            "Epoch [14042/20000], Training Loss: 0.4369\n",
            "Epoch [14043/20000], Training Loss: 0.4906\n",
            "Epoch [14044/20000], Training Loss: 0.5156\n",
            "Epoch [14045/20000], Training Loss: 0.4527\n",
            "Epoch [14046/20000], Training Loss: 0.5039\n",
            "Epoch [14047/20000], Training Loss: 0.4753\n",
            "Epoch [14048/20000], Training Loss: 0.4205\n",
            "Epoch [14049/20000], Training Loss: 0.5304\n",
            "Epoch [14050/20000], Training Loss: 0.4995\n",
            "Epoch [14051/20000], Training Loss: 0.4797\n",
            "Epoch [14052/20000], Training Loss: 0.4564\n",
            "Epoch [14053/20000], Training Loss: 0.5205\n",
            "Epoch [14054/20000], Training Loss: 0.4922\n",
            "Epoch [14055/20000], Training Loss: 0.4740\n",
            "Epoch [14056/20000], Training Loss: 0.5023\n",
            "Epoch [14057/20000], Training Loss: 0.4586\n",
            "Epoch [14058/20000], Training Loss: 0.5235\n",
            "Epoch [14059/20000], Training Loss: 0.4903\n",
            "Epoch [14060/20000], Training Loss: 0.4840\n",
            "Epoch [14061/20000], Training Loss: 0.4643\n",
            "Epoch [14062/20000], Training Loss: 0.4503\n",
            "Epoch [14063/20000], Training Loss: 0.4807\n",
            "Epoch [14064/20000], Training Loss: 0.5040\n",
            "Epoch [14065/20000], Training Loss: 0.5497\n",
            "Epoch [14066/20000], Training Loss: 0.5009\n",
            "Epoch [14067/20000], Training Loss: 0.5000\n",
            "Epoch [14068/20000], Training Loss: 0.5161\n",
            "Epoch [14069/20000], Training Loss: 0.4635\n",
            "Epoch [14070/20000], Training Loss: 0.4644\n",
            "Epoch [14071/20000], Training Loss: 0.5168\n",
            "Epoch [14072/20000], Training Loss: 0.5045\n",
            "Epoch [14073/20000], Training Loss: 0.5110\n",
            "Epoch [14074/20000], Training Loss: 0.5324\n",
            "Epoch [14075/20000], Training Loss: 0.4994\n",
            "Epoch [14076/20000], Training Loss: 0.4718\n",
            "Epoch [14077/20000], Training Loss: 0.4606\n",
            "Epoch [14078/20000], Training Loss: 0.4868\n",
            "Epoch [14079/20000], Training Loss: 0.5057\n",
            "Epoch [14080/20000], Training Loss: 0.4947\n",
            "Epoch [14081/20000], Training Loss: 0.4940\n",
            "Epoch [14082/20000], Training Loss: 0.4872\n",
            "Epoch [14083/20000], Training Loss: 0.4944\n",
            "Epoch [14084/20000], Training Loss: 0.4986\n",
            "Epoch [14085/20000], Training Loss: 0.5018\n",
            "Epoch [14086/20000], Training Loss: 0.4655\n",
            "Epoch [14087/20000], Training Loss: 0.4427\n",
            "Epoch [14088/20000], Training Loss: 0.4917\n",
            "Epoch [14089/20000], Training Loss: 0.4601\n",
            "Epoch [14090/20000], Training Loss: 0.4859\n",
            "Epoch [14091/20000], Training Loss: 0.4577\n",
            "Epoch [14092/20000], Training Loss: 0.4562\n",
            "Epoch [14093/20000], Training Loss: 0.4993\n",
            "Epoch [14094/20000], Training Loss: 0.4856\n",
            "Epoch [14095/20000], Training Loss: 0.4827\n",
            "Epoch [14096/20000], Training Loss: 0.4852\n",
            "Epoch [14097/20000], Training Loss: 0.5194\n",
            "Epoch [14098/20000], Training Loss: 0.4807\n",
            "Epoch [14099/20000], Training Loss: 0.4828\n",
            "Epoch [14100/20000], Training Loss: 0.4560\n",
            "Epoch [14101/20000], Training Loss: 0.4742\n",
            "Epoch [14102/20000], Training Loss: 0.4827\n",
            "Epoch [14103/20000], Training Loss: 0.4725\n",
            "Epoch [14104/20000], Training Loss: 0.4760\n",
            "Epoch [14105/20000], Training Loss: 0.5119\n",
            "Epoch [14106/20000], Training Loss: 0.5310\n",
            "Epoch [14107/20000], Training Loss: 0.4983\n",
            "Epoch [14108/20000], Training Loss: 0.5591\n",
            "Epoch [14109/20000], Training Loss: 0.4882\n",
            "Epoch [14110/20000], Training Loss: 0.5090\n",
            "Epoch [14111/20000], Training Loss: 0.5217\n",
            "Epoch [14112/20000], Training Loss: 0.5011\n",
            "Epoch [14113/20000], Training Loss: 0.4410\n",
            "Epoch [14114/20000], Training Loss: 0.4570\n",
            "Epoch [14115/20000], Training Loss: 0.4880\n",
            "Epoch [14116/20000], Training Loss: 0.5049\n",
            "Epoch [14117/20000], Training Loss: 0.4873\n",
            "Epoch [14118/20000], Training Loss: 0.5026\n",
            "Epoch [14119/20000], Training Loss: 0.4591\n",
            "Epoch [14120/20000], Training Loss: 0.4693\n",
            "Epoch [14121/20000], Training Loss: 0.4946\n",
            "Epoch [14122/20000], Training Loss: 0.5062\n",
            "Epoch [14123/20000], Training Loss: 0.4858\n",
            "Epoch [14124/20000], Training Loss: 0.4909\n",
            "Epoch [14125/20000], Training Loss: 0.5454\n",
            "Epoch [14126/20000], Training Loss: 0.5031\n",
            "Epoch [14127/20000], Training Loss: 0.4731\n",
            "Epoch [14128/20000], Training Loss: 0.5149\n",
            "Epoch [14129/20000], Training Loss: 0.4905\n",
            "Epoch [14130/20000], Training Loss: 0.5242\n",
            "Epoch [14131/20000], Training Loss: 0.5227\n",
            "Epoch [14132/20000], Training Loss: 0.4784\n",
            "Epoch [14133/20000], Training Loss: 0.5344\n",
            "Epoch [14134/20000], Training Loss: 0.5068\n",
            "Epoch [14135/20000], Training Loss: 0.5319\n",
            "Epoch [14136/20000], Training Loss: 0.4588\n",
            "Epoch [14137/20000], Training Loss: 0.4736\n",
            "Epoch [14138/20000], Training Loss: 0.5499\n",
            "Epoch [14139/20000], Training Loss: 0.4939\n",
            "Epoch [14140/20000], Training Loss: 0.5587\n",
            "Epoch [14141/20000], Training Loss: 0.4841\n",
            "Epoch [14142/20000], Training Loss: 0.4898\n",
            "Epoch [14143/20000], Training Loss: 0.5202\n",
            "Epoch [14144/20000], Training Loss: 0.4994\n",
            "Epoch [14145/20000], Training Loss: 0.4648\n",
            "Epoch [14146/20000], Training Loss: 0.4454\n",
            "Epoch [14147/20000], Training Loss: 0.4556\n",
            "Epoch [14148/20000], Training Loss: 0.4670\n",
            "Epoch [14149/20000], Training Loss: 0.4742\n",
            "Epoch [14150/20000], Training Loss: 0.5197\n",
            "Epoch [14151/20000], Training Loss: 0.4733\n",
            "Epoch [14152/20000], Training Loss: 0.4676\n",
            "Epoch [14153/20000], Training Loss: 0.5373\n",
            "Epoch [14154/20000], Training Loss: 0.5279\n",
            "Epoch [14155/20000], Training Loss: 0.4577\n",
            "Epoch [14156/20000], Training Loss: 0.5113\n",
            "Epoch [14157/20000], Training Loss: 0.4877\n",
            "Epoch [14158/20000], Training Loss: 0.4857\n",
            "Epoch [14159/20000], Training Loss: 0.4796\n",
            "Epoch [14160/20000], Training Loss: 0.4754\n",
            "Epoch [14161/20000], Training Loss: 0.5055\n",
            "Epoch [14162/20000], Training Loss: 0.4371\n",
            "Epoch [14163/20000], Training Loss: 0.5185\n",
            "Epoch [14164/20000], Training Loss: 0.4677\n",
            "Epoch [14165/20000], Training Loss: 0.4598\n",
            "Epoch [14166/20000], Training Loss: 0.5016\n",
            "Epoch [14167/20000], Training Loss: 0.5082\n",
            "Epoch [14168/20000], Training Loss: 0.5037\n",
            "Epoch [14169/20000], Training Loss: 0.4919\n",
            "Epoch [14170/20000], Training Loss: 0.5122\n",
            "Epoch [14171/20000], Training Loss: 0.4696\n",
            "Epoch [14172/20000], Training Loss: 0.4734\n",
            "Epoch [14173/20000], Training Loss: 0.4784\n",
            "Epoch [14174/20000], Training Loss: 0.4596\n",
            "Epoch [14175/20000], Training Loss: 0.4707\n",
            "Epoch [14176/20000], Training Loss: 0.4634\n",
            "Epoch [14177/20000], Training Loss: 0.4933\n",
            "Epoch [14178/20000], Training Loss: 0.4873\n",
            "Epoch [14179/20000], Training Loss: 0.4637\n",
            "Epoch [14180/20000], Training Loss: 0.4766\n",
            "Epoch [14181/20000], Training Loss: 0.4896\n",
            "Epoch [14182/20000], Training Loss: 0.4923\n",
            "Epoch [14183/20000], Training Loss: 0.4926\n",
            "Epoch [14184/20000], Training Loss: 0.4587\n",
            "Epoch [14185/20000], Training Loss: 0.4712\n",
            "Epoch [14186/20000], Training Loss: 0.5054\n",
            "Epoch [14187/20000], Training Loss: 0.4601\n",
            "Epoch [14188/20000], Training Loss: 0.4579\n",
            "Epoch [14189/20000], Training Loss: 0.4648\n",
            "Epoch [14190/20000], Training Loss: 0.4529\n",
            "Epoch [14191/20000], Training Loss: 0.4590\n",
            "Epoch [14192/20000], Training Loss: 0.4914\n",
            "Epoch [14193/20000], Training Loss: 0.4878\n",
            "Epoch [14194/20000], Training Loss: 0.5316\n",
            "Epoch [14195/20000], Training Loss: 0.5065\n",
            "Epoch [14196/20000], Training Loss: 0.4919\n",
            "Epoch [14197/20000], Training Loss: 0.5568\n",
            "Epoch [14198/20000], Training Loss: 0.4985\n",
            "Epoch [14199/20000], Training Loss: 0.4610\n",
            "Epoch [14200/20000], Training Loss: 0.4675\n",
            "Epoch [14201/20000], Training Loss: 0.4747\n",
            "Epoch [14202/20000], Training Loss: 0.5258\n",
            "Epoch [14203/20000], Training Loss: 0.5275\n",
            "Epoch [14204/20000], Training Loss: 0.5169\n",
            "Epoch [14205/20000], Training Loss: 0.4425\n",
            "Epoch [14206/20000], Training Loss: 0.5271\n",
            "Epoch [14207/20000], Training Loss: 0.5014\n",
            "Epoch [14208/20000], Training Loss: 0.4647\n",
            "Epoch [14209/20000], Training Loss: 0.4917\n",
            "Epoch [14210/20000], Training Loss: 0.4592\n",
            "Epoch [14211/20000], Training Loss: 0.5090\n",
            "Epoch [14212/20000], Training Loss: 0.4800\n",
            "Epoch [14213/20000], Training Loss: 0.4907\n",
            "Epoch [14214/20000], Training Loss: 0.4755\n",
            "Epoch [14215/20000], Training Loss: 0.4444\n",
            "Epoch [14216/20000], Training Loss: 0.4955\n",
            "Epoch [14217/20000], Training Loss: 0.4774\n",
            "Epoch [14218/20000], Training Loss: 0.4711\n",
            "Epoch [14219/20000], Training Loss: 0.4492\n",
            "Epoch [14220/20000], Training Loss: 0.4833\n",
            "Epoch [14221/20000], Training Loss: 0.4847\n",
            "Epoch [14222/20000], Training Loss: 0.5041\n",
            "Epoch [14223/20000], Training Loss: 0.4534\n",
            "Epoch [14224/20000], Training Loss: 0.5599\n",
            "Epoch [14225/20000], Training Loss: 0.4726\n",
            "Epoch [14226/20000], Training Loss: 0.4727\n",
            "Epoch [14227/20000], Training Loss: 0.5562\n",
            "Epoch [14228/20000], Training Loss: 0.4747\n",
            "Epoch [14229/20000], Training Loss: 0.4603\n",
            "Epoch [14230/20000], Training Loss: 0.4934\n",
            "Epoch [14231/20000], Training Loss: 0.4834\n",
            "Epoch [14232/20000], Training Loss: 0.4907\n",
            "Epoch [14233/20000], Training Loss: 0.4753\n",
            "Epoch [14234/20000], Training Loss: 0.5314\n",
            "Epoch [14235/20000], Training Loss: 0.5084\n",
            "Epoch [14236/20000], Training Loss: 0.5222\n",
            "Epoch [14237/20000], Training Loss: 0.4825\n",
            "Epoch [14238/20000], Training Loss: 0.4814\n",
            "Epoch [14239/20000], Training Loss: 0.4606\n",
            "Epoch [14240/20000], Training Loss: 0.4634\n",
            "Epoch [14241/20000], Training Loss: 0.4918\n",
            "Epoch [14242/20000], Training Loss: 0.4626\n",
            "Epoch [14243/20000], Training Loss: 0.5128\n",
            "Epoch [14244/20000], Training Loss: 0.4832\n",
            "Epoch [14245/20000], Training Loss: 0.4834\n",
            "Epoch [14246/20000], Training Loss: 0.4637\n",
            "Epoch [14247/20000], Training Loss: 0.5263\n",
            "Epoch [14248/20000], Training Loss: 0.4471\n",
            "Epoch [14249/20000], Training Loss: 0.4983\n",
            "Epoch [14250/20000], Training Loss: 0.5316\n",
            "Epoch [14251/20000], Training Loss: 0.4988\n",
            "Epoch [14252/20000], Training Loss: 0.4820\n",
            "Epoch [14253/20000], Training Loss: 0.5106\n",
            "Epoch [14254/20000], Training Loss: 0.5183\n",
            "Epoch [14255/20000], Training Loss: 0.5077\n",
            "Epoch [14256/20000], Training Loss: 0.5217\n",
            "Epoch [14257/20000], Training Loss: 0.4805\n",
            "Epoch [14258/20000], Training Loss: 0.4950\n",
            "Epoch [14259/20000], Training Loss: 0.4407\n",
            "Epoch [14260/20000], Training Loss: 0.5288\n",
            "Epoch [14261/20000], Training Loss: 0.4732\n",
            "Epoch [14262/20000], Training Loss: 0.4761\n",
            "Epoch [14263/20000], Training Loss: 0.5013\n",
            "Epoch [14264/20000], Training Loss: 0.4884\n",
            "Epoch [14265/20000], Training Loss: 0.4880\n",
            "Epoch [14266/20000], Training Loss: 0.5290\n",
            "Epoch [14267/20000], Training Loss: 0.4886\n",
            "Epoch [14268/20000], Training Loss: 0.5323\n",
            "Epoch [14269/20000], Training Loss: 0.4779\n",
            "Epoch [14270/20000], Training Loss: 0.4644\n",
            "Epoch [14271/20000], Training Loss: 0.5066\n",
            "Epoch [14272/20000], Training Loss: 0.4749\n",
            "Epoch [14273/20000], Training Loss: 0.5067\n",
            "Epoch [14274/20000], Training Loss: 0.5131\n",
            "Epoch [14275/20000], Training Loss: 0.4755\n",
            "Epoch [14276/20000], Training Loss: 0.4654\n",
            "Epoch [14277/20000], Training Loss: 0.4597\n",
            "Epoch [14278/20000], Training Loss: 0.4867\n",
            "Epoch [14279/20000], Training Loss: 0.5076\n",
            "Epoch [14280/20000], Training Loss: 0.4759\n",
            "Epoch [14281/20000], Training Loss: 0.4971\n",
            "Epoch [14282/20000], Training Loss: 0.5103\n",
            "Epoch [14283/20000], Training Loss: 0.4993\n",
            "Epoch [14284/20000], Training Loss: 0.4680\n",
            "Epoch [14285/20000], Training Loss: 0.4878\n",
            "Epoch [14286/20000], Training Loss: 0.4623\n",
            "Epoch [14287/20000], Training Loss: 0.4601\n",
            "Epoch [14288/20000], Training Loss: 0.4380\n",
            "Epoch [14289/20000], Training Loss: 0.4656\n",
            "Epoch [14290/20000], Training Loss: 0.4892\n",
            "Epoch [14291/20000], Training Loss: 0.4518\n",
            "Epoch [14292/20000], Training Loss: 0.4895\n",
            "Epoch [14293/20000], Training Loss: 0.5098\n",
            "Epoch [14294/20000], Training Loss: 0.4517\n",
            "Epoch [14295/20000], Training Loss: 0.5193\n",
            "Epoch [14296/20000], Training Loss: 0.4978\n",
            "Epoch [14297/20000], Training Loss: 0.5096\n",
            "Epoch [14298/20000], Training Loss: 0.5123\n",
            "Epoch [14299/20000], Training Loss: 0.4569\n",
            "Epoch [14300/20000], Training Loss: 0.5103\n",
            "Epoch [14301/20000], Training Loss: 0.5368\n",
            "Epoch [14302/20000], Training Loss: 0.4526\n",
            "Epoch [14303/20000], Training Loss: 0.4673\n",
            "Epoch [14304/20000], Training Loss: 0.4810\n",
            "Epoch [14305/20000], Training Loss: 0.4854\n",
            "Epoch [14306/20000], Training Loss: 0.4809\n",
            "Epoch [14307/20000], Training Loss: 0.4720\n",
            "Epoch [14308/20000], Training Loss: 0.4775\n",
            "Epoch [14309/20000], Training Loss: 0.4852\n",
            "Epoch [14310/20000], Training Loss: 0.4622\n",
            "Epoch [14311/20000], Training Loss: 0.5128\n",
            "Epoch [14312/20000], Training Loss: 0.4651\n",
            "Epoch [14313/20000], Training Loss: 0.5145\n",
            "Epoch [14314/20000], Training Loss: 0.4695\n",
            "Epoch [14315/20000], Training Loss: 0.5119\n",
            "Epoch [14316/20000], Training Loss: 0.4584\n",
            "Epoch [14317/20000], Training Loss: 0.4997\n",
            "Epoch [14318/20000], Training Loss: 0.4846\n",
            "Epoch [14319/20000], Training Loss: 0.4632\n",
            "Epoch [14320/20000], Training Loss: 0.5239\n",
            "Epoch [14321/20000], Training Loss: 0.5187\n",
            "Epoch [14322/20000], Training Loss: 0.5453\n",
            "Epoch [14323/20000], Training Loss: 0.5032\n",
            "Epoch [14324/20000], Training Loss: 0.5140\n",
            "Epoch [14325/20000], Training Loss: 0.4542\n",
            "Epoch [14326/20000], Training Loss: 0.5132\n",
            "Epoch [14327/20000], Training Loss: 0.5194\n",
            "Epoch [14328/20000], Training Loss: 0.5360\n",
            "Epoch [14329/20000], Training Loss: 0.4562\n",
            "Epoch [14330/20000], Training Loss: 0.5059\n",
            "Epoch [14331/20000], Training Loss: 0.4844\n",
            "Epoch [14332/20000], Training Loss: 0.5290\n",
            "Epoch [14333/20000], Training Loss: 0.4923\n",
            "Epoch [14334/20000], Training Loss: 0.5139\n",
            "Epoch [14335/20000], Training Loss: 0.4741\n",
            "Epoch [14336/20000], Training Loss: 0.4574\n",
            "Epoch [14337/20000], Training Loss: 0.4519\n",
            "Epoch [14338/20000], Training Loss: 0.5046\n",
            "Epoch [14339/20000], Training Loss: 0.4783\n",
            "Epoch [14340/20000], Training Loss: 0.5321\n",
            "Epoch [14341/20000], Training Loss: 0.4957\n",
            "Epoch [14342/20000], Training Loss: 0.4881\n",
            "Epoch [14343/20000], Training Loss: 0.4647\n",
            "Epoch [14344/20000], Training Loss: 0.4705\n",
            "Epoch [14345/20000], Training Loss: 0.5468\n",
            "Epoch [14346/20000], Training Loss: 0.4744\n",
            "Epoch [14347/20000], Training Loss: 0.4363\n",
            "Epoch [14348/20000], Training Loss: 0.5398\n",
            "Epoch [14349/20000], Training Loss: 0.4894\n",
            "Epoch [14350/20000], Training Loss: 0.4686\n",
            "Epoch [14351/20000], Training Loss: 0.5390\n",
            "Epoch [14352/20000], Training Loss: 0.4907\n",
            "Epoch [14353/20000], Training Loss: 0.4884\n",
            "Epoch [14354/20000], Training Loss: 0.4809\n",
            "Epoch [14355/20000], Training Loss: 0.5076\n",
            "Epoch [14356/20000], Training Loss: 0.4828\n",
            "Epoch [14357/20000], Training Loss: 0.5280\n",
            "Epoch [14358/20000], Training Loss: 0.5208\n",
            "Epoch [14359/20000], Training Loss: 0.4961\n",
            "Epoch [14360/20000], Training Loss: 0.5420\n",
            "Epoch [14361/20000], Training Loss: 0.4555\n",
            "Epoch [14362/20000], Training Loss: 0.4773\n",
            "Epoch [14363/20000], Training Loss: 0.4742\n",
            "Epoch [14364/20000], Training Loss: 0.5037\n",
            "Epoch [14365/20000], Training Loss: 0.4709\n",
            "Epoch [14366/20000], Training Loss: 0.4701\n",
            "Epoch [14367/20000], Training Loss: 0.4938\n",
            "Epoch [14368/20000], Training Loss: 0.5286\n",
            "Epoch [14369/20000], Training Loss: 0.5222\n",
            "Epoch [14370/20000], Training Loss: 0.5099\n",
            "Epoch [14371/20000], Training Loss: 0.4826\n",
            "Epoch [14372/20000], Training Loss: 0.5297\n",
            "Epoch [14373/20000], Training Loss: 0.4959\n",
            "Epoch [14374/20000], Training Loss: 0.4590\n",
            "Epoch [14375/20000], Training Loss: 0.4871\n",
            "Epoch [14376/20000], Training Loss: 0.4796\n",
            "Epoch [14377/20000], Training Loss: 0.4907\n",
            "Epoch [14378/20000], Training Loss: 0.5326\n",
            "Epoch [14379/20000], Training Loss: 0.4829\n",
            "Epoch [14380/20000], Training Loss: 0.4668\n",
            "Epoch [14381/20000], Training Loss: 0.4975\n",
            "Epoch [14382/20000], Training Loss: 0.5074\n",
            "Epoch [14383/20000], Training Loss: 0.4802\n",
            "Epoch [14384/20000], Training Loss: 0.4646\n",
            "Epoch [14385/20000], Training Loss: 0.4962\n",
            "Epoch [14386/20000], Training Loss: 0.4517\n",
            "Epoch [14387/20000], Training Loss: 0.4803\n",
            "Epoch [14388/20000], Training Loss: 0.5342\n",
            "Epoch [14389/20000], Training Loss: 0.4779\n",
            "Epoch [14390/20000], Training Loss: 0.4519\n",
            "Epoch [14391/20000], Training Loss: 0.5129\n",
            "Epoch [14392/20000], Training Loss: 0.4935\n",
            "Epoch [14393/20000], Training Loss: 0.4797\n",
            "Epoch [14394/20000], Training Loss: 0.4689\n",
            "Epoch [14395/20000], Training Loss: 0.5068\n",
            "Epoch [14396/20000], Training Loss: 0.5278\n",
            "Epoch [14397/20000], Training Loss: 0.5316\n",
            "Epoch [14398/20000], Training Loss: 0.4779\n",
            "Epoch [14399/20000], Training Loss: 0.4580\n",
            "Epoch [14400/20000], Training Loss: 0.4331\n",
            "Epoch [14401/20000], Training Loss: 0.4707\n",
            "Epoch [14402/20000], Training Loss: 0.5038\n",
            "Epoch [14403/20000], Training Loss: 0.5019\n",
            "Epoch [14404/20000], Training Loss: 0.4790\n",
            "Epoch [14405/20000], Training Loss: 0.5006\n",
            "Epoch [14406/20000], Training Loss: 0.4606\n",
            "Epoch [14407/20000], Training Loss: 0.4813\n",
            "Epoch [14408/20000], Training Loss: 0.4596\n",
            "Epoch [14409/20000], Training Loss: 0.4701\n",
            "Epoch [14410/20000], Training Loss: 0.4450\n",
            "Epoch [14411/20000], Training Loss: 0.4841\n",
            "Epoch [14412/20000], Training Loss: 0.4728\n",
            "Epoch [14413/20000], Training Loss: 0.4437\n",
            "Epoch [14414/20000], Training Loss: 0.4668\n",
            "Epoch [14415/20000], Training Loss: 0.4667\n",
            "Epoch [14416/20000], Training Loss: 0.4657\n",
            "Epoch [14417/20000], Training Loss: 0.5233\n",
            "Epoch [14418/20000], Training Loss: 0.5364\n",
            "Epoch [14419/20000], Training Loss: 0.4507\n",
            "Epoch [14420/20000], Training Loss: 0.5057\n",
            "Epoch [14421/20000], Training Loss: 0.5083\n",
            "Epoch [14422/20000], Training Loss: 0.5116\n",
            "Epoch [14423/20000], Training Loss: 0.5012\n",
            "Epoch [14424/20000], Training Loss: 0.4741\n",
            "Epoch [14425/20000], Training Loss: 0.5069\n",
            "Epoch [14426/20000], Training Loss: 0.5053\n",
            "Epoch [14427/20000], Training Loss: 0.4671\n",
            "Epoch [14428/20000], Training Loss: 0.4517\n",
            "Epoch [14429/20000], Training Loss: 0.4973\n",
            "Epoch [14430/20000], Training Loss: 0.4799\n",
            "Epoch [14431/20000], Training Loss: 0.4894\n",
            "Epoch [14432/20000], Training Loss: 0.4874\n",
            "Epoch [14433/20000], Training Loss: 0.4629\n",
            "Epoch [14434/20000], Training Loss: 0.5062\n",
            "Epoch [14435/20000], Training Loss: 0.5398\n",
            "Epoch [14436/20000], Training Loss: 0.4697\n",
            "Epoch [14437/20000], Training Loss: 0.4775\n",
            "Epoch [14438/20000], Training Loss: 0.5207\n",
            "Epoch [14439/20000], Training Loss: 0.4654\n",
            "Epoch [14440/20000], Training Loss: 0.4693\n",
            "Epoch [14441/20000], Training Loss: 0.4897\n",
            "Epoch [14442/20000], Training Loss: 0.4594\n",
            "Epoch [14443/20000], Training Loss: 0.5565\n",
            "Epoch [14444/20000], Training Loss: 0.4926\n",
            "Epoch [14445/20000], Training Loss: 0.5018\n",
            "Epoch [14446/20000], Training Loss: 0.4828\n",
            "Epoch [14447/20000], Training Loss: 0.4712\n",
            "Epoch [14448/20000], Training Loss: 0.4609\n",
            "Epoch [14449/20000], Training Loss: 0.4508\n",
            "Epoch [14450/20000], Training Loss: 0.4962\n",
            "Epoch [14451/20000], Training Loss: 0.4718\n",
            "Epoch [14452/20000], Training Loss: 0.4632\n",
            "Epoch [14453/20000], Training Loss: 0.5013\n",
            "Epoch [14454/20000], Training Loss: 0.4662\n",
            "Epoch [14455/20000], Training Loss: 0.5072\n",
            "Epoch [14456/20000], Training Loss: 0.5106\n",
            "Epoch [14457/20000], Training Loss: 0.4842\n",
            "Epoch [14458/20000], Training Loss: 0.5089\n",
            "Epoch [14459/20000], Training Loss: 0.4338\n",
            "Epoch [14460/20000], Training Loss: 0.5208\n",
            "Epoch [14461/20000], Training Loss: 0.4384\n",
            "Epoch [14462/20000], Training Loss: 0.4798\n",
            "Epoch [14463/20000], Training Loss: 0.4536\n",
            "Epoch [14464/20000], Training Loss: 0.4773\n",
            "Epoch [14465/20000], Training Loss: 0.5336\n",
            "Epoch [14466/20000], Training Loss: 0.5235\n",
            "Epoch [14467/20000], Training Loss: 0.5095\n",
            "Epoch [14468/20000], Training Loss: 0.4533\n",
            "Epoch [14469/20000], Training Loss: 0.4751\n",
            "Epoch [14470/20000], Training Loss: 0.4747\n",
            "Epoch [14471/20000], Training Loss: 0.5268\n",
            "Epoch [14472/20000], Training Loss: 0.4939\n",
            "Epoch [14473/20000], Training Loss: 0.4795\n",
            "Epoch [14474/20000], Training Loss: 0.4996\n",
            "Epoch [14475/20000], Training Loss: 0.4831\n",
            "Epoch [14476/20000], Training Loss: 0.4739\n",
            "Epoch [14477/20000], Training Loss: 0.4295\n",
            "Epoch [14478/20000], Training Loss: 0.4669\n",
            "Epoch [14479/20000], Training Loss: 0.5053\n",
            "Epoch [14480/20000], Training Loss: 0.4748\n",
            "Epoch [14481/20000], Training Loss: 0.4640\n",
            "Epoch [14482/20000], Training Loss: 0.4342\n",
            "Epoch [14483/20000], Training Loss: 0.4642\n",
            "Epoch [14484/20000], Training Loss: 0.5163\n",
            "Epoch [14485/20000], Training Loss: 0.4677\n",
            "Epoch [14486/20000], Training Loss: 0.5043\n",
            "Epoch [14487/20000], Training Loss: 0.4784\n",
            "Epoch [14488/20000], Training Loss: 0.4610\n",
            "Epoch [14489/20000], Training Loss: 0.4778\n",
            "Epoch [14490/20000], Training Loss: 0.4427\n",
            "Epoch [14491/20000], Training Loss: 0.4726\n",
            "Epoch [14492/20000], Training Loss: 0.4633\n",
            "Epoch [14493/20000], Training Loss: 0.4529\n",
            "Epoch [14494/20000], Training Loss: 0.5021\n",
            "Epoch [14495/20000], Training Loss: 0.4417\n",
            "Epoch [14496/20000], Training Loss: 0.5046\n",
            "Epoch [14497/20000], Training Loss: 0.4993\n",
            "Epoch [14498/20000], Training Loss: 0.4951\n",
            "Epoch [14499/20000], Training Loss: 0.5446\n",
            "Epoch [14500/20000], Training Loss: 0.4756\n",
            "Epoch [14501/20000], Training Loss: 0.4957\n",
            "Epoch [14502/20000], Training Loss: 0.4841\n",
            "Epoch [14503/20000], Training Loss: 0.4514\n",
            "Epoch [14504/20000], Training Loss: 0.5057\n",
            "Epoch [14505/20000], Training Loss: 0.4976\n",
            "Epoch [14506/20000], Training Loss: 0.5170\n",
            "Epoch [14507/20000], Training Loss: 0.4782\n",
            "Epoch [14508/20000], Training Loss: 0.4904\n",
            "Epoch [14509/20000], Training Loss: 0.4792\n",
            "Epoch [14510/20000], Training Loss: 0.4968\n",
            "Epoch [14511/20000], Training Loss: 0.5264\n",
            "Epoch [14512/20000], Training Loss: 0.4924\n",
            "Epoch [14513/20000], Training Loss: 0.5251\n",
            "Epoch [14514/20000], Training Loss: 0.5358\n",
            "Epoch [14515/20000], Training Loss: 0.4764\n",
            "Epoch [14516/20000], Training Loss: 0.4508\n",
            "Epoch [14517/20000], Training Loss: 0.4791\n",
            "Epoch [14518/20000], Training Loss: 0.4954\n",
            "Epoch [14519/20000], Training Loss: 0.5172\n",
            "Epoch [14520/20000], Training Loss: 0.4973\n",
            "Epoch [14521/20000], Training Loss: 0.4786\n",
            "Epoch [14522/20000], Training Loss: 0.5280\n",
            "Epoch [14523/20000], Training Loss: 0.4777\n",
            "Epoch [14524/20000], Training Loss: 0.4822\n",
            "Epoch [14525/20000], Training Loss: 0.4941\n",
            "Epoch [14526/20000], Training Loss: 0.4811\n",
            "Epoch [14527/20000], Training Loss: 0.4989\n",
            "Epoch [14528/20000], Training Loss: 0.5083\n",
            "Epoch [14529/20000], Training Loss: 0.4646\n",
            "Epoch [14530/20000], Training Loss: 0.4873\n",
            "Epoch [14531/20000], Training Loss: 0.4895\n",
            "Epoch [14532/20000], Training Loss: 0.4997\n",
            "Epoch [14533/20000], Training Loss: 0.4603\n",
            "Epoch [14534/20000], Training Loss: 0.4587\n",
            "Epoch [14535/20000], Training Loss: 0.4616\n",
            "Epoch [14536/20000], Training Loss: 0.5435\n",
            "Epoch [14537/20000], Training Loss: 0.5238\n",
            "Epoch [14538/20000], Training Loss: 0.5109\n",
            "Epoch [14539/20000], Training Loss: 0.4753\n",
            "Epoch [14540/20000], Training Loss: 0.5136\n",
            "Epoch [14541/20000], Training Loss: 0.5443\n",
            "Epoch [14542/20000], Training Loss: 0.4531\n",
            "Epoch [14543/20000], Training Loss: 0.5498\n",
            "Epoch [14544/20000], Training Loss: 0.4901\n",
            "Epoch [14545/20000], Training Loss: 0.4812\n",
            "Epoch [14546/20000], Training Loss: 0.5114\n",
            "Epoch [14547/20000], Training Loss: 0.4666\n",
            "Epoch [14548/20000], Training Loss: 0.5067\n",
            "Epoch [14549/20000], Training Loss: 0.4813\n",
            "Epoch [14550/20000], Training Loss: 0.4979\n",
            "Epoch [14551/20000], Training Loss: 0.4882\n",
            "Epoch [14552/20000], Training Loss: 0.4718\n",
            "Epoch [14553/20000], Training Loss: 0.5428\n",
            "Epoch [14554/20000], Training Loss: 0.5014\n",
            "Epoch [14555/20000], Training Loss: 0.5322\n",
            "Epoch [14556/20000], Training Loss: 0.4921\n",
            "Epoch [14557/20000], Training Loss: 0.5150\n",
            "Epoch [14558/20000], Training Loss: 0.4726\n",
            "Epoch [14559/20000], Training Loss: 0.4722\n",
            "Epoch [14560/20000], Training Loss: 0.4675\n",
            "Epoch [14561/20000], Training Loss: 0.5044\n",
            "Epoch [14562/20000], Training Loss: 0.4976\n",
            "Epoch [14563/20000], Training Loss: 0.4642\n",
            "Epoch [14564/20000], Training Loss: 0.4525\n",
            "Epoch [14565/20000], Training Loss: 0.4740\n",
            "Epoch [14566/20000], Training Loss: 0.4960\n",
            "Epoch [14567/20000], Training Loss: 0.5113\n",
            "Epoch [14568/20000], Training Loss: 0.4866\n",
            "Epoch [14569/20000], Training Loss: 0.4765\n",
            "Epoch [14570/20000], Training Loss: 0.5197\n",
            "Epoch [14571/20000], Training Loss: 0.5165\n",
            "Epoch [14572/20000], Training Loss: 0.4986\n",
            "Epoch [14573/20000], Training Loss: 0.4728\n",
            "Epoch [14574/20000], Training Loss: 0.4845\n",
            "Epoch [14575/20000], Training Loss: 0.4647\n",
            "Epoch [14576/20000], Training Loss: 0.4623\n",
            "Epoch [14577/20000], Training Loss: 0.4998\n",
            "Epoch [14578/20000], Training Loss: 0.5130\n",
            "Epoch [14579/20000], Training Loss: 0.5326\n",
            "Epoch [14580/20000], Training Loss: 0.4898\n",
            "Epoch [14581/20000], Training Loss: 0.4803\n",
            "Epoch [14582/20000], Training Loss: 0.5425\n",
            "Epoch [14583/20000], Training Loss: 0.4626\n",
            "Epoch [14584/20000], Training Loss: 0.4619\n",
            "Epoch [14585/20000], Training Loss: 0.4940\n",
            "Epoch [14586/20000], Training Loss: 0.5042\n",
            "Epoch [14587/20000], Training Loss: 0.4934\n",
            "Epoch [14588/20000], Training Loss: 0.4741\n",
            "Epoch [14589/20000], Training Loss: 0.4846\n",
            "Epoch [14590/20000], Training Loss: 0.4480\n",
            "Epoch [14591/20000], Training Loss: 0.5221\n",
            "Epoch [14592/20000], Training Loss: 0.4585\n",
            "Epoch [14593/20000], Training Loss: 0.5013\n",
            "Epoch [14594/20000], Training Loss: 0.4400\n",
            "Epoch [14595/20000], Training Loss: 0.4938\n",
            "Epoch [14596/20000], Training Loss: 0.5031\n",
            "Epoch [14597/20000], Training Loss: 0.4756\n",
            "Epoch [14598/20000], Training Loss: 0.4689\n",
            "Epoch [14599/20000], Training Loss: 0.4977\n",
            "Epoch [14600/20000], Training Loss: 0.4495\n",
            "Epoch [14601/20000], Training Loss: 0.4708\n",
            "Epoch [14602/20000], Training Loss: 0.4811\n",
            "Epoch [14603/20000], Training Loss: 0.4990\n",
            "Epoch [14604/20000], Training Loss: 0.4558\n",
            "Epoch [14605/20000], Training Loss: 0.4735\n",
            "Epoch [14606/20000], Training Loss: 0.5106\n",
            "Epoch [14607/20000], Training Loss: 0.4674\n",
            "Epoch [14608/20000], Training Loss: 0.4911\n",
            "Epoch [14609/20000], Training Loss: 0.4423\n",
            "Epoch [14610/20000], Training Loss: 0.4486\n",
            "Epoch [14611/20000], Training Loss: 0.4874\n",
            "Epoch [14612/20000], Training Loss: 0.4940\n",
            "Epoch [14613/20000], Training Loss: 0.4677\n",
            "Epoch [14614/20000], Training Loss: 0.4870\n",
            "Epoch [14615/20000], Training Loss: 0.4652\n",
            "Epoch [14616/20000], Training Loss: 0.4844\n",
            "Epoch [14617/20000], Training Loss: 0.5190\n",
            "Epoch [14618/20000], Training Loss: 0.4817\n",
            "Epoch [14619/20000], Training Loss: 0.4682\n",
            "Epoch [14620/20000], Training Loss: 0.4673\n",
            "Epoch [14621/20000], Training Loss: 0.4554\n",
            "Epoch [14622/20000], Training Loss: 0.5036\n",
            "Epoch [14623/20000], Training Loss: 0.4486\n",
            "Epoch [14624/20000], Training Loss: 0.5060\n",
            "Epoch [14625/20000], Training Loss: 0.5226\n",
            "Epoch [14626/20000], Training Loss: 0.4768\n",
            "Epoch [14627/20000], Training Loss: 0.4345\n",
            "Epoch [14628/20000], Training Loss: 0.5061\n",
            "Epoch [14629/20000], Training Loss: 0.5457\n",
            "Epoch [14630/20000], Training Loss: 0.4677\n",
            "Epoch [14631/20000], Training Loss: 0.4980\n",
            "Epoch [14632/20000], Training Loss: 0.4942\n",
            "Epoch [14633/20000], Training Loss: 0.5145\n",
            "Epoch [14634/20000], Training Loss: 0.5077\n",
            "Epoch [14635/20000], Training Loss: 0.4837\n",
            "Epoch [14636/20000], Training Loss: 0.4845\n",
            "Epoch [14637/20000], Training Loss: 0.4528\n",
            "Epoch [14638/20000], Training Loss: 0.4398\n",
            "Epoch [14639/20000], Training Loss: 0.4398\n",
            "Epoch [14640/20000], Training Loss: 0.4582\n",
            "Epoch [14641/20000], Training Loss: 0.4401\n",
            "Epoch [14642/20000], Training Loss: 0.5023\n",
            "Epoch [14643/20000], Training Loss: 0.4826\n",
            "Epoch [14644/20000], Training Loss: 0.4792\n",
            "Epoch [14645/20000], Training Loss: 0.5003\n",
            "Epoch [14646/20000], Training Loss: 0.4569\n",
            "Epoch [14647/20000], Training Loss: 0.4476\n",
            "Epoch [14648/20000], Training Loss: 0.4764\n",
            "Epoch [14649/20000], Training Loss: 0.4685\n",
            "Epoch [14650/20000], Training Loss: 0.5217\n",
            "Epoch [14651/20000], Training Loss: 0.4677\n",
            "Epoch [14652/20000], Training Loss: 0.4358\n",
            "Epoch [14653/20000], Training Loss: 0.5003\n",
            "Epoch [14654/20000], Training Loss: 0.4739\n",
            "Epoch [14655/20000], Training Loss: 0.4715\n",
            "Epoch [14656/20000], Training Loss: 0.4867\n",
            "Epoch [14657/20000], Training Loss: 0.5194\n",
            "Epoch [14658/20000], Training Loss: 0.5220\n",
            "Epoch [14659/20000], Training Loss: 0.4652\n",
            "Epoch [14660/20000], Training Loss: 0.4977\n",
            "Epoch [14661/20000], Training Loss: 0.4586\n",
            "Epoch [14662/20000], Training Loss: 0.4736\n",
            "Epoch [14663/20000], Training Loss: 0.4751\n",
            "Epoch [14664/20000], Training Loss: 0.4669\n",
            "Epoch [14665/20000], Training Loss: 0.5183\n",
            "Epoch [14666/20000], Training Loss: 0.4809\n",
            "Epoch [14667/20000], Training Loss: 0.4904\n",
            "Epoch [14668/20000], Training Loss: 0.4995\n",
            "Epoch [14669/20000], Training Loss: 0.4668\n",
            "Epoch [14670/20000], Training Loss: 0.4646\n",
            "Epoch [14671/20000], Training Loss: 0.4741\n",
            "Epoch [14672/20000], Training Loss: 0.4574\n",
            "Epoch [14673/20000], Training Loss: 0.5337\n",
            "Epoch [14674/20000], Training Loss: 0.4691\n",
            "Epoch [14675/20000], Training Loss: 0.4916\n",
            "Epoch [14676/20000], Training Loss: 0.5021\n",
            "Epoch [14677/20000], Training Loss: 0.5441\n",
            "Epoch [14678/20000], Training Loss: 0.5419\n",
            "Epoch [14679/20000], Training Loss: 0.4773\n",
            "Epoch [14680/20000], Training Loss: 0.4766\n",
            "Epoch [14681/20000], Training Loss: 0.4938\n",
            "Epoch [14682/20000], Training Loss: 0.4651\n",
            "Epoch [14683/20000], Training Loss: 0.4960\n",
            "Epoch [14684/20000], Training Loss: 0.4782\n",
            "Epoch [14685/20000], Training Loss: 0.5029\n",
            "Epoch [14686/20000], Training Loss: 0.5142\n",
            "Epoch [14687/20000], Training Loss: 0.4451\n",
            "Epoch [14688/20000], Training Loss: 0.4728\n",
            "Epoch [14689/20000], Training Loss: 0.5197\n",
            "Epoch [14690/20000], Training Loss: 0.4831\n",
            "Epoch [14691/20000], Training Loss: 0.4659\n",
            "Epoch [14692/20000], Training Loss: 0.5029\n",
            "Epoch [14693/20000], Training Loss: 0.5112\n",
            "Epoch [14694/20000], Training Loss: 0.4947\n",
            "Epoch [14695/20000], Training Loss: 0.5057\n",
            "Epoch [14696/20000], Training Loss: 0.4720\n",
            "Epoch [14697/20000], Training Loss: 0.4686\n",
            "Epoch [14698/20000], Training Loss: 0.4708\n",
            "Epoch [14699/20000], Training Loss: 0.5035\n",
            "Epoch [14700/20000], Training Loss: 0.5363\n",
            "Epoch [14701/20000], Training Loss: 0.5103\n",
            "Epoch [14702/20000], Training Loss: 0.5188\n",
            "Epoch [14703/20000], Training Loss: 0.5413\n",
            "Epoch [14704/20000], Training Loss: 0.5318\n",
            "Epoch [14705/20000], Training Loss: 0.4426\n",
            "Epoch [14706/20000], Training Loss: 0.5055\n",
            "Epoch [14707/20000], Training Loss: 0.4983\n",
            "Epoch [14708/20000], Training Loss: 0.4327\n",
            "Epoch [14709/20000], Training Loss: 0.4939\n",
            "Epoch [14710/20000], Training Loss: 0.4391\n",
            "Epoch [14711/20000], Training Loss: 0.5360\n",
            "Epoch [14712/20000], Training Loss: 0.4738\n",
            "Epoch [14713/20000], Training Loss: 0.5362\n",
            "Epoch [14714/20000], Training Loss: 0.4781\n",
            "Epoch [14715/20000], Training Loss: 0.4778\n",
            "Epoch [14716/20000], Training Loss: 0.4787\n",
            "Epoch [14717/20000], Training Loss: 0.4574\n",
            "Epoch [14718/20000], Training Loss: 0.4719\n",
            "Epoch [14719/20000], Training Loss: 0.4914\n",
            "Epoch [14720/20000], Training Loss: 0.5113\n",
            "Epoch [14721/20000], Training Loss: 0.4900\n",
            "Epoch [14722/20000], Training Loss: 0.4580\n",
            "Epoch [14723/20000], Training Loss: 0.4705\n",
            "Epoch [14724/20000], Training Loss: 0.4590\n",
            "Epoch [14725/20000], Training Loss: 0.4695\n",
            "Epoch [14726/20000], Training Loss: 0.4687\n",
            "Epoch [14727/20000], Training Loss: 0.4773\n",
            "Epoch [14728/20000], Training Loss: 0.5161\n",
            "Epoch [14729/20000], Training Loss: 0.4815\n",
            "Epoch [14730/20000], Training Loss: 0.4531\n",
            "Epoch [14731/20000], Training Loss: 0.4786\n",
            "Epoch [14732/20000], Training Loss: 0.4782\n",
            "Epoch [14733/20000], Training Loss: 0.5021\n",
            "Epoch [14734/20000], Training Loss: 0.5034\n",
            "Epoch [14735/20000], Training Loss: 0.4977\n",
            "Epoch [14736/20000], Training Loss: 0.4523\n",
            "Epoch [14737/20000], Training Loss: 0.5011\n",
            "Epoch [14738/20000], Training Loss: 0.4719\n",
            "Epoch [14739/20000], Training Loss: 0.4776\n",
            "Epoch [14740/20000], Training Loss: 0.5306\n",
            "Epoch [14741/20000], Training Loss: 0.4735\n",
            "Epoch [14742/20000], Training Loss: 0.5082\n",
            "Epoch [14743/20000], Training Loss: 0.4914\n",
            "Epoch [14744/20000], Training Loss: 0.4677\n",
            "Epoch [14745/20000], Training Loss: 0.4671\n",
            "Epoch [14746/20000], Training Loss: 0.4850\n",
            "Epoch [14747/20000], Training Loss: 0.5096\n",
            "Epoch [14748/20000], Training Loss: 0.5236\n",
            "Epoch [14749/20000], Training Loss: 0.4791\n",
            "Epoch [14750/20000], Training Loss: 0.5166\n",
            "Epoch [14751/20000], Training Loss: 0.5123\n",
            "Epoch [14752/20000], Training Loss: 0.5193\n",
            "Epoch [14753/20000], Training Loss: 0.4773\n",
            "Epoch [14754/20000], Training Loss: 0.4522\n",
            "Epoch [14755/20000], Training Loss: 0.4951\n",
            "Epoch [14756/20000], Training Loss: 0.4692\n",
            "Epoch [14757/20000], Training Loss: 0.4825\n",
            "Epoch [14758/20000], Training Loss: 0.4590\n",
            "Epoch [14759/20000], Training Loss: 0.4854\n",
            "Epoch [14760/20000], Training Loss: 0.5328\n",
            "Epoch [14761/20000], Training Loss: 0.4372\n",
            "Epoch [14762/20000], Training Loss: 0.5025\n",
            "Epoch [14763/20000], Training Loss: 0.5194\n",
            "Epoch [14764/20000], Training Loss: 0.4978\n",
            "Epoch [14765/20000], Training Loss: 0.4763\n",
            "Epoch [14766/20000], Training Loss: 0.4936\n",
            "Epoch [14767/20000], Training Loss: 0.4806\n",
            "Epoch [14768/20000], Training Loss: 0.4734\n",
            "Epoch [14769/20000], Training Loss: 0.4558\n",
            "Epoch [14770/20000], Training Loss: 0.4881\n",
            "Epoch [14771/20000], Training Loss: 0.4826\n",
            "Epoch [14772/20000], Training Loss: 0.4770\n",
            "Epoch [14773/20000], Training Loss: 0.4967\n",
            "Epoch [14774/20000], Training Loss: 0.4689\n",
            "Epoch [14775/20000], Training Loss: 0.5088\n",
            "Epoch [14776/20000], Training Loss: 0.4737\n",
            "Epoch [14777/20000], Training Loss: 0.5034\n",
            "Epoch [14778/20000], Training Loss: 0.5149\n",
            "Epoch [14779/20000], Training Loss: 0.4826\n",
            "Epoch [14780/20000], Training Loss: 0.5041\n",
            "Epoch [14781/20000], Training Loss: 0.5153\n",
            "Epoch [14782/20000], Training Loss: 0.5376\n",
            "Epoch [14783/20000], Training Loss: 0.4830\n",
            "Epoch [14784/20000], Training Loss: 0.4829\n",
            "Epoch [14785/20000], Training Loss: 0.4587\n",
            "Epoch [14786/20000], Training Loss: 0.4811\n",
            "Epoch [14787/20000], Training Loss: 0.4589\n",
            "Epoch [14788/20000], Training Loss: 0.5093\n",
            "Epoch [14789/20000], Training Loss: 0.4814\n",
            "Epoch [14790/20000], Training Loss: 0.4535\n",
            "Epoch [14791/20000], Training Loss: 0.4822\n",
            "Epoch [14792/20000], Training Loss: 0.4708\n",
            "Epoch [14793/20000], Training Loss: 0.4560\n",
            "Epoch [14794/20000], Training Loss: 0.4996\n",
            "Epoch [14795/20000], Training Loss: 0.5035\n",
            "Epoch [14796/20000], Training Loss: 0.4976\n",
            "Epoch [14797/20000], Training Loss: 0.5161\n",
            "Epoch [14798/20000], Training Loss: 0.4847\n",
            "Epoch [14799/20000], Training Loss: 0.5055\n",
            "Epoch [14800/20000], Training Loss: 0.4831\n",
            "Epoch [14801/20000], Training Loss: 0.5090\n",
            "Epoch [14802/20000], Training Loss: 0.5022\n",
            "Epoch [14803/20000], Training Loss: 0.5205\n",
            "Epoch [14804/20000], Training Loss: 0.5036\n",
            "Epoch [14805/20000], Training Loss: 0.4776\n",
            "Epoch [14806/20000], Training Loss: 0.4816\n",
            "Epoch [14807/20000], Training Loss: 0.4696\n",
            "Epoch [14808/20000], Training Loss: 0.4721\n",
            "Epoch [14809/20000], Training Loss: 0.4465\n",
            "Epoch [14810/20000], Training Loss: 0.4812\n",
            "Epoch [14811/20000], Training Loss: 0.4989\n",
            "Epoch [14812/20000], Training Loss: 0.5340\n",
            "Epoch [14813/20000], Training Loss: 0.4930\n",
            "Epoch [14814/20000], Training Loss: 0.4912\n",
            "Epoch [14815/20000], Training Loss: 0.4603\n",
            "Epoch [14816/20000], Training Loss: 0.5148\n",
            "Epoch [14817/20000], Training Loss: 0.4745\n",
            "Epoch [14818/20000], Training Loss: 0.4596\n",
            "Epoch [14819/20000], Training Loss: 0.5019\n",
            "Epoch [14820/20000], Training Loss: 0.5277\n",
            "Epoch [14821/20000], Training Loss: 0.4942\n",
            "Epoch [14822/20000], Training Loss: 0.4757\n",
            "Epoch [14823/20000], Training Loss: 0.4671\n",
            "Epoch [14824/20000], Training Loss: 0.4773\n",
            "Epoch [14825/20000], Training Loss: 0.5157\n",
            "Epoch [14826/20000], Training Loss: 0.4642\n",
            "Epoch [14827/20000], Training Loss: 0.5071\n",
            "Epoch [14828/20000], Training Loss: 0.4591\n",
            "Epoch [14829/20000], Training Loss: 0.4813\n",
            "Epoch [14830/20000], Training Loss: 0.5284\n",
            "Epoch [14831/20000], Training Loss: 0.4880\n",
            "Epoch [14832/20000], Training Loss: 0.4689\n",
            "Epoch [14833/20000], Training Loss: 0.4753\n",
            "Epoch [14834/20000], Training Loss: 0.4422\n",
            "Epoch [14835/20000], Training Loss: 0.4907\n",
            "Epoch [14836/20000], Training Loss: 0.4982\n",
            "Epoch [14837/20000], Training Loss: 0.4902\n",
            "Epoch [14838/20000], Training Loss: 0.4642\n",
            "Epoch [14839/20000], Training Loss: 0.4624\n",
            "Epoch [14840/20000], Training Loss: 0.4869\n",
            "Epoch [14841/20000], Training Loss: 0.5375\n",
            "Epoch [14842/20000], Training Loss: 0.5085\n",
            "Epoch [14843/20000], Training Loss: 0.4555\n",
            "Epoch [14844/20000], Training Loss: 0.5272\n",
            "Epoch [14845/20000], Training Loss: 0.4416\n",
            "Epoch [14846/20000], Training Loss: 0.4976\n",
            "Epoch [14847/20000], Training Loss: 0.5164\n",
            "Epoch [14848/20000], Training Loss: 0.4673\n",
            "Epoch [14849/20000], Training Loss: 0.4775\n",
            "Epoch [14850/20000], Training Loss: 0.4976\n",
            "Epoch [14851/20000], Training Loss: 0.4876\n",
            "Epoch [14852/20000], Training Loss: 0.4916\n",
            "Epoch [14853/20000], Training Loss: 0.4876\n",
            "Epoch [14854/20000], Training Loss: 0.4549\n",
            "Epoch [14855/20000], Training Loss: 0.4612\n",
            "Epoch [14856/20000], Training Loss: 0.4358\n",
            "Epoch [14857/20000], Training Loss: 0.4442\n",
            "Epoch [14858/20000], Training Loss: 0.4828\n",
            "Epoch [14859/20000], Training Loss: 0.4658\n",
            "Epoch [14860/20000], Training Loss: 0.4701\n",
            "Epoch [14861/20000], Training Loss: 0.5033\n",
            "Epoch [14862/20000], Training Loss: 0.4862\n",
            "Epoch [14863/20000], Training Loss: 0.4697\n",
            "Epoch [14864/20000], Training Loss: 0.4806\n",
            "Epoch [14865/20000], Training Loss: 0.4443\n",
            "Epoch [14866/20000], Training Loss: 0.4778\n",
            "Epoch [14867/20000], Training Loss: 0.4966\n",
            "Epoch [14868/20000], Training Loss: 0.4755\n",
            "Epoch [14869/20000], Training Loss: 0.4528\n",
            "Epoch [14870/20000], Training Loss: 0.4961\n",
            "Epoch [14871/20000], Training Loss: 0.4401\n",
            "Epoch [14872/20000], Training Loss: 0.5082\n",
            "Epoch [14873/20000], Training Loss: 0.5164\n",
            "Epoch [14874/20000], Training Loss: 0.4787\n",
            "Epoch [14875/20000], Training Loss: 0.4913\n",
            "Epoch [14876/20000], Training Loss: 0.4998\n",
            "Epoch [14877/20000], Training Loss: 0.5108\n",
            "Epoch [14878/20000], Training Loss: 0.5013\n",
            "Epoch [14879/20000], Training Loss: 0.5163\n",
            "Epoch [14880/20000], Training Loss: 0.4695\n",
            "Epoch [14881/20000], Training Loss: 0.4601\n",
            "Epoch [14882/20000], Training Loss: 0.5065\n",
            "Epoch [14883/20000], Training Loss: 0.4565\n",
            "Epoch [14884/20000], Training Loss: 0.5337\n",
            "Epoch [14885/20000], Training Loss: 0.5430\n",
            "Epoch [14886/20000], Training Loss: 0.5189\n",
            "Epoch [14887/20000], Training Loss: 0.5306\n",
            "Epoch [14888/20000], Training Loss: 0.5273\n",
            "Epoch [14889/20000], Training Loss: 0.5015\n",
            "Epoch [14890/20000], Training Loss: 0.5198\n",
            "Epoch [14891/20000], Training Loss: 0.4986\n",
            "Epoch [14892/20000], Training Loss: 0.5392\n",
            "Epoch [14893/20000], Training Loss: 0.4637\n",
            "Epoch [14894/20000], Training Loss: 0.5156\n",
            "Epoch [14895/20000], Training Loss: 0.5015\n",
            "Epoch [14896/20000], Training Loss: 0.5373\n",
            "Epoch [14897/20000], Training Loss: 0.4591\n",
            "Epoch [14898/20000], Training Loss: 0.4612\n",
            "Epoch [14899/20000], Training Loss: 0.4762\n",
            "Epoch [14900/20000], Training Loss: 0.4649\n",
            "Epoch [14901/20000], Training Loss: 0.4948\n",
            "Epoch [14902/20000], Training Loss: 0.4817\n",
            "Epoch [14903/20000], Training Loss: 0.4854\n",
            "Epoch [14904/20000], Training Loss: 0.5061\n",
            "Epoch [14905/20000], Training Loss: 0.5065\n",
            "Epoch [14906/20000], Training Loss: 0.4764\n",
            "Epoch [14907/20000], Training Loss: 0.4988\n",
            "Epoch [14908/20000], Training Loss: 0.4547\n",
            "Epoch [14909/20000], Training Loss: 0.4685\n",
            "Epoch [14910/20000], Training Loss: 0.4405\n",
            "Epoch [14911/20000], Training Loss: 0.5219\n",
            "Epoch [14912/20000], Training Loss: 0.4357\n",
            "Epoch [14913/20000], Training Loss: 0.4909\n",
            "Epoch [14914/20000], Training Loss: 0.5126\n",
            "Epoch [14915/20000], Training Loss: 0.5315\n",
            "Epoch [14916/20000], Training Loss: 0.5386\n",
            "Epoch [14917/20000], Training Loss: 0.4970\n",
            "Epoch [14918/20000], Training Loss: 0.4684\n",
            "Epoch [14919/20000], Training Loss: 0.5085\n",
            "Epoch [14920/20000], Training Loss: 0.5154\n",
            "Epoch [14921/20000], Training Loss: 0.4677\n",
            "Epoch [14922/20000], Training Loss: 0.4974\n",
            "Epoch [14923/20000], Training Loss: 0.4548\n",
            "Epoch [14924/20000], Training Loss: 0.4988\n",
            "Epoch [14925/20000], Training Loss: 0.5086\n",
            "Epoch [14926/20000], Training Loss: 0.5464\n",
            "Epoch [14927/20000], Training Loss: 0.4739\n",
            "Epoch [14928/20000], Training Loss: 0.4709\n",
            "Epoch [14929/20000], Training Loss: 0.4736\n",
            "Epoch [14930/20000], Training Loss: 0.4939\n",
            "Epoch [14931/20000], Training Loss: 0.4619\n",
            "Epoch [14932/20000], Training Loss: 0.5355\n",
            "Epoch [14933/20000], Training Loss: 0.4827\n",
            "Epoch [14934/20000], Training Loss: 0.5290\n",
            "Epoch [14935/20000], Training Loss: 0.4974\n",
            "Epoch [14936/20000], Training Loss: 0.4552\n",
            "Epoch [14937/20000], Training Loss: 0.5172\n",
            "Epoch [14938/20000], Training Loss: 0.4607\n",
            "Epoch [14939/20000], Training Loss: 0.4713\n",
            "Epoch [14940/20000], Training Loss: 0.4489\n",
            "Epoch [14941/20000], Training Loss: 0.5376\n",
            "Epoch [14942/20000], Training Loss: 0.4703\n",
            "Epoch [14943/20000], Training Loss: 0.4650\n",
            "Epoch [14944/20000], Training Loss: 0.5197\n",
            "Epoch [14945/20000], Training Loss: 0.4918\n",
            "Epoch [14946/20000], Training Loss: 0.4497\n",
            "Epoch [14947/20000], Training Loss: 0.4643\n",
            "Epoch [14948/20000], Training Loss: 0.4903\n",
            "Epoch [14949/20000], Training Loss: 0.4905\n",
            "Epoch [14950/20000], Training Loss: 0.4992\n",
            "Epoch [14951/20000], Training Loss: 0.4537\n",
            "Epoch [14952/20000], Training Loss: 0.4684\n",
            "Epoch [14953/20000], Training Loss: 0.4602\n",
            "Epoch [14954/20000], Training Loss: 0.4602\n",
            "Epoch [14955/20000], Training Loss: 0.4718\n",
            "Epoch [14956/20000], Training Loss: 0.4890\n",
            "Epoch [14957/20000], Training Loss: 0.4532\n",
            "Epoch [14958/20000], Training Loss: 0.5190\n",
            "Epoch [14959/20000], Training Loss: 0.4631\n",
            "Epoch [14960/20000], Training Loss: 0.5145\n",
            "Epoch [14961/20000], Training Loss: 0.4724\n",
            "Epoch [14962/20000], Training Loss: 0.5136\n",
            "Epoch [14963/20000], Training Loss: 0.4817\n",
            "Epoch [14964/20000], Training Loss: 0.5059\n",
            "Epoch [14965/20000], Training Loss: 0.4586\n",
            "Epoch [14966/20000], Training Loss: 0.4664\n",
            "Epoch [14967/20000], Training Loss: 0.5059\n",
            "Epoch [14968/20000], Training Loss: 0.4601\n",
            "Epoch [14969/20000], Training Loss: 0.4996\n",
            "Epoch [14970/20000], Training Loss: 0.4970\n",
            "Epoch [14971/20000], Training Loss: 0.4955\n",
            "Epoch [14972/20000], Training Loss: 0.4605\n",
            "Epoch [14973/20000], Training Loss: 0.5065\n",
            "Epoch [14974/20000], Training Loss: 0.4748\n",
            "Epoch [14975/20000], Training Loss: 0.4774\n",
            "Epoch [14976/20000], Training Loss: 0.4536\n",
            "Epoch [14977/20000], Training Loss: 0.4570\n",
            "Epoch [14978/20000], Training Loss: 0.5056\n",
            "Epoch [14979/20000], Training Loss: 0.4919\n",
            "Epoch [14980/20000], Training Loss: 0.5113\n",
            "Epoch [14981/20000], Training Loss: 0.4607\n",
            "Epoch [14982/20000], Training Loss: 0.5075\n",
            "Epoch [14983/20000], Training Loss: 0.5388\n",
            "Epoch [14984/20000], Training Loss: 0.4737\n",
            "Epoch [14985/20000], Training Loss: 0.5133\n",
            "Epoch [14986/20000], Training Loss: 0.4989\n",
            "Epoch [14987/20000], Training Loss: 0.4787\n",
            "Epoch [14988/20000], Training Loss: 0.4569\n",
            "Epoch [14989/20000], Training Loss: 0.4758\n",
            "Epoch [14990/20000], Training Loss: 0.4965\n",
            "Epoch [14991/20000], Training Loss: 0.5147\n",
            "Epoch [14992/20000], Training Loss: 0.4797\n",
            "Epoch [14993/20000], Training Loss: 0.4372\n",
            "Epoch [14994/20000], Training Loss: 0.4700\n",
            "Epoch [14995/20000], Training Loss: 0.4915\n",
            "Epoch [14996/20000], Training Loss: 0.5268\n",
            "Epoch [14997/20000], Training Loss: 0.5444\n",
            "Epoch [14998/20000], Training Loss: 0.4805\n",
            "Epoch [14999/20000], Training Loss: 0.4522\n",
            "Epoch [15000/20000], Training Loss: 0.4678\n",
            "Epoch [15001/20000], Training Loss: 0.5129\n",
            "Epoch [15002/20000], Training Loss: 0.5033\n",
            "Epoch [15003/20000], Training Loss: 0.4898\n",
            "Epoch [15004/20000], Training Loss: 0.4783\n",
            "Epoch [15005/20000], Training Loss: 0.4922\n",
            "Epoch [15006/20000], Training Loss: 0.4763\n",
            "Epoch [15007/20000], Training Loss: 0.4562\n",
            "Epoch [15008/20000], Training Loss: 0.4878\n",
            "Epoch [15009/20000], Training Loss: 0.5002\n",
            "Epoch [15010/20000], Training Loss: 0.4878\n",
            "Epoch [15011/20000], Training Loss: 0.5107\n",
            "Epoch [15012/20000], Training Loss: 0.4859\n",
            "Epoch [15013/20000], Training Loss: 0.4643\n",
            "Epoch [15014/20000], Training Loss: 0.4860\n",
            "Epoch [15015/20000], Training Loss: 0.4759\n",
            "Epoch [15016/20000], Training Loss: 0.4895\n",
            "Epoch [15017/20000], Training Loss: 0.5092\n",
            "Epoch [15018/20000], Training Loss: 0.4662\n",
            "Epoch [15019/20000], Training Loss: 0.5163\n",
            "Epoch [15020/20000], Training Loss: 0.5062\n",
            "Epoch [15021/20000], Training Loss: 0.4499\n",
            "Epoch [15022/20000], Training Loss: 0.4557\n",
            "Epoch [15023/20000], Training Loss: 0.5148\n",
            "Epoch [15024/20000], Training Loss: 0.5068\n",
            "Epoch [15025/20000], Training Loss: 0.5196\n",
            "Epoch [15026/20000], Training Loss: 0.4951\n",
            "Epoch [15027/20000], Training Loss: 0.4845\n",
            "Epoch [15028/20000], Training Loss: 0.4619\n",
            "Epoch [15029/20000], Training Loss: 0.4786\n",
            "Epoch [15030/20000], Training Loss: 0.4907\n",
            "Epoch [15031/20000], Training Loss: 0.5077\n",
            "Epoch [15032/20000], Training Loss: 0.5022\n",
            "Epoch [15033/20000], Training Loss: 0.5179\n",
            "Epoch [15034/20000], Training Loss: 0.4935\n",
            "Epoch [15035/20000], Training Loss: 0.5197\n",
            "Epoch [15036/20000], Training Loss: 0.5327\n",
            "Epoch [15037/20000], Training Loss: 0.4622\n",
            "Epoch [15038/20000], Training Loss: 0.5379\n",
            "Epoch [15039/20000], Training Loss: 0.4593\n",
            "Epoch [15040/20000], Training Loss: 0.4942\n",
            "Epoch [15041/20000], Training Loss: 0.4894\n",
            "Epoch [15042/20000], Training Loss: 0.4976\n",
            "Epoch [15043/20000], Training Loss: 0.4911\n",
            "Epoch [15044/20000], Training Loss: 0.4947\n",
            "Epoch [15045/20000], Training Loss: 0.4956\n",
            "Epoch [15046/20000], Training Loss: 0.5029\n",
            "Epoch [15047/20000], Training Loss: 0.5008\n",
            "Epoch [15048/20000], Training Loss: 0.4944\n",
            "Epoch [15049/20000], Training Loss: 0.4688\n",
            "Epoch [15050/20000], Training Loss: 0.4869\n",
            "Epoch [15051/20000], Training Loss: 0.4260\n",
            "Epoch [15052/20000], Training Loss: 0.4537\n",
            "Epoch [15053/20000], Training Loss: 0.4876\n",
            "Epoch [15054/20000], Training Loss: 0.4782\n",
            "Epoch [15055/20000], Training Loss: 0.4612\n",
            "Epoch [15056/20000], Training Loss: 0.4914\n",
            "Epoch [15057/20000], Training Loss: 0.4521\n",
            "Epoch [15058/20000], Training Loss: 0.5041\n",
            "Epoch [15059/20000], Training Loss: 0.5031\n",
            "Epoch [15060/20000], Training Loss: 0.5009\n",
            "Epoch [15061/20000], Training Loss: 0.4668\n",
            "Epoch [15062/20000], Training Loss: 0.4672\n",
            "Epoch [15063/20000], Training Loss: 0.5398\n",
            "Epoch [15064/20000], Training Loss: 0.5240\n",
            "Epoch [15065/20000], Training Loss: 0.4646\n",
            "Epoch [15066/20000], Training Loss: 0.5160\n",
            "Epoch [15067/20000], Training Loss: 0.4894\n",
            "Epoch [15068/20000], Training Loss: 0.4640\n",
            "Epoch [15069/20000], Training Loss: 0.5128\n",
            "Epoch [15070/20000], Training Loss: 0.5115\n",
            "Epoch [15071/20000], Training Loss: 0.4744\n",
            "Epoch [15072/20000], Training Loss: 0.4752\n",
            "Epoch [15073/20000], Training Loss: 0.4765\n",
            "Epoch [15074/20000], Training Loss: 0.4822\n",
            "Epoch [15075/20000], Training Loss: 0.5135\n",
            "Epoch [15076/20000], Training Loss: 0.4361\n",
            "Epoch [15077/20000], Training Loss: 0.5063\n",
            "Epoch [15078/20000], Training Loss: 0.4930\n",
            "Epoch [15079/20000], Training Loss: 0.4812\n",
            "Epoch [15080/20000], Training Loss: 0.4704\n",
            "Epoch [15081/20000], Training Loss: 0.4981\n",
            "Epoch [15082/20000], Training Loss: 0.4459\n",
            "Epoch [15083/20000], Training Loss: 0.5320\n",
            "Epoch [15084/20000], Training Loss: 0.5011\n",
            "Epoch [15085/20000], Training Loss: 0.5155\n",
            "Epoch [15086/20000], Training Loss: 0.4394\n",
            "Epoch [15087/20000], Training Loss: 0.5067\n",
            "Epoch [15088/20000], Training Loss: 0.4974\n",
            "Epoch [15089/20000], Training Loss: 0.4892\n",
            "Epoch [15090/20000], Training Loss: 0.4736\n",
            "Epoch [15091/20000], Training Loss: 0.4706\n",
            "Epoch [15092/20000], Training Loss: 0.4899\n",
            "Epoch [15093/20000], Training Loss: 0.5234\n",
            "Epoch [15094/20000], Training Loss: 0.5137\n",
            "Epoch [15095/20000], Training Loss: 0.4669\n",
            "Epoch [15096/20000], Training Loss: 0.4652\n",
            "Epoch [15097/20000], Training Loss: 0.4897\n",
            "Epoch [15098/20000], Training Loss: 0.4980\n",
            "Epoch [15099/20000], Training Loss: 0.4959\n",
            "Epoch [15100/20000], Training Loss: 0.4745\n",
            "Epoch [15101/20000], Training Loss: 0.4784\n",
            "Epoch [15102/20000], Training Loss: 0.5157\n",
            "Epoch [15103/20000], Training Loss: 0.4862\n",
            "Epoch [15104/20000], Training Loss: 0.4959\n",
            "Epoch [15105/20000], Training Loss: 0.4516\n",
            "Epoch [15106/20000], Training Loss: 0.5021\n",
            "Epoch [15107/20000], Training Loss: 0.4737\n",
            "Epoch [15108/20000], Training Loss: 0.4835\n",
            "Epoch [15109/20000], Training Loss: 0.4957\n",
            "Epoch [15110/20000], Training Loss: 0.5123\n",
            "Epoch [15111/20000], Training Loss: 0.4575\n",
            "Epoch [15112/20000], Training Loss: 0.4676\n",
            "Epoch [15113/20000], Training Loss: 0.5219\n",
            "Epoch [15114/20000], Training Loss: 0.4540\n",
            "Epoch [15115/20000], Training Loss: 0.4986\n",
            "Epoch [15116/20000], Training Loss: 0.4833\n",
            "Epoch [15117/20000], Training Loss: 0.4829\n",
            "Epoch [15118/20000], Training Loss: 0.4807\n",
            "Epoch [15119/20000], Training Loss: 0.5236\n",
            "Epoch [15120/20000], Training Loss: 0.4748\n",
            "Epoch [15121/20000], Training Loss: 0.4947\n",
            "Epoch [15122/20000], Training Loss: 0.4756\n",
            "Epoch [15123/20000], Training Loss: 0.4416\n",
            "Epoch [15124/20000], Training Loss: 0.4785\n",
            "Epoch [15125/20000], Training Loss: 0.4977\n",
            "Epoch [15126/20000], Training Loss: 0.5050\n",
            "Epoch [15127/20000], Training Loss: 0.4662\n",
            "Epoch [15128/20000], Training Loss: 0.4916\n",
            "Epoch [15129/20000], Training Loss: 0.4901\n",
            "Epoch [15130/20000], Training Loss: 0.5368\n",
            "Epoch [15131/20000], Training Loss: 0.5138\n",
            "Epoch [15132/20000], Training Loss: 0.5379\n",
            "Epoch [15133/20000], Training Loss: 0.5111\n",
            "Epoch [15134/20000], Training Loss: 0.5311\n",
            "Epoch [15135/20000], Training Loss: 0.4985\n",
            "Epoch [15136/20000], Training Loss: 0.4419\n",
            "Epoch [15137/20000], Training Loss: 0.4774\n",
            "Epoch [15138/20000], Training Loss: 0.4963\n",
            "Epoch [15139/20000], Training Loss: 0.4685\n",
            "Epoch [15140/20000], Training Loss: 0.4545\n",
            "Epoch [15141/20000], Training Loss: 0.4820\n",
            "Epoch [15142/20000], Training Loss: 0.5605\n",
            "Epoch [15143/20000], Training Loss: 0.4708\n",
            "Epoch [15144/20000], Training Loss: 0.4927\n",
            "Epoch [15145/20000], Training Loss: 0.4683\n",
            "Epoch [15146/20000], Training Loss: 0.4961\n",
            "Epoch [15147/20000], Training Loss: 0.4803\n",
            "Epoch [15148/20000], Training Loss: 0.4411\n",
            "Epoch [15149/20000], Training Loss: 0.4718\n",
            "Epoch [15150/20000], Training Loss: 0.4633\n",
            "Epoch [15151/20000], Training Loss: 0.4731\n",
            "Epoch [15152/20000], Training Loss: 0.4526\n",
            "Epoch [15153/20000], Training Loss: 0.4844\n",
            "Epoch [15154/20000], Training Loss: 0.4673\n",
            "Epoch [15155/20000], Training Loss: 0.4977\n",
            "Epoch [15156/20000], Training Loss: 0.4810\n",
            "Epoch [15157/20000], Training Loss: 0.5128\n",
            "Epoch [15158/20000], Training Loss: 0.5019\n",
            "Epoch [15159/20000], Training Loss: 0.4942\n",
            "Epoch [15160/20000], Training Loss: 0.4512\n",
            "Epoch [15161/20000], Training Loss: 0.4861\n",
            "Epoch [15162/20000], Training Loss: 0.5051\n",
            "Epoch [15163/20000], Training Loss: 0.5149\n",
            "Epoch [15164/20000], Training Loss: 0.4966\n",
            "Epoch [15165/20000], Training Loss: 0.5195\n",
            "Epoch [15166/20000], Training Loss: 0.5022\n",
            "Epoch [15167/20000], Training Loss: 0.4854\n",
            "Epoch [15168/20000], Training Loss: 0.4972\n",
            "Epoch [15169/20000], Training Loss: 0.4837\n",
            "Epoch [15170/20000], Training Loss: 0.5111\n",
            "Epoch [15171/20000], Training Loss: 0.5259\n",
            "Epoch [15172/20000], Training Loss: 0.5515\n",
            "Epoch [15173/20000], Training Loss: 0.5165\n",
            "Epoch [15174/20000], Training Loss: 0.4839\n",
            "Epoch [15175/20000], Training Loss: 0.5034\n",
            "Epoch [15176/20000], Training Loss: 0.5127\n",
            "Epoch [15177/20000], Training Loss: 0.4580\n",
            "Epoch [15178/20000], Training Loss: 0.4871\n",
            "Epoch [15179/20000], Training Loss: 0.4843\n",
            "Epoch [15180/20000], Training Loss: 0.5220\n",
            "Epoch [15181/20000], Training Loss: 0.5012\n",
            "Epoch [15182/20000], Training Loss: 0.5052\n",
            "Epoch [15183/20000], Training Loss: 0.4729\n",
            "Epoch [15184/20000], Training Loss: 0.5081\n",
            "Epoch [15185/20000], Training Loss: 0.4846\n",
            "Epoch [15186/20000], Training Loss: 0.5306\n",
            "Epoch [15187/20000], Training Loss: 0.5015\n",
            "Epoch [15188/20000], Training Loss: 0.4876\n",
            "Epoch [15189/20000], Training Loss: 0.4507\n",
            "Epoch [15190/20000], Training Loss: 0.4467\n",
            "Epoch [15191/20000], Training Loss: 0.5200\n",
            "Epoch [15192/20000], Training Loss: 0.4767\n",
            "Epoch [15193/20000], Training Loss: 0.4874\n",
            "Epoch [15194/20000], Training Loss: 0.4838\n",
            "Epoch [15195/20000], Training Loss: 0.4994\n",
            "Epoch [15196/20000], Training Loss: 0.5356\n",
            "Epoch [15197/20000], Training Loss: 0.4889\n",
            "Epoch [15198/20000], Training Loss: 0.4726\n",
            "Epoch [15199/20000], Training Loss: 0.4970\n",
            "Epoch [15200/20000], Training Loss: 0.4872\n",
            "Epoch [15201/20000], Training Loss: 0.4606\n",
            "Epoch [15202/20000], Training Loss: 0.4803\n",
            "Epoch [15203/20000], Training Loss: 0.4690\n",
            "Epoch [15204/20000], Training Loss: 0.4478\n",
            "Epoch [15205/20000], Training Loss: 0.5199\n",
            "Epoch [15206/20000], Training Loss: 0.4939\n",
            "Epoch [15207/20000], Training Loss: 0.4900\n",
            "Epoch [15208/20000], Training Loss: 0.4557\n",
            "Epoch [15209/20000], Training Loss: 0.4803\n",
            "Epoch [15210/20000], Training Loss: 0.5450\n",
            "Epoch [15211/20000], Training Loss: 0.5000\n",
            "Epoch [15212/20000], Training Loss: 0.4798\n",
            "Epoch [15213/20000], Training Loss: 0.5163\n",
            "Epoch [15214/20000], Training Loss: 0.4593\n",
            "Epoch [15215/20000], Training Loss: 0.4997\n",
            "Epoch [15216/20000], Training Loss: 0.4769\n",
            "Epoch [15217/20000], Training Loss: 0.5088\n",
            "Epoch [15218/20000], Training Loss: 0.4806\n",
            "Epoch [15219/20000], Training Loss: 0.4892\n",
            "Epoch [15220/20000], Training Loss: 0.4978\n",
            "Epoch [15221/20000], Training Loss: 0.4490\n",
            "Epoch [15222/20000], Training Loss: 0.4678\n",
            "Epoch [15223/20000], Training Loss: 0.5177\n",
            "Epoch [15224/20000], Training Loss: 0.5187\n",
            "Epoch [15225/20000], Training Loss: 0.4602\n",
            "Epoch [15226/20000], Training Loss: 0.5030\n",
            "Epoch [15227/20000], Training Loss: 0.4820\n",
            "Epoch [15228/20000], Training Loss: 0.4973\n",
            "Epoch [15229/20000], Training Loss: 0.5047\n",
            "Epoch [15230/20000], Training Loss: 0.4712\n",
            "Epoch [15231/20000], Training Loss: 0.4534\n",
            "Epoch [15232/20000], Training Loss: 0.4646\n",
            "Epoch [15233/20000], Training Loss: 0.4663\n",
            "Epoch [15234/20000], Training Loss: 0.4813\n",
            "Epoch [15235/20000], Training Loss: 0.5083\n",
            "Epoch [15236/20000], Training Loss: 0.5147\n",
            "Epoch [15237/20000], Training Loss: 0.5102\n",
            "Epoch [15238/20000], Training Loss: 0.5261\n",
            "Epoch [15239/20000], Training Loss: 0.5131\n",
            "Epoch [15240/20000], Training Loss: 0.4563\n",
            "Epoch [15241/20000], Training Loss: 0.4725\n",
            "Epoch [15242/20000], Training Loss: 0.4968\n",
            "Epoch [15243/20000], Training Loss: 0.4629\n",
            "Epoch [15244/20000], Training Loss: 0.5210\n",
            "Epoch [15245/20000], Training Loss: 0.5171\n",
            "Epoch [15246/20000], Training Loss: 0.4995\n",
            "Epoch [15247/20000], Training Loss: 0.5023\n",
            "Epoch [15248/20000], Training Loss: 0.4780\n",
            "Epoch [15249/20000], Training Loss: 0.4614\n",
            "Epoch [15250/20000], Training Loss: 0.5326\n",
            "Epoch [15251/20000], Training Loss: 0.4893\n",
            "Epoch [15252/20000], Training Loss: 0.4867\n",
            "Epoch [15253/20000], Training Loss: 0.4817\n",
            "Epoch [15254/20000], Training Loss: 0.4538\n",
            "Epoch [15255/20000], Training Loss: 0.4781\n",
            "Epoch [15256/20000], Training Loss: 0.4746\n",
            "Epoch [15257/20000], Training Loss: 0.5024\n",
            "Epoch [15258/20000], Training Loss: 0.4910\n",
            "Epoch [15259/20000], Training Loss: 0.4836\n",
            "Epoch [15260/20000], Training Loss: 0.5455\n",
            "Epoch [15261/20000], Training Loss: 0.4722\n",
            "Epoch [15262/20000], Training Loss: 0.5007\n",
            "Epoch [15263/20000], Training Loss: 0.4514\n",
            "Epoch [15264/20000], Training Loss: 0.4386\n",
            "Epoch [15265/20000], Training Loss: 0.4928\n",
            "Epoch [15266/20000], Training Loss: 0.5210\n",
            "Epoch [15267/20000], Training Loss: 0.5078\n",
            "Epoch [15268/20000], Training Loss: 0.4734\n",
            "Epoch [15269/20000], Training Loss: 0.4440\n",
            "Epoch [15270/20000], Training Loss: 0.4951\n",
            "Epoch [15271/20000], Training Loss: 0.5281\n",
            "Epoch [15272/20000], Training Loss: 0.4815\n",
            "Epoch [15273/20000], Training Loss: 0.4434\n",
            "Epoch [15274/20000], Training Loss: 0.4606\n",
            "Epoch [15275/20000], Training Loss: 0.4624\n",
            "Epoch [15276/20000], Training Loss: 0.5030\n",
            "Epoch [15277/20000], Training Loss: 0.4826\n",
            "Epoch [15278/20000], Training Loss: 0.5131\n",
            "Epoch [15279/20000], Training Loss: 0.4992\n",
            "Epoch [15280/20000], Training Loss: 0.4773\n",
            "Epoch [15281/20000], Training Loss: 0.5196\n",
            "Epoch [15282/20000], Training Loss: 0.4800\n",
            "Epoch [15283/20000], Training Loss: 0.5351\n",
            "Epoch [15284/20000], Training Loss: 0.5099\n",
            "Epoch [15285/20000], Training Loss: 0.4852\n",
            "Epoch [15286/20000], Training Loss: 0.4894\n",
            "Epoch [15287/20000], Training Loss: 0.5143\n",
            "Epoch [15288/20000], Training Loss: 0.4583\n",
            "Epoch [15289/20000], Training Loss: 0.4884\n",
            "Epoch [15290/20000], Training Loss: 0.4737\n",
            "Epoch [15291/20000], Training Loss: 0.5110\n",
            "Epoch [15292/20000], Training Loss: 0.4724\n",
            "Epoch [15293/20000], Training Loss: 0.4776\n",
            "Epoch [15294/20000], Training Loss: 0.5312\n",
            "Epoch [15295/20000], Training Loss: 0.5261\n",
            "Epoch [15296/20000], Training Loss: 0.4785\n",
            "Epoch [15297/20000], Training Loss: 0.4528\n",
            "Epoch [15298/20000], Training Loss: 0.5440\n",
            "Epoch [15299/20000], Training Loss: 0.5530\n",
            "Epoch [15300/20000], Training Loss: 0.4706\n",
            "Epoch [15301/20000], Training Loss: 0.5546\n",
            "Epoch [15302/20000], Training Loss: 0.4511\n",
            "Epoch [15303/20000], Training Loss: 0.5368\n",
            "Epoch [15304/20000], Training Loss: 0.4671\n",
            "Epoch [15305/20000], Training Loss: 0.5128\n",
            "Epoch [15306/20000], Training Loss: 0.4811\n",
            "Epoch [15307/20000], Training Loss: 0.5207\n",
            "Epoch [15308/20000], Training Loss: 0.4892\n",
            "Epoch [15309/20000], Training Loss: 0.4827\n",
            "Epoch [15310/20000], Training Loss: 0.4906\n",
            "Epoch [15311/20000], Training Loss: 0.5136\n",
            "Epoch [15312/20000], Training Loss: 0.4880\n",
            "Epoch [15313/20000], Training Loss: 0.4986\n",
            "Epoch [15314/20000], Training Loss: 0.4688\n",
            "Epoch [15315/20000], Training Loss: 0.4917\n",
            "Epoch [15316/20000], Training Loss: 0.4707\n",
            "Epoch [15317/20000], Training Loss: 0.5049\n",
            "Epoch [15318/20000], Training Loss: 0.4686\n",
            "Epoch [15319/20000], Training Loss: 0.5550\n",
            "Epoch [15320/20000], Training Loss: 0.4900\n",
            "Epoch [15321/20000], Training Loss: 0.4924\n",
            "Epoch [15322/20000], Training Loss: 0.5085\n",
            "Epoch [15323/20000], Training Loss: 0.5270\n",
            "Epoch [15324/20000], Training Loss: 0.4607\n",
            "Epoch [15325/20000], Training Loss: 0.4882\n",
            "Epoch [15326/20000], Training Loss: 0.5037\n",
            "Epoch [15327/20000], Training Loss: 0.4907\n",
            "Epoch [15328/20000], Training Loss: 0.5138\n",
            "Epoch [15329/20000], Training Loss: 0.5337\n",
            "Epoch [15330/20000], Training Loss: 0.5449\n",
            "Epoch [15331/20000], Training Loss: 0.5367\n",
            "Epoch [15332/20000], Training Loss: 0.4631\n",
            "Epoch [15333/20000], Training Loss: 0.4456\n",
            "Epoch [15334/20000], Training Loss: 0.4524\n",
            "Epoch [15335/20000], Training Loss: 0.4997\n",
            "Epoch [15336/20000], Training Loss: 0.5041\n",
            "Epoch [15337/20000], Training Loss: 0.4819\n",
            "Epoch [15338/20000], Training Loss: 0.5111\n",
            "Epoch [15339/20000], Training Loss: 0.4912\n",
            "Epoch [15340/20000], Training Loss: 0.4646\n",
            "Epoch [15341/20000], Training Loss: 0.4481\n",
            "Epoch [15342/20000], Training Loss: 0.4952\n",
            "Epoch [15343/20000], Training Loss: 0.4479\n",
            "Epoch [15344/20000], Training Loss: 0.5221\n",
            "Epoch [15345/20000], Training Loss: 0.5195\n",
            "Epoch [15346/20000], Training Loss: 0.5249\n",
            "Epoch [15347/20000], Training Loss: 0.4815\n",
            "Epoch [15348/20000], Training Loss: 0.4652\n",
            "Epoch [15349/20000], Training Loss: 0.4548\n",
            "Epoch [15350/20000], Training Loss: 0.5491\n",
            "Epoch [15351/20000], Training Loss: 0.4885\n",
            "Epoch [15352/20000], Training Loss: 0.4640\n",
            "Epoch [15353/20000], Training Loss: 0.4625\n",
            "Epoch [15354/20000], Training Loss: 0.4844\n",
            "Epoch [15355/20000], Training Loss: 0.5109\n",
            "Epoch [15356/20000], Training Loss: 0.5057\n",
            "Epoch [15357/20000], Training Loss: 0.5159\n",
            "Epoch [15358/20000], Training Loss: 0.4534\n",
            "Epoch [15359/20000], Training Loss: 0.5275\n",
            "Epoch [15360/20000], Training Loss: 0.5184\n",
            "Epoch [15361/20000], Training Loss: 0.4466\n",
            "Epoch [15362/20000], Training Loss: 0.4903\n",
            "Epoch [15363/20000], Training Loss: 0.4650\n",
            "Epoch [15364/20000], Training Loss: 0.5033\n",
            "Epoch [15365/20000], Training Loss: 0.4690\n",
            "Epoch [15366/20000], Training Loss: 0.5702\n",
            "Epoch [15367/20000], Training Loss: 0.4921\n",
            "Epoch [15368/20000], Training Loss: 0.5051\n",
            "Epoch [15369/20000], Training Loss: 0.4837\n",
            "Epoch [15370/20000], Training Loss: 0.5092\n",
            "Epoch [15371/20000], Training Loss: 0.4902\n",
            "Epoch [15372/20000], Training Loss: 0.4773\n",
            "Epoch [15373/20000], Training Loss: 0.4567\n",
            "Epoch [15374/20000], Training Loss: 0.4845\n",
            "Epoch [15375/20000], Training Loss: 0.5406\n",
            "Epoch [15376/20000], Training Loss: 0.4965\n",
            "Epoch [15377/20000], Training Loss: 0.4673\n",
            "Epoch [15378/20000], Training Loss: 0.5210\n",
            "Epoch [15379/20000], Training Loss: 0.5193\n",
            "Epoch [15380/20000], Training Loss: 0.4795\n",
            "Epoch [15381/20000], Training Loss: 0.4674\n",
            "Epoch [15382/20000], Training Loss: 0.4736\n",
            "Epoch [15383/20000], Training Loss: 0.5032\n",
            "Epoch [15384/20000], Training Loss: 0.5039\n",
            "Epoch [15385/20000], Training Loss: 0.4667\n",
            "Epoch [15386/20000], Training Loss: 0.4563\n",
            "Epoch [15387/20000], Training Loss: 0.5085\n",
            "Epoch [15388/20000], Training Loss: 0.5084\n",
            "Epoch [15389/20000], Training Loss: 0.4445\n",
            "Epoch [15390/20000], Training Loss: 0.4824\n",
            "Epoch [15391/20000], Training Loss: 0.4717\n",
            "Epoch [15392/20000], Training Loss: 0.5014\n",
            "Epoch [15393/20000], Training Loss: 0.5001\n",
            "Epoch [15394/20000], Training Loss: 0.5324\n",
            "Epoch [15395/20000], Training Loss: 0.4735\n",
            "Epoch [15396/20000], Training Loss: 0.4894\n",
            "Epoch [15397/20000], Training Loss: 0.4607\n",
            "Epoch [15398/20000], Training Loss: 0.4966\n",
            "Epoch [15399/20000], Training Loss: 0.4948\n",
            "Epoch [15400/20000], Training Loss: 0.4930\n",
            "Epoch [15401/20000], Training Loss: 0.5412\n",
            "Epoch [15402/20000], Training Loss: 0.4973\n",
            "Epoch [15403/20000], Training Loss: 0.4694\n",
            "Epoch [15404/20000], Training Loss: 0.4786\n",
            "Epoch [15405/20000], Training Loss: 0.5078\n",
            "Epoch [15406/20000], Training Loss: 0.4997\n",
            "Epoch [15407/20000], Training Loss: 0.5386\n",
            "Epoch [15408/20000], Training Loss: 0.5427\n",
            "Epoch [15409/20000], Training Loss: 0.4869\n",
            "Epoch [15410/20000], Training Loss: 0.4961\n",
            "Epoch [15411/20000], Training Loss: 0.4589\n",
            "Epoch [15412/20000], Training Loss: 0.4753\n",
            "Epoch [15413/20000], Training Loss: 0.5322\n",
            "Epoch [15414/20000], Training Loss: 0.4605\n",
            "Epoch [15415/20000], Training Loss: 0.5400\n",
            "Epoch [15416/20000], Training Loss: 0.4876\n",
            "Epoch [15417/20000], Training Loss: 0.4660\n",
            "Epoch [15418/20000], Training Loss: 0.4814\n",
            "Epoch [15419/20000], Training Loss: 0.4509\n",
            "Epoch [15420/20000], Training Loss: 0.4716\n",
            "Epoch [15421/20000], Training Loss: 0.5250\n",
            "Epoch [15422/20000], Training Loss: 0.4689\n",
            "Epoch [15423/20000], Training Loss: 0.4779\n",
            "Epoch [15424/20000], Training Loss: 0.4943\n",
            "Epoch [15425/20000], Training Loss: 0.4887\n",
            "Epoch [15426/20000], Training Loss: 0.4893\n",
            "Epoch [15427/20000], Training Loss: 0.4765\n",
            "Epoch [15428/20000], Training Loss: 0.4525\n",
            "Epoch [15429/20000], Training Loss: 0.4689\n",
            "Epoch [15430/20000], Training Loss: 0.4692\n",
            "Epoch [15431/20000], Training Loss: 0.4795\n",
            "Epoch [15432/20000], Training Loss: 0.4888\n",
            "Epoch [15433/20000], Training Loss: 0.5526\n",
            "Epoch [15434/20000], Training Loss: 0.4903\n",
            "Epoch [15435/20000], Training Loss: 0.5121\n",
            "Epoch [15436/20000], Training Loss: 0.5050\n",
            "Epoch [15437/20000], Training Loss: 0.4577\n",
            "Epoch [15438/20000], Training Loss: 0.5031\n",
            "Epoch [15439/20000], Training Loss: 0.4542\n",
            "Epoch [15440/20000], Training Loss: 0.5054\n",
            "Epoch [15441/20000], Training Loss: 0.4571\n",
            "Epoch [15442/20000], Training Loss: 0.4520\n",
            "Epoch [15443/20000], Training Loss: 0.5323\n",
            "Epoch [15444/20000], Training Loss: 0.4951\n",
            "Epoch [15445/20000], Training Loss: 0.4447\n",
            "Epoch [15446/20000], Training Loss: 0.4933\n",
            "Epoch [15447/20000], Training Loss: 0.4474\n",
            "Epoch [15448/20000], Training Loss: 0.4821\n",
            "Epoch [15449/20000], Training Loss: 0.4923\n",
            "Epoch [15450/20000], Training Loss: 0.4532\n",
            "Epoch [15451/20000], Training Loss: 0.4584\n",
            "Epoch [15452/20000], Training Loss: 0.4907\n",
            "Epoch [15453/20000], Training Loss: 0.5178\n",
            "Epoch [15454/20000], Training Loss: 0.5081\n",
            "Epoch [15455/20000], Training Loss: 0.4733\n",
            "Epoch [15456/20000], Training Loss: 0.4526\n",
            "Epoch [15457/20000], Training Loss: 0.4911\n",
            "Epoch [15458/20000], Training Loss: 0.4823\n",
            "Epoch [15459/20000], Training Loss: 0.4862\n",
            "Epoch [15460/20000], Training Loss: 0.4531\n",
            "Epoch [15461/20000], Training Loss: 0.4645\n",
            "Epoch [15462/20000], Training Loss: 0.4950\n",
            "Epoch [15463/20000], Training Loss: 0.4969\n",
            "Epoch [15464/20000], Training Loss: 0.5186\n",
            "Epoch [15465/20000], Training Loss: 0.5354\n",
            "Epoch [15466/20000], Training Loss: 0.4541\n",
            "Epoch [15467/20000], Training Loss: 0.4732\n",
            "Epoch [15468/20000], Training Loss: 0.4800\n",
            "Epoch [15469/20000], Training Loss: 0.5419\n",
            "Epoch [15470/20000], Training Loss: 0.4781\n",
            "Epoch [15471/20000], Training Loss: 0.4988\n",
            "Epoch [15472/20000], Training Loss: 0.5473\n",
            "Epoch [15473/20000], Training Loss: 0.5508\n",
            "Epoch [15474/20000], Training Loss: 0.4827\n",
            "Epoch [15475/20000], Training Loss: 0.5514\n",
            "Epoch [15476/20000], Training Loss: 0.4940\n",
            "Epoch [15477/20000], Training Loss: 0.4863\n",
            "Epoch [15478/20000], Training Loss: 0.4972\n",
            "Epoch [15479/20000], Training Loss: 0.4885\n",
            "Epoch [15480/20000], Training Loss: 0.4902\n",
            "Epoch [15481/20000], Training Loss: 0.4992\n",
            "Epoch [15482/20000], Training Loss: 0.4828\n",
            "Epoch [15483/20000], Training Loss: 0.4680\n",
            "Epoch [15484/20000], Training Loss: 0.4739\n",
            "Epoch [15485/20000], Training Loss: 0.5300\n",
            "Epoch [15486/20000], Training Loss: 0.5020\n",
            "Epoch [15487/20000], Training Loss: 0.4820\n",
            "Epoch [15488/20000], Training Loss: 0.5093\n",
            "Epoch [15489/20000], Training Loss: 0.4909\n",
            "Epoch [15490/20000], Training Loss: 0.4940\n",
            "Epoch [15491/20000], Training Loss: 0.4763\n",
            "Epoch [15492/20000], Training Loss: 0.4851\n",
            "Epoch [15493/20000], Training Loss: 0.4809\n",
            "Epoch [15494/20000], Training Loss: 0.4765\n",
            "Epoch [15495/20000], Training Loss: 0.5049\n",
            "Epoch [15496/20000], Training Loss: 0.5129\n",
            "Epoch [15497/20000], Training Loss: 0.4811\n",
            "Epoch [15498/20000], Training Loss: 0.4773\n",
            "Epoch [15499/20000], Training Loss: 0.5077\n",
            "Epoch [15500/20000], Training Loss: 0.4784\n",
            "Epoch [15501/20000], Training Loss: 0.4931\n",
            "Epoch [15502/20000], Training Loss: 0.4934\n",
            "Epoch [15503/20000], Training Loss: 0.4692\n",
            "Epoch [15504/20000], Training Loss: 0.5097\n",
            "Epoch [15505/20000], Training Loss: 0.4966\n",
            "Epoch [15506/20000], Training Loss: 0.4942\n",
            "Epoch [15507/20000], Training Loss: 0.4629\n",
            "Epoch [15508/20000], Training Loss: 0.5069\n",
            "Epoch [15509/20000], Training Loss: 0.4794\n",
            "Epoch [15510/20000], Training Loss: 0.4780\n",
            "Epoch [15511/20000], Training Loss: 0.4612\n",
            "Epoch [15512/20000], Training Loss: 0.4548\n",
            "Epoch [15513/20000], Training Loss: 0.4924\n",
            "Epoch [15514/20000], Training Loss: 0.4934\n",
            "Epoch [15515/20000], Training Loss: 0.4862\n",
            "Epoch [15516/20000], Training Loss: 0.4750\n",
            "Epoch [15517/20000], Training Loss: 0.5403\n",
            "Epoch [15518/20000], Training Loss: 0.4599\n",
            "Epoch [15519/20000], Training Loss: 0.4729\n",
            "Epoch [15520/20000], Training Loss: 0.4806\n",
            "Epoch [15521/20000], Training Loss: 0.5009\n",
            "Epoch [15522/20000], Training Loss: 0.5014\n",
            "Epoch [15523/20000], Training Loss: 0.4880\n",
            "Epoch [15524/20000], Training Loss: 0.4439\n",
            "Epoch [15525/20000], Training Loss: 0.5011\n",
            "Epoch [15526/20000], Training Loss: 0.5271\n",
            "Epoch [15527/20000], Training Loss: 0.4718\n",
            "Epoch [15528/20000], Training Loss: 0.5251\n",
            "Epoch [15529/20000], Training Loss: 0.5343\n",
            "Epoch [15530/20000], Training Loss: 0.4520\n",
            "Epoch [15531/20000], Training Loss: 0.4977\n",
            "Epoch [15532/20000], Training Loss: 0.5069\n",
            "Epoch [15533/20000], Training Loss: 0.5202\n",
            "Epoch [15534/20000], Training Loss: 0.4736\n",
            "Epoch [15535/20000], Training Loss: 0.4914\n",
            "Epoch [15536/20000], Training Loss: 0.5025\n",
            "Epoch [15537/20000], Training Loss: 0.4957\n",
            "Epoch [15538/20000], Training Loss: 0.5072\n",
            "Epoch [15539/20000], Training Loss: 0.4802\n",
            "Epoch [15540/20000], Training Loss: 0.4565\n",
            "Epoch [15541/20000], Training Loss: 0.4524\n",
            "Epoch [15542/20000], Training Loss: 0.4350\n",
            "Epoch [15543/20000], Training Loss: 0.4578\n",
            "Epoch [15544/20000], Training Loss: 0.5044\n",
            "Epoch [15545/20000], Training Loss: 0.5193\n",
            "Epoch [15546/20000], Training Loss: 0.5270\n",
            "Epoch [15547/20000], Training Loss: 0.4528\n",
            "Epoch [15548/20000], Training Loss: 0.4873\n",
            "Epoch [15549/20000], Training Loss: 0.4467\n",
            "Epoch [15550/20000], Training Loss: 0.4423\n",
            "Epoch [15551/20000], Training Loss: 0.4763\n",
            "Epoch [15552/20000], Training Loss: 0.4880\n",
            "Epoch [15553/20000], Training Loss: 0.4828\n",
            "Epoch [15554/20000], Training Loss: 0.4597\n",
            "Epoch [15555/20000], Training Loss: 0.5135\n",
            "Epoch [15556/20000], Training Loss: 0.4640\n",
            "Epoch [15557/20000], Training Loss: 0.5271\n",
            "Epoch [15558/20000], Training Loss: 0.5243\n",
            "Epoch [15559/20000], Training Loss: 0.4813\n",
            "Epoch [15560/20000], Training Loss: 0.4887\n",
            "Epoch [15561/20000], Training Loss: 0.4849\n",
            "Epoch [15562/20000], Training Loss: 0.4910\n",
            "Epoch [15563/20000], Training Loss: 0.4950\n",
            "Epoch [15564/20000], Training Loss: 0.4870\n",
            "Epoch [15565/20000], Training Loss: 0.4763\n",
            "Epoch [15566/20000], Training Loss: 0.4832\n",
            "Epoch [15567/20000], Training Loss: 0.4694\n",
            "Epoch [15568/20000], Training Loss: 0.5124\n",
            "Epoch [15569/20000], Training Loss: 0.4844\n",
            "Epoch [15570/20000], Training Loss: 0.4981\n",
            "Epoch [15571/20000], Training Loss: 0.4710\n",
            "Epoch [15572/20000], Training Loss: 0.5264\n",
            "Epoch [15573/20000], Training Loss: 0.5315\n",
            "Epoch [15574/20000], Training Loss: 0.4617\n",
            "Epoch [15575/20000], Training Loss: 0.4586\n",
            "Epoch [15576/20000], Training Loss: 0.5305\n",
            "Epoch [15577/20000], Training Loss: 0.5410\n",
            "Epoch [15578/20000], Training Loss: 0.4813\n",
            "Epoch [15579/20000], Training Loss: 0.4831\n",
            "Epoch [15580/20000], Training Loss: 0.5110\n",
            "Epoch [15581/20000], Training Loss: 0.5085\n",
            "Epoch [15582/20000], Training Loss: 0.4711\n",
            "Epoch [15583/20000], Training Loss: 0.4873\n",
            "Epoch [15584/20000], Training Loss: 0.4879\n",
            "Epoch [15585/20000], Training Loss: 0.4721\n",
            "Epoch [15586/20000], Training Loss: 0.5094\n",
            "Epoch [15587/20000], Training Loss: 0.5080\n",
            "Epoch [15588/20000], Training Loss: 0.5005\n",
            "Epoch [15589/20000], Training Loss: 0.4996\n",
            "Epoch [15590/20000], Training Loss: 0.4521\n",
            "Epoch [15591/20000], Training Loss: 0.5142\n",
            "Epoch [15592/20000], Training Loss: 0.5076\n",
            "Epoch [15593/20000], Training Loss: 0.4928\n",
            "Epoch [15594/20000], Training Loss: 0.4763\n",
            "Epoch [15595/20000], Training Loss: 0.4834\n",
            "Epoch [15596/20000], Training Loss: 0.5304\n",
            "Epoch [15597/20000], Training Loss: 0.4907\n",
            "Epoch [15598/20000], Training Loss: 0.5073\n",
            "Epoch [15599/20000], Training Loss: 0.4829\n",
            "Epoch [15600/20000], Training Loss: 0.4725\n",
            "Epoch [15601/20000], Training Loss: 0.5517\n",
            "Epoch [15602/20000], Training Loss: 0.4986\n",
            "Epoch [15603/20000], Training Loss: 0.4742\n",
            "Epoch [15604/20000], Training Loss: 0.5161\n",
            "Epoch [15605/20000], Training Loss: 0.4995\n",
            "Epoch [15606/20000], Training Loss: 0.4665\n",
            "Epoch [15607/20000], Training Loss: 0.5215\n",
            "Epoch [15608/20000], Training Loss: 0.5382\n",
            "Epoch [15609/20000], Training Loss: 0.4545\n",
            "Epoch [15610/20000], Training Loss: 0.4560\n",
            "Epoch [15611/20000], Training Loss: 0.5156\n",
            "Epoch [15612/20000], Training Loss: 0.4767\n",
            "Epoch [15613/20000], Training Loss: 0.5277\n",
            "Epoch [15614/20000], Training Loss: 0.4660\n",
            "Epoch [15615/20000], Training Loss: 0.4747\n",
            "Epoch [15616/20000], Training Loss: 0.4899\n",
            "Epoch [15617/20000], Training Loss: 0.5207\n",
            "Epoch [15618/20000], Training Loss: 0.5090\n",
            "Epoch [15619/20000], Training Loss: 0.4952\n",
            "Epoch [15620/20000], Training Loss: 0.5427\n",
            "Epoch [15621/20000], Training Loss: 0.4961\n",
            "Epoch [15622/20000], Training Loss: 0.4784\n",
            "Epoch [15623/20000], Training Loss: 0.4975\n",
            "Epoch [15624/20000], Training Loss: 0.4857\n",
            "Epoch [15625/20000], Training Loss: 0.4620\n",
            "Epoch [15626/20000], Training Loss: 0.4728\n",
            "Epoch [15627/20000], Training Loss: 0.4909\n",
            "Epoch [15628/20000], Training Loss: 0.5134\n",
            "Epoch [15629/20000], Training Loss: 0.4884\n",
            "Epoch [15630/20000], Training Loss: 0.4970\n",
            "Epoch [15631/20000], Training Loss: 0.4610\n",
            "Epoch [15632/20000], Training Loss: 0.4494\n",
            "Epoch [15633/20000], Training Loss: 0.5189\n",
            "Epoch [15634/20000], Training Loss: 0.4538\n",
            "Epoch [15635/20000], Training Loss: 0.4913\n",
            "Epoch [15636/20000], Training Loss: 0.4773\n",
            "Epoch [15637/20000], Training Loss: 0.5331\n",
            "Epoch [15638/20000], Training Loss: 0.4755\n",
            "Epoch [15639/20000], Training Loss: 0.4761\n",
            "Epoch [15640/20000], Training Loss: 0.4638\n",
            "Epoch [15641/20000], Training Loss: 0.4785\n",
            "Epoch [15642/20000], Training Loss: 0.4344\n",
            "Epoch [15643/20000], Training Loss: 0.4795\n",
            "Epoch [15644/20000], Training Loss: 0.4615\n",
            "Epoch [15645/20000], Training Loss: 0.4990\n",
            "Epoch [15646/20000], Training Loss: 0.4986\n",
            "Epoch [15647/20000], Training Loss: 0.4840\n",
            "Epoch [15648/20000], Training Loss: 0.5099\n",
            "Epoch [15649/20000], Training Loss: 0.4817\n",
            "Epoch [15650/20000], Training Loss: 0.4604\n",
            "Epoch [15651/20000], Training Loss: 0.4817\n",
            "Epoch [15652/20000], Training Loss: 0.4878\n",
            "Epoch [15653/20000], Training Loss: 0.4945\n",
            "Epoch [15654/20000], Training Loss: 0.5025\n",
            "Epoch [15655/20000], Training Loss: 0.4889\n",
            "Epoch [15656/20000], Training Loss: 0.4735\n",
            "Epoch [15657/20000], Training Loss: 0.4649\n",
            "Epoch [15658/20000], Training Loss: 0.4656\n",
            "Epoch [15659/20000], Training Loss: 0.5053\n",
            "Epoch [15660/20000], Training Loss: 0.5233\n",
            "Epoch [15661/20000], Training Loss: 0.4613\n",
            "Epoch [15662/20000], Training Loss: 0.5357\n",
            "Epoch [15663/20000], Training Loss: 0.5052\n",
            "Epoch [15664/20000], Training Loss: 0.4611\n",
            "Epoch [15665/20000], Training Loss: 0.4928\n",
            "Epoch [15666/20000], Training Loss: 0.4946\n",
            "Epoch [15667/20000], Training Loss: 0.4677\n",
            "Epoch [15668/20000], Training Loss: 0.4987\n",
            "Epoch [15669/20000], Training Loss: 0.5112\n",
            "Epoch [15670/20000], Training Loss: 0.4727\n",
            "Epoch [15671/20000], Training Loss: 0.4901\n",
            "Epoch [15672/20000], Training Loss: 0.4857\n",
            "Epoch [15673/20000], Training Loss: 0.4705\n",
            "Epoch [15674/20000], Training Loss: 0.5163\n",
            "Epoch [15675/20000], Training Loss: 0.5025\n",
            "Epoch [15676/20000], Training Loss: 0.5186\n",
            "Epoch [15677/20000], Training Loss: 0.5310\n",
            "Epoch [15678/20000], Training Loss: 0.4560\n",
            "Epoch [15679/20000], Training Loss: 0.4877\n",
            "Epoch [15680/20000], Training Loss: 0.5327\n",
            "Epoch [15681/20000], Training Loss: 0.4715\n",
            "Epoch [15682/20000], Training Loss: 0.4794\n",
            "Epoch [15683/20000], Training Loss: 0.4945\n",
            "Epoch [15684/20000], Training Loss: 0.4685\n",
            "Epoch [15685/20000], Training Loss: 0.5026\n",
            "Epoch [15686/20000], Training Loss: 0.5156\n",
            "Epoch [15687/20000], Training Loss: 0.4531\n",
            "Epoch [15688/20000], Training Loss: 0.4778\n",
            "Epoch [15689/20000], Training Loss: 0.4699\n",
            "Epoch [15690/20000], Training Loss: 0.5192\n",
            "Epoch [15691/20000], Training Loss: 0.4587\n",
            "Epoch [15692/20000], Training Loss: 0.4930\n",
            "Epoch [15693/20000], Training Loss: 0.4804\n",
            "Epoch [15694/20000], Training Loss: 0.5083\n",
            "Epoch [15695/20000], Training Loss: 0.4355\n",
            "Epoch [15696/20000], Training Loss: 0.4356\n",
            "Epoch [15697/20000], Training Loss: 0.4978\n",
            "Epoch [15698/20000], Training Loss: 0.5005\n",
            "Epoch [15699/20000], Training Loss: 0.4735\n",
            "Epoch [15700/20000], Training Loss: 0.5334\n",
            "Epoch [15701/20000], Training Loss: 0.5067\n",
            "Epoch [15702/20000], Training Loss: 0.5098\n",
            "Epoch [15703/20000], Training Loss: 0.4934\n",
            "Epoch [15704/20000], Training Loss: 0.5115\n",
            "Epoch [15705/20000], Training Loss: 0.4722\n",
            "Epoch [15706/20000], Training Loss: 0.4712\n",
            "Epoch [15707/20000], Training Loss: 0.4703\n",
            "Epoch [15708/20000], Training Loss: 0.4349\n",
            "Epoch [15709/20000], Training Loss: 0.4429\n",
            "Epoch [15710/20000], Training Loss: 0.5147\n",
            "Epoch [15711/20000], Training Loss: 0.4870\n",
            "Epoch [15712/20000], Training Loss: 0.4920\n",
            "Epoch [15713/20000], Training Loss: 0.5535\n",
            "Epoch [15714/20000], Training Loss: 0.5206\n",
            "Epoch [15715/20000], Training Loss: 0.4911\n",
            "Epoch [15716/20000], Training Loss: 0.4998\n",
            "Epoch [15717/20000], Training Loss: 0.4906\n",
            "Epoch [15718/20000], Training Loss: 0.4790\n",
            "Epoch [15719/20000], Training Loss: 0.4886\n",
            "Epoch [15720/20000], Training Loss: 0.5098\n",
            "Epoch [15721/20000], Training Loss: 0.4588\n",
            "Epoch [15722/20000], Training Loss: 0.4856\n",
            "Epoch [15723/20000], Training Loss: 0.4728\n",
            "Epoch [15724/20000], Training Loss: 0.5099\n",
            "Epoch [15725/20000], Training Loss: 0.4581\n",
            "Epoch [15726/20000], Training Loss: 0.5327\n",
            "Epoch [15727/20000], Training Loss: 0.4893\n",
            "Epoch [15728/20000], Training Loss: 0.5212\n",
            "Epoch [15729/20000], Training Loss: 0.4754\n",
            "Epoch [15730/20000], Training Loss: 0.4814\n",
            "Epoch [15731/20000], Training Loss: 0.4785\n",
            "Epoch [15732/20000], Training Loss: 0.4495\n",
            "Epoch [15733/20000], Training Loss: 0.5222\n",
            "Epoch [15734/20000], Training Loss: 0.5016\n",
            "Epoch [15735/20000], Training Loss: 0.5405\n",
            "Epoch [15736/20000], Training Loss: 0.4790\n",
            "Epoch [15737/20000], Training Loss: 0.5118\n",
            "Epoch [15738/20000], Training Loss: 0.4726\n",
            "Epoch [15739/20000], Training Loss: 0.4992\n",
            "Epoch [15740/20000], Training Loss: 0.5245\n",
            "Epoch [15741/20000], Training Loss: 0.5565\n",
            "Epoch [15742/20000], Training Loss: 0.4967\n",
            "Epoch [15743/20000], Training Loss: 0.5063\n",
            "Epoch [15744/20000], Training Loss: 0.4809\n",
            "Epoch [15745/20000], Training Loss: 0.5228\n",
            "Epoch [15746/20000], Training Loss: 0.4996\n",
            "Epoch [15747/20000], Training Loss: 0.4580\n",
            "Epoch [15748/20000], Training Loss: 0.4905\n",
            "Epoch [15749/20000], Training Loss: 0.4957\n",
            "Epoch [15750/20000], Training Loss: 0.4644\n",
            "Epoch [15751/20000], Training Loss: 0.4921\n",
            "Epoch [15752/20000], Training Loss: 0.5115\n",
            "Epoch [15753/20000], Training Loss: 0.5068\n",
            "Epoch [15754/20000], Training Loss: 0.4576\n",
            "Epoch [15755/20000], Training Loss: 0.5383\n",
            "Epoch [15756/20000], Training Loss: 0.5064\n",
            "Epoch [15757/20000], Training Loss: 0.5309\n",
            "Epoch [15758/20000], Training Loss: 0.5007\n",
            "Epoch [15759/20000], Training Loss: 0.5212\n",
            "Epoch [15760/20000], Training Loss: 0.4686\n",
            "Epoch [15761/20000], Training Loss: 0.4914\n",
            "Epoch [15762/20000], Training Loss: 0.5176\n",
            "Epoch [15763/20000], Training Loss: 0.4641\n",
            "Epoch [15764/20000], Training Loss: 0.4535\n",
            "Epoch [15765/20000], Training Loss: 0.4412\n",
            "Epoch [15766/20000], Training Loss: 0.4830\n",
            "Epoch [15767/20000], Training Loss: 0.4932\n",
            "Epoch [15768/20000], Training Loss: 0.4476\n",
            "Epoch [15769/20000], Training Loss: 0.4882\n",
            "Epoch [15770/20000], Training Loss: 0.5185\n",
            "Epoch [15771/20000], Training Loss: 0.4878\n",
            "Epoch [15772/20000], Training Loss: 0.4611\n",
            "Epoch [15773/20000], Training Loss: 0.4922\n",
            "Epoch [15774/20000], Training Loss: 0.5419\n",
            "Epoch [15775/20000], Training Loss: 0.5035\n",
            "Epoch [15776/20000], Training Loss: 0.5058\n",
            "Epoch [15777/20000], Training Loss: 0.4768\n",
            "Epoch [15778/20000], Training Loss: 0.5063\n",
            "Epoch [15779/20000], Training Loss: 0.5121\n",
            "Epoch [15780/20000], Training Loss: 0.4927\n",
            "Epoch [15781/20000], Training Loss: 0.4647\n",
            "Epoch [15782/20000], Training Loss: 0.5241\n",
            "Epoch [15783/20000], Training Loss: 0.4793\n",
            "Epoch [15784/20000], Training Loss: 0.4923\n",
            "Epoch [15785/20000], Training Loss: 0.4573\n",
            "Epoch [15786/20000], Training Loss: 0.5361\n",
            "Epoch [15787/20000], Training Loss: 0.4995\n",
            "Epoch [15788/20000], Training Loss: 0.5049\n",
            "Epoch [15789/20000], Training Loss: 0.4785\n",
            "Epoch [15790/20000], Training Loss: 0.5338\n",
            "Epoch [15791/20000], Training Loss: 0.4676\n",
            "Epoch [15792/20000], Training Loss: 0.5291\n",
            "Epoch [15793/20000], Training Loss: 0.5003\n",
            "Epoch [15794/20000], Training Loss: 0.4908\n",
            "Epoch [15795/20000], Training Loss: 0.4587\n",
            "Epoch [15796/20000], Training Loss: 0.4514\n",
            "Epoch [15797/20000], Training Loss: 0.4735\n",
            "Epoch [15798/20000], Training Loss: 0.5325\n",
            "Epoch [15799/20000], Training Loss: 0.4901\n",
            "Epoch [15800/20000], Training Loss: 0.4925\n",
            "Epoch [15801/20000], Training Loss: 0.4855\n",
            "Epoch [15802/20000], Training Loss: 0.4889\n",
            "Epoch [15803/20000], Training Loss: 0.5005\n",
            "Epoch [15804/20000], Training Loss: 0.4633\n",
            "Epoch [15805/20000], Training Loss: 0.4896\n",
            "Epoch [15806/20000], Training Loss: 0.4773\n",
            "Epoch [15807/20000], Training Loss: 0.5112\n",
            "Epoch [15808/20000], Training Loss: 0.4760\n",
            "Epoch [15809/20000], Training Loss: 0.4954\n",
            "Epoch [15810/20000], Training Loss: 0.4375\n",
            "Epoch [15811/20000], Training Loss: 0.5226\n",
            "Epoch [15812/20000], Training Loss: 0.5003\n",
            "Epoch [15813/20000], Training Loss: 0.4810\n",
            "Epoch [15814/20000], Training Loss: 0.4517\n",
            "Epoch [15815/20000], Training Loss: 0.4576\n",
            "Epoch [15816/20000], Training Loss: 0.4660\n",
            "Epoch [15817/20000], Training Loss: 0.4934\n",
            "Epoch [15818/20000], Training Loss: 0.4606\n",
            "Epoch [15819/20000], Training Loss: 0.4401\n",
            "Epoch [15820/20000], Training Loss: 0.4792\n",
            "Epoch [15821/20000], Training Loss: 0.5249\n",
            "Epoch [15822/20000], Training Loss: 0.4852\n",
            "Epoch [15823/20000], Training Loss: 0.4659\n",
            "Epoch [15824/20000], Training Loss: 0.4763\n",
            "Epoch [15825/20000], Training Loss: 0.4799\n",
            "Epoch [15826/20000], Training Loss: 0.4802\n",
            "Epoch [15827/20000], Training Loss: 0.4559\n",
            "Epoch [15828/20000], Training Loss: 0.5130\n",
            "Epoch [15829/20000], Training Loss: 0.5062\n",
            "Epoch [15830/20000], Training Loss: 0.4507\n",
            "Epoch [15831/20000], Training Loss: 0.4984\n",
            "Epoch [15832/20000], Training Loss: 0.5177\n",
            "Epoch [15833/20000], Training Loss: 0.4645\n",
            "Epoch [15834/20000], Training Loss: 0.4952\n",
            "Epoch [15835/20000], Training Loss: 0.5141\n",
            "Epoch [15836/20000], Training Loss: 0.4724\n",
            "Epoch [15837/20000], Training Loss: 0.4853\n",
            "Epoch [15838/20000], Training Loss: 0.4907\n",
            "Epoch [15839/20000], Training Loss: 0.4610\n",
            "Epoch [15840/20000], Training Loss: 0.4912\n",
            "Epoch [15841/20000], Training Loss: 0.4927\n",
            "Epoch [15842/20000], Training Loss: 0.5122\n",
            "Epoch [15843/20000], Training Loss: 0.4690\n",
            "Epoch [15844/20000], Training Loss: 0.5096\n",
            "Epoch [15845/20000], Training Loss: 0.4824\n",
            "Epoch [15846/20000], Training Loss: 0.4813\n",
            "Epoch [15847/20000], Training Loss: 0.4436\n",
            "Epoch [15848/20000], Training Loss: 0.5007\n",
            "Epoch [15849/20000], Training Loss: 0.4537\n",
            "Epoch [15850/20000], Training Loss: 0.4507\n",
            "Epoch [15851/20000], Training Loss: 0.5479\n",
            "Epoch [15852/20000], Training Loss: 0.4279\n",
            "Epoch [15853/20000], Training Loss: 0.5098\n",
            "Epoch [15854/20000], Training Loss: 0.4713\n",
            "Epoch [15855/20000], Training Loss: 0.4809\n",
            "Epoch [15856/20000], Training Loss: 0.4685\n",
            "Epoch [15857/20000], Training Loss: 0.4750\n",
            "Epoch [15858/20000], Training Loss: 0.4722\n",
            "Epoch [15859/20000], Training Loss: 0.4648\n",
            "Epoch [15860/20000], Training Loss: 0.5038\n",
            "Epoch [15861/20000], Training Loss: 0.4678\n",
            "Epoch [15862/20000], Training Loss: 0.5059\n",
            "Epoch [15863/20000], Training Loss: 0.4999\n",
            "Epoch [15864/20000], Training Loss: 0.4384\n",
            "Epoch [15865/20000], Training Loss: 0.5332\n",
            "Epoch [15866/20000], Training Loss: 0.4889\n",
            "Epoch [15867/20000], Training Loss: 0.5125\n",
            "Epoch [15868/20000], Training Loss: 0.5270\n",
            "Epoch [15869/20000], Training Loss: 0.4379\n",
            "Epoch [15870/20000], Training Loss: 0.4956\n",
            "Epoch [15871/20000], Training Loss: 0.4959\n",
            "Epoch [15872/20000], Training Loss: 0.4348\n",
            "Epoch [15873/20000], Training Loss: 0.5159\n",
            "Epoch [15874/20000], Training Loss: 0.4674\n",
            "Epoch [15875/20000], Training Loss: 0.4607\n",
            "Epoch [15876/20000], Training Loss: 0.4769\n",
            "Epoch [15877/20000], Training Loss: 0.4340\n",
            "Epoch [15878/20000], Training Loss: 0.5260\n",
            "Epoch [15879/20000], Training Loss: 0.4703\n",
            "Epoch [15880/20000], Training Loss: 0.4878\n",
            "Epoch [15881/20000], Training Loss: 0.4938\n",
            "Epoch [15882/20000], Training Loss: 0.4970\n",
            "Epoch [15883/20000], Training Loss: 0.4952\n",
            "Epoch [15884/20000], Training Loss: 0.4702\n",
            "Epoch [15885/20000], Training Loss: 0.4962\n",
            "Epoch [15886/20000], Training Loss: 0.4882\n",
            "Epoch [15887/20000], Training Loss: 0.5009\n",
            "Epoch [15888/20000], Training Loss: 0.4946\n",
            "Epoch [15889/20000], Training Loss: 0.4895\n",
            "Epoch [15890/20000], Training Loss: 0.4538\n",
            "Epoch [15891/20000], Training Loss: 0.4827\n",
            "Epoch [15892/20000], Training Loss: 0.4601\n",
            "Epoch [15893/20000], Training Loss: 0.5417\n",
            "Epoch [15894/20000], Training Loss: 0.4559\n",
            "Epoch [15895/20000], Training Loss: 0.4668\n",
            "Epoch [15896/20000], Training Loss: 0.4674\n",
            "Epoch [15897/20000], Training Loss: 0.5011\n",
            "Epoch [15898/20000], Training Loss: 0.5103\n",
            "Epoch [15899/20000], Training Loss: 0.4896\n",
            "Epoch [15900/20000], Training Loss: 0.5360\n",
            "Epoch [15901/20000], Training Loss: 0.5057\n",
            "Epoch [15902/20000], Training Loss: 0.4771\n",
            "Epoch [15903/20000], Training Loss: 0.4984\n",
            "Epoch [15904/20000], Training Loss: 0.4838\n",
            "Epoch [15905/20000], Training Loss: 0.4813\n",
            "Epoch [15906/20000], Training Loss: 0.4756\n",
            "Epoch [15907/20000], Training Loss: 0.4783\n",
            "Epoch [15908/20000], Training Loss: 0.5061\n",
            "Epoch [15909/20000], Training Loss: 0.4946\n",
            "Epoch [15910/20000], Training Loss: 0.4794\n",
            "Epoch [15911/20000], Training Loss: 0.5402\n",
            "Epoch [15912/20000], Training Loss: 0.4527\n",
            "Epoch [15913/20000], Training Loss: 0.4782\n",
            "Epoch [15914/20000], Training Loss: 0.5000\n",
            "Epoch [15915/20000], Training Loss: 0.4513\n",
            "Epoch [15916/20000], Training Loss: 0.4753\n",
            "Epoch [15917/20000], Training Loss: 0.5078\n",
            "Epoch [15918/20000], Training Loss: 0.4409\n",
            "Epoch [15919/20000], Training Loss: 0.4969\n",
            "Epoch [15920/20000], Training Loss: 0.5353\n",
            "Epoch [15921/20000], Training Loss: 0.4848\n",
            "Epoch [15922/20000], Training Loss: 0.4946\n",
            "Epoch [15923/20000], Training Loss: 0.5234\n",
            "Epoch [15924/20000], Training Loss: 0.5071\n",
            "Epoch [15925/20000], Training Loss: 0.4648\n",
            "Epoch [15926/20000], Training Loss: 0.4746\n",
            "Epoch [15927/20000], Training Loss: 0.4718\n",
            "Epoch [15928/20000], Training Loss: 0.5234\n",
            "Epoch [15929/20000], Training Loss: 0.4616\n",
            "Epoch [15930/20000], Training Loss: 0.4785\n",
            "Epoch [15931/20000], Training Loss: 0.5032\n",
            "Epoch [15932/20000], Training Loss: 0.5139\n",
            "Epoch [15933/20000], Training Loss: 0.5071\n",
            "Epoch [15934/20000], Training Loss: 0.4475\n",
            "Epoch [15935/20000], Training Loss: 0.5579\n",
            "Epoch [15936/20000], Training Loss: 0.5450\n",
            "Epoch [15937/20000], Training Loss: 0.4894\n",
            "Epoch [15938/20000], Training Loss: 0.4915\n",
            "Epoch [15939/20000], Training Loss: 0.4849\n",
            "Epoch [15940/20000], Training Loss: 0.4831\n",
            "Epoch [15941/20000], Training Loss: 0.5036\n",
            "Epoch [15942/20000], Training Loss: 0.5021\n",
            "Epoch [15943/20000], Training Loss: 0.4496\n",
            "Epoch [15944/20000], Training Loss: 0.4940\n",
            "Epoch [15945/20000], Training Loss: 0.5373\n",
            "Epoch [15946/20000], Training Loss: 0.4590\n",
            "Epoch [15947/20000], Training Loss: 0.4584\n",
            "Epoch [15948/20000], Training Loss: 0.5257\n",
            "Epoch [15949/20000], Training Loss: 0.4964\n",
            "Epoch [15950/20000], Training Loss: 0.4581\n",
            "Epoch [15951/20000], Training Loss: 0.5103\n",
            "Epoch [15952/20000], Training Loss: 0.4854\n",
            "Epoch [15953/20000], Training Loss: 0.4545\n",
            "Epoch [15954/20000], Training Loss: 0.4970\n",
            "Epoch [15955/20000], Training Loss: 0.4925\n",
            "Epoch [15956/20000], Training Loss: 0.5623\n",
            "Epoch [15957/20000], Training Loss: 0.4923\n",
            "Epoch [15958/20000], Training Loss: 0.4760\n",
            "Epoch [15959/20000], Training Loss: 0.5266\n",
            "Epoch [15960/20000], Training Loss: 0.5216\n",
            "Epoch [15961/20000], Training Loss: 0.4925\n",
            "Epoch [15962/20000], Training Loss: 0.5031\n",
            "Epoch [15963/20000], Training Loss: 0.4744\n",
            "Epoch [15964/20000], Training Loss: 0.5102\n",
            "Epoch [15965/20000], Training Loss: 0.4806\n",
            "Epoch [15966/20000], Training Loss: 0.5108\n",
            "Epoch [15967/20000], Training Loss: 0.5116\n",
            "Epoch [15968/20000], Training Loss: 0.4796\n",
            "Epoch [15969/20000], Training Loss: 0.4953\n",
            "Epoch [15970/20000], Training Loss: 0.4773\n",
            "Epoch [15971/20000], Training Loss: 0.4797\n",
            "Epoch [15972/20000], Training Loss: 0.5136\n",
            "Epoch [15973/20000], Training Loss: 0.4974\n",
            "Epoch [15974/20000], Training Loss: 0.5008\n",
            "Epoch [15975/20000], Training Loss: 0.5155\n",
            "Epoch [15976/20000], Training Loss: 0.5292\n",
            "Epoch [15977/20000], Training Loss: 0.4923\n",
            "Epoch [15978/20000], Training Loss: 0.4406\n",
            "Epoch [15979/20000], Training Loss: 0.5140\n",
            "Epoch [15980/20000], Training Loss: 0.4702\n",
            "Epoch [15981/20000], Training Loss: 0.5104\n",
            "Epoch [15982/20000], Training Loss: 0.4843\n",
            "Epoch [15983/20000], Training Loss: 0.4679\n",
            "Epoch [15984/20000], Training Loss: 0.4705\n",
            "Epoch [15985/20000], Training Loss: 0.4535\n",
            "Epoch [15986/20000], Training Loss: 0.5273\n",
            "Epoch [15987/20000], Training Loss: 0.4958\n",
            "Epoch [15988/20000], Training Loss: 0.5023\n",
            "Epoch [15989/20000], Training Loss: 0.4400\n",
            "Epoch [15990/20000], Training Loss: 0.4764\n",
            "Epoch [15991/20000], Training Loss: 0.4756\n",
            "Epoch [15992/20000], Training Loss: 0.4839\n",
            "Epoch [15993/20000], Training Loss: 0.4721\n",
            "Epoch [15994/20000], Training Loss: 0.4495\n",
            "Epoch [15995/20000], Training Loss: 0.4761\n",
            "Epoch [15996/20000], Training Loss: 0.4743\n",
            "Epoch [15997/20000], Training Loss: 0.4485\n",
            "Epoch [15998/20000], Training Loss: 0.5123\n",
            "Epoch [15999/20000], Training Loss: 0.4402\n",
            "Epoch [16000/20000], Training Loss: 0.5013\n",
            "Epoch [16001/20000], Training Loss: 0.4480\n",
            "Epoch [16002/20000], Training Loss: 0.4835\n",
            "Epoch [16003/20000], Training Loss: 0.4564\n",
            "Epoch [16004/20000], Training Loss: 0.4585\n",
            "Epoch [16005/20000], Training Loss: 0.4754\n",
            "Epoch [16006/20000], Training Loss: 0.4601\n",
            "Epoch [16007/20000], Training Loss: 0.4754\n",
            "Epoch [16008/20000], Training Loss: 0.4879\n",
            "Epoch [16009/20000], Training Loss: 0.5041\n",
            "Epoch [16010/20000], Training Loss: 0.5039\n",
            "Epoch [16011/20000], Training Loss: 0.5224\n",
            "Epoch [16012/20000], Training Loss: 0.4927\n",
            "Epoch [16013/20000], Training Loss: 0.5132\n",
            "Epoch [16014/20000], Training Loss: 0.4836\n",
            "Epoch [16015/20000], Training Loss: 0.4621\n",
            "Epoch [16016/20000], Training Loss: 0.5069\n",
            "Epoch [16017/20000], Training Loss: 0.4791\n",
            "Epoch [16018/20000], Training Loss: 0.4851\n",
            "Epoch [16019/20000], Training Loss: 0.4700\n",
            "Epoch [16020/20000], Training Loss: 0.4702\n",
            "Epoch [16021/20000], Training Loss: 0.4875\n",
            "Epoch [16022/20000], Training Loss: 0.5040\n",
            "Epoch [16023/20000], Training Loss: 0.4986\n",
            "Epoch [16024/20000], Training Loss: 0.4445\n",
            "Epoch [16025/20000], Training Loss: 0.4862\n",
            "Epoch [16026/20000], Training Loss: 0.4531\n",
            "Epoch [16027/20000], Training Loss: 0.4870\n",
            "Epoch [16028/20000], Training Loss: 0.4820\n",
            "Epoch [16029/20000], Training Loss: 0.4544\n",
            "Epoch [16030/20000], Training Loss: 0.4764\n",
            "Epoch [16031/20000], Training Loss: 0.5064\n",
            "Epoch [16032/20000], Training Loss: 0.4780\n",
            "Epoch [16033/20000], Training Loss: 0.5016\n",
            "Epoch [16034/20000], Training Loss: 0.4750\n",
            "Epoch [16035/20000], Training Loss: 0.4861\n",
            "Epoch [16036/20000], Training Loss: 0.4835\n",
            "Epoch [16037/20000], Training Loss: 0.4986\n",
            "Epoch [16038/20000], Training Loss: 0.4739\n",
            "Epoch [16039/20000], Training Loss: 0.5131\n",
            "Epoch [16040/20000], Training Loss: 0.5139\n",
            "Epoch [16041/20000], Training Loss: 0.4782\n",
            "Epoch [16042/20000], Training Loss: 0.5265\n",
            "Epoch [16043/20000], Training Loss: 0.4838\n",
            "Epoch [16044/20000], Training Loss: 0.5370\n",
            "Epoch [16045/20000], Training Loss: 0.4577\n",
            "Epoch [16046/20000], Training Loss: 0.4537\n",
            "Epoch [16047/20000], Training Loss: 0.4769\n",
            "Epoch [16048/20000], Training Loss: 0.5309\n",
            "Epoch [16049/20000], Training Loss: 0.4732\n",
            "Epoch [16050/20000], Training Loss: 0.4995\n",
            "Epoch [16051/20000], Training Loss: 0.4405\n",
            "Epoch [16052/20000], Training Loss: 0.5260\n",
            "Epoch [16053/20000], Training Loss: 0.4930\n",
            "Epoch [16054/20000], Training Loss: 0.4963\n",
            "Epoch [16055/20000], Training Loss: 0.4562\n",
            "Epoch [16056/20000], Training Loss: 0.5131\n",
            "Epoch [16057/20000], Training Loss: 0.4805\n",
            "Epoch [16058/20000], Training Loss: 0.4820\n",
            "Epoch [16059/20000], Training Loss: 0.5529\n",
            "Epoch [16060/20000], Training Loss: 0.4684\n",
            "Epoch [16061/20000], Training Loss: 0.5298\n",
            "Epoch [16062/20000], Training Loss: 0.4860\n",
            "Epoch [16063/20000], Training Loss: 0.5088\n",
            "Epoch [16064/20000], Training Loss: 0.4949\n",
            "Epoch [16065/20000], Training Loss: 0.4744\n",
            "Epoch [16066/20000], Training Loss: 0.5329\n",
            "Epoch [16067/20000], Training Loss: 0.4494\n",
            "Epoch [16068/20000], Training Loss: 0.4595\n",
            "Epoch [16069/20000], Training Loss: 0.4478\n",
            "Epoch [16070/20000], Training Loss: 0.4570\n",
            "Epoch [16071/20000], Training Loss: 0.5246\n",
            "Epoch [16072/20000], Training Loss: 0.4764\n",
            "Epoch [16073/20000], Training Loss: 0.5345\n",
            "Epoch [16074/20000], Training Loss: 0.5017\n",
            "Epoch [16075/20000], Training Loss: 0.4962\n",
            "Epoch [16076/20000], Training Loss: 0.5345\n",
            "Epoch [16077/20000], Training Loss: 0.4884\n",
            "Epoch [16078/20000], Training Loss: 0.4704\n",
            "Epoch [16079/20000], Training Loss: 0.4941\n",
            "Epoch [16080/20000], Training Loss: 0.5496\n",
            "Epoch [16081/20000], Training Loss: 0.4894\n",
            "Epoch [16082/20000], Training Loss: 0.5206\n",
            "Epoch [16083/20000], Training Loss: 0.5082\n",
            "Epoch [16084/20000], Training Loss: 0.5181\n",
            "Epoch [16085/20000], Training Loss: 0.5115\n",
            "Epoch [16086/20000], Training Loss: 0.5267\n",
            "Epoch [16087/20000], Training Loss: 0.4820\n",
            "Epoch [16088/20000], Training Loss: 0.4801\n",
            "Epoch [16089/20000], Training Loss: 0.4795\n",
            "Epoch [16090/20000], Training Loss: 0.5012\n",
            "Epoch [16091/20000], Training Loss: 0.4585\n",
            "Epoch [16092/20000], Training Loss: 0.4930\n",
            "Epoch [16093/20000], Training Loss: 0.4984\n",
            "Epoch [16094/20000], Training Loss: 0.4289\n",
            "Epoch [16095/20000], Training Loss: 0.4905\n",
            "Epoch [16096/20000], Training Loss: 0.4851\n",
            "Epoch [16097/20000], Training Loss: 0.4561\n",
            "Epoch [16098/20000], Training Loss: 0.4927\n",
            "Epoch [16099/20000], Training Loss: 0.4879\n",
            "Epoch [16100/20000], Training Loss: 0.5325\n",
            "Epoch [16101/20000], Training Loss: 0.5079\n",
            "Epoch [16102/20000], Training Loss: 0.4843\n",
            "Epoch [16103/20000], Training Loss: 0.4486\n",
            "Epoch [16104/20000], Training Loss: 0.4886\n",
            "Epoch [16105/20000], Training Loss: 0.4975\n",
            "Epoch [16106/20000], Training Loss: 0.4334\n",
            "Epoch [16107/20000], Training Loss: 0.4852\n",
            "Epoch [16108/20000], Training Loss: 0.4920\n",
            "Epoch [16109/20000], Training Loss: 0.4729\n",
            "Epoch [16110/20000], Training Loss: 0.5258\n",
            "Epoch [16111/20000], Training Loss: 0.5031\n",
            "Epoch [16112/20000], Training Loss: 0.4678\n",
            "Epoch [16113/20000], Training Loss: 0.4974\n",
            "Epoch [16114/20000], Training Loss: 0.4623\n",
            "Epoch [16115/20000], Training Loss: 0.4712\n",
            "Epoch [16116/20000], Training Loss: 0.4786\n",
            "Epoch [16117/20000], Training Loss: 0.4511\n",
            "Epoch [16118/20000], Training Loss: 0.4622\n",
            "Epoch [16119/20000], Training Loss: 0.5154\n",
            "Epoch [16120/20000], Training Loss: 0.4752\n",
            "Epoch [16121/20000], Training Loss: 0.4873\n",
            "Epoch [16122/20000], Training Loss: 0.4846\n",
            "Epoch [16123/20000], Training Loss: 0.4743\n",
            "Epoch [16124/20000], Training Loss: 0.4595\n",
            "Epoch [16125/20000], Training Loss: 0.5074\n",
            "Epoch [16126/20000], Training Loss: 0.4752\n",
            "Epoch [16127/20000], Training Loss: 0.4611\n",
            "Epoch [16128/20000], Training Loss: 0.5140\n",
            "Epoch [16129/20000], Training Loss: 0.4736\n",
            "Epoch [16130/20000], Training Loss: 0.5353\n",
            "Epoch [16131/20000], Training Loss: 0.4747\n",
            "Epoch [16132/20000], Training Loss: 0.5173\n",
            "Epoch [16133/20000], Training Loss: 0.4874\n",
            "Epoch [16134/20000], Training Loss: 0.4751\n",
            "Epoch [16135/20000], Training Loss: 0.5116\n",
            "Epoch [16136/20000], Training Loss: 0.4817\n",
            "Epoch [16137/20000], Training Loss: 0.5072\n",
            "Epoch [16138/20000], Training Loss: 0.4477\n",
            "Epoch [16139/20000], Training Loss: 0.4787\n",
            "Epoch [16140/20000], Training Loss: 0.4418\n",
            "Epoch [16141/20000], Training Loss: 0.5046\n",
            "Epoch [16142/20000], Training Loss: 0.4933\n",
            "Epoch [16143/20000], Training Loss: 0.5328\n",
            "Epoch [16144/20000], Training Loss: 0.5198\n",
            "Epoch [16145/20000], Training Loss: 0.4962\n",
            "Epoch [16146/20000], Training Loss: 0.5301\n",
            "Epoch [16147/20000], Training Loss: 0.4731\n",
            "Epoch [16148/20000], Training Loss: 0.5050\n",
            "Epoch [16149/20000], Training Loss: 0.5084\n",
            "Epoch [16150/20000], Training Loss: 0.4922\n",
            "Epoch [16151/20000], Training Loss: 0.5138\n",
            "Epoch [16152/20000], Training Loss: 0.5054\n",
            "Epoch [16153/20000], Training Loss: 0.4785\n",
            "Epoch [16154/20000], Training Loss: 0.4641\n",
            "Epoch [16155/20000], Training Loss: 0.4391\n",
            "Epoch [16156/20000], Training Loss: 0.4388\n",
            "Epoch [16157/20000], Training Loss: 0.4330\n",
            "Epoch [16158/20000], Training Loss: 0.5054\n",
            "Epoch [16159/20000], Training Loss: 0.5103\n",
            "Epoch [16160/20000], Training Loss: 0.5084\n",
            "Epoch [16161/20000], Training Loss: 0.4696\n",
            "Epoch [16162/20000], Training Loss: 0.5169\n",
            "Epoch [16163/20000], Training Loss: 0.4576\n",
            "Epoch [16164/20000], Training Loss: 0.4763\n",
            "Epoch [16165/20000], Training Loss: 0.4461\n",
            "Epoch [16166/20000], Training Loss: 0.4821\n",
            "Epoch [16167/20000], Training Loss: 0.4744\n",
            "Epoch [16168/20000], Training Loss: 0.4714\n",
            "Epoch [16169/20000], Training Loss: 0.4549\n",
            "Epoch [16170/20000], Training Loss: 0.4838\n",
            "Epoch [16171/20000], Training Loss: 0.4835\n",
            "Epoch [16172/20000], Training Loss: 0.4345\n",
            "Epoch [16173/20000], Training Loss: 0.5079\n",
            "Epoch [16174/20000], Training Loss: 0.5118\n",
            "Epoch [16175/20000], Training Loss: 0.5165\n",
            "Epoch [16176/20000], Training Loss: 0.4610\n",
            "Epoch [16177/20000], Training Loss: 0.4841\n",
            "Epoch [16178/20000], Training Loss: 0.4623\n",
            "Epoch [16179/20000], Training Loss: 0.4572\n",
            "Epoch [16180/20000], Training Loss: 0.4967\n",
            "Epoch [16181/20000], Training Loss: 0.4934\n",
            "Epoch [16182/20000], Training Loss: 0.5030\n",
            "Epoch [16183/20000], Training Loss: 0.5094\n",
            "Epoch [16184/20000], Training Loss: 0.4948\n",
            "Epoch [16185/20000], Training Loss: 0.4682\n",
            "Epoch [16186/20000], Training Loss: 0.5139\n",
            "Epoch [16187/20000], Training Loss: 0.4620\n",
            "Epoch [16188/20000], Training Loss: 0.4500\n",
            "Epoch [16189/20000], Training Loss: 0.4988\n",
            "Epoch [16190/20000], Training Loss: 0.5318\n",
            "Epoch [16191/20000], Training Loss: 0.4919\n",
            "Epoch [16192/20000], Training Loss: 0.4866\n",
            "Epoch [16193/20000], Training Loss: 0.4965\n",
            "Epoch [16194/20000], Training Loss: 0.4467\n",
            "Epoch [16195/20000], Training Loss: 0.5099\n",
            "Epoch [16196/20000], Training Loss: 0.4739\n",
            "Epoch [16197/20000], Training Loss: 0.4800\n",
            "Epoch [16198/20000], Training Loss: 0.4822\n",
            "Epoch [16199/20000], Training Loss: 0.5218\n",
            "Epoch [16200/20000], Training Loss: 0.4695\n",
            "Epoch [16201/20000], Training Loss: 0.4522\n",
            "Epoch [16202/20000], Training Loss: 0.4542\n",
            "Epoch [16203/20000], Training Loss: 0.4841\n",
            "Epoch [16204/20000], Training Loss: 0.4781\n",
            "Epoch [16205/20000], Training Loss: 0.4820\n",
            "Epoch [16206/20000], Training Loss: 0.4595\n",
            "Epoch [16207/20000], Training Loss: 0.4950\n",
            "Epoch [16208/20000], Training Loss: 0.4429\n",
            "Epoch [16209/20000], Training Loss: 0.4700\n",
            "Epoch [16210/20000], Training Loss: 0.4462\n",
            "Epoch [16211/20000], Training Loss: 0.4381\n",
            "Epoch [16212/20000], Training Loss: 0.4883\n",
            "Epoch [16213/20000], Training Loss: 0.4768\n",
            "Epoch [16214/20000], Training Loss: 0.5657\n",
            "Epoch [16215/20000], Training Loss: 0.4931\n",
            "Epoch [16216/20000], Training Loss: 0.4807\n",
            "Epoch [16217/20000], Training Loss: 0.5191\n",
            "Epoch [16218/20000], Training Loss: 0.5080\n",
            "Epoch [16219/20000], Training Loss: 0.4491\n",
            "Epoch [16220/20000], Training Loss: 0.4941\n",
            "Epoch [16221/20000], Training Loss: 0.5072\n",
            "Epoch [16222/20000], Training Loss: 0.4790\n",
            "Epoch [16223/20000], Training Loss: 0.5061\n",
            "Epoch [16224/20000], Training Loss: 0.4564\n",
            "Epoch [16225/20000], Training Loss: 0.4867\n",
            "Epoch [16226/20000], Training Loss: 0.5192\n",
            "Epoch [16227/20000], Training Loss: 0.4700\n",
            "Epoch [16228/20000], Training Loss: 0.4816\n",
            "Epoch [16229/20000], Training Loss: 0.5019\n",
            "Epoch [16230/20000], Training Loss: 0.5331\n",
            "Epoch [16231/20000], Training Loss: 0.4833\n",
            "Epoch [16232/20000], Training Loss: 0.4750\n",
            "Epoch [16233/20000], Training Loss: 0.5270\n",
            "Epoch [16234/20000], Training Loss: 0.5188\n",
            "Epoch [16235/20000], Training Loss: 0.4517\n",
            "Epoch [16236/20000], Training Loss: 0.4736\n",
            "Epoch [16237/20000], Training Loss: 0.4872\n",
            "Epoch [16238/20000], Training Loss: 0.5068\n",
            "Epoch [16239/20000], Training Loss: 0.5026\n",
            "Epoch [16240/20000], Training Loss: 0.5049\n",
            "Epoch [16241/20000], Training Loss: 0.4960\n",
            "Epoch [16242/20000], Training Loss: 0.5111\n",
            "Epoch [16243/20000], Training Loss: 0.4571\n",
            "Epoch [16244/20000], Training Loss: 0.4929\n",
            "Epoch [16245/20000], Training Loss: 0.4956\n",
            "Epoch [16246/20000], Training Loss: 0.5159\n",
            "Epoch [16247/20000], Training Loss: 0.4387\n",
            "Epoch [16248/20000], Training Loss: 0.4589\n",
            "Epoch [16249/20000], Training Loss: 0.4637\n",
            "Epoch [16250/20000], Training Loss: 0.5320\n",
            "Epoch [16251/20000], Training Loss: 0.5008\n",
            "Epoch [16252/20000], Training Loss: 0.4873\n",
            "Epoch [16253/20000], Training Loss: 0.4765\n",
            "Epoch [16254/20000], Training Loss: 0.4648\n",
            "Epoch [16255/20000], Training Loss: 0.4422\n",
            "Epoch [16256/20000], Training Loss: 0.4888\n",
            "Epoch [16257/20000], Training Loss: 0.4808\n",
            "Epoch [16258/20000], Training Loss: 0.4847\n",
            "Epoch [16259/20000], Training Loss: 0.4726\n",
            "Epoch [16260/20000], Training Loss: 0.4880\n",
            "Epoch [16261/20000], Training Loss: 0.4705\n",
            "Epoch [16262/20000], Training Loss: 0.5008\n",
            "Epoch [16263/20000], Training Loss: 0.4821\n",
            "Epoch [16264/20000], Training Loss: 0.4995\n",
            "Epoch [16265/20000], Training Loss: 0.4868\n",
            "Epoch [16266/20000], Training Loss: 0.4992\n",
            "Epoch [16267/20000], Training Loss: 0.4962\n",
            "Epoch [16268/20000], Training Loss: 0.4626\n",
            "Epoch [16269/20000], Training Loss: 0.4480\n",
            "Epoch [16270/20000], Training Loss: 0.4766\n",
            "Epoch [16271/20000], Training Loss: 0.4652\n",
            "Epoch [16272/20000], Training Loss: 0.4388\n",
            "Epoch [16273/20000], Training Loss: 0.5004\n",
            "Epoch [16274/20000], Training Loss: 0.5210\n",
            "Epoch [16275/20000], Training Loss: 0.4749\n",
            "Epoch [16276/20000], Training Loss: 0.4852\n",
            "Epoch [16277/20000], Training Loss: 0.4569\n",
            "Epoch [16278/20000], Training Loss: 0.5219\n",
            "Epoch [16279/20000], Training Loss: 0.4413\n",
            "Epoch [16280/20000], Training Loss: 0.4528\n",
            "Epoch [16281/20000], Training Loss: 0.5059\n",
            "Epoch [16282/20000], Training Loss: 0.4575\n",
            "Epoch [16283/20000], Training Loss: 0.4576\n",
            "Epoch [16284/20000], Training Loss: 0.4825\n",
            "Epoch [16285/20000], Training Loss: 0.4926\n",
            "Epoch [16286/20000], Training Loss: 0.4910\n",
            "Epoch [16287/20000], Training Loss: 0.4994\n",
            "Epoch [16288/20000], Training Loss: 0.5093\n",
            "Epoch [16289/20000], Training Loss: 0.4640\n",
            "Epoch [16290/20000], Training Loss: 0.4493\n",
            "Epoch [16291/20000], Training Loss: 0.4770\n",
            "Epoch [16292/20000], Training Loss: 0.5047\n",
            "Epoch [16293/20000], Training Loss: 0.4895\n",
            "Epoch [16294/20000], Training Loss: 0.4797\n",
            "Epoch [16295/20000], Training Loss: 0.4746\n",
            "Epoch [16296/20000], Training Loss: 0.4881\n",
            "Epoch [16297/20000], Training Loss: 0.4948\n",
            "Epoch [16298/20000], Training Loss: 0.5221\n",
            "Epoch [16299/20000], Training Loss: 0.4712\n",
            "Epoch [16300/20000], Training Loss: 0.4792\n",
            "Epoch [16301/20000], Training Loss: 0.4831\n",
            "Epoch [16302/20000], Training Loss: 0.4996\n",
            "Epoch [16303/20000], Training Loss: 0.5280\n",
            "Epoch [16304/20000], Training Loss: 0.4732\n",
            "Epoch [16305/20000], Training Loss: 0.4545\n",
            "Epoch [16306/20000], Training Loss: 0.4407\n",
            "Epoch [16307/20000], Training Loss: 0.4537\n",
            "Epoch [16308/20000], Training Loss: 0.4762\n",
            "Epoch [16309/20000], Training Loss: 0.5372\n",
            "Epoch [16310/20000], Training Loss: 0.4853\n",
            "Epoch [16311/20000], Training Loss: 0.5001\n",
            "Epoch [16312/20000], Training Loss: 0.5348\n",
            "Epoch [16313/20000], Training Loss: 0.4762\n",
            "Epoch [16314/20000], Training Loss: 0.4914\n",
            "Epoch [16315/20000], Training Loss: 0.5262\n",
            "Epoch [16316/20000], Training Loss: 0.4868\n",
            "Epoch [16317/20000], Training Loss: 0.4853\n",
            "Epoch [16318/20000], Training Loss: 0.4864\n",
            "Epoch [16319/20000], Training Loss: 0.4799\n",
            "Epoch [16320/20000], Training Loss: 0.5230\n",
            "Epoch [16321/20000], Training Loss: 0.4622\n",
            "Epoch [16322/20000], Training Loss: 0.5073\n",
            "Epoch [16323/20000], Training Loss: 0.5044\n",
            "Epoch [16324/20000], Training Loss: 0.4823\n",
            "Epoch [16325/20000], Training Loss: 0.4979\n",
            "Epoch [16326/20000], Training Loss: 0.4773\n",
            "Epoch [16327/20000], Training Loss: 0.4739\n",
            "Epoch [16328/20000], Training Loss: 0.4661\n",
            "Epoch [16329/20000], Training Loss: 0.4773\n",
            "Epoch [16330/20000], Training Loss: 0.5134\n",
            "Epoch [16331/20000], Training Loss: 0.4588\n",
            "Epoch [16332/20000], Training Loss: 0.4409\n",
            "Epoch [16333/20000], Training Loss: 0.4743\n",
            "Epoch [16334/20000], Training Loss: 0.4953\n",
            "Epoch [16335/20000], Training Loss: 0.4853\n",
            "Epoch [16336/20000], Training Loss: 0.4943\n",
            "Epoch [16337/20000], Training Loss: 0.4666\n",
            "Epoch [16338/20000], Training Loss: 0.4608\n",
            "Epoch [16339/20000], Training Loss: 0.5169\n",
            "Epoch [16340/20000], Training Loss: 0.4940\n",
            "Epoch [16341/20000], Training Loss: 0.4552\n",
            "Epoch [16342/20000], Training Loss: 0.4440\n",
            "Epoch [16343/20000], Training Loss: 0.4916\n",
            "Epoch [16344/20000], Training Loss: 0.5109\n",
            "Epoch [16345/20000], Training Loss: 0.5040\n",
            "Epoch [16346/20000], Training Loss: 0.4726\n",
            "Epoch [16347/20000], Training Loss: 0.4551\n",
            "Epoch [16348/20000], Training Loss: 0.4777\n",
            "Epoch [16349/20000], Training Loss: 0.5258\n",
            "Epoch [16350/20000], Training Loss: 0.4889\n",
            "Epoch [16351/20000], Training Loss: 0.4773\n",
            "Epoch [16352/20000], Training Loss: 0.5272\n",
            "Epoch [16353/20000], Training Loss: 0.4348\n",
            "Epoch [16354/20000], Training Loss: 0.4978\n",
            "Epoch [16355/20000], Training Loss: 0.4721\n",
            "Epoch [16356/20000], Training Loss: 0.4867\n",
            "Epoch [16357/20000], Training Loss: 0.5014\n",
            "Epoch [16358/20000], Training Loss: 0.4922\n",
            "Epoch [16359/20000], Training Loss: 0.4644\n",
            "Epoch [16360/20000], Training Loss: 0.5264\n",
            "Epoch [16361/20000], Training Loss: 0.4616\n",
            "Epoch [16362/20000], Training Loss: 0.4733\n",
            "Epoch [16363/20000], Training Loss: 0.5177\n",
            "Epoch [16364/20000], Training Loss: 0.4929\n",
            "Epoch [16365/20000], Training Loss: 0.5179\n",
            "Epoch [16366/20000], Training Loss: 0.5127\n",
            "Epoch [16367/20000], Training Loss: 0.4973\n",
            "Epoch [16368/20000], Training Loss: 0.4825\n",
            "Epoch [16369/20000], Training Loss: 0.4800\n",
            "Epoch [16370/20000], Training Loss: 0.5297\n",
            "Epoch [16371/20000], Training Loss: 0.5242\n",
            "Epoch [16372/20000], Training Loss: 0.5385\n",
            "Epoch [16373/20000], Training Loss: 0.4763\n",
            "Epoch [16374/20000], Training Loss: 0.4888\n",
            "Epoch [16375/20000], Training Loss: 0.4557\n",
            "Epoch [16376/20000], Training Loss: 0.4935\n",
            "Epoch [16377/20000], Training Loss: 0.4812\n",
            "Epoch [16378/20000], Training Loss: 0.4668\n",
            "Epoch [16379/20000], Training Loss: 0.5044\n",
            "Epoch [16380/20000], Training Loss: 0.4537\n",
            "Epoch [16381/20000], Training Loss: 0.4795\n",
            "Epoch [16382/20000], Training Loss: 0.4835\n",
            "Epoch [16383/20000], Training Loss: 0.4881\n",
            "Epoch [16384/20000], Training Loss: 0.4691\n",
            "Epoch [16385/20000], Training Loss: 0.4869\n",
            "Epoch [16386/20000], Training Loss: 0.5010\n",
            "Epoch [16387/20000], Training Loss: 0.5117\n",
            "Epoch [16388/20000], Training Loss: 0.4787\n",
            "Epoch [16389/20000], Training Loss: 0.4604\n",
            "Epoch [16390/20000], Training Loss: 0.5211\n",
            "Epoch [16391/20000], Training Loss: 0.5147\n",
            "Epoch [16392/20000], Training Loss: 0.4819\n",
            "Epoch [16393/20000], Training Loss: 0.4782\n",
            "Epoch [16394/20000], Training Loss: 0.4923\n",
            "Epoch [16395/20000], Training Loss: 0.5115\n",
            "Epoch [16396/20000], Training Loss: 0.4899\n",
            "Epoch [16397/20000], Training Loss: 0.4765\n",
            "Epoch [16398/20000], Training Loss: 0.4506\n",
            "Epoch [16399/20000], Training Loss: 0.5672\n",
            "Epoch [16400/20000], Training Loss: 0.4785\n",
            "Epoch [16401/20000], Training Loss: 0.5330\n",
            "Epoch [16402/20000], Training Loss: 0.4634\n",
            "Epoch [16403/20000], Training Loss: 0.5154\n",
            "Epoch [16404/20000], Training Loss: 0.5129\n",
            "Epoch [16405/20000], Training Loss: 0.4775\n",
            "Epoch [16406/20000], Training Loss: 0.4973\n",
            "Epoch [16407/20000], Training Loss: 0.5049\n",
            "Epoch [16408/20000], Training Loss: 0.5142\n",
            "Epoch [16409/20000], Training Loss: 0.5362\n",
            "Epoch [16410/20000], Training Loss: 0.5246\n",
            "Epoch [16411/20000], Training Loss: 0.4721\n",
            "Epoch [16412/20000], Training Loss: 0.4857\n",
            "Epoch [16413/20000], Training Loss: 0.5076\n",
            "Epoch [16414/20000], Training Loss: 0.4446\n",
            "Epoch [16415/20000], Training Loss: 0.5366\n",
            "Epoch [16416/20000], Training Loss: 0.4822\n",
            "Epoch [16417/20000], Training Loss: 0.4862\n",
            "Epoch [16418/20000], Training Loss: 0.4670\n",
            "Epoch [16419/20000], Training Loss: 0.4761\n",
            "Epoch [16420/20000], Training Loss: 0.4552\n",
            "Epoch [16421/20000], Training Loss: 0.4796\n",
            "Epoch [16422/20000], Training Loss: 0.4854\n",
            "Epoch [16423/20000], Training Loss: 0.4674\n",
            "Epoch [16424/20000], Training Loss: 0.4965\n",
            "Epoch [16425/20000], Training Loss: 0.4887\n",
            "Epoch [16426/20000], Training Loss: 0.5220\n",
            "Epoch [16427/20000], Training Loss: 0.4597\n",
            "Epoch [16428/20000], Training Loss: 0.4716\n",
            "Epoch [16429/20000], Training Loss: 0.4573\n",
            "Epoch [16430/20000], Training Loss: 0.4610\n",
            "Epoch [16431/20000], Training Loss: 0.4933\n",
            "Epoch [16432/20000], Training Loss: 0.4515\n",
            "Epoch [16433/20000], Training Loss: 0.5250\n",
            "Epoch [16434/20000], Training Loss: 0.4699\n",
            "Epoch [16435/20000], Training Loss: 0.5664\n",
            "Epoch [16436/20000], Training Loss: 0.4403\n",
            "Epoch [16437/20000], Training Loss: 0.4606\n",
            "Epoch [16438/20000], Training Loss: 0.4908\n",
            "Epoch [16439/20000], Training Loss: 0.4846\n",
            "Epoch [16440/20000], Training Loss: 0.4863\n",
            "Epoch [16441/20000], Training Loss: 0.5278\n",
            "Epoch [16442/20000], Training Loss: 0.4713\n",
            "Epoch [16443/20000], Training Loss: 0.4715\n",
            "Epoch [16444/20000], Training Loss: 0.4965\n",
            "Epoch [16445/20000], Training Loss: 0.4868\n",
            "Epoch [16446/20000], Training Loss: 0.5332\n",
            "Epoch [16447/20000], Training Loss: 0.5230\n",
            "Epoch [16448/20000], Training Loss: 0.4709\n",
            "Epoch [16449/20000], Training Loss: 0.4790\n",
            "Epoch [16450/20000], Training Loss: 0.4617\n",
            "Epoch [16451/20000], Training Loss: 0.4603\n",
            "Epoch [16452/20000], Training Loss: 0.4820\n",
            "Epoch [16453/20000], Training Loss: 0.5017\n",
            "Epoch [16454/20000], Training Loss: 0.4935\n",
            "Epoch [16455/20000], Training Loss: 0.4869\n",
            "Epoch [16456/20000], Training Loss: 0.5172\n",
            "Epoch [16457/20000], Training Loss: 0.4976\n",
            "Epoch [16458/20000], Training Loss: 0.4518\n",
            "Epoch [16459/20000], Training Loss: 0.4980\n",
            "Epoch [16460/20000], Training Loss: 0.5012\n",
            "Epoch [16461/20000], Training Loss: 0.4958\n",
            "Epoch [16462/20000], Training Loss: 0.4761\n",
            "Epoch [16463/20000], Training Loss: 0.5277\n",
            "Epoch [16464/20000], Training Loss: 0.4350\n",
            "Epoch [16465/20000], Training Loss: 0.5102\n",
            "Epoch [16466/20000], Training Loss: 0.4950\n",
            "Epoch [16467/20000], Training Loss: 0.5082\n",
            "Epoch [16468/20000], Training Loss: 0.4907\n",
            "Epoch [16469/20000], Training Loss: 0.4877\n",
            "Epoch [16470/20000], Training Loss: 0.4985\n",
            "Epoch [16471/20000], Training Loss: 0.4504\n",
            "Epoch [16472/20000], Training Loss: 0.4935\n",
            "Epoch [16473/20000], Training Loss: 0.4969\n",
            "Epoch [16474/20000], Training Loss: 0.4969\n",
            "Epoch [16475/20000], Training Loss: 0.4915\n",
            "Epoch [16476/20000], Training Loss: 0.4628\n",
            "Epoch [16477/20000], Training Loss: 0.4473\n",
            "Epoch [16478/20000], Training Loss: 0.4800\n",
            "Epoch [16479/20000], Training Loss: 0.4795\n",
            "Epoch [16480/20000], Training Loss: 0.4587\n",
            "Epoch [16481/20000], Training Loss: 0.4867\n",
            "Epoch [16482/20000], Training Loss: 0.5051\n",
            "Epoch [16483/20000], Training Loss: 0.4529\n",
            "Epoch [16484/20000], Training Loss: 0.4434\n",
            "Epoch [16485/20000], Training Loss: 0.5040\n",
            "Epoch [16486/20000], Training Loss: 0.4713\n",
            "Epoch [16487/20000], Training Loss: 0.4751\n",
            "Epoch [16488/20000], Training Loss: 0.5344\n",
            "Epoch [16489/20000], Training Loss: 0.4878\n",
            "Epoch [16490/20000], Training Loss: 0.4875\n",
            "Epoch [16491/20000], Training Loss: 0.5014\n",
            "Epoch [16492/20000], Training Loss: 0.4484\n",
            "Epoch [16493/20000], Training Loss: 0.5026\n",
            "Epoch [16494/20000], Training Loss: 0.4785\n",
            "Epoch [16495/20000], Training Loss: 0.5065\n",
            "Epoch [16496/20000], Training Loss: 0.5209\n",
            "Epoch [16497/20000], Training Loss: 0.4846\n",
            "Epoch [16498/20000], Training Loss: 0.4806\n",
            "Epoch [16499/20000], Training Loss: 0.4745\n",
            "Epoch [16500/20000], Training Loss: 0.5109\n",
            "Epoch [16501/20000], Training Loss: 0.4820\n",
            "Epoch [16502/20000], Training Loss: 0.4700\n",
            "Epoch [16503/20000], Training Loss: 0.4508\n",
            "Epoch [16504/20000], Training Loss: 0.4711\n",
            "Epoch [16505/20000], Training Loss: 0.4901\n",
            "Epoch [16506/20000], Training Loss: 0.4611\n",
            "Epoch [16507/20000], Training Loss: 0.5023\n",
            "Epoch [16508/20000], Training Loss: 0.4919\n",
            "Epoch [16509/20000], Training Loss: 0.5011\n",
            "Epoch [16510/20000], Training Loss: 0.4829\n",
            "Epoch [16511/20000], Training Loss: 0.5304\n",
            "Epoch [16512/20000], Training Loss: 0.5078\n",
            "Epoch [16513/20000], Training Loss: 0.5185\n",
            "Epoch [16514/20000], Training Loss: 0.5152\n",
            "Epoch [16515/20000], Training Loss: 0.5122\n",
            "Epoch [16516/20000], Training Loss: 0.4735\n",
            "Epoch [16517/20000], Training Loss: 0.4951\n",
            "Epoch [16518/20000], Training Loss: 0.5195\n",
            "Epoch [16519/20000], Training Loss: 0.4690\n",
            "Epoch [16520/20000], Training Loss: 0.4646\n",
            "Epoch [16521/20000], Training Loss: 0.4983\n",
            "Epoch [16522/20000], Training Loss: 0.4887\n",
            "Epoch [16523/20000], Training Loss: 0.5106\n",
            "Epoch [16524/20000], Training Loss: 0.4640\n",
            "Epoch [16525/20000], Training Loss: 0.4967\n",
            "Epoch [16526/20000], Training Loss: 0.4678\n",
            "Epoch [16527/20000], Training Loss: 0.4457\n",
            "Epoch [16528/20000], Training Loss: 0.5298\n",
            "Epoch [16529/20000], Training Loss: 0.5241\n",
            "Epoch [16530/20000], Training Loss: 0.4873\n",
            "Epoch [16531/20000], Training Loss: 0.4762\n",
            "Epoch [16532/20000], Training Loss: 0.4944\n",
            "Epoch [16533/20000], Training Loss: 0.5046\n",
            "Epoch [16534/20000], Training Loss: 0.5029\n",
            "Epoch [16535/20000], Training Loss: 0.5090\n",
            "Epoch [16536/20000], Training Loss: 0.4923\n",
            "Epoch [16537/20000], Training Loss: 0.4537\n",
            "Epoch [16538/20000], Training Loss: 0.4960\n",
            "Epoch [16539/20000], Training Loss: 0.5436\n",
            "Epoch [16540/20000], Training Loss: 0.5259\n",
            "Epoch [16541/20000], Training Loss: 0.4863\n",
            "Epoch [16542/20000], Training Loss: 0.4767\n",
            "Epoch [16543/20000], Training Loss: 0.4571\n",
            "Epoch [16544/20000], Training Loss: 0.4804\n",
            "Epoch [16545/20000], Training Loss: 0.4788\n",
            "Epoch [16546/20000], Training Loss: 0.4331\n",
            "Epoch [16547/20000], Training Loss: 0.5126\n",
            "Epoch [16548/20000], Training Loss: 0.4370\n",
            "Epoch [16549/20000], Training Loss: 0.4700\n",
            "Epoch [16550/20000], Training Loss: 0.5301\n",
            "Epoch [16551/20000], Training Loss: 0.4467\n",
            "Epoch [16552/20000], Training Loss: 0.4544\n",
            "Epoch [16553/20000], Training Loss: 0.4726\n",
            "Epoch [16554/20000], Training Loss: 0.4576\n",
            "Epoch [16555/20000], Training Loss: 0.4756\n",
            "Epoch [16556/20000], Training Loss: 0.5412\n",
            "Epoch [16557/20000], Training Loss: 0.5471\n",
            "Epoch [16558/20000], Training Loss: 0.4655\n",
            "Epoch [16559/20000], Training Loss: 0.5349\n",
            "Epoch [16560/20000], Training Loss: 0.4638\n",
            "Epoch [16561/20000], Training Loss: 0.5272\n",
            "Epoch [16562/20000], Training Loss: 0.5065\n",
            "Epoch [16563/20000], Training Loss: 0.4819\n",
            "Epoch [16564/20000], Training Loss: 0.4690\n",
            "Epoch [16565/20000], Training Loss: 0.4942\n",
            "Epoch [16566/20000], Training Loss: 0.4701\n",
            "Epoch [16567/20000], Training Loss: 0.4482\n",
            "Epoch [16568/20000], Training Loss: 0.4822\n",
            "Epoch [16569/20000], Training Loss: 0.4928\n",
            "Epoch [16570/20000], Training Loss: 0.5304\n",
            "Epoch [16571/20000], Training Loss: 0.4932\n",
            "Epoch [16572/20000], Training Loss: 0.4866\n",
            "Epoch [16573/20000], Training Loss: 0.4663\n",
            "Epoch [16574/20000], Training Loss: 0.4923\n",
            "Epoch [16575/20000], Training Loss: 0.4758\n",
            "Epoch [16576/20000], Training Loss: 0.4988\n",
            "Epoch [16577/20000], Training Loss: 0.4507\n",
            "Epoch [16578/20000], Training Loss: 0.4896\n",
            "Epoch [16579/20000], Training Loss: 0.5125\n",
            "Epoch [16580/20000], Training Loss: 0.5319\n",
            "Epoch [16581/20000], Training Loss: 0.4629\n",
            "Epoch [16582/20000], Training Loss: 0.5380\n",
            "Epoch [16583/20000], Training Loss: 0.5016\n",
            "Epoch [16584/20000], Training Loss: 0.4904\n",
            "Epoch [16585/20000], Training Loss: 0.4996\n",
            "Epoch [16586/20000], Training Loss: 0.4808\n",
            "Epoch [16587/20000], Training Loss: 0.4752\n",
            "Epoch [16588/20000], Training Loss: 0.4651\n",
            "Epoch [16589/20000], Training Loss: 0.5189\n",
            "Epoch [16590/20000], Training Loss: 0.4503\n",
            "Epoch [16591/20000], Training Loss: 0.5130\n",
            "Epoch [16592/20000], Training Loss: 0.4972\n",
            "Epoch [16593/20000], Training Loss: 0.4905\n",
            "Epoch [16594/20000], Training Loss: 0.5111\n",
            "Epoch [16595/20000], Training Loss: 0.4863\n",
            "Epoch [16596/20000], Training Loss: 0.4746\n",
            "Epoch [16597/20000], Training Loss: 0.4497\n",
            "Epoch [16598/20000], Training Loss: 0.4740\n",
            "Epoch [16599/20000], Training Loss: 0.4981\n",
            "Epoch [16600/20000], Training Loss: 0.4820\n",
            "Epoch [16601/20000], Training Loss: 0.4863\n",
            "Epoch [16602/20000], Training Loss: 0.4926\n",
            "Epoch [16603/20000], Training Loss: 0.5131\n",
            "Epoch [16604/20000], Training Loss: 0.4747\n",
            "Epoch [16605/20000], Training Loss: 0.4621\n",
            "Epoch [16606/20000], Training Loss: 0.4655\n",
            "Epoch [16607/20000], Training Loss: 0.4789\n",
            "Epoch [16608/20000], Training Loss: 0.4571\n",
            "Epoch [16609/20000], Training Loss: 0.5231\n",
            "Epoch [16610/20000], Training Loss: 0.5168\n",
            "Epoch [16611/20000], Training Loss: 0.4381\n",
            "Epoch [16612/20000], Training Loss: 0.4913\n",
            "Epoch [16613/20000], Training Loss: 0.4535\n",
            "Epoch [16614/20000], Training Loss: 0.5086\n",
            "Epoch [16615/20000], Training Loss: 0.5142\n",
            "Epoch [16616/20000], Training Loss: 0.4622\n",
            "Epoch [16617/20000], Training Loss: 0.4915\n",
            "Epoch [16618/20000], Training Loss: 0.4575\n",
            "Epoch [16619/20000], Training Loss: 0.4501\n",
            "Epoch [16620/20000], Training Loss: 0.4658\n",
            "Epoch [16621/20000], Training Loss: 0.4748\n",
            "Epoch [16622/20000], Training Loss: 0.4761\n",
            "Epoch [16623/20000], Training Loss: 0.4663\n",
            "Epoch [16624/20000], Training Loss: 0.5513\n",
            "Epoch [16625/20000], Training Loss: 0.4620\n",
            "Epoch [16626/20000], Training Loss: 0.4879\n",
            "Epoch [16627/20000], Training Loss: 0.4912\n",
            "Epoch [16628/20000], Training Loss: 0.4562\n",
            "Epoch [16629/20000], Training Loss: 0.5099\n",
            "Epoch [16630/20000], Training Loss: 0.4430\n",
            "Epoch [16631/20000], Training Loss: 0.4944\n",
            "Epoch [16632/20000], Training Loss: 0.4978\n",
            "Epoch [16633/20000], Training Loss: 0.4791\n",
            "Epoch [16634/20000], Training Loss: 0.4590\n",
            "Epoch [16635/20000], Training Loss: 0.5058\n",
            "Epoch [16636/20000], Training Loss: 0.4865\n",
            "Epoch [16637/20000], Training Loss: 0.4870\n",
            "Epoch [16638/20000], Training Loss: 0.5124\n",
            "Epoch [16639/20000], Training Loss: 0.4904\n",
            "Epoch [16640/20000], Training Loss: 0.4977\n",
            "Epoch [16641/20000], Training Loss: 0.5070\n",
            "Epoch [16642/20000], Training Loss: 0.5088\n",
            "Epoch [16643/20000], Training Loss: 0.5231\n",
            "Epoch [16644/20000], Training Loss: 0.4732\n",
            "Epoch [16645/20000], Training Loss: 0.5085\n",
            "Epoch [16646/20000], Training Loss: 0.4687\n",
            "Epoch [16647/20000], Training Loss: 0.4705\n",
            "Epoch [16648/20000], Training Loss: 0.4967\n",
            "Epoch [16649/20000], Training Loss: 0.4607\n",
            "Epoch [16650/20000], Training Loss: 0.4684\n",
            "Epoch [16651/20000], Training Loss: 0.4836\n",
            "Epoch [16652/20000], Training Loss: 0.4543\n",
            "Epoch [16653/20000], Training Loss: 0.4545\n",
            "Epoch [16654/20000], Training Loss: 0.4356\n",
            "Epoch [16655/20000], Training Loss: 0.5009\n",
            "Epoch [16656/20000], Training Loss: 0.4801\n",
            "Epoch [16657/20000], Training Loss: 0.4438\n",
            "Epoch [16658/20000], Training Loss: 0.4880\n",
            "Epoch [16659/20000], Training Loss: 0.4872\n",
            "Epoch [16660/20000], Training Loss: 0.5115\n",
            "Epoch [16661/20000], Training Loss: 0.5383\n",
            "Epoch [16662/20000], Training Loss: 0.5183\n",
            "Epoch [16663/20000], Training Loss: 0.4625\n",
            "Epoch [16664/20000], Training Loss: 0.4624\n",
            "Epoch [16665/20000], Training Loss: 0.4983\n",
            "Epoch [16666/20000], Training Loss: 0.5432\n",
            "Epoch [16667/20000], Training Loss: 0.4636\n",
            "Epoch [16668/20000], Training Loss: 0.4998\n",
            "Epoch [16669/20000], Training Loss: 0.5182\n",
            "Epoch [16670/20000], Training Loss: 0.4794\n",
            "Epoch [16671/20000], Training Loss: 0.4857\n",
            "Epoch [16672/20000], Training Loss: 0.5162\n",
            "Epoch [16673/20000], Training Loss: 0.4672\n",
            "Epoch [16674/20000], Training Loss: 0.4774\n",
            "Epoch [16675/20000], Training Loss: 0.5392\n",
            "Epoch [16676/20000], Training Loss: 0.4838\n",
            "Epoch [16677/20000], Training Loss: 0.4283\n",
            "Epoch [16678/20000], Training Loss: 0.4808\n",
            "Epoch [16679/20000], Training Loss: 0.4776\n",
            "Epoch [16680/20000], Training Loss: 0.4534\n",
            "Epoch [16681/20000], Training Loss: 0.4757\n",
            "Epoch [16682/20000], Training Loss: 0.5121\n",
            "Epoch [16683/20000], Training Loss: 0.4721\n",
            "Epoch [16684/20000], Training Loss: 0.5466\n",
            "Epoch [16685/20000], Training Loss: 0.4857\n",
            "Epoch [16686/20000], Training Loss: 0.5051\n",
            "Epoch [16687/20000], Training Loss: 0.5072\n",
            "Epoch [16688/20000], Training Loss: 0.5178\n",
            "Epoch [16689/20000], Training Loss: 0.5147\n",
            "Epoch [16690/20000], Training Loss: 0.4606\n",
            "Epoch [16691/20000], Training Loss: 0.4810\n",
            "Epoch [16692/20000], Training Loss: 0.4657\n",
            "Epoch [16693/20000], Training Loss: 0.4537\n",
            "Epoch [16694/20000], Training Loss: 0.5123\n",
            "Epoch [16695/20000], Training Loss: 0.5522\n",
            "Epoch [16696/20000], Training Loss: 0.4597\n",
            "Epoch [16697/20000], Training Loss: 0.5441\n",
            "Epoch [16698/20000], Training Loss: 0.5084\n",
            "Epoch [16699/20000], Training Loss: 0.4655\n",
            "Epoch [16700/20000], Training Loss: 0.5090\n",
            "Epoch [16701/20000], Training Loss: 0.4841\n",
            "Epoch [16702/20000], Training Loss: 0.5113\n",
            "Epoch [16703/20000], Training Loss: 0.4501\n",
            "Epoch [16704/20000], Training Loss: 0.4663\n",
            "Epoch [16705/20000], Training Loss: 0.5033\n",
            "Epoch [16706/20000], Training Loss: 0.4689\n",
            "Epoch [16707/20000], Training Loss: 0.4554\n",
            "Epoch [16708/20000], Training Loss: 0.4559\n",
            "Epoch [16709/20000], Training Loss: 0.5030\n",
            "Epoch [16710/20000], Training Loss: 0.4849\n",
            "Epoch [16711/20000], Training Loss: 0.5159\n",
            "Epoch [16712/20000], Training Loss: 0.4960\n",
            "Epoch [16713/20000], Training Loss: 0.4712\n",
            "Epoch [16714/20000], Training Loss: 0.5235\n",
            "Epoch [16715/20000], Training Loss: 0.5230\n",
            "Epoch [16716/20000], Training Loss: 0.4711\n",
            "Epoch [16717/20000], Training Loss: 0.5012\n",
            "Epoch [16718/20000], Training Loss: 0.4791\n",
            "Epoch [16719/20000], Training Loss: 0.4636\n",
            "Epoch [16720/20000], Training Loss: 0.4931\n",
            "Epoch [16721/20000], Training Loss: 0.5190\n",
            "Epoch [16722/20000], Training Loss: 0.4687\n",
            "Epoch [16723/20000], Training Loss: 0.4863\n",
            "Epoch [16724/20000], Training Loss: 0.4863\n",
            "Epoch [16725/20000], Training Loss: 0.4517\n",
            "Epoch [16726/20000], Training Loss: 0.4552\n",
            "Epoch [16727/20000], Training Loss: 0.4812\n",
            "Epoch [16728/20000], Training Loss: 0.5206\n",
            "Epoch [16729/20000], Training Loss: 0.4602\n",
            "Epoch [16730/20000], Training Loss: 0.5150\n",
            "Epoch [16731/20000], Training Loss: 0.4853\n",
            "Epoch [16732/20000], Training Loss: 0.4859\n",
            "Epoch [16733/20000], Training Loss: 0.4707\n",
            "Epoch [16734/20000], Training Loss: 0.4444\n",
            "Epoch [16735/20000], Training Loss: 0.4869\n",
            "Epoch [16736/20000], Training Loss: 0.5402\n",
            "Epoch [16737/20000], Training Loss: 0.4878\n",
            "Epoch [16738/20000], Training Loss: 0.4651\n",
            "Epoch [16739/20000], Training Loss: 0.4850\n",
            "Epoch [16740/20000], Training Loss: 0.4511\n",
            "Epoch [16741/20000], Training Loss: 0.4977\n",
            "Epoch [16742/20000], Training Loss: 0.4805\n",
            "Epoch [16743/20000], Training Loss: 0.4529\n",
            "Epoch [16744/20000], Training Loss: 0.4921\n",
            "Epoch [16745/20000], Training Loss: 0.4587\n",
            "Epoch [16746/20000], Training Loss: 0.4748\n",
            "Epoch [16747/20000], Training Loss: 0.5166\n",
            "Epoch [16748/20000], Training Loss: 0.4895\n",
            "Epoch [16749/20000], Training Loss: 0.4364\n",
            "Epoch [16750/20000], Training Loss: 0.4735\n",
            "Epoch [16751/20000], Training Loss: 0.4909\n",
            "Epoch [16752/20000], Training Loss: 0.4561\n",
            "Epoch [16753/20000], Training Loss: 0.4916\n",
            "Epoch [16754/20000], Training Loss: 0.4637\n",
            "Epoch [16755/20000], Training Loss: 0.4987\n",
            "Epoch [16756/20000], Training Loss: 0.4873\n",
            "Epoch [16757/20000], Training Loss: 0.5106\n",
            "Epoch [16758/20000], Training Loss: 0.5015\n",
            "Epoch [16759/20000], Training Loss: 0.4804\n",
            "Epoch [16760/20000], Training Loss: 0.4769\n",
            "Epoch [16761/20000], Training Loss: 0.5162\n",
            "Epoch [16762/20000], Training Loss: 0.5354\n",
            "Epoch [16763/20000], Training Loss: 0.5210\n",
            "Epoch [16764/20000], Training Loss: 0.5014\n",
            "Epoch [16765/20000], Training Loss: 0.5191\n",
            "Epoch [16766/20000], Training Loss: 0.5424\n",
            "Epoch [16767/20000], Training Loss: 0.4831\n",
            "Epoch [16768/20000], Training Loss: 0.5235\n",
            "Epoch [16769/20000], Training Loss: 0.5226\n",
            "Epoch [16770/20000], Training Loss: 0.4629\n",
            "Epoch [16771/20000], Training Loss: 0.5144\n",
            "Epoch [16772/20000], Training Loss: 0.4705\n",
            "Epoch [16773/20000], Training Loss: 0.5013\n",
            "Epoch [16774/20000], Training Loss: 0.4939\n",
            "Epoch [16775/20000], Training Loss: 0.4862\n",
            "Epoch [16776/20000], Training Loss: 0.5170\n",
            "Epoch [16777/20000], Training Loss: 0.5292\n",
            "Epoch [16778/20000], Training Loss: 0.4805\n",
            "Epoch [16779/20000], Training Loss: 0.4484\n",
            "Epoch [16780/20000], Training Loss: 0.5264\n",
            "Epoch [16781/20000], Training Loss: 0.4997\n",
            "Epoch [16782/20000], Training Loss: 0.4699\n",
            "Epoch [16783/20000], Training Loss: 0.5026\n",
            "Epoch [16784/20000], Training Loss: 0.5459\n",
            "Epoch [16785/20000], Training Loss: 0.5392\n",
            "Epoch [16786/20000], Training Loss: 0.4460\n",
            "Epoch [16787/20000], Training Loss: 0.5128\n",
            "Epoch [16788/20000], Training Loss: 0.4645\n",
            "Epoch [16789/20000], Training Loss: 0.5003\n",
            "Epoch [16790/20000], Training Loss: 0.5248\n",
            "Epoch [16791/20000], Training Loss: 0.4994\n",
            "Epoch [16792/20000], Training Loss: 0.4783\n",
            "Epoch [16793/20000], Training Loss: 0.4763\n",
            "Epoch [16794/20000], Training Loss: 0.4567\n",
            "Epoch [16795/20000], Training Loss: 0.5365\n",
            "Epoch [16796/20000], Training Loss: 0.4924\n",
            "Epoch [16797/20000], Training Loss: 0.5156\n",
            "Epoch [16798/20000], Training Loss: 0.4885\n",
            "Epoch [16799/20000], Training Loss: 0.5274\n",
            "Epoch [16800/20000], Training Loss: 0.4701\n",
            "Epoch [16801/20000], Training Loss: 0.4811\n",
            "Epoch [16802/20000], Training Loss: 0.4353\n",
            "Epoch [16803/20000], Training Loss: 0.4744\n",
            "Epoch [16804/20000], Training Loss: 0.4565\n",
            "Epoch [16805/20000], Training Loss: 0.4733\n",
            "Epoch [16806/20000], Training Loss: 0.5113\n",
            "Epoch [16807/20000], Training Loss: 0.4653\n",
            "Epoch [16808/20000], Training Loss: 0.4975\n",
            "Epoch [16809/20000], Training Loss: 0.5105\n",
            "Epoch [16810/20000], Training Loss: 0.4955\n",
            "Epoch [16811/20000], Training Loss: 0.4665\n",
            "Epoch [16812/20000], Training Loss: 0.5032\n",
            "Epoch [16813/20000], Training Loss: 0.4408\n",
            "Epoch [16814/20000], Training Loss: 0.4683\n",
            "Epoch [16815/20000], Training Loss: 0.4372\n",
            "Epoch [16816/20000], Training Loss: 0.5132\n",
            "Epoch [16817/20000], Training Loss: 0.4544\n",
            "Epoch [16818/20000], Training Loss: 0.4866\n",
            "Epoch [16819/20000], Training Loss: 0.4743\n",
            "Epoch [16820/20000], Training Loss: 0.4514\n",
            "Epoch [16821/20000], Training Loss: 0.5342\n",
            "Epoch [16822/20000], Training Loss: 0.5146\n",
            "Epoch [16823/20000], Training Loss: 0.5135\n",
            "Epoch [16824/20000], Training Loss: 0.5161\n",
            "Epoch [16825/20000], Training Loss: 0.4670\n",
            "Epoch [16826/20000], Training Loss: 0.4682\n",
            "Epoch [16827/20000], Training Loss: 0.4421\n",
            "Epoch [16828/20000], Training Loss: 0.4924\n",
            "Epoch [16829/20000], Training Loss: 0.4904\n",
            "Epoch [16830/20000], Training Loss: 0.5238\n",
            "Epoch [16831/20000], Training Loss: 0.4642\n",
            "Epoch [16832/20000], Training Loss: 0.4556\n",
            "Epoch [16833/20000], Training Loss: 0.5013\n",
            "Epoch [16834/20000], Training Loss: 0.4919\n",
            "Epoch [16835/20000], Training Loss: 0.4765\n",
            "Epoch [16836/20000], Training Loss: 0.4567\n",
            "Epoch [16837/20000], Training Loss: 0.4979\n",
            "Epoch [16838/20000], Training Loss: 0.4845\n",
            "Epoch [16839/20000], Training Loss: 0.4563\n",
            "Epoch [16840/20000], Training Loss: 0.5079\n",
            "Epoch [16841/20000], Training Loss: 0.4991\n",
            "Epoch [16842/20000], Training Loss: 0.4576\n",
            "Epoch [16843/20000], Training Loss: 0.5004\n",
            "Epoch [16844/20000], Training Loss: 0.4604\n",
            "Epoch [16845/20000], Training Loss: 0.5118\n",
            "Epoch [16846/20000], Training Loss: 0.4763\n",
            "Epoch [16847/20000], Training Loss: 0.4906\n",
            "Epoch [16848/20000], Training Loss: 0.4242\n",
            "Epoch [16849/20000], Training Loss: 0.4910\n",
            "Epoch [16850/20000], Training Loss: 0.4642\n",
            "Epoch [16851/20000], Training Loss: 0.5413\n",
            "Epoch [16852/20000], Training Loss: 0.5046\n",
            "Epoch [16853/20000], Training Loss: 0.4802\n",
            "Epoch [16854/20000], Training Loss: 0.5153\n",
            "Epoch [16855/20000], Training Loss: 0.5327\n",
            "Epoch [16856/20000], Training Loss: 0.4912\n",
            "Epoch [16857/20000], Training Loss: 0.5032\n",
            "Epoch [16858/20000], Training Loss: 0.4549\n",
            "Epoch [16859/20000], Training Loss: 0.5271\n",
            "Epoch [16860/20000], Training Loss: 0.5215\n",
            "Epoch [16861/20000], Training Loss: 0.4705\n",
            "Epoch [16862/20000], Training Loss: 0.4894\n",
            "Epoch [16863/20000], Training Loss: 0.4912\n",
            "Epoch [16864/20000], Training Loss: 0.4866\n",
            "Epoch [16865/20000], Training Loss: 0.4858\n",
            "Epoch [16866/20000], Training Loss: 0.5264\n",
            "Epoch [16867/20000], Training Loss: 0.4580\n",
            "Epoch [16868/20000], Training Loss: 0.5280\n",
            "Epoch [16869/20000], Training Loss: 0.4585\n",
            "Epoch [16870/20000], Training Loss: 0.4708\n",
            "Epoch [16871/20000], Training Loss: 0.4908\n",
            "Epoch [16872/20000], Training Loss: 0.4764\n",
            "Epoch [16873/20000], Training Loss: 0.5521\n",
            "Epoch [16874/20000], Training Loss: 0.5013\n",
            "Epoch [16875/20000], Training Loss: 0.5509\n",
            "Epoch [16876/20000], Training Loss: 0.4815\n",
            "Epoch [16877/20000], Training Loss: 0.4916\n",
            "Epoch [16878/20000], Training Loss: 0.4785\n",
            "Epoch [16879/20000], Training Loss: 0.4617\n",
            "Epoch [16880/20000], Training Loss: 0.5194\n",
            "Epoch [16881/20000], Training Loss: 0.4933\n",
            "Epoch [16882/20000], Training Loss: 0.4725\n",
            "Epoch [16883/20000], Training Loss: 0.4920\n",
            "Epoch [16884/20000], Training Loss: 0.4661\n",
            "Epoch [16885/20000], Training Loss: 0.4640\n",
            "Epoch [16886/20000], Training Loss: 0.4826\n",
            "Epoch [16887/20000], Training Loss: 0.4940\n",
            "Epoch [16888/20000], Training Loss: 0.4739\n",
            "Epoch [16889/20000], Training Loss: 0.4565\n",
            "Epoch [16890/20000], Training Loss: 0.4797\n",
            "Epoch [16891/20000], Training Loss: 0.5312\n",
            "Epoch [16892/20000], Training Loss: 0.4428\n",
            "Epoch [16893/20000], Training Loss: 0.4475\n",
            "Epoch [16894/20000], Training Loss: 0.4823\n",
            "Epoch [16895/20000], Training Loss: 0.5518\n",
            "Epoch [16896/20000], Training Loss: 0.5140\n",
            "Epoch [16897/20000], Training Loss: 0.4686\n",
            "Epoch [16898/20000], Training Loss: 0.5251\n",
            "Epoch [16899/20000], Training Loss: 0.5192\n",
            "Epoch [16900/20000], Training Loss: 0.5028\n",
            "Epoch [16901/20000], Training Loss: 0.4468\n",
            "Epoch [16902/20000], Training Loss: 0.4390\n",
            "Epoch [16903/20000], Training Loss: 0.5085\n",
            "Epoch [16904/20000], Training Loss: 0.4876\n",
            "Epoch [16905/20000], Training Loss: 0.4553\n",
            "Epoch [16906/20000], Training Loss: 0.4700\n",
            "Epoch [16907/20000], Training Loss: 0.4725\n",
            "Epoch [16908/20000], Training Loss: 0.4755\n",
            "Epoch [16909/20000], Training Loss: 0.5082\n",
            "Epoch [16910/20000], Training Loss: 0.5020\n",
            "Epoch [16911/20000], Training Loss: 0.4798\n",
            "Epoch [16912/20000], Training Loss: 0.5157\n",
            "Epoch [16913/20000], Training Loss: 0.5125\n",
            "Epoch [16914/20000], Training Loss: 0.5349\n",
            "Epoch [16915/20000], Training Loss: 0.5219\n",
            "Epoch [16916/20000], Training Loss: 0.4900\n",
            "Epoch [16917/20000], Training Loss: 0.5024\n",
            "Epoch [16918/20000], Training Loss: 0.4593\n",
            "Epoch [16919/20000], Training Loss: 0.5047\n",
            "Epoch [16920/20000], Training Loss: 0.4729\n",
            "Epoch [16921/20000], Training Loss: 0.4675\n",
            "Epoch [16922/20000], Training Loss: 0.5033\n",
            "Epoch [16923/20000], Training Loss: 0.4627\n",
            "Epoch [16924/20000], Training Loss: 0.5337\n",
            "Epoch [16925/20000], Training Loss: 0.4802\n",
            "Epoch [16926/20000], Training Loss: 0.4654\n",
            "Epoch [16927/20000], Training Loss: 0.4578\n",
            "Epoch [16928/20000], Training Loss: 0.5049\n",
            "Epoch [16929/20000], Training Loss: 0.4908\n",
            "Epoch [16930/20000], Training Loss: 0.4750\n",
            "Epoch [16931/20000], Training Loss: 0.4620\n",
            "Epoch [16932/20000], Training Loss: 0.5128\n",
            "Epoch [16933/20000], Training Loss: 0.5273\n",
            "Epoch [16934/20000], Training Loss: 0.4707\n",
            "Epoch [16935/20000], Training Loss: 0.4671\n",
            "Epoch [16936/20000], Training Loss: 0.4714\n",
            "Epoch [16937/20000], Training Loss: 0.4727\n",
            "Epoch [16938/20000], Training Loss: 0.5226\n",
            "Epoch [16939/20000], Training Loss: 0.4505\n",
            "Epoch [16940/20000], Training Loss: 0.5011\n",
            "Epoch [16941/20000], Training Loss: 0.4899\n",
            "Epoch [16942/20000], Training Loss: 0.4702\n",
            "Epoch [16943/20000], Training Loss: 0.4871\n",
            "Epoch [16944/20000], Training Loss: 0.4684\n",
            "Epoch [16945/20000], Training Loss: 0.4975\n",
            "Epoch [16946/20000], Training Loss: 0.4973\n",
            "Epoch [16947/20000], Training Loss: 0.4657\n",
            "Epoch [16948/20000], Training Loss: 0.5369\n",
            "Epoch [16949/20000], Training Loss: 0.4390\n",
            "Epoch [16950/20000], Training Loss: 0.4433\n",
            "Epoch [16951/20000], Training Loss: 0.4956\n",
            "Epoch [16952/20000], Training Loss: 0.5149\n",
            "Epoch [16953/20000], Training Loss: 0.4864\n",
            "Epoch [16954/20000], Training Loss: 0.5029\n",
            "Epoch [16955/20000], Training Loss: 0.4995\n",
            "Epoch [16956/20000], Training Loss: 0.4850\n",
            "Epoch [16957/20000], Training Loss: 0.4994\n",
            "Epoch [16958/20000], Training Loss: 0.5185\n",
            "Epoch [16959/20000], Training Loss: 0.4896\n",
            "Epoch [16960/20000], Training Loss: 0.4886\n",
            "Epoch [16961/20000], Training Loss: 0.5093\n",
            "Epoch [16962/20000], Training Loss: 0.4884\n",
            "Epoch [16963/20000], Training Loss: 0.5084\n",
            "Epoch [16964/20000], Training Loss: 0.4797\n",
            "Epoch [16965/20000], Training Loss: 0.4866\n",
            "Epoch [16966/20000], Training Loss: 0.5129\n",
            "Epoch [16967/20000], Training Loss: 0.4455\n",
            "Epoch [16968/20000], Training Loss: 0.4941\n",
            "Epoch [16969/20000], Training Loss: 0.4631\n",
            "Epoch [16970/20000], Training Loss: 0.4872\n",
            "Epoch [16971/20000], Training Loss: 0.5025\n",
            "Epoch [16972/20000], Training Loss: 0.4730\n",
            "Epoch [16973/20000], Training Loss: 0.4641\n",
            "Epoch [16974/20000], Training Loss: 0.4904\n",
            "Epoch [16975/20000], Training Loss: 0.5137\n",
            "Epoch [16976/20000], Training Loss: 0.4892\n",
            "Epoch [16977/20000], Training Loss: 0.4707\n",
            "Epoch [16978/20000], Training Loss: 0.4339\n",
            "Epoch [16979/20000], Training Loss: 0.4922\n",
            "Epoch [16980/20000], Training Loss: 0.4684\n",
            "Epoch [16981/20000], Training Loss: 0.4687\n",
            "Epoch [16982/20000], Training Loss: 0.4849\n",
            "Epoch [16983/20000], Training Loss: 0.4513\n",
            "Epoch [16984/20000], Training Loss: 0.4855\n",
            "Epoch [16985/20000], Training Loss: 0.5191\n",
            "Epoch [16986/20000], Training Loss: 0.5031\n",
            "Epoch [16987/20000], Training Loss: 0.4905\n",
            "Epoch [16988/20000], Training Loss: 0.5040\n",
            "Epoch [16989/20000], Training Loss: 0.4859\n",
            "Epoch [16990/20000], Training Loss: 0.5302\n",
            "Epoch [16991/20000], Training Loss: 0.4963\n",
            "Epoch [16992/20000], Training Loss: 0.4954\n",
            "Epoch [16993/20000], Training Loss: 0.4772\n",
            "Epoch [16994/20000], Training Loss: 0.4611\n",
            "Epoch [16995/20000], Training Loss: 0.5066\n",
            "Epoch [16996/20000], Training Loss: 0.4532\n",
            "Epoch [16997/20000], Training Loss: 0.4799\n",
            "Epoch [16998/20000], Training Loss: 0.5124\n",
            "Epoch [16999/20000], Training Loss: 0.4632\n",
            "Epoch [17000/20000], Training Loss: 0.5015\n",
            "Epoch [17001/20000], Training Loss: 0.4855\n",
            "Epoch [17002/20000], Training Loss: 0.4864\n",
            "Epoch [17003/20000], Training Loss: 0.4393\n",
            "Epoch [17004/20000], Training Loss: 0.5108\n",
            "Epoch [17005/20000], Training Loss: 0.5126\n",
            "Epoch [17006/20000], Training Loss: 0.5388\n",
            "Epoch [17007/20000], Training Loss: 0.5049\n",
            "Epoch [17008/20000], Training Loss: 0.5139\n",
            "Epoch [17009/20000], Training Loss: 0.4956\n",
            "Epoch [17010/20000], Training Loss: 0.4399\n",
            "Epoch [17011/20000], Training Loss: 0.5074\n",
            "Epoch [17012/20000], Training Loss: 0.4793\n",
            "Epoch [17013/20000], Training Loss: 0.4626\n",
            "Epoch [17014/20000], Training Loss: 0.4652\n",
            "Epoch [17015/20000], Training Loss: 0.4465\n",
            "Epoch [17016/20000], Training Loss: 0.4834\n",
            "Epoch [17017/20000], Training Loss: 0.4663\n",
            "Epoch [17018/20000], Training Loss: 0.4946\n",
            "Epoch [17019/20000], Training Loss: 0.5279\n",
            "Epoch [17020/20000], Training Loss: 0.4842\n",
            "Epoch [17021/20000], Training Loss: 0.5013\n",
            "Epoch [17022/20000], Training Loss: 0.5087\n",
            "Epoch [17023/20000], Training Loss: 0.5130\n",
            "Epoch [17024/20000], Training Loss: 0.5117\n",
            "Epoch [17025/20000], Training Loss: 0.4573\n",
            "Epoch [17026/20000], Training Loss: 0.4470\n",
            "Epoch [17027/20000], Training Loss: 0.5057\n",
            "Epoch [17028/20000], Training Loss: 0.5079\n",
            "Epoch [17029/20000], Training Loss: 0.4853\n",
            "Epoch [17030/20000], Training Loss: 0.4725\n",
            "Epoch [17031/20000], Training Loss: 0.4679\n",
            "Epoch [17032/20000], Training Loss: 0.4747\n",
            "Epoch [17033/20000], Training Loss: 0.4542\n",
            "Epoch [17034/20000], Training Loss: 0.4429\n",
            "Epoch [17035/20000], Training Loss: 0.5024\n",
            "Epoch [17036/20000], Training Loss: 0.5223\n",
            "Epoch [17037/20000], Training Loss: 0.5048\n",
            "Epoch [17038/20000], Training Loss: 0.4849\n",
            "Epoch [17039/20000], Training Loss: 0.5065\n",
            "Epoch [17040/20000], Training Loss: 0.4681\n",
            "Epoch [17041/20000], Training Loss: 0.5175\n",
            "Epoch [17042/20000], Training Loss: 0.5160\n",
            "Epoch [17043/20000], Training Loss: 0.4760\n",
            "Epoch [17044/20000], Training Loss: 0.4935\n",
            "Epoch [17045/20000], Training Loss: 0.4462\n",
            "Epoch [17046/20000], Training Loss: 0.5388\n",
            "Epoch [17047/20000], Training Loss: 0.5072\n",
            "Epoch [17048/20000], Training Loss: 0.5114\n",
            "Epoch [17049/20000], Training Loss: 0.4959\n",
            "Epoch [17050/20000], Training Loss: 0.4897\n",
            "Epoch [17051/20000], Training Loss: 0.4753\n",
            "Epoch [17052/20000], Training Loss: 0.4430\n",
            "Epoch [17053/20000], Training Loss: 0.4695\n",
            "Epoch [17054/20000], Training Loss: 0.5160\n",
            "Epoch [17055/20000], Training Loss: 0.5071\n",
            "Epoch [17056/20000], Training Loss: 0.4823\n",
            "Epoch [17057/20000], Training Loss: 0.4703\n",
            "Epoch [17058/20000], Training Loss: 0.4678\n",
            "Epoch [17059/20000], Training Loss: 0.4917\n",
            "Epoch [17060/20000], Training Loss: 0.4759\n",
            "Epoch [17061/20000], Training Loss: 0.4942\n",
            "Epoch [17062/20000], Training Loss: 0.4708\n",
            "Epoch [17063/20000], Training Loss: 0.5289\n",
            "Epoch [17064/20000], Training Loss: 0.4715\n",
            "Epoch [17065/20000], Training Loss: 0.4837\n",
            "Epoch [17066/20000], Training Loss: 0.4995\n",
            "Epoch [17067/20000], Training Loss: 0.4347\n",
            "Epoch [17068/20000], Training Loss: 0.4750\n",
            "Epoch [17069/20000], Training Loss: 0.4919\n",
            "Epoch [17070/20000], Training Loss: 0.5173\n",
            "Epoch [17071/20000], Training Loss: 0.4671\n",
            "Epoch [17072/20000], Training Loss: 0.4890\n",
            "Epoch [17073/20000], Training Loss: 0.5347\n",
            "Epoch [17074/20000], Training Loss: 0.4745\n",
            "Epoch [17075/20000], Training Loss: 0.4671\n",
            "Epoch [17076/20000], Training Loss: 0.5380\n",
            "Epoch [17077/20000], Training Loss: 0.4644\n",
            "Epoch [17078/20000], Training Loss: 0.4851\n",
            "Epoch [17079/20000], Training Loss: 0.4748\n",
            "Epoch [17080/20000], Training Loss: 0.4847\n",
            "Epoch [17081/20000], Training Loss: 0.4681\n",
            "Epoch [17082/20000], Training Loss: 0.4735\n",
            "Epoch [17083/20000], Training Loss: 0.4596\n",
            "Epoch [17084/20000], Training Loss: 0.4457\n",
            "Epoch [17085/20000], Training Loss: 0.4855\n",
            "Epoch [17086/20000], Training Loss: 0.4639\n",
            "Epoch [17087/20000], Training Loss: 0.5155\n",
            "Epoch [17088/20000], Training Loss: 0.5106\n",
            "Epoch [17089/20000], Training Loss: 0.5073\n",
            "Epoch [17090/20000], Training Loss: 0.4665\n",
            "Epoch [17091/20000], Training Loss: 0.4911\n",
            "Epoch [17092/20000], Training Loss: 0.4999\n",
            "Epoch [17093/20000], Training Loss: 0.4966\n",
            "Epoch [17094/20000], Training Loss: 0.4699\n",
            "Epoch [17095/20000], Training Loss: 0.5019\n",
            "Epoch [17096/20000], Training Loss: 0.4441\n",
            "Epoch [17097/20000], Training Loss: 0.4865\n",
            "Epoch [17098/20000], Training Loss: 0.5172\n",
            "Epoch [17099/20000], Training Loss: 0.4804\n",
            "Epoch [17100/20000], Training Loss: 0.4720\n",
            "Epoch [17101/20000], Training Loss: 0.5059\n",
            "Epoch [17102/20000], Training Loss: 0.5147\n",
            "Epoch [17103/20000], Training Loss: 0.5082\n",
            "Epoch [17104/20000], Training Loss: 0.4763\n",
            "Epoch [17105/20000], Training Loss: 0.4670\n",
            "Epoch [17106/20000], Training Loss: 0.4940\n",
            "Epoch [17107/20000], Training Loss: 0.5307\n",
            "Epoch [17108/20000], Training Loss: 0.5331\n",
            "Epoch [17109/20000], Training Loss: 0.4336\n",
            "Epoch [17110/20000], Training Loss: 0.5026\n",
            "Epoch [17111/20000], Training Loss: 0.4621\n",
            "Epoch [17112/20000], Training Loss: 0.5014\n",
            "Epoch [17113/20000], Training Loss: 0.5060\n",
            "Epoch [17114/20000], Training Loss: 0.4481\n",
            "Epoch [17115/20000], Training Loss: 0.5346\n",
            "Epoch [17116/20000], Training Loss: 0.4685\n",
            "Epoch [17117/20000], Training Loss: 0.4568\n",
            "Epoch [17118/20000], Training Loss: 0.4771\n",
            "Epoch [17119/20000], Training Loss: 0.4934\n",
            "Epoch [17120/20000], Training Loss: 0.4706\n",
            "Epoch [17121/20000], Training Loss: 0.4926\n",
            "Epoch [17122/20000], Training Loss: 0.4786\n",
            "Epoch [17123/20000], Training Loss: 0.4641\n",
            "Epoch [17124/20000], Training Loss: 0.4789\n",
            "Epoch [17125/20000], Training Loss: 0.4830\n",
            "Epoch [17126/20000], Training Loss: 0.4618\n",
            "Epoch [17127/20000], Training Loss: 0.4950\n",
            "Epoch [17128/20000], Training Loss: 0.4815\n",
            "Epoch [17129/20000], Training Loss: 0.4813\n",
            "Epoch [17130/20000], Training Loss: 0.4529\n",
            "Epoch [17131/20000], Training Loss: 0.4938\n",
            "Epoch [17132/20000], Training Loss: 0.4636\n",
            "Epoch [17133/20000], Training Loss: 0.4785\n",
            "Epoch [17134/20000], Training Loss: 0.4937\n",
            "Epoch [17135/20000], Training Loss: 0.4616\n",
            "Epoch [17136/20000], Training Loss: 0.4855\n",
            "Epoch [17137/20000], Training Loss: 0.4990\n",
            "Epoch [17138/20000], Training Loss: 0.4982\n",
            "Epoch [17139/20000], Training Loss: 0.4504\n",
            "Epoch [17140/20000], Training Loss: 0.4486\n",
            "Epoch [17141/20000], Training Loss: 0.5167\n",
            "Epoch [17142/20000], Training Loss: 0.5061\n",
            "Epoch [17143/20000], Training Loss: 0.4945\n",
            "Epoch [17144/20000], Training Loss: 0.4465\n",
            "Epoch [17145/20000], Training Loss: 0.4578\n",
            "Epoch [17146/20000], Training Loss: 0.5037\n",
            "Epoch [17147/20000], Training Loss: 0.4618\n",
            "Epoch [17148/20000], Training Loss: 0.4666\n",
            "Epoch [17149/20000], Training Loss: 0.4516\n",
            "Epoch [17150/20000], Training Loss: 0.4770\n",
            "Epoch [17151/20000], Training Loss: 0.5401\n",
            "Epoch [17152/20000], Training Loss: 0.5032\n",
            "Epoch [17153/20000], Training Loss: 0.5144\n",
            "Epoch [17154/20000], Training Loss: 0.4922\n",
            "Epoch [17155/20000], Training Loss: 0.4981\n",
            "Epoch [17156/20000], Training Loss: 0.5033\n",
            "Epoch [17157/20000], Training Loss: 0.5001\n",
            "Epoch [17158/20000], Training Loss: 0.5081\n",
            "Epoch [17159/20000], Training Loss: 0.5236\n",
            "Epoch [17160/20000], Training Loss: 0.4700\n",
            "Epoch [17161/20000], Training Loss: 0.5048\n",
            "Epoch [17162/20000], Training Loss: 0.4438\n",
            "Epoch [17163/20000], Training Loss: 0.5208\n",
            "Epoch [17164/20000], Training Loss: 0.4897\n",
            "Epoch [17165/20000], Training Loss: 0.4730\n",
            "Epoch [17166/20000], Training Loss: 0.4644\n",
            "Epoch [17167/20000], Training Loss: 0.4950\n",
            "Epoch [17168/20000], Training Loss: 0.5228\n",
            "Epoch [17169/20000], Training Loss: 0.4775\n",
            "Epoch [17170/20000], Training Loss: 0.5376\n",
            "Epoch [17171/20000], Training Loss: 0.5213\n",
            "Epoch [17172/20000], Training Loss: 0.4872\n",
            "Epoch [17173/20000], Training Loss: 0.5068\n",
            "Epoch [17174/20000], Training Loss: 0.4696\n",
            "Epoch [17175/20000], Training Loss: 0.4600\n",
            "Epoch [17176/20000], Training Loss: 0.5292\n",
            "Epoch [17177/20000], Training Loss: 0.4666\n",
            "Epoch [17178/20000], Training Loss: 0.4623\n",
            "Epoch [17179/20000], Training Loss: 0.4859\n",
            "Epoch [17180/20000], Training Loss: 0.4646\n",
            "Epoch [17181/20000], Training Loss: 0.5053\n",
            "Epoch [17182/20000], Training Loss: 0.5200\n",
            "Epoch [17183/20000], Training Loss: 0.4617\n",
            "Epoch [17184/20000], Training Loss: 0.5059\n",
            "Epoch [17185/20000], Training Loss: 0.4724\n",
            "Epoch [17186/20000], Training Loss: 0.4907\n",
            "Epoch [17187/20000], Training Loss: 0.5028\n",
            "Epoch [17188/20000], Training Loss: 0.5357\n",
            "Epoch [17189/20000], Training Loss: 0.4337\n",
            "Epoch [17190/20000], Training Loss: 0.5256\n",
            "Epoch [17191/20000], Training Loss: 0.4893\n",
            "Epoch [17192/20000], Training Loss: 0.4964\n",
            "Epoch [17193/20000], Training Loss: 0.4513\n",
            "Epoch [17194/20000], Training Loss: 0.4826\n",
            "Epoch [17195/20000], Training Loss: 0.4688\n",
            "Epoch [17196/20000], Training Loss: 0.4807\n",
            "Epoch [17197/20000], Training Loss: 0.4682\n",
            "Epoch [17198/20000], Training Loss: 0.4954\n",
            "Epoch [17199/20000], Training Loss: 0.4912\n",
            "Epoch [17200/20000], Training Loss: 0.5365\n",
            "Epoch [17201/20000], Training Loss: 0.5373\n",
            "Epoch [17202/20000], Training Loss: 0.4703\n",
            "Epoch [17203/20000], Training Loss: 0.4820\n",
            "Epoch [17204/20000], Training Loss: 0.5027\n",
            "Epoch [17205/20000], Training Loss: 0.4323\n",
            "Epoch [17206/20000], Training Loss: 0.4882\n",
            "Epoch [17207/20000], Training Loss: 0.4485\n",
            "Epoch [17208/20000], Training Loss: 0.4658\n",
            "Epoch [17209/20000], Training Loss: 0.4888\n",
            "Epoch [17210/20000], Training Loss: 0.4318\n",
            "Epoch [17211/20000], Training Loss: 0.4818\n",
            "Epoch [17212/20000], Training Loss: 0.4717\n",
            "Epoch [17213/20000], Training Loss: 0.4783\n",
            "Epoch [17214/20000], Training Loss: 0.4531\n",
            "Epoch [17215/20000], Training Loss: 0.4846\n",
            "Epoch [17216/20000], Training Loss: 0.4736\n",
            "Epoch [17217/20000], Training Loss: 0.5000\n",
            "Epoch [17218/20000], Training Loss: 0.4656\n",
            "Epoch [17219/20000], Training Loss: 0.4880\n",
            "Epoch [17220/20000], Training Loss: 0.4692\n",
            "Epoch [17221/20000], Training Loss: 0.5028\n",
            "Epoch [17222/20000], Training Loss: 0.5161\n",
            "Epoch [17223/20000], Training Loss: 0.4789\n",
            "Epoch [17224/20000], Training Loss: 0.4824\n",
            "Epoch [17225/20000], Training Loss: 0.5087\n",
            "Epoch [17226/20000], Training Loss: 0.4763\n",
            "Epoch [17227/20000], Training Loss: 0.5101\n",
            "Epoch [17228/20000], Training Loss: 0.4848\n",
            "Epoch [17229/20000], Training Loss: 0.5006\n",
            "Epoch [17230/20000], Training Loss: 0.5051\n",
            "Epoch [17231/20000], Training Loss: 0.4571\n",
            "Epoch [17232/20000], Training Loss: 0.4634\n",
            "Epoch [17233/20000], Training Loss: 0.4397\n",
            "Epoch [17234/20000], Training Loss: 0.5293\n",
            "Epoch [17235/20000], Training Loss: 0.5188\n",
            "Epoch [17236/20000], Training Loss: 0.5034\n",
            "Epoch [17237/20000], Training Loss: 0.4845\n",
            "Epoch [17238/20000], Training Loss: 0.4931\n",
            "Epoch [17239/20000], Training Loss: 0.4760\n",
            "Epoch [17240/20000], Training Loss: 0.5005\n",
            "Epoch [17241/20000], Training Loss: 0.4858\n",
            "Epoch [17242/20000], Training Loss: 0.4830\n",
            "Epoch [17243/20000], Training Loss: 0.4521\n",
            "Epoch [17244/20000], Training Loss: 0.4870\n",
            "Epoch [17245/20000], Training Loss: 0.4466\n",
            "Epoch [17246/20000], Training Loss: 0.5337\n",
            "Epoch [17247/20000], Training Loss: 0.4683\n",
            "Epoch [17248/20000], Training Loss: 0.4751\n",
            "Epoch [17249/20000], Training Loss: 0.4960\n",
            "Epoch [17250/20000], Training Loss: 0.4644\n",
            "Epoch [17251/20000], Training Loss: 0.4823\n",
            "Epoch [17252/20000], Training Loss: 0.5192\n",
            "Epoch [17253/20000], Training Loss: 0.4602\n",
            "Epoch [17254/20000], Training Loss: 0.4823\n",
            "Epoch [17255/20000], Training Loss: 0.4736\n",
            "Epoch [17256/20000], Training Loss: 0.4928\n",
            "Epoch [17257/20000], Training Loss: 0.4948\n",
            "Epoch [17258/20000], Training Loss: 0.4552\n",
            "Epoch [17259/20000], Training Loss: 0.4968\n",
            "Epoch [17260/20000], Training Loss: 0.4794\n",
            "Epoch [17261/20000], Training Loss: 0.4929\n",
            "Epoch [17262/20000], Training Loss: 0.4499\n",
            "Epoch [17263/20000], Training Loss: 0.4684\n",
            "Epoch [17264/20000], Training Loss: 0.4908\n",
            "Epoch [17265/20000], Training Loss: 0.4897\n",
            "Epoch [17266/20000], Training Loss: 0.4897\n",
            "Epoch [17267/20000], Training Loss: 0.4829\n",
            "Epoch [17268/20000], Training Loss: 0.4699\n",
            "Epoch [17269/20000], Training Loss: 0.5104\n",
            "Epoch [17270/20000], Training Loss: 0.5086\n",
            "Epoch [17271/20000], Training Loss: 0.5075\n",
            "Epoch [17272/20000], Training Loss: 0.4598\n",
            "Epoch [17273/20000], Training Loss: 0.5156\n",
            "Epoch [17274/20000], Training Loss: 0.5106\n",
            "Epoch [17275/20000], Training Loss: 0.4745\n",
            "Epoch [17276/20000], Training Loss: 0.4621\n",
            "Epoch [17277/20000], Training Loss: 0.5248\n",
            "Epoch [17278/20000], Training Loss: 0.4552\n",
            "Epoch [17279/20000], Training Loss: 0.4773\n",
            "Epoch [17280/20000], Training Loss: 0.4748\n",
            "Epoch [17281/20000], Training Loss: 0.4523\n",
            "Epoch [17282/20000], Training Loss: 0.4608\n",
            "Epoch [17283/20000], Training Loss: 0.4770\n",
            "Epoch [17284/20000], Training Loss: 0.4545\n",
            "Epoch [17285/20000], Training Loss: 0.5076\n",
            "Epoch [17286/20000], Training Loss: 0.5121\n",
            "Epoch [17287/20000], Training Loss: 0.4848\n",
            "Epoch [17288/20000], Training Loss: 0.4457\n",
            "Epoch [17289/20000], Training Loss: 0.4802\n",
            "Epoch [17290/20000], Training Loss: 0.5317\n",
            "Epoch [17291/20000], Training Loss: 0.4652\n",
            "Epoch [17292/20000], Training Loss: 0.4675\n",
            "Epoch [17293/20000], Training Loss: 0.4494\n",
            "Epoch [17294/20000], Training Loss: 0.5290\n",
            "Epoch [17295/20000], Training Loss: 0.5183\n",
            "Epoch [17296/20000], Training Loss: 0.4242\n",
            "Epoch [17297/20000], Training Loss: 0.4686\n",
            "Epoch [17298/20000], Training Loss: 0.4526\n",
            "Epoch [17299/20000], Training Loss: 0.4724\n",
            "Epoch [17300/20000], Training Loss: 0.5216\n",
            "Epoch [17301/20000], Training Loss: 0.4858\n",
            "Epoch [17302/20000], Training Loss: 0.4539\n",
            "Epoch [17303/20000], Training Loss: 0.4728\n",
            "Epoch [17304/20000], Training Loss: 0.4551\n",
            "Epoch [17305/20000], Training Loss: 0.5093\n",
            "Epoch [17306/20000], Training Loss: 0.4628\n",
            "Epoch [17307/20000], Training Loss: 0.4448\n",
            "Epoch [17308/20000], Training Loss: 0.4719\n",
            "Epoch [17309/20000], Training Loss: 0.5249\n",
            "Epoch [17310/20000], Training Loss: 0.4780\n",
            "Epoch [17311/20000], Training Loss: 0.4745\n",
            "Epoch [17312/20000], Training Loss: 0.4962\n",
            "Epoch [17313/20000], Training Loss: 0.4786\n",
            "Epoch [17314/20000], Training Loss: 0.5056\n",
            "Epoch [17315/20000], Training Loss: 0.5114\n",
            "Epoch [17316/20000], Training Loss: 0.4949\n",
            "Epoch [17317/20000], Training Loss: 0.4823\n",
            "Epoch [17318/20000], Training Loss: 0.4949\n",
            "Epoch [17319/20000], Training Loss: 0.4909\n",
            "Epoch [17320/20000], Training Loss: 0.5060\n",
            "Epoch [17321/20000], Training Loss: 0.5034\n",
            "Epoch [17322/20000], Training Loss: 0.4746\n",
            "Epoch [17323/20000], Training Loss: 0.4821\n",
            "Epoch [17324/20000], Training Loss: 0.4544\n",
            "Epoch [17325/20000], Training Loss: 0.4940\n",
            "Epoch [17326/20000], Training Loss: 0.5016\n",
            "Epoch [17327/20000], Training Loss: 0.4589\n",
            "Epoch [17328/20000], Training Loss: 0.5254\n",
            "Epoch [17329/20000], Training Loss: 0.4925\n",
            "Epoch [17330/20000], Training Loss: 0.4876\n",
            "Epoch [17331/20000], Training Loss: 0.5253\n",
            "Epoch [17332/20000], Training Loss: 0.4659\n",
            "Epoch [17333/20000], Training Loss: 0.5089\n",
            "Epoch [17334/20000], Training Loss: 0.4799\n",
            "Epoch [17335/20000], Training Loss: 0.4993\n",
            "Epoch [17336/20000], Training Loss: 0.5333\n",
            "Epoch [17337/20000], Training Loss: 0.4389\n",
            "Epoch [17338/20000], Training Loss: 0.4753\n",
            "Epoch [17339/20000], Training Loss: 0.4910\n",
            "Epoch [17340/20000], Training Loss: 0.5003\n",
            "Epoch [17341/20000], Training Loss: 0.4815\n",
            "Epoch [17342/20000], Training Loss: 0.5076\n",
            "Epoch [17343/20000], Training Loss: 0.4685\n",
            "Epoch [17344/20000], Training Loss: 0.4818\n",
            "Epoch [17345/20000], Training Loss: 0.4977\n",
            "Epoch [17346/20000], Training Loss: 0.4827\n",
            "Epoch [17347/20000], Training Loss: 0.5005\n",
            "Epoch [17348/20000], Training Loss: 0.4572\n",
            "Epoch [17349/20000], Training Loss: 0.5004\n",
            "Epoch [17350/20000], Training Loss: 0.5249\n",
            "Epoch [17351/20000], Training Loss: 0.4963\n",
            "Epoch [17352/20000], Training Loss: 0.4803\n",
            "Epoch [17353/20000], Training Loss: 0.4800\n",
            "Epoch [17354/20000], Training Loss: 0.4905\n",
            "Epoch [17355/20000], Training Loss: 0.4677\n",
            "Epoch [17356/20000], Training Loss: 0.5415\n",
            "Epoch [17357/20000], Training Loss: 0.4320\n",
            "Epoch [17358/20000], Training Loss: 0.4593\n",
            "Epoch [17359/20000], Training Loss: 0.5291\n",
            "Epoch [17360/20000], Training Loss: 0.4615\n",
            "Epoch [17361/20000], Training Loss: 0.4937\n",
            "Epoch [17362/20000], Training Loss: 0.4593\n",
            "Epoch [17363/20000], Training Loss: 0.4602\n",
            "Epoch [17364/20000], Training Loss: 0.4932\n",
            "Epoch [17365/20000], Training Loss: 0.4729\n",
            "Epoch [17366/20000], Training Loss: 0.4496\n",
            "Epoch [17367/20000], Training Loss: 0.4423\n",
            "Epoch [17368/20000], Training Loss: 0.5187\n",
            "Epoch [17369/20000], Training Loss: 0.4804\n",
            "Epoch [17370/20000], Training Loss: 0.4832\n",
            "Epoch [17371/20000], Training Loss: 0.4963\n",
            "Epoch [17372/20000], Training Loss: 0.4911\n",
            "Epoch [17373/20000], Training Loss: 0.4603\n",
            "Epoch [17374/20000], Training Loss: 0.4747\n",
            "Epoch [17375/20000], Training Loss: 0.4554\n",
            "Epoch [17376/20000], Training Loss: 0.4788\n",
            "Epoch [17377/20000], Training Loss: 0.4644\n",
            "Epoch [17378/20000], Training Loss: 0.5381\n",
            "Epoch [17379/20000], Training Loss: 0.4455\n",
            "Epoch [17380/20000], Training Loss: 0.4837\n",
            "Epoch [17381/20000], Training Loss: 0.5125\n",
            "Epoch [17382/20000], Training Loss: 0.4805\n",
            "Epoch [17383/20000], Training Loss: 0.4840\n",
            "Epoch [17384/20000], Training Loss: 0.4601\n",
            "Epoch [17385/20000], Training Loss: 0.5149\n",
            "Epoch [17386/20000], Training Loss: 0.4592\n",
            "Epoch [17387/20000], Training Loss: 0.4950\n",
            "Epoch [17388/20000], Training Loss: 0.4690\n",
            "Epoch [17389/20000], Training Loss: 0.4959\n",
            "Epoch [17390/20000], Training Loss: 0.5151\n",
            "Epoch [17391/20000], Training Loss: 0.5001\n",
            "Epoch [17392/20000], Training Loss: 0.5128\n",
            "Epoch [17393/20000], Training Loss: 0.4758\n",
            "Epoch [17394/20000], Training Loss: 0.4702\n",
            "Epoch [17395/20000], Training Loss: 0.4879\n",
            "Epoch [17396/20000], Training Loss: 0.4768\n",
            "Epoch [17397/20000], Training Loss: 0.5065\n",
            "Epoch [17398/20000], Training Loss: 0.4860\n",
            "Epoch [17399/20000], Training Loss: 0.5243\n",
            "Epoch [17400/20000], Training Loss: 0.4736\n",
            "Epoch [17401/20000], Training Loss: 0.5063\n",
            "Epoch [17402/20000], Training Loss: 0.4822\n",
            "Epoch [17403/20000], Training Loss: 0.4637\n",
            "Epoch [17404/20000], Training Loss: 0.4534\n",
            "Epoch [17405/20000], Training Loss: 0.4471\n",
            "Epoch [17406/20000], Training Loss: 0.4679\n",
            "Epoch [17407/20000], Training Loss: 0.5248\n",
            "Epoch [17408/20000], Training Loss: 0.4778\n",
            "Epoch [17409/20000], Training Loss: 0.4586\n",
            "Epoch [17410/20000], Training Loss: 0.4726\n",
            "Epoch [17411/20000], Training Loss: 0.4889\n",
            "Epoch [17412/20000], Training Loss: 0.4895\n",
            "Epoch [17413/20000], Training Loss: 0.4765\n",
            "Epoch [17414/20000], Training Loss: 0.5301\n",
            "Epoch [17415/20000], Training Loss: 0.4679\n",
            "Epoch [17416/20000], Training Loss: 0.4574\n",
            "Epoch [17417/20000], Training Loss: 0.4890\n",
            "Epoch [17418/20000], Training Loss: 0.4530\n",
            "Epoch [17419/20000], Training Loss: 0.4936\n",
            "Epoch [17420/20000], Training Loss: 0.4712\n",
            "Epoch [17421/20000], Training Loss: 0.5009\n",
            "Epoch [17422/20000], Training Loss: 0.4710\n",
            "Epoch [17423/20000], Training Loss: 0.4565\n",
            "Epoch [17424/20000], Training Loss: 0.4975\n",
            "Epoch [17425/20000], Training Loss: 0.5037\n",
            "Epoch [17426/20000], Training Loss: 0.4462\n",
            "Epoch [17427/20000], Training Loss: 0.4720\n",
            "Epoch [17428/20000], Training Loss: 0.4578\n",
            "Epoch [17429/20000], Training Loss: 0.4498\n",
            "Epoch [17430/20000], Training Loss: 0.4892\n",
            "Epoch [17431/20000], Training Loss: 0.4754\n",
            "Epoch [17432/20000], Training Loss: 0.5393\n",
            "Epoch [17433/20000], Training Loss: 0.5356\n",
            "Epoch [17434/20000], Training Loss: 0.5427\n",
            "Epoch [17435/20000], Training Loss: 0.4919\n",
            "Epoch [17436/20000], Training Loss: 0.5170\n",
            "Epoch [17437/20000], Training Loss: 0.5202\n",
            "Epoch [17438/20000], Training Loss: 0.5337\n",
            "Epoch [17439/20000], Training Loss: 0.4773\n",
            "Epoch [17440/20000], Training Loss: 0.4740\n",
            "Epoch [17441/20000], Training Loss: 0.4575\n",
            "Epoch [17442/20000], Training Loss: 0.4803\n",
            "Epoch [17443/20000], Training Loss: 0.4706\n",
            "Epoch [17444/20000], Training Loss: 0.5302\n",
            "Epoch [17445/20000], Training Loss: 0.4701\n",
            "Epoch [17446/20000], Training Loss: 0.4552\n",
            "Epoch [17447/20000], Training Loss: 0.4950\n",
            "Epoch [17448/20000], Training Loss: 0.4983\n",
            "Epoch [17449/20000], Training Loss: 0.5149\n",
            "Epoch [17450/20000], Training Loss: 0.4588\n",
            "Epoch [17451/20000], Training Loss: 0.5074\n",
            "Epoch [17452/20000], Training Loss: 0.4488\n",
            "Epoch [17453/20000], Training Loss: 0.4929\n",
            "Epoch [17454/20000], Training Loss: 0.4533\n",
            "Epoch [17455/20000], Training Loss: 0.5431\n",
            "Epoch [17456/20000], Training Loss: 0.4566\n",
            "Epoch [17457/20000], Training Loss: 0.4900\n",
            "Epoch [17458/20000], Training Loss: 0.4843\n",
            "Epoch [17459/20000], Training Loss: 0.4509\n",
            "Epoch [17460/20000], Training Loss: 0.4894\n",
            "Epoch [17461/20000], Training Loss: 0.4765\n",
            "Epoch [17462/20000], Training Loss: 0.5512\n",
            "Epoch [17463/20000], Training Loss: 0.4577\n",
            "Epoch [17464/20000], Training Loss: 0.4995\n",
            "Epoch [17465/20000], Training Loss: 0.4960\n",
            "Epoch [17466/20000], Training Loss: 0.4746\n",
            "Epoch [17467/20000], Training Loss: 0.4896\n",
            "Epoch [17468/20000], Training Loss: 0.4656\n",
            "Epoch [17469/20000], Training Loss: 0.4264\n",
            "Epoch [17470/20000], Training Loss: 0.4751\n",
            "Epoch [17471/20000], Training Loss: 0.4861\n",
            "Epoch [17472/20000], Training Loss: 0.4632\n",
            "Epoch [17473/20000], Training Loss: 0.4820\n",
            "Epoch [17474/20000], Training Loss: 0.4792\n",
            "Epoch [17475/20000], Training Loss: 0.4955\n",
            "Epoch [17476/20000], Training Loss: 0.4846\n",
            "Epoch [17477/20000], Training Loss: 0.4975\n",
            "Epoch [17478/20000], Training Loss: 0.5029\n",
            "Epoch [17479/20000], Training Loss: 0.4992\n",
            "Epoch [17480/20000], Training Loss: 0.4761\n",
            "Epoch [17481/20000], Training Loss: 0.4684\n",
            "Epoch [17482/20000], Training Loss: 0.5094\n",
            "Epoch [17483/20000], Training Loss: 0.4937\n",
            "Epoch [17484/20000], Training Loss: 0.4819\n",
            "Epoch [17485/20000], Training Loss: 0.5183\n",
            "Epoch [17486/20000], Training Loss: 0.4697\n",
            "Epoch [17487/20000], Training Loss: 0.5183\n",
            "Epoch [17488/20000], Training Loss: 0.4829\n",
            "Epoch [17489/20000], Training Loss: 0.4818\n",
            "Epoch [17490/20000], Training Loss: 0.5196\n",
            "Epoch [17491/20000], Training Loss: 0.4490\n",
            "Epoch [17492/20000], Training Loss: 0.4972\n",
            "Epoch [17493/20000], Training Loss: 0.4881\n",
            "Epoch [17494/20000], Training Loss: 0.5522\n",
            "Epoch [17495/20000], Training Loss: 0.4394\n",
            "Epoch [17496/20000], Training Loss: 0.4753\n",
            "Epoch [17497/20000], Training Loss: 0.4820\n",
            "Epoch [17498/20000], Training Loss: 0.4680\n",
            "Epoch [17499/20000], Training Loss: 0.5332\n",
            "Epoch [17500/20000], Training Loss: 0.4952\n",
            "Epoch [17501/20000], Training Loss: 0.4614\n",
            "Epoch [17502/20000], Training Loss: 0.4840\n",
            "Epoch [17503/20000], Training Loss: 0.4820\n",
            "Epoch [17504/20000], Training Loss: 0.5303\n",
            "Epoch [17505/20000], Training Loss: 0.4607\n",
            "Epoch [17506/20000], Training Loss: 0.5180\n",
            "Epoch [17507/20000], Training Loss: 0.4710\n",
            "Epoch [17508/20000], Training Loss: 0.5130\n",
            "Epoch [17509/20000], Training Loss: 0.4538\n",
            "Epoch [17510/20000], Training Loss: 0.4473\n",
            "Epoch [17511/20000], Training Loss: 0.4915\n",
            "Epoch [17512/20000], Training Loss: 0.4832\n",
            "Epoch [17513/20000], Training Loss: 0.4897\n",
            "Epoch [17514/20000], Training Loss: 0.4628\n",
            "Epoch [17515/20000], Training Loss: 0.5273\n",
            "Epoch [17516/20000], Training Loss: 0.4704\n",
            "Epoch [17517/20000], Training Loss: 0.5304\n",
            "Epoch [17518/20000], Training Loss: 0.5295\n",
            "Epoch [17519/20000], Training Loss: 0.4673\n",
            "Epoch [17520/20000], Training Loss: 0.5135\n",
            "Epoch [17521/20000], Training Loss: 0.4806\n",
            "Epoch [17522/20000], Training Loss: 0.4522\n",
            "Epoch [17523/20000], Training Loss: 0.4612\n",
            "Epoch [17524/20000], Training Loss: 0.5345\n",
            "Epoch [17525/20000], Training Loss: 0.5065\n",
            "Epoch [17526/20000], Training Loss: 0.4645\n",
            "Epoch [17527/20000], Training Loss: 0.5056\n",
            "Epoch [17528/20000], Training Loss: 0.5028\n",
            "Epoch [17529/20000], Training Loss: 0.4912\n",
            "Epoch [17530/20000], Training Loss: 0.4723\n",
            "Epoch [17531/20000], Training Loss: 0.4632\n",
            "Epoch [17532/20000], Training Loss: 0.4541\n",
            "Epoch [17533/20000], Training Loss: 0.5305\n",
            "Epoch [17534/20000], Training Loss: 0.4764\n",
            "Epoch [17535/20000], Training Loss: 0.5055\n",
            "Epoch [17536/20000], Training Loss: 0.4529\n",
            "Epoch [17537/20000], Training Loss: 0.5142\n",
            "Epoch [17538/20000], Training Loss: 0.4662\n",
            "Epoch [17539/20000], Training Loss: 0.4509\n",
            "Epoch [17540/20000], Training Loss: 0.4961\n",
            "Epoch [17541/20000], Training Loss: 0.4285\n",
            "Epoch [17542/20000], Training Loss: 0.5126\n",
            "Epoch [17543/20000], Training Loss: 0.4616\n",
            "Epoch [17544/20000], Training Loss: 0.4648\n",
            "Epoch [17545/20000], Training Loss: 0.4499\n",
            "Epoch [17546/20000], Training Loss: 0.4963\n",
            "Epoch [17547/20000], Training Loss: 0.4475\n",
            "Epoch [17548/20000], Training Loss: 0.4764\n",
            "Epoch [17549/20000], Training Loss: 0.4902\n",
            "Epoch [17550/20000], Training Loss: 0.4918\n",
            "Epoch [17551/20000], Training Loss: 0.4641\n",
            "Epoch [17552/20000], Training Loss: 0.5544\n",
            "Epoch [17553/20000], Training Loss: 0.5090\n",
            "Epoch [17554/20000], Training Loss: 0.4827\n",
            "Epoch [17555/20000], Training Loss: 0.5069\n",
            "Epoch [17556/20000], Training Loss: 0.4818\n",
            "Epoch [17557/20000], Training Loss: 0.4795\n",
            "Epoch [17558/20000], Training Loss: 0.4891\n",
            "Epoch [17559/20000], Training Loss: 0.4589\n",
            "Epoch [17560/20000], Training Loss: 0.5264\n",
            "Epoch [17561/20000], Training Loss: 0.4739\n",
            "Epoch [17562/20000], Training Loss: 0.4850\n",
            "Epoch [17563/20000], Training Loss: 0.5080\n",
            "Epoch [17564/20000], Training Loss: 0.4710\n",
            "Epoch [17565/20000], Training Loss: 0.5066\n",
            "Epoch [17566/20000], Training Loss: 0.5157\n",
            "Epoch [17567/20000], Training Loss: 0.5127\n",
            "Epoch [17568/20000], Training Loss: 0.4734\n",
            "Epoch [17569/20000], Training Loss: 0.5161\n",
            "Epoch [17570/20000], Training Loss: 0.4709\n",
            "Epoch [17571/20000], Training Loss: 0.4902\n",
            "Epoch [17572/20000], Training Loss: 0.5335\n",
            "Epoch [17573/20000], Training Loss: 0.4467\n",
            "Epoch [17574/20000], Training Loss: 0.5046\n",
            "Epoch [17575/20000], Training Loss: 0.4556\n",
            "Epoch [17576/20000], Training Loss: 0.4770\n",
            "Epoch [17577/20000], Training Loss: 0.5079\n",
            "Epoch [17578/20000], Training Loss: 0.4572\n",
            "Epoch [17579/20000], Training Loss: 0.4699\n",
            "Epoch [17580/20000], Training Loss: 0.4829\n",
            "Epoch [17581/20000], Training Loss: 0.5182\n",
            "Epoch [17582/20000], Training Loss: 0.4606\n",
            "Epoch [17583/20000], Training Loss: 0.5081\n",
            "Epoch [17584/20000], Training Loss: 0.4796\n",
            "Epoch [17585/20000], Training Loss: 0.4746\n",
            "Epoch [17586/20000], Training Loss: 0.4841\n",
            "Epoch [17587/20000], Training Loss: 0.5381\n",
            "Epoch [17588/20000], Training Loss: 0.4879\n",
            "Epoch [17589/20000], Training Loss: 0.5047\n",
            "Epoch [17590/20000], Training Loss: 0.4889\n",
            "Epoch [17591/20000], Training Loss: 0.4874\n",
            "Epoch [17592/20000], Training Loss: 0.4991\n",
            "Epoch [17593/20000], Training Loss: 0.4972\n",
            "Epoch [17594/20000], Training Loss: 0.5396\n",
            "Epoch [17595/20000], Training Loss: 0.4609\n",
            "Epoch [17596/20000], Training Loss: 0.5137\n",
            "Epoch [17597/20000], Training Loss: 0.4779\n",
            "Epoch [17598/20000], Training Loss: 0.4840\n",
            "Epoch [17599/20000], Training Loss: 0.4593\n",
            "Epoch [17600/20000], Training Loss: 0.5002\n",
            "Epoch [17601/20000], Training Loss: 0.4766\n",
            "Epoch [17602/20000], Training Loss: 0.4551\n",
            "Epoch [17603/20000], Training Loss: 0.4985\n",
            "Epoch [17604/20000], Training Loss: 0.5244\n",
            "Epoch [17605/20000], Training Loss: 0.5247\n",
            "Epoch [17606/20000], Training Loss: 0.5177\n",
            "Epoch [17607/20000], Training Loss: 0.4817\n",
            "Epoch [17608/20000], Training Loss: 0.5123\n",
            "Epoch [17609/20000], Training Loss: 0.4604\n",
            "Epoch [17610/20000], Training Loss: 0.4780\n",
            "Epoch [17611/20000], Training Loss: 0.5051\n",
            "Epoch [17612/20000], Training Loss: 0.4775\n",
            "Epoch [17613/20000], Training Loss: 0.4804\n",
            "Epoch [17614/20000], Training Loss: 0.5042\n",
            "Epoch [17615/20000], Training Loss: 0.4866\n",
            "Epoch [17616/20000], Training Loss: 0.5471\n",
            "Epoch [17617/20000], Training Loss: 0.4983\n",
            "Epoch [17618/20000], Training Loss: 0.4724\n",
            "Epoch [17619/20000], Training Loss: 0.5119\n",
            "Epoch [17620/20000], Training Loss: 0.4596\n",
            "Epoch [17621/20000], Training Loss: 0.4454\n",
            "Epoch [17622/20000], Training Loss: 0.4995\n",
            "Epoch [17623/20000], Training Loss: 0.4931\n",
            "Epoch [17624/20000], Training Loss: 0.4917\n",
            "Epoch [17625/20000], Training Loss: 0.4531\n",
            "Epoch [17626/20000], Training Loss: 0.4969\n",
            "Epoch [17627/20000], Training Loss: 0.5123\n",
            "Epoch [17628/20000], Training Loss: 0.5068\n",
            "Epoch [17629/20000], Training Loss: 0.5221\n",
            "Epoch [17630/20000], Training Loss: 0.5097\n",
            "Epoch [17631/20000], Training Loss: 0.5210\n",
            "Epoch [17632/20000], Training Loss: 0.4813\n",
            "Epoch [17633/20000], Training Loss: 0.4912\n",
            "Epoch [17634/20000], Training Loss: 0.4880\n",
            "Epoch [17635/20000], Training Loss: 0.4755\n",
            "Epoch [17636/20000], Training Loss: 0.5225\n",
            "Epoch [17637/20000], Training Loss: 0.5008\n",
            "Epoch [17638/20000], Training Loss: 0.4997\n",
            "Epoch [17639/20000], Training Loss: 0.5035\n",
            "Epoch [17640/20000], Training Loss: 0.4590\n",
            "Epoch [17641/20000], Training Loss: 0.4875\n",
            "Epoch [17642/20000], Training Loss: 0.4749\n",
            "Epoch [17643/20000], Training Loss: 0.4571\n",
            "Epoch [17644/20000], Training Loss: 0.4624\n",
            "Epoch [17645/20000], Training Loss: 0.5304\n",
            "Epoch [17646/20000], Training Loss: 0.5033\n",
            "Epoch [17647/20000], Training Loss: 0.4572\n",
            "Epoch [17648/20000], Training Loss: 0.4828\n",
            "Epoch [17649/20000], Training Loss: 0.4474\n",
            "Epoch [17650/20000], Training Loss: 0.5043\n",
            "Epoch [17651/20000], Training Loss: 0.4866\n",
            "Epoch [17652/20000], Training Loss: 0.4953\n",
            "Epoch [17653/20000], Training Loss: 0.5001\n",
            "Epoch [17654/20000], Training Loss: 0.5020\n",
            "Epoch [17655/20000], Training Loss: 0.5073\n",
            "Epoch [17656/20000], Training Loss: 0.4758\n",
            "Epoch [17657/20000], Training Loss: 0.4938\n",
            "Epoch [17658/20000], Training Loss: 0.4753\n",
            "Epoch [17659/20000], Training Loss: 0.4785\n",
            "Epoch [17660/20000], Training Loss: 0.4670\n",
            "Epoch [17661/20000], Training Loss: 0.5135\n",
            "Epoch [17662/20000], Training Loss: 0.4785\n",
            "Epoch [17663/20000], Training Loss: 0.4937\n",
            "Epoch [17664/20000], Training Loss: 0.4799\n",
            "Epoch [17665/20000], Training Loss: 0.4875\n",
            "Epoch [17666/20000], Training Loss: 0.5275\n",
            "Epoch [17667/20000], Training Loss: 0.4640\n",
            "Epoch [17668/20000], Training Loss: 0.5016\n",
            "Epoch [17669/20000], Training Loss: 0.4679\n",
            "Epoch [17670/20000], Training Loss: 0.4735\n",
            "Epoch [17671/20000], Training Loss: 0.4693\n",
            "Epoch [17672/20000], Training Loss: 0.5207\n",
            "Epoch [17673/20000], Training Loss: 0.4785\n",
            "Epoch [17674/20000], Training Loss: 0.4859\n",
            "Epoch [17675/20000], Training Loss: 0.4534\n",
            "Epoch [17676/20000], Training Loss: 0.4848\n",
            "Epoch [17677/20000], Training Loss: 0.4782\n",
            "Epoch [17678/20000], Training Loss: 0.4744\n",
            "Epoch [17679/20000], Training Loss: 0.4674\n",
            "Epoch [17680/20000], Training Loss: 0.4805\n",
            "Epoch [17681/20000], Training Loss: 0.5073\n",
            "Epoch [17682/20000], Training Loss: 0.4657\n",
            "Epoch [17683/20000], Training Loss: 0.4609\n",
            "Epoch [17684/20000], Training Loss: 0.4766\n",
            "Epoch [17685/20000], Training Loss: 0.4733\n",
            "Epoch [17686/20000], Training Loss: 0.4860\n",
            "Epoch [17687/20000], Training Loss: 0.5082\n",
            "Epoch [17688/20000], Training Loss: 0.4472\n",
            "Epoch [17689/20000], Training Loss: 0.5009\n",
            "Epoch [17690/20000], Training Loss: 0.4632\n",
            "Epoch [17691/20000], Training Loss: 0.4843\n",
            "Epoch [17692/20000], Training Loss: 0.5228\n",
            "Epoch [17693/20000], Training Loss: 0.4892\n",
            "Epoch [17694/20000], Training Loss: 0.4972\n",
            "Epoch [17695/20000], Training Loss: 0.4931\n",
            "Epoch [17696/20000], Training Loss: 0.5090\n",
            "Epoch [17697/20000], Training Loss: 0.4807\n",
            "Epoch [17698/20000], Training Loss: 0.4721\n",
            "Epoch [17699/20000], Training Loss: 0.5501\n",
            "Epoch [17700/20000], Training Loss: 0.5035\n",
            "Epoch [17701/20000], Training Loss: 0.5060\n",
            "Epoch [17702/20000], Training Loss: 0.4601\n",
            "Epoch [17703/20000], Training Loss: 0.4853\n",
            "Epoch [17704/20000], Training Loss: 0.5139\n",
            "Epoch [17705/20000], Training Loss: 0.4823\n",
            "Epoch [17706/20000], Training Loss: 0.5160\n",
            "Epoch [17707/20000], Training Loss: 0.4982\n",
            "Epoch [17708/20000], Training Loss: 0.5118\n",
            "Epoch [17709/20000], Training Loss: 0.5168\n",
            "Epoch [17710/20000], Training Loss: 0.4573\n",
            "Epoch [17711/20000], Training Loss: 0.5139\n",
            "Epoch [17712/20000], Training Loss: 0.4745\n",
            "Epoch [17713/20000], Training Loss: 0.5432\n",
            "Epoch [17714/20000], Training Loss: 0.4776\n",
            "Epoch [17715/20000], Training Loss: 0.4819\n",
            "Epoch [17716/20000], Training Loss: 0.4754\n",
            "Epoch [17717/20000], Training Loss: 0.5108\n",
            "Epoch [17718/20000], Training Loss: 0.4817\n",
            "Epoch [17719/20000], Training Loss: 0.4794\n",
            "Epoch [17720/20000], Training Loss: 0.5041\n",
            "Epoch [17721/20000], Training Loss: 0.4877\n",
            "Epoch [17722/20000], Training Loss: 0.4944\n",
            "Epoch [17723/20000], Training Loss: 0.4936\n",
            "Epoch [17724/20000], Training Loss: 0.5083\n",
            "Epoch [17725/20000], Training Loss: 0.4933\n",
            "Epoch [17726/20000], Training Loss: 0.4751\n",
            "Epoch [17727/20000], Training Loss: 0.4925\n",
            "Epoch [17728/20000], Training Loss: 0.4986\n",
            "Epoch [17729/20000], Training Loss: 0.4842\n",
            "Epoch [17730/20000], Training Loss: 0.5022\n",
            "Epoch [17731/20000], Training Loss: 0.5120\n",
            "Epoch [17732/20000], Training Loss: 0.4588\n",
            "Epoch [17733/20000], Training Loss: 0.5018\n",
            "Epoch [17734/20000], Training Loss: 0.4606\n",
            "Epoch [17735/20000], Training Loss: 0.4690\n",
            "Epoch [17736/20000], Training Loss: 0.4602\n",
            "Epoch [17737/20000], Training Loss: 0.4925\n",
            "Epoch [17738/20000], Training Loss: 0.4653\n",
            "Epoch [17739/20000], Training Loss: 0.4806\n",
            "Epoch [17740/20000], Training Loss: 0.5078\n",
            "Epoch [17741/20000], Training Loss: 0.4939\n",
            "Epoch [17742/20000], Training Loss: 0.5256\n",
            "Epoch [17743/20000], Training Loss: 0.5090\n",
            "Epoch [17744/20000], Training Loss: 0.4709\n",
            "Epoch [17745/20000], Training Loss: 0.4977\n",
            "Epoch [17746/20000], Training Loss: 0.4713\n",
            "Epoch [17747/20000], Training Loss: 0.4633\n",
            "Epoch [17748/20000], Training Loss: 0.4954\n",
            "Epoch [17749/20000], Training Loss: 0.4975\n",
            "Epoch [17750/20000], Training Loss: 0.4830\n",
            "Epoch [17751/20000], Training Loss: 0.4647\n",
            "Epoch [17752/20000], Training Loss: 0.5506\n",
            "Epoch [17753/20000], Training Loss: 0.4834\n",
            "Epoch [17754/20000], Training Loss: 0.4883\n",
            "Epoch [17755/20000], Training Loss: 0.4863\n",
            "Epoch [17756/20000], Training Loss: 0.5272\n",
            "Epoch [17757/20000], Training Loss: 0.4821\n",
            "Epoch [17758/20000], Training Loss: 0.5216\n",
            "Epoch [17759/20000], Training Loss: 0.4986\n",
            "Epoch [17760/20000], Training Loss: 0.4638\n",
            "Epoch [17761/20000], Training Loss: 0.4603\n",
            "Epoch [17762/20000], Training Loss: 0.4581\n",
            "Epoch [17763/20000], Training Loss: 0.5107\n",
            "Epoch [17764/20000], Training Loss: 0.5002\n",
            "Epoch [17765/20000], Training Loss: 0.4894\n",
            "Epoch [17766/20000], Training Loss: 0.5109\n",
            "Epoch [17767/20000], Training Loss: 0.4548\n",
            "Epoch [17768/20000], Training Loss: 0.4851\n",
            "Epoch [17769/20000], Training Loss: 0.4972\n",
            "Epoch [17770/20000], Training Loss: 0.4797\n",
            "Epoch [17771/20000], Training Loss: 0.5181\n",
            "Epoch [17772/20000], Training Loss: 0.4936\n",
            "Epoch [17773/20000], Training Loss: 0.4429\n",
            "Epoch [17774/20000], Training Loss: 0.4950\n",
            "Epoch [17775/20000], Training Loss: 0.4271\n",
            "Epoch [17776/20000], Training Loss: 0.5140\n",
            "Epoch [17777/20000], Training Loss: 0.5099\n",
            "Epoch [17778/20000], Training Loss: 0.4881\n",
            "Epoch [17779/20000], Training Loss: 0.5161\n",
            "Epoch [17780/20000], Training Loss: 0.4962\n",
            "Epoch [17781/20000], Training Loss: 0.4671\n",
            "Epoch [17782/20000], Training Loss: 0.4886\n",
            "Epoch [17783/20000], Training Loss: 0.5083\n",
            "Epoch [17784/20000], Training Loss: 0.4920\n",
            "Epoch [17785/20000], Training Loss: 0.4957\n",
            "Epoch [17786/20000], Training Loss: 0.4383\n",
            "Epoch [17787/20000], Training Loss: 0.5286\n",
            "Epoch [17788/20000], Training Loss: 0.4660\n",
            "Epoch [17789/20000], Training Loss: 0.4905\n",
            "Epoch [17790/20000], Training Loss: 0.4967\n",
            "Epoch [17791/20000], Training Loss: 0.4747\n",
            "Epoch [17792/20000], Training Loss: 0.5073\n",
            "Epoch [17793/20000], Training Loss: 0.5029\n",
            "Epoch [17794/20000], Training Loss: 0.4818\n",
            "Epoch [17795/20000], Training Loss: 0.4968\n",
            "Epoch [17796/20000], Training Loss: 0.5106\n",
            "Epoch [17797/20000], Training Loss: 0.4778\n",
            "Epoch [17798/20000], Training Loss: 0.4476\n",
            "Epoch [17799/20000], Training Loss: 0.4673\n",
            "Epoch [17800/20000], Training Loss: 0.4953\n",
            "Epoch [17801/20000], Training Loss: 0.4995\n",
            "Epoch [17802/20000], Training Loss: 0.4882\n",
            "Epoch [17803/20000], Training Loss: 0.4942\n",
            "Epoch [17804/20000], Training Loss: 0.4684\n",
            "Epoch [17805/20000], Training Loss: 0.4945\n",
            "Epoch [17806/20000], Training Loss: 0.4664\n",
            "Epoch [17807/20000], Training Loss: 0.5082\n",
            "Epoch [17808/20000], Training Loss: 0.5237\n",
            "Epoch [17809/20000], Training Loss: 0.4723\n",
            "Epoch [17810/20000], Training Loss: 0.5031\n",
            "Epoch [17811/20000], Training Loss: 0.4515\n",
            "Epoch [17812/20000], Training Loss: 0.5013\n",
            "Epoch [17813/20000], Training Loss: 0.4819\n",
            "Epoch [17814/20000], Training Loss: 0.5347\n",
            "Epoch [17815/20000], Training Loss: 0.4877\n",
            "Epoch [17816/20000], Training Loss: 0.4471\n",
            "Epoch [17817/20000], Training Loss: 0.4714\n",
            "Epoch [17818/20000], Training Loss: 0.4619\n",
            "Epoch [17819/20000], Training Loss: 0.4422\n",
            "Epoch [17820/20000], Training Loss: 0.4878\n",
            "Epoch [17821/20000], Training Loss: 0.4897\n",
            "Epoch [17822/20000], Training Loss: 0.4866\n",
            "Epoch [17823/20000], Training Loss: 0.5008\n",
            "Epoch [17824/20000], Training Loss: 0.4884\n",
            "Epoch [17825/20000], Training Loss: 0.5029\n",
            "Epoch [17826/20000], Training Loss: 0.4939\n",
            "Epoch [17827/20000], Training Loss: 0.4736\n",
            "Epoch [17828/20000], Training Loss: 0.4932\n",
            "Epoch [17829/20000], Training Loss: 0.5223\n",
            "Epoch [17830/20000], Training Loss: 0.4931\n",
            "Epoch [17831/20000], Training Loss: 0.4981\n",
            "Epoch [17832/20000], Training Loss: 0.4471\n",
            "Epoch [17833/20000], Training Loss: 0.4678\n",
            "Epoch [17834/20000], Training Loss: 0.4535\n",
            "Epoch [17835/20000], Training Loss: 0.5154\n",
            "Epoch [17836/20000], Training Loss: 0.4851\n",
            "Epoch [17837/20000], Training Loss: 0.4580\n",
            "Epoch [17838/20000], Training Loss: 0.5011\n",
            "Epoch [17839/20000], Training Loss: 0.5185\n",
            "Epoch [17840/20000], Training Loss: 0.4767\n",
            "Epoch [17841/20000], Training Loss: 0.5005\n",
            "Epoch [17842/20000], Training Loss: 0.4756\n",
            "Epoch [17843/20000], Training Loss: 0.5202\n",
            "Epoch [17844/20000], Training Loss: 0.5001\n",
            "Epoch [17845/20000], Training Loss: 0.5062\n",
            "Epoch [17846/20000], Training Loss: 0.4832\n",
            "Epoch [17847/20000], Training Loss: 0.4609\n",
            "Epoch [17848/20000], Training Loss: 0.4954\n",
            "Epoch [17849/20000], Training Loss: 0.4826\n",
            "Epoch [17850/20000], Training Loss: 0.4884\n",
            "Epoch [17851/20000], Training Loss: 0.4845\n",
            "Epoch [17852/20000], Training Loss: 0.4540\n",
            "Epoch [17853/20000], Training Loss: 0.4855\n",
            "Epoch [17854/20000], Training Loss: 0.5158\n",
            "Epoch [17855/20000], Training Loss: 0.4731\n",
            "Epoch [17856/20000], Training Loss: 0.4858\n",
            "Epoch [17857/20000], Training Loss: 0.4633\n",
            "Epoch [17858/20000], Training Loss: 0.4661\n",
            "Epoch [17859/20000], Training Loss: 0.5176\n",
            "Epoch [17860/20000], Training Loss: 0.5284\n",
            "Epoch [17861/20000], Training Loss: 0.4658\n",
            "Epoch [17862/20000], Training Loss: 0.4636\n",
            "Epoch [17863/20000], Training Loss: 0.5333\n",
            "Epoch [17864/20000], Training Loss: 0.4715\n",
            "Epoch [17865/20000], Training Loss: 0.4769\n",
            "Epoch [17866/20000], Training Loss: 0.5020\n",
            "Epoch [17867/20000], Training Loss: 0.5057\n",
            "Epoch [17868/20000], Training Loss: 0.4931\n",
            "Epoch [17869/20000], Training Loss: 0.5203\n",
            "Epoch [17870/20000], Training Loss: 0.5040\n",
            "Epoch [17871/20000], Training Loss: 0.5119\n",
            "Epoch [17872/20000], Training Loss: 0.5237\n",
            "Epoch [17873/20000], Training Loss: 0.4465\n",
            "Epoch [17874/20000], Training Loss: 0.5078\n",
            "Epoch [17875/20000], Training Loss: 0.4655\n",
            "Epoch [17876/20000], Training Loss: 0.5103\n",
            "Epoch [17877/20000], Training Loss: 0.4755\n",
            "Epoch [17878/20000], Training Loss: 0.4918\n",
            "Epoch [17879/20000], Training Loss: 0.4861\n",
            "Epoch [17880/20000], Training Loss: 0.4660\n",
            "Epoch [17881/20000], Training Loss: 0.4851\n",
            "Epoch [17882/20000], Training Loss: 0.4570\n",
            "Epoch [17883/20000], Training Loss: 0.4571\n",
            "Epoch [17884/20000], Training Loss: 0.4374\n",
            "Epoch [17885/20000], Training Loss: 0.5102\n",
            "Epoch [17886/20000], Training Loss: 0.4640\n",
            "Epoch [17887/20000], Training Loss: 0.4524\n",
            "Epoch [17888/20000], Training Loss: 0.4300\n",
            "Epoch [17889/20000], Training Loss: 0.5230\n",
            "Epoch [17890/20000], Training Loss: 0.5170\n",
            "Epoch [17891/20000], Training Loss: 0.4854\n",
            "Epoch [17892/20000], Training Loss: 0.5142\n",
            "Epoch [17893/20000], Training Loss: 0.4617\n",
            "Epoch [17894/20000], Training Loss: 0.5181\n",
            "Epoch [17895/20000], Training Loss: 0.4870\n",
            "Epoch [17896/20000], Training Loss: 0.4833\n",
            "Epoch [17897/20000], Training Loss: 0.5186\n",
            "Epoch [17898/20000], Training Loss: 0.4519\n",
            "Epoch [17899/20000], Training Loss: 0.4780\n",
            "Epoch [17900/20000], Training Loss: 0.5242\n",
            "Epoch [17901/20000], Training Loss: 0.4950\n",
            "Epoch [17902/20000], Training Loss: 0.4604\n",
            "Epoch [17903/20000], Training Loss: 0.4395\n",
            "Epoch [17904/20000], Training Loss: 0.4989\n",
            "Epoch [17905/20000], Training Loss: 0.4916\n",
            "Epoch [17906/20000], Training Loss: 0.4956\n",
            "Epoch [17907/20000], Training Loss: 0.4992\n",
            "Epoch [17908/20000], Training Loss: 0.4741\n",
            "Epoch [17909/20000], Training Loss: 0.4909\n",
            "Epoch [17910/20000], Training Loss: 0.4583\n",
            "Epoch [17911/20000], Training Loss: 0.5384\n",
            "Epoch [17912/20000], Training Loss: 0.4863\n",
            "Epoch [17913/20000], Training Loss: 0.5078\n",
            "Epoch [17914/20000], Training Loss: 0.4995\n",
            "Epoch [17915/20000], Training Loss: 0.4582\n",
            "Epoch [17916/20000], Training Loss: 0.4901\n",
            "Epoch [17917/20000], Training Loss: 0.4780\n",
            "Epoch [17918/20000], Training Loss: 0.4624\n",
            "Epoch [17919/20000], Training Loss: 0.4777\n",
            "Epoch [17920/20000], Training Loss: 0.5004\n",
            "Epoch [17921/20000], Training Loss: 0.4446\n",
            "Epoch [17922/20000], Training Loss: 0.4964\n",
            "Epoch [17923/20000], Training Loss: 0.4861\n",
            "Epoch [17924/20000], Training Loss: 0.4721\n",
            "Epoch [17925/20000], Training Loss: 0.5146\n",
            "Epoch [17926/20000], Training Loss: 0.4915\n",
            "Epoch [17927/20000], Training Loss: 0.4353\n",
            "Epoch [17928/20000], Training Loss: 0.4702\n",
            "Epoch [17929/20000], Training Loss: 0.4989\n",
            "Epoch [17930/20000], Training Loss: 0.4960\n",
            "Epoch [17931/20000], Training Loss: 0.4713\n",
            "Epoch [17932/20000], Training Loss: 0.4677\n",
            "Epoch [17933/20000], Training Loss: 0.4910\n",
            "Epoch [17934/20000], Training Loss: 0.4657\n",
            "Epoch [17935/20000], Training Loss: 0.4867\n",
            "Epoch [17936/20000], Training Loss: 0.5173\n",
            "Epoch [17937/20000], Training Loss: 0.4848\n",
            "Epoch [17938/20000], Training Loss: 0.4634\n",
            "Epoch [17939/20000], Training Loss: 0.5036\n",
            "Epoch [17940/20000], Training Loss: 0.4577\n",
            "Epoch [17941/20000], Training Loss: 0.5132\n",
            "Epoch [17942/20000], Training Loss: 0.4599\n",
            "Epoch [17943/20000], Training Loss: 0.5018\n",
            "Epoch [17944/20000], Training Loss: 0.4709\n",
            "Epoch [17945/20000], Training Loss: 0.4862\n",
            "Epoch [17946/20000], Training Loss: 0.4595\n",
            "Epoch [17947/20000], Training Loss: 0.5081\n",
            "Epoch [17948/20000], Training Loss: 0.4668\n",
            "Epoch [17949/20000], Training Loss: 0.4831\n",
            "Epoch [17950/20000], Training Loss: 0.5081\n",
            "Epoch [17951/20000], Training Loss: 0.5047\n",
            "Epoch [17952/20000], Training Loss: 0.4672\n",
            "Epoch [17953/20000], Training Loss: 0.5218\n",
            "Epoch [17954/20000], Training Loss: 0.4932\n",
            "Epoch [17955/20000], Training Loss: 0.5208\n",
            "Epoch [17956/20000], Training Loss: 0.5130\n",
            "Epoch [17957/20000], Training Loss: 0.4667\n",
            "Epoch [17958/20000], Training Loss: 0.4710\n",
            "Epoch [17959/20000], Training Loss: 0.4956\n",
            "Epoch [17960/20000], Training Loss: 0.5012\n",
            "Epoch [17961/20000], Training Loss: 0.4699\n",
            "Epoch [17962/20000], Training Loss: 0.4674\n",
            "Epoch [17963/20000], Training Loss: 0.4768\n",
            "Epoch [17964/20000], Training Loss: 0.4893\n",
            "Epoch [17965/20000], Training Loss: 0.5118\n",
            "Epoch [17966/20000], Training Loss: 0.5140\n",
            "Epoch [17967/20000], Training Loss: 0.5107\n",
            "Epoch [17968/20000], Training Loss: 0.5167\n",
            "Epoch [17969/20000], Training Loss: 0.5356\n",
            "Epoch [17970/20000], Training Loss: 0.5131\n",
            "Epoch [17971/20000], Training Loss: 0.4904\n",
            "Epoch [17972/20000], Training Loss: 0.4835\n",
            "Epoch [17973/20000], Training Loss: 0.5077\n",
            "Epoch [17974/20000], Training Loss: 0.5109\n",
            "Epoch [17975/20000], Training Loss: 0.5229\n",
            "Epoch [17976/20000], Training Loss: 0.4799\n",
            "Epoch [17977/20000], Training Loss: 0.4768\n",
            "Epoch [17978/20000], Training Loss: 0.5298\n",
            "Epoch [17979/20000], Training Loss: 0.5145\n",
            "Epoch [17980/20000], Training Loss: 0.5135\n",
            "Epoch [17981/20000], Training Loss: 0.4585\n",
            "Epoch [17982/20000], Training Loss: 0.5350\n",
            "Epoch [17983/20000], Training Loss: 0.4947\n",
            "Epoch [17984/20000], Training Loss: 0.4551\n",
            "Epoch [17985/20000], Training Loss: 0.4623\n",
            "Epoch [17986/20000], Training Loss: 0.4637\n",
            "Epoch [17987/20000], Training Loss: 0.4752\n",
            "Epoch [17988/20000], Training Loss: 0.4937\n",
            "Epoch [17989/20000], Training Loss: 0.5009\n",
            "Epoch [17990/20000], Training Loss: 0.5201\n",
            "Epoch [17991/20000], Training Loss: 0.4744\n",
            "Epoch [17992/20000], Training Loss: 0.4833\n",
            "Epoch [17993/20000], Training Loss: 0.5236\n",
            "Epoch [17994/20000], Training Loss: 0.4660\n",
            "Epoch [17995/20000], Training Loss: 0.4776\n",
            "Epoch [17996/20000], Training Loss: 0.4845\n",
            "Epoch [17997/20000], Training Loss: 0.4909\n",
            "Epoch [17998/20000], Training Loss: 0.4947\n",
            "Epoch [17999/20000], Training Loss: 0.4438\n",
            "Epoch [18000/20000], Training Loss: 0.4999\n",
            "Epoch [18001/20000], Training Loss: 0.4711\n",
            "Epoch [18002/20000], Training Loss: 0.5083\n",
            "Epoch [18003/20000], Training Loss: 0.4905\n",
            "Epoch [18004/20000], Training Loss: 0.4958\n",
            "Epoch [18005/20000], Training Loss: 0.5124\n",
            "Epoch [18006/20000], Training Loss: 0.5166\n",
            "Epoch [18007/20000], Training Loss: 0.4957\n",
            "Epoch [18008/20000], Training Loss: 0.4550\n",
            "Epoch [18009/20000], Training Loss: 0.4937\n",
            "Epoch [18010/20000], Training Loss: 0.4514\n",
            "Epoch [18011/20000], Training Loss: 0.4740\n",
            "Epoch [18012/20000], Training Loss: 0.4967\n",
            "Epoch [18013/20000], Training Loss: 0.4878\n",
            "Epoch [18014/20000], Training Loss: 0.4733\n",
            "Epoch [18015/20000], Training Loss: 0.5273\n",
            "Epoch [18016/20000], Training Loss: 0.4908\n",
            "Epoch [18017/20000], Training Loss: 0.5121\n",
            "Epoch [18018/20000], Training Loss: 0.4592\n",
            "Epoch [18019/20000], Training Loss: 0.4794\n",
            "Epoch [18020/20000], Training Loss: 0.4633\n",
            "Epoch [18021/20000], Training Loss: 0.5014\n",
            "Epoch [18022/20000], Training Loss: 0.4493\n",
            "Epoch [18023/20000], Training Loss: 0.4650\n",
            "Epoch [18024/20000], Training Loss: 0.4864\n",
            "Epoch [18025/20000], Training Loss: 0.5072\n",
            "Epoch [18026/20000], Training Loss: 0.4430\n",
            "Epoch [18027/20000], Training Loss: 0.4889\n",
            "Epoch [18028/20000], Training Loss: 0.4667\n",
            "Epoch [18029/20000], Training Loss: 0.4953\n",
            "Epoch [18030/20000], Training Loss: 0.4603\n",
            "Epoch [18031/20000], Training Loss: 0.4830\n",
            "Epoch [18032/20000], Training Loss: 0.5320\n",
            "Epoch [18033/20000], Training Loss: 0.4446\n",
            "Epoch [18034/20000], Training Loss: 0.4667\n",
            "Epoch [18035/20000], Training Loss: 0.5052\n",
            "Epoch [18036/20000], Training Loss: 0.5482\n",
            "Epoch [18037/20000], Training Loss: 0.5069\n",
            "Epoch [18038/20000], Training Loss: 0.5178\n",
            "Epoch [18039/20000], Training Loss: 0.4768\n",
            "Epoch [18040/20000], Training Loss: 0.4983\n",
            "Epoch [18041/20000], Training Loss: 0.4922\n",
            "Epoch [18042/20000], Training Loss: 0.4867\n",
            "Epoch [18043/20000], Training Loss: 0.4827\n",
            "Epoch [18044/20000], Training Loss: 0.4809\n",
            "Epoch [18045/20000], Training Loss: 0.4944\n",
            "Epoch [18046/20000], Training Loss: 0.4951\n",
            "Epoch [18047/20000], Training Loss: 0.5057\n",
            "Epoch [18048/20000], Training Loss: 0.4703\n",
            "Epoch [18049/20000], Training Loss: 0.5137\n",
            "Epoch [18050/20000], Training Loss: 0.4871\n",
            "Epoch [18051/20000], Training Loss: 0.4910\n",
            "Epoch [18052/20000], Training Loss: 0.4948\n",
            "Epoch [18053/20000], Training Loss: 0.4614\n",
            "Epoch [18054/20000], Training Loss: 0.5280\n",
            "Epoch [18055/20000], Training Loss: 0.4490\n",
            "Epoch [18056/20000], Training Loss: 0.4408\n",
            "Epoch [18057/20000], Training Loss: 0.5062\n",
            "Epoch [18058/20000], Training Loss: 0.4945\n",
            "Epoch [18059/20000], Training Loss: 0.4688\n",
            "Epoch [18060/20000], Training Loss: 0.4705\n",
            "Epoch [18061/20000], Training Loss: 0.5250\n",
            "Epoch [18062/20000], Training Loss: 0.4634\n",
            "Epoch [18063/20000], Training Loss: 0.4461\n",
            "Epoch [18064/20000], Training Loss: 0.4748\n",
            "Epoch [18065/20000], Training Loss: 0.5062\n",
            "Epoch [18066/20000], Training Loss: 0.4854\n",
            "Epoch [18067/20000], Training Loss: 0.5051\n",
            "Epoch [18068/20000], Training Loss: 0.4792\n",
            "Epoch [18069/20000], Training Loss: 0.4753\n",
            "Epoch [18070/20000], Training Loss: 0.4624\n",
            "Epoch [18071/20000], Training Loss: 0.4968\n",
            "Epoch [18072/20000], Training Loss: 0.5329\n",
            "Epoch [18073/20000], Training Loss: 0.4658\n",
            "Epoch [18074/20000], Training Loss: 0.5179\n",
            "Epoch [18075/20000], Training Loss: 0.4682\n",
            "Epoch [18076/20000], Training Loss: 0.5062\n",
            "Epoch [18077/20000], Training Loss: 0.4734\n",
            "Epoch [18078/20000], Training Loss: 0.4398\n",
            "Epoch [18079/20000], Training Loss: 0.4744\n",
            "Epoch [18080/20000], Training Loss: 0.4875\n",
            "Epoch [18081/20000], Training Loss: 0.4632\n",
            "Epoch [18082/20000], Training Loss: 0.4732\n",
            "Epoch [18083/20000], Training Loss: 0.4875\n",
            "Epoch [18084/20000], Training Loss: 0.4715\n",
            "Epoch [18085/20000], Training Loss: 0.4687\n",
            "Epoch [18086/20000], Training Loss: 0.4665\n",
            "Epoch [18087/20000], Training Loss: 0.4728\n",
            "Epoch [18088/20000], Training Loss: 0.4760\n",
            "Epoch [18089/20000], Training Loss: 0.5263\n",
            "Epoch [18090/20000], Training Loss: 0.4654\n",
            "Epoch [18091/20000], Training Loss: 0.4695\n",
            "Epoch [18092/20000], Training Loss: 0.4923\n",
            "Epoch [18093/20000], Training Loss: 0.4952\n",
            "Epoch [18094/20000], Training Loss: 0.5093\n",
            "Epoch [18095/20000], Training Loss: 0.4401\n",
            "Epoch [18096/20000], Training Loss: 0.4948\n",
            "Epoch [18097/20000], Training Loss: 0.4511\n",
            "Epoch [18098/20000], Training Loss: 0.4891\n",
            "Epoch [18099/20000], Training Loss: 0.5239\n",
            "Epoch [18100/20000], Training Loss: 0.4963\n",
            "Epoch [18101/20000], Training Loss: 0.4655\n",
            "Epoch [18102/20000], Training Loss: 0.5170\n",
            "Epoch [18103/20000], Training Loss: 0.4796\n",
            "Epoch [18104/20000], Training Loss: 0.4997\n",
            "Epoch [18105/20000], Training Loss: 0.5276\n",
            "Epoch [18106/20000], Training Loss: 0.5659\n",
            "Epoch [18107/20000], Training Loss: 0.4775\n",
            "Epoch [18108/20000], Training Loss: 0.5178\n",
            "Epoch [18109/20000], Training Loss: 0.5083\n",
            "Epoch [18110/20000], Training Loss: 0.5107\n",
            "Epoch [18111/20000], Training Loss: 0.4777\n",
            "Epoch [18112/20000], Training Loss: 0.4729\n",
            "Epoch [18113/20000], Training Loss: 0.4724\n",
            "Epoch [18114/20000], Training Loss: 0.4842\n",
            "Epoch [18115/20000], Training Loss: 0.5024\n",
            "Epoch [18116/20000], Training Loss: 0.4770\n",
            "Epoch [18117/20000], Training Loss: 0.4409\n",
            "Epoch [18118/20000], Training Loss: 0.5018\n",
            "Epoch [18119/20000], Training Loss: 0.5050\n",
            "Epoch [18120/20000], Training Loss: 0.5115\n",
            "Epoch [18121/20000], Training Loss: 0.4582\n",
            "Epoch [18122/20000], Training Loss: 0.4735\n",
            "Epoch [18123/20000], Training Loss: 0.4940\n",
            "Epoch [18124/20000], Training Loss: 0.4564\n",
            "Epoch [18125/20000], Training Loss: 0.4595\n",
            "Epoch [18126/20000], Training Loss: 0.4885\n",
            "Epoch [18127/20000], Training Loss: 0.5112\n",
            "Epoch [18128/20000], Training Loss: 0.4571\n",
            "Epoch [18129/20000], Training Loss: 0.4861\n",
            "Epoch [18130/20000], Training Loss: 0.4552\n",
            "Epoch [18131/20000], Training Loss: 0.4876\n",
            "Epoch [18132/20000], Training Loss: 0.4991\n",
            "Epoch [18133/20000], Training Loss: 0.5111\n",
            "Epoch [18134/20000], Training Loss: 0.4798\n",
            "Epoch [18135/20000], Training Loss: 0.5084\n",
            "Epoch [18136/20000], Training Loss: 0.4732\n",
            "Epoch [18137/20000], Training Loss: 0.4834\n",
            "Epoch [18138/20000], Training Loss: 0.4797\n",
            "Epoch [18139/20000], Training Loss: 0.4775\n",
            "Epoch [18140/20000], Training Loss: 0.4803\n",
            "Epoch [18141/20000], Training Loss: 0.4684\n",
            "Epoch [18142/20000], Training Loss: 0.4655\n",
            "Epoch [18143/20000], Training Loss: 0.4805\n",
            "Epoch [18144/20000], Training Loss: 0.5399\n",
            "Epoch [18145/20000], Training Loss: 0.4428\n",
            "Epoch [18146/20000], Training Loss: 0.5066\n",
            "Epoch [18147/20000], Training Loss: 0.4598\n",
            "Epoch [18148/20000], Training Loss: 0.4954\n",
            "Epoch [18149/20000], Training Loss: 0.4817\n",
            "Epoch [18150/20000], Training Loss: 0.4774\n",
            "Epoch [18151/20000], Training Loss: 0.4840\n",
            "Epoch [18152/20000], Training Loss: 0.4901\n",
            "Epoch [18153/20000], Training Loss: 0.4826\n",
            "Epoch [18154/20000], Training Loss: 0.4665\n",
            "Epoch [18155/20000], Training Loss: 0.4497\n",
            "Epoch [18156/20000], Training Loss: 0.5138\n",
            "Epoch [18157/20000], Training Loss: 0.4963\n",
            "Epoch [18158/20000], Training Loss: 0.4913\n",
            "Epoch [18159/20000], Training Loss: 0.5068\n",
            "Epoch [18160/20000], Training Loss: 0.5232\n",
            "Epoch [18161/20000], Training Loss: 0.4827\n",
            "Epoch [18162/20000], Training Loss: 0.5533\n",
            "Epoch [18163/20000], Training Loss: 0.4918\n",
            "Epoch [18164/20000], Training Loss: 0.5041\n",
            "Epoch [18165/20000], Training Loss: 0.4364\n",
            "Epoch [18166/20000], Training Loss: 0.4728\n",
            "Epoch [18167/20000], Training Loss: 0.5147\n",
            "Epoch [18168/20000], Training Loss: 0.4883\n",
            "Epoch [18169/20000], Training Loss: 0.5078\n",
            "Epoch [18170/20000], Training Loss: 0.4969\n",
            "Epoch [18171/20000], Training Loss: 0.4761\n",
            "Epoch [18172/20000], Training Loss: 0.5015\n",
            "Epoch [18173/20000], Training Loss: 0.4581\n",
            "Epoch [18174/20000], Training Loss: 0.4857\n",
            "Epoch [18175/20000], Training Loss: 0.4924\n",
            "Epoch [18176/20000], Training Loss: 0.4916\n",
            "Epoch [18177/20000], Training Loss: 0.4586\n",
            "Epoch [18178/20000], Training Loss: 0.4733\n",
            "Epoch [18179/20000], Training Loss: 0.5067\n",
            "Epoch [18180/20000], Training Loss: 0.4689\n",
            "Epoch [18181/20000], Training Loss: 0.5315\n",
            "Epoch [18182/20000], Training Loss: 0.4589\n",
            "Epoch [18183/20000], Training Loss: 0.4799\n",
            "Epoch [18184/20000], Training Loss: 0.4447\n",
            "Epoch [18185/20000], Training Loss: 0.4921\n",
            "Epoch [18186/20000], Training Loss: 0.5204\n",
            "Epoch [18187/20000], Training Loss: 0.4681\n",
            "Epoch [18188/20000], Training Loss: 0.4656\n",
            "Epoch [18189/20000], Training Loss: 0.4517\n",
            "Epoch [18190/20000], Training Loss: 0.4966\n",
            "Epoch [18191/20000], Training Loss: 0.4572\n",
            "Epoch [18192/20000], Training Loss: 0.4765\n",
            "Epoch [18193/20000], Training Loss: 0.5116\n",
            "Epoch [18194/20000], Training Loss: 0.4821\n",
            "Epoch [18195/20000], Training Loss: 0.4802\n",
            "Epoch [18196/20000], Training Loss: 0.4817\n",
            "Epoch [18197/20000], Training Loss: 0.4832\n",
            "Epoch [18198/20000], Training Loss: 0.5123\n",
            "Epoch [18199/20000], Training Loss: 0.5293\n",
            "Epoch [18200/20000], Training Loss: 0.4945\n",
            "Epoch [18201/20000], Training Loss: 0.5484\n",
            "Epoch [18202/20000], Training Loss: 0.4812\n",
            "Epoch [18203/20000], Training Loss: 0.5087\n",
            "Epoch [18204/20000], Training Loss: 0.4989\n",
            "Epoch [18205/20000], Training Loss: 0.5164\n",
            "Epoch [18206/20000], Training Loss: 0.4338\n",
            "Epoch [18207/20000], Training Loss: 0.4971\n",
            "Epoch [18208/20000], Training Loss: 0.5145\n",
            "Epoch [18209/20000], Training Loss: 0.4506\n",
            "Epoch [18210/20000], Training Loss: 0.4992\n",
            "Epoch [18211/20000], Training Loss: 0.4746\n",
            "Epoch [18212/20000], Training Loss: 0.5483\n",
            "Epoch [18213/20000], Training Loss: 0.4914\n",
            "Epoch [18214/20000], Training Loss: 0.4625\n",
            "Epoch [18215/20000], Training Loss: 0.4865\n",
            "Epoch [18216/20000], Training Loss: 0.4963\n",
            "Epoch [18217/20000], Training Loss: 0.4779\n",
            "Epoch [18218/20000], Training Loss: 0.4777\n",
            "Epoch [18219/20000], Training Loss: 0.4868\n",
            "Epoch [18220/20000], Training Loss: 0.5343\n",
            "Epoch [18221/20000], Training Loss: 0.4913\n",
            "Epoch [18222/20000], Training Loss: 0.5482\n",
            "Epoch [18223/20000], Training Loss: 0.4786\n",
            "Epoch [18224/20000], Training Loss: 0.4576\n",
            "Epoch [18225/20000], Training Loss: 0.4311\n",
            "Epoch [18226/20000], Training Loss: 0.4993\n",
            "Epoch [18227/20000], Training Loss: 0.4922\n",
            "Epoch [18228/20000], Training Loss: 0.4843\n",
            "Epoch [18229/20000], Training Loss: 0.4530\n",
            "Epoch [18230/20000], Training Loss: 0.4604\n",
            "Epoch [18231/20000], Training Loss: 0.5671\n",
            "Epoch [18232/20000], Training Loss: 0.4903\n",
            "Epoch [18233/20000], Training Loss: 0.4949\n",
            "Epoch [18234/20000], Training Loss: 0.4791\n",
            "Epoch [18235/20000], Training Loss: 0.5050\n",
            "Epoch [18236/20000], Training Loss: 0.5039\n",
            "Epoch [18237/20000], Training Loss: 0.4607\n",
            "Epoch [18238/20000], Training Loss: 0.5062\n",
            "Epoch [18239/20000], Training Loss: 0.5196\n",
            "Epoch [18240/20000], Training Loss: 0.4828\n",
            "Epoch [18241/20000], Training Loss: 0.5318\n",
            "Epoch [18242/20000], Training Loss: 0.5226\n",
            "Epoch [18243/20000], Training Loss: 0.5004\n",
            "Epoch [18244/20000], Training Loss: 0.5010\n",
            "Epoch [18245/20000], Training Loss: 0.4791\n",
            "Epoch [18246/20000], Training Loss: 0.4816\n",
            "Epoch [18247/20000], Training Loss: 0.4770\n",
            "Epoch [18248/20000], Training Loss: 0.4965\n",
            "Epoch [18249/20000], Training Loss: 0.4500\n",
            "Epoch [18250/20000], Training Loss: 0.4931\n",
            "Epoch [18251/20000], Training Loss: 0.4532\n",
            "Epoch [18252/20000], Training Loss: 0.4572\n",
            "Epoch [18253/20000], Training Loss: 0.4978\n",
            "Epoch [18254/20000], Training Loss: 0.4819\n",
            "Epoch [18255/20000], Training Loss: 0.4983\n",
            "Epoch [18256/20000], Training Loss: 0.5009\n",
            "Epoch [18257/20000], Training Loss: 0.4832\n",
            "Epoch [18258/20000], Training Loss: 0.5083\n",
            "Epoch [18259/20000], Training Loss: 0.4914\n",
            "Epoch [18260/20000], Training Loss: 0.5127\n",
            "Epoch [18261/20000], Training Loss: 0.5048\n",
            "Epoch [18262/20000], Training Loss: 0.4878\n",
            "Epoch [18263/20000], Training Loss: 0.5021\n",
            "Epoch [18264/20000], Training Loss: 0.5037\n",
            "Epoch [18265/20000], Training Loss: 0.4714\n",
            "Epoch [18266/20000], Training Loss: 0.4730\n",
            "Epoch [18267/20000], Training Loss: 0.4513\n",
            "Epoch [18268/20000], Training Loss: 0.4812\n",
            "Epoch [18269/20000], Training Loss: 0.4791\n",
            "Epoch [18270/20000], Training Loss: 0.5246\n",
            "Epoch [18271/20000], Training Loss: 0.4499\n",
            "Epoch [18272/20000], Training Loss: 0.4827\n",
            "Epoch [18273/20000], Training Loss: 0.4925\n",
            "Epoch [18274/20000], Training Loss: 0.4349\n",
            "Epoch [18275/20000], Training Loss: 0.5226\n",
            "Epoch [18276/20000], Training Loss: 0.4584\n",
            "Epoch [18277/20000], Training Loss: 0.5158\n",
            "Epoch [18278/20000], Training Loss: 0.5542\n",
            "Epoch [18279/20000], Training Loss: 0.4993\n",
            "Epoch [18280/20000], Training Loss: 0.4810\n",
            "Epoch [18281/20000], Training Loss: 0.4622\n",
            "Epoch [18282/20000], Training Loss: 0.4924\n",
            "Epoch [18283/20000], Training Loss: 0.4398\n",
            "Epoch [18284/20000], Training Loss: 0.4757\n",
            "Epoch [18285/20000], Training Loss: 0.4944\n",
            "Epoch [18286/20000], Training Loss: 0.4819\n",
            "Epoch [18287/20000], Training Loss: 0.4700\n",
            "Epoch [18288/20000], Training Loss: 0.4942\n",
            "Epoch [18289/20000], Training Loss: 0.4617\n",
            "Epoch [18290/20000], Training Loss: 0.4820\n",
            "Epoch [18291/20000], Training Loss: 0.4861\n",
            "Epoch [18292/20000], Training Loss: 0.5080\n",
            "Epoch [18293/20000], Training Loss: 0.4899\n",
            "Epoch [18294/20000], Training Loss: 0.5388\n",
            "Epoch [18295/20000], Training Loss: 0.4814\n",
            "Epoch [18296/20000], Training Loss: 0.4816\n",
            "Epoch [18297/20000], Training Loss: 0.5042\n",
            "Epoch [18298/20000], Training Loss: 0.5210\n",
            "Epoch [18299/20000], Training Loss: 0.4809\n",
            "Epoch [18300/20000], Training Loss: 0.5060\n",
            "Epoch [18301/20000], Training Loss: 0.4726\n",
            "Epoch [18302/20000], Training Loss: 0.4995\n",
            "Epoch [18303/20000], Training Loss: 0.5087\n",
            "Epoch [18304/20000], Training Loss: 0.4925\n",
            "Epoch [18305/20000], Training Loss: 0.4639\n",
            "Epoch [18306/20000], Training Loss: 0.5180\n",
            "Epoch [18307/20000], Training Loss: 0.4819\n",
            "Epoch [18308/20000], Training Loss: 0.4958\n",
            "Epoch [18309/20000], Training Loss: 0.4810\n",
            "Epoch [18310/20000], Training Loss: 0.4792\n",
            "Epoch [18311/20000], Training Loss: 0.4732\n",
            "Epoch [18312/20000], Training Loss: 0.4943\n",
            "Epoch [18313/20000], Training Loss: 0.4635\n",
            "Epoch [18314/20000], Training Loss: 0.4683\n",
            "Epoch [18315/20000], Training Loss: 0.5013\n",
            "Epoch [18316/20000], Training Loss: 0.5208\n",
            "Epoch [18317/20000], Training Loss: 0.5039\n",
            "Epoch [18318/20000], Training Loss: 0.4837\n",
            "Epoch [18319/20000], Training Loss: 0.4954\n",
            "Epoch [18320/20000], Training Loss: 0.5179\n",
            "Epoch [18321/20000], Training Loss: 0.4576\n",
            "Epoch [18322/20000], Training Loss: 0.4573\n",
            "Epoch [18323/20000], Training Loss: 0.5222\n",
            "Epoch [18324/20000], Training Loss: 0.4943\n",
            "Epoch [18325/20000], Training Loss: 0.4767\n",
            "Epoch [18326/20000], Training Loss: 0.4611\n",
            "Epoch [18327/20000], Training Loss: 0.4549\n",
            "Epoch [18328/20000], Training Loss: 0.4908\n",
            "Epoch [18329/20000], Training Loss: 0.4823\n",
            "Epoch [18330/20000], Training Loss: 0.4883\n",
            "Epoch [18331/20000], Training Loss: 0.4736\n",
            "Epoch [18332/20000], Training Loss: 0.4704\n",
            "Epoch [18333/20000], Training Loss: 0.4965\n",
            "Epoch [18334/20000], Training Loss: 0.4900\n",
            "Epoch [18335/20000], Training Loss: 0.4811\n",
            "Epoch [18336/20000], Training Loss: 0.4825\n",
            "Epoch [18337/20000], Training Loss: 0.5025\n",
            "Epoch [18338/20000], Training Loss: 0.5197\n",
            "Epoch [18339/20000], Training Loss: 0.4966\n",
            "Epoch [18340/20000], Training Loss: 0.4894\n",
            "Epoch [18341/20000], Training Loss: 0.4374\n",
            "Epoch [18342/20000], Training Loss: 0.4779\n",
            "Epoch [18343/20000], Training Loss: 0.4734\n",
            "Epoch [18344/20000], Training Loss: 0.4783\n",
            "Epoch [18345/20000], Training Loss: 0.4935\n",
            "Epoch [18346/20000], Training Loss: 0.4871\n",
            "Epoch [18347/20000], Training Loss: 0.5010\n",
            "Epoch [18348/20000], Training Loss: 0.4429\n",
            "Epoch [18349/20000], Training Loss: 0.4453\n",
            "Epoch [18350/20000], Training Loss: 0.4376\n",
            "Epoch [18351/20000], Training Loss: 0.4647\n",
            "Epoch [18352/20000], Training Loss: 0.4759\n",
            "Epoch [18353/20000], Training Loss: 0.4587\n",
            "Epoch [18354/20000], Training Loss: 0.4847\n",
            "Epoch [18355/20000], Training Loss: 0.4357\n",
            "Epoch [18356/20000], Training Loss: 0.4568\n",
            "Epoch [18357/20000], Training Loss: 0.4554\n",
            "Epoch [18358/20000], Training Loss: 0.5097\n",
            "Epoch [18359/20000], Training Loss: 0.4882\n",
            "Epoch [18360/20000], Training Loss: 0.4995\n",
            "Epoch [18361/20000], Training Loss: 0.5145\n",
            "Epoch [18362/20000], Training Loss: 0.4708\n",
            "Epoch [18363/20000], Training Loss: 0.4803\n",
            "Epoch [18364/20000], Training Loss: 0.4935\n",
            "Epoch [18365/20000], Training Loss: 0.4910\n",
            "Epoch [18366/20000], Training Loss: 0.5536\n",
            "Epoch [18367/20000], Training Loss: 0.4712\n",
            "Epoch [18368/20000], Training Loss: 0.5123\n",
            "Epoch [18369/20000], Training Loss: 0.5130\n",
            "Epoch [18370/20000], Training Loss: 0.4577\n",
            "Epoch [18371/20000], Training Loss: 0.4644\n",
            "Epoch [18372/20000], Training Loss: 0.4943\n",
            "Epoch [18373/20000], Training Loss: 0.4612\n",
            "Epoch [18374/20000], Training Loss: 0.4948\n",
            "Epoch [18375/20000], Training Loss: 0.4823\n",
            "Epoch [18376/20000], Training Loss: 0.4949\n",
            "Epoch [18377/20000], Training Loss: 0.4520\n",
            "Epoch [18378/20000], Training Loss: 0.4695\n",
            "Epoch [18379/20000], Training Loss: 0.4830\n",
            "Epoch [18380/20000], Training Loss: 0.4583\n",
            "Epoch [18381/20000], Training Loss: 0.4944\n",
            "Epoch [18382/20000], Training Loss: 0.4625\n",
            "Epoch [18383/20000], Training Loss: 0.4968\n",
            "Epoch [18384/20000], Training Loss: 0.5168\n",
            "Epoch [18385/20000], Training Loss: 0.5360\n",
            "Epoch [18386/20000], Training Loss: 0.4471\n",
            "Epoch [18387/20000], Training Loss: 0.4844\n",
            "Epoch [18388/20000], Training Loss: 0.5205\n",
            "Epoch [18389/20000], Training Loss: 0.5101\n",
            "Epoch [18390/20000], Training Loss: 0.4834\n",
            "Epoch [18391/20000], Training Loss: 0.5045\n",
            "Epoch [18392/20000], Training Loss: 0.4889\n",
            "Epoch [18393/20000], Training Loss: 0.4895\n",
            "Epoch [18394/20000], Training Loss: 0.4680\n",
            "Epoch [18395/20000], Training Loss: 0.4721\n",
            "Epoch [18396/20000], Training Loss: 0.4601\n",
            "Epoch [18397/20000], Training Loss: 0.5049\n",
            "Epoch [18398/20000], Training Loss: 0.4926\n",
            "Epoch [18399/20000], Training Loss: 0.4953\n",
            "Epoch [18400/20000], Training Loss: 0.4987\n",
            "Epoch [18401/20000], Training Loss: 0.5295\n",
            "Epoch [18402/20000], Training Loss: 0.5113\n",
            "Epoch [18403/20000], Training Loss: 0.5492\n",
            "Epoch [18404/20000], Training Loss: 0.5090\n",
            "Epoch [18405/20000], Training Loss: 0.4681\n",
            "Epoch [18406/20000], Training Loss: 0.4728\n",
            "Epoch [18407/20000], Training Loss: 0.4444\n",
            "Epoch [18408/20000], Training Loss: 0.5101\n",
            "Epoch [18409/20000], Training Loss: 0.5051\n",
            "Epoch [18410/20000], Training Loss: 0.4858\n",
            "Epoch [18411/20000], Training Loss: 0.5129\n",
            "Epoch [18412/20000], Training Loss: 0.5229\n",
            "Epoch [18413/20000], Training Loss: 0.5353\n",
            "Epoch [18414/20000], Training Loss: 0.4598\n",
            "Epoch [18415/20000], Training Loss: 0.5219\n",
            "Epoch [18416/20000], Training Loss: 0.5287\n",
            "Epoch [18417/20000], Training Loss: 0.4566\n",
            "Epoch [18418/20000], Training Loss: 0.4842\n",
            "Epoch [18419/20000], Training Loss: 0.4991\n",
            "Epoch [18420/20000], Training Loss: 0.5087\n",
            "Epoch [18421/20000], Training Loss: 0.4723\n",
            "Epoch [18422/20000], Training Loss: 0.5070\n",
            "Epoch [18423/20000], Training Loss: 0.5516\n",
            "Epoch [18424/20000], Training Loss: 0.4768\n",
            "Epoch [18425/20000], Training Loss: 0.4616\n",
            "Epoch [18426/20000], Training Loss: 0.4888\n",
            "Epoch [18427/20000], Training Loss: 0.4911\n",
            "Epoch [18428/20000], Training Loss: 0.4565\n",
            "Epoch [18429/20000], Training Loss: 0.4836\n",
            "Epoch [18430/20000], Training Loss: 0.4814\n",
            "Epoch [18431/20000], Training Loss: 0.4746\n",
            "Epoch [18432/20000], Training Loss: 0.4626\n",
            "Epoch [18433/20000], Training Loss: 0.4738\n",
            "Epoch [18434/20000], Training Loss: 0.4616\n",
            "Epoch [18435/20000], Training Loss: 0.4691\n",
            "Epoch [18436/20000], Training Loss: 0.5081\n",
            "Epoch [18437/20000], Training Loss: 0.5113\n",
            "Epoch [18438/20000], Training Loss: 0.4943\n",
            "Epoch [18439/20000], Training Loss: 0.5189\n",
            "Epoch [18440/20000], Training Loss: 0.5423\n",
            "Epoch [18441/20000], Training Loss: 0.4945\n",
            "Epoch [18442/20000], Training Loss: 0.4520\n",
            "Epoch [18443/20000], Training Loss: 0.5256\n",
            "Epoch [18444/20000], Training Loss: 0.4739\n",
            "Epoch [18445/20000], Training Loss: 0.5184\n",
            "Epoch [18446/20000], Training Loss: 0.4782\n",
            "Epoch [18447/20000], Training Loss: 0.5315\n",
            "Epoch [18448/20000], Training Loss: 0.5049\n",
            "Epoch [18449/20000], Training Loss: 0.5284\n",
            "Epoch [18450/20000], Training Loss: 0.4843\n",
            "Epoch [18451/20000], Training Loss: 0.5137\n",
            "Epoch [18452/20000], Training Loss: 0.4503\n",
            "Epoch [18453/20000], Training Loss: 0.5156\n",
            "Epoch [18454/20000], Training Loss: 0.5423\n",
            "Epoch [18455/20000], Training Loss: 0.4916\n",
            "Epoch [18456/20000], Training Loss: 0.4974\n",
            "Epoch [18457/20000], Training Loss: 0.5396\n",
            "Epoch [18458/20000], Training Loss: 0.5065\n",
            "Epoch [18459/20000], Training Loss: 0.5202\n",
            "Epoch [18460/20000], Training Loss: 0.4698\n",
            "Epoch [18461/20000], Training Loss: 0.5278\n",
            "Epoch [18462/20000], Training Loss: 0.4902\n",
            "Epoch [18463/20000], Training Loss: 0.4284\n",
            "Epoch [18464/20000], Training Loss: 0.5426\n",
            "Epoch [18465/20000], Training Loss: 0.4523\n",
            "Epoch [18466/20000], Training Loss: 0.4471\n",
            "Epoch [18467/20000], Training Loss: 0.4413\n",
            "Epoch [18468/20000], Training Loss: 0.5180\n",
            "Epoch [18469/20000], Training Loss: 0.5170\n",
            "Epoch [18470/20000], Training Loss: 0.5088\n",
            "Epoch [18471/20000], Training Loss: 0.4921\n",
            "Epoch [18472/20000], Training Loss: 0.5207\n",
            "Epoch [18473/20000], Training Loss: 0.4646\n",
            "Epoch [18474/20000], Training Loss: 0.4748\n",
            "Epoch [18475/20000], Training Loss: 0.5026\n",
            "Epoch [18476/20000], Training Loss: 0.5406\n",
            "Epoch [18477/20000], Training Loss: 0.4646\n",
            "Epoch [18478/20000], Training Loss: 0.4915\n",
            "Epoch [18479/20000], Training Loss: 0.5001\n",
            "Epoch [18480/20000], Training Loss: 0.4437\n",
            "Epoch [18481/20000], Training Loss: 0.5085\n",
            "Epoch [18482/20000], Training Loss: 0.5181\n",
            "Epoch [18483/20000], Training Loss: 0.5249\n",
            "Epoch [18484/20000], Training Loss: 0.5158\n",
            "Epoch [18485/20000], Training Loss: 0.4516\n",
            "Epoch [18486/20000], Training Loss: 0.4683\n",
            "Epoch [18487/20000], Training Loss: 0.5029\n",
            "Epoch [18488/20000], Training Loss: 0.4948\n",
            "Epoch [18489/20000], Training Loss: 0.4847\n",
            "Epoch [18490/20000], Training Loss: 0.4590\n",
            "Epoch [18491/20000], Training Loss: 0.5329\n",
            "Epoch [18492/20000], Training Loss: 0.4332\n",
            "Epoch [18493/20000], Training Loss: 0.4636\n",
            "Epoch [18494/20000], Training Loss: 0.4669\n",
            "Epoch [18495/20000], Training Loss: 0.4471\n",
            "Epoch [18496/20000], Training Loss: 0.5190\n",
            "Epoch [18497/20000], Training Loss: 0.4737\n",
            "Epoch [18498/20000], Training Loss: 0.5520\n",
            "Epoch [18499/20000], Training Loss: 0.4669\n",
            "Epoch [18500/20000], Training Loss: 0.4766\n",
            "Epoch [18501/20000], Training Loss: 0.4783\n",
            "Epoch [18502/20000], Training Loss: 0.4959\n",
            "Epoch [18503/20000], Training Loss: 0.4790\n",
            "Epoch [18504/20000], Training Loss: 0.4477\n",
            "Epoch [18505/20000], Training Loss: 0.5443\n",
            "Epoch [18506/20000], Training Loss: 0.5219\n",
            "Epoch [18507/20000], Training Loss: 0.4523\n",
            "Epoch [18508/20000], Training Loss: 0.4786\n",
            "Epoch [18509/20000], Training Loss: 0.4753\n",
            "Epoch [18510/20000], Training Loss: 0.4978\n",
            "Epoch [18511/20000], Training Loss: 0.4633\n",
            "Epoch [18512/20000], Training Loss: 0.4958\n",
            "Epoch [18513/20000], Training Loss: 0.5159\n",
            "Epoch [18514/20000], Training Loss: 0.5377\n",
            "Epoch [18515/20000], Training Loss: 0.4929\n",
            "Epoch [18516/20000], Training Loss: 0.4689\n",
            "Epoch [18517/20000], Training Loss: 0.4815\n",
            "Epoch [18518/20000], Training Loss: 0.4951\n",
            "Epoch [18519/20000], Training Loss: 0.4327\n",
            "Epoch [18520/20000], Training Loss: 0.4944\n",
            "Epoch [18521/20000], Training Loss: 0.4842\n",
            "Epoch [18522/20000], Training Loss: 0.4904\n",
            "Epoch [18523/20000], Training Loss: 0.5048\n",
            "Epoch [18524/20000], Training Loss: 0.5000\n",
            "Epoch [18525/20000], Training Loss: 0.4840\n",
            "Epoch [18526/20000], Training Loss: 0.5378\n",
            "Epoch [18527/20000], Training Loss: 0.4614\n",
            "Epoch [18528/20000], Training Loss: 0.4779\n",
            "Epoch [18529/20000], Training Loss: 0.4886\n",
            "Epoch [18530/20000], Training Loss: 0.4695\n",
            "Epoch [18531/20000], Training Loss: 0.4905\n",
            "Epoch [18532/20000], Training Loss: 0.4610\n",
            "Epoch [18533/20000], Training Loss: 0.4875\n",
            "Epoch [18534/20000], Training Loss: 0.5074\n",
            "Epoch [18535/20000], Training Loss: 0.5029\n",
            "Epoch [18536/20000], Training Loss: 0.4788\n",
            "Epoch [18537/20000], Training Loss: 0.4730\n",
            "Epoch [18538/20000], Training Loss: 0.4676\n",
            "Epoch [18539/20000], Training Loss: 0.4603\n",
            "Epoch [18540/20000], Training Loss: 0.5225\n",
            "Epoch [18541/20000], Training Loss: 0.5348\n",
            "Epoch [18542/20000], Training Loss: 0.4736\n",
            "Epoch [18543/20000], Training Loss: 0.4435\n",
            "Epoch [18544/20000], Training Loss: 0.4546\n",
            "Epoch [18545/20000], Training Loss: 0.4695\n",
            "Epoch [18546/20000], Training Loss: 0.5319\n",
            "Epoch [18547/20000], Training Loss: 0.4844\n",
            "Epoch [18548/20000], Training Loss: 0.4598\n",
            "Epoch [18549/20000], Training Loss: 0.4800\n",
            "Epoch [18550/20000], Training Loss: 0.5039\n",
            "Epoch [18551/20000], Training Loss: 0.4695\n",
            "Epoch [18552/20000], Training Loss: 0.4348\n",
            "Epoch [18553/20000], Training Loss: 0.5094\n",
            "Epoch [18554/20000], Training Loss: 0.4860\n",
            "Epoch [18555/20000], Training Loss: 0.5068\n",
            "Epoch [18556/20000], Training Loss: 0.5091\n",
            "Epoch [18557/20000], Training Loss: 0.4739\n",
            "Epoch [18558/20000], Training Loss: 0.5499\n",
            "Epoch [18559/20000], Training Loss: 0.5333\n",
            "Epoch [18560/20000], Training Loss: 0.5392\n",
            "Epoch [18561/20000], Training Loss: 0.4719\n",
            "Epoch [18562/20000], Training Loss: 0.5347\n",
            "Epoch [18563/20000], Training Loss: 0.4745\n",
            "Epoch [18564/20000], Training Loss: 0.4876\n",
            "Epoch [18565/20000], Training Loss: 0.4801\n",
            "Epoch [18566/20000], Training Loss: 0.4908\n",
            "Epoch [18567/20000], Training Loss: 0.5017\n",
            "Epoch [18568/20000], Training Loss: 0.4994\n",
            "Epoch [18569/20000], Training Loss: 0.5083\n",
            "Epoch [18570/20000], Training Loss: 0.5144\n",
            "Epoch [18571/20000], Training Loss: 0.4448\n",
            "Epoch [18572/20000], Training Loss: 0.5163\n",
            "Epoch [18573/20000], Training Loss: 0.4655\n",
            "Epoch [18574/20000], Training Loss: 0.4774\n",
            "Epoch [18575/20000], Training Loss: 0.4842\n",
            "Epoch [18576/20000], Training Loss: 0.4714\n",
            "Epoch [18577/20000], Training Loss: 0.4766\n",
            "Epoch [18578/20000], Training Loss: 0.4350\n",
            "Epoch [18579/20000], Training Loss: 0.4870\n",
            "Epoch [18580/20000], Training Loss: 0.5040\n",
            "Epoch [18581/20000], Training Loss: 0.5072\n",
            "Epoch [18582/20000], Training Loss: 0.4693\n",
            "Epoch [18583/20000], Training Loss: 0.4818\n",
            "Epoch [18584/20000], Training Loss: 0.5098\n",
            "Epoch [18585/20000], Training Loss: 0.4468\n",
            "Epoch [18586/20000], Training Loss: 0.4567\n",
            "Epoch [18587/20000], Training Loss: 0.4535\n",
            "Epoch [18588/20000], Training Loss: 0.5279\n",
            "Epoch [18589/20000], Training Loss: 0.4874\n",
            "Epoch [18590/20000], Training Loss: 0.5266\n",
            "Epoch [18591/20000], Training Loss: 0.4594\n",
            "Epoch [18592/20000], Training Loss: 0.5268\n",
            "Epoch [18593/20000], Training Loss: 0.4804\n",
            "Epoch [18594/20000], Training Loss: 0.4936\n",
            "Epoch [18595/20000], Training Loss: 0.4988\n",
            "Epoch [18596/20000], Training Loss: 0.4537\n",
            "Epoch [18597/20000], Training Loss: 0.5165\n",
            "Epoch [18598/20000], Training Loss: 0.5126\n",
            "Epoch [18599/20000], Training Loss: 0.5337\n",
            "Epoch [18600/20000], Training Loss: 0.4643\n",
            "Epoch [18601/20000], Training Loss: 0.5507\n",
            "Epoch [18602/20000], Training Loss: 0.4446\n",
            "Epoch [18603/20000], Training Loss: 0.4557\n",
            "Epoch [18604/20000], Training Loss: 0.5091\n",
            "Epoch [18605/20000], Training Loss: 0.4913\n",
            "Epoch [18606/20000], Training Loss: 0.4553\n",
            "Epoch [18607/20000], Training Loss: 0.5293\n",
            "Epoch [18608/20000], Training Loss: 0.4445\n",
            "Epoch [18609/20000], Training Loss: 0.5120\n",
            "Epoch [18610/20000], Training Loss: 0.4941\n",
            "Epoch [18611/20000], Training Loss: 0.4505\n",
            "Epoch [18612/20000], Training Loss: 0.4765\n",
            "Epoch [18613/20000], Training Loss: 0.4635\n",
            "Epoch [18614/20000], Training Loss: 0.4742\n",
            "Epoch [18615/20000], Training Loss: 0.4950\n",
            "Epoch [18616/20000], Training Loss: 0.4974\n",
            "Epoch [18617/20000], Training Loss: 0.4651\n",
            "Epoch [18618/20000], Training Loss: 0.4637\n",
            "Epoch [18619/20000], Training Loss: 0.4947\n",
            "Epoch [18620/20000], Training Loss: 0.5120\n",
            "Epoch [18621/20000], Training Loss: 0.4644\n",
            "Epoch [18622/20000], Training Loss: 0.4463\n",
            "Epoch [18623/20000], Training Loss: 0.4927\n",
            "Epoch [18624/20000], Training Loss: 0.4904\n",
            "Epoch [18625/20000], Training Loss: 0.5040\n",
            "Epoch [18626/20000], Training Loss: 0.4899\n",
            "Epoch [18627/20000], Training Loss: 0.4522\n",
            "Epoch [18628/20000], Training Loss: 0.5147\n",
            "Epoch [18629/20000], Training Loss: 0.4886\n",
            "Epoch [18630/20000], Training Loss: 0.5400\n",
            "Epoch [18631/20000], Training Loss: 0.5270\n",
            "Epoch [18632/20000], Training Loss: 0.4212\n",
            "Epoch [18633/20000], Training Loss: 0.4683\n",
            "Epoch [18634/20000], Training Loss: 0.5445\n",
            "Epoch [18635/20000], Training Loss: 0.4679\n",
            "Epoch [18636/20000], Training Loss: 0.4789\n",
            "Epoch [18637/20000], Training Loss: 0.4946\n",
            "Epoch [18638/20000], Training Loss: 0.5060\n",
            "Epoch [18639/20000], Training Loss: 0.4667\n",
            "Epoch [18640/20000], Training Loss: 0.4627\n",
            "Epoch [18641/20000], Training Loss: 0.5013\n",
            "Epoch [18642/20000], Training Loss: 0.4793\n",
            "Epoch [18643/20000], Training Loss: 0.5015\n",
            "Epoch [18644/20000], Training Loss: 0.4583\n",
            "Epoch [18645/20000], Training Loss: 0.4666\n",
            "Epoch [18646/20000], Training Loss: 0.5303\n",
            "Epoch [18647/20000], Training Loss: 0.4900\n",
            "Epoch [18648/20000], Training Loss: 0.4481\n",
            "Epoch [18649/20000], Training Loss: 0.4935\n",
            "Epoch [18650/20000], Training Loss: 0.5060\n",
            "Epoch [18651/20000], Training Loss: 0.5104\n",
            "Epoch [18652/20000], Training Loss: 0.5343\n",
            "Epoch [18653/20000], Training Loss: 0.4454\n",
            "Epoch [18654/20000], Training Loss: 0.4696\n",
            "Epoch [18655/20000], Training Loss: 0.4906\n",
            "Epoch [18656/20000], Training Loss: 0.4730\n",
            "Epoch [18657/20000], Training Loss: 0.5126\n",
            "Epoch [18658/20000], Training Loss: 0.4678\n",
            "Epoch [18659/20000], Training Loss: 0.4802\n",
            "Epoch [18660/20000], Training Loss: 0.4851\n",
            "Epoch [18661/20000], Training Loss: 0.5035\n",
            "Epoch [18662/20000], Training Loss: 0.4670\n",
            "Epoch [18663/20000], Training Loss: 0.4896\n",
            "Epoch [18664/20000], Training Loss: 0.5121\n",
            "Epoch [18665/20000], Training Loss: 0.5112\n",
            "Epoch [18666/20000], Training Loss: 0.5312\n",
            "Epoch [18667/20000], Training Loss: 0.4913\n",
            "Epoch [18668/20000], Training Loss: 0.4573\n",
            "Epoch [18669/20000], Training Loss: 0.4652\n",
            "Epoch [18670/20000], Training Loss: 0.5044\n",
            "Epoch [18671/20000], Training Loss: 0.4955\n",
            "Epoch [18672/20000], Training Loss: 0.4839\n",
            "Epoch [18673/20000], Training Loss: 0.4942\n",
            "Epoch [18674/20000], Training Loss: 0.4982\n",
            "Epoch [18675/20000], Training Loss: 0.5115\n",
            "Epoch [18676/20000], Training Loss: 0.5210\n",
            "Epoch [18677/20000], Training Loss: 0.4652\n",
            "Epoch [18678/20000], Training Loss: 0.4786\n",
            "Epoch [18679/20000], Training Loss: 0.4831\n",
            "Epoch [18680/20000], Training Loss: 0.5517\n",
            "Epoch [18681/20000], Training Loss: 0.4902\n",
            "Epoch [18682/20000], Training Loss: 0.4719\n",
            "Epoch [18683/20000], Training Loss: 0.5451\n",
            "Epoch [18684/20000], Training Loss: 0.4398\n",
            "Epoch [18685/20000], Training Loss: 0.4573\n",
            "Epoch [18686/20000], Training Loss: 0.4816\n",
            "Epoch [18687/20000], Training Loss: 0.4907\n",
            "Epoch [18688/20000], Training Loss: 0.4839\n",
            "Epoch [18689/20000], Training Loss: 0.4869\n",
            "Epoch [18690/20000], Training Loss: 0.5261\n",
            "Epoch [18691/20000], Training Loss: 0.4763\n",
            "Epoch [18692/20000], Training Loss: 0.4619\n",
            "Epoch [18693/20000], Training Loss: 0.4920\n",
            "Epoch [18694/20000], Training Loss: 0.5052\n",
            "Epoch [18695/20000], Training Loss: 0.4898\n",
            "Epoch [18696/20000], Training Loss: 0.5165\n",
            "Epoch [18697/20000], Training Loss: 0.4826\n",
            "Epoch [18698/20000], Training Loss: 0.5029\n",
            "Epoch [18699/20000], Training Loss: 0.5049\n",
            "Epoch [18700/20000], Training Loss: 0.5214\n",
            "Epoch [18701/20000], Training Loss: 0.4882\n",
            "Epoch [18702/20000], Training Loss: 0.4654\n",
            "Epoch [18703/20000], Training Loss: 0.4783\n",
            "Epoch [18704/20000], Training Loss: 0.4633\n",
            "Epoch [18705/20000], Training Loss: 0.4872\n",
            "Epoch [18706/20000], Training Loss: 0.5067\n",
            "Epoch [18707/20000], Training Loss: 0.4568\n",
            "Epoch [18708/20000], Training Loss: 0.4589\n",
            "Epoch [18709/20000], Training Loss: 0.4715\n",
            "Epoch [18710/20000], Training Loss: 0.5528\n",
            "Epoch [18711/20000], Training Loss: 0.5353\n",
            "Epoch [18712/20000], Training Loss: 0.4893\n",
            "Epoch [18713/20000], Training Loss: 0.4726\n",
            "Epoch [18714/20000], Training Loss: 0.4664\n",
            "Epoch [18715/20000], Training Loss: 0.5326\n",
            "Epoch [18716/20000], Training Loss: 0.5310\n",
            "Epoch [18717/20000], Training Loss: 0.4989\n",
            "Epoch [18718/20000], Training Loss: 0.4639\n",
            "Epoch [18719/20000], Training Loss: 0.4754\n",
            "Epoch [18720/20000], Training Loss: 0.5019\n",
            "Epoch [18721/20000], Training Loss: 0.4965\n",
            "Epoch [18722/20000], Training Loss: 0.4868\n",
            "Epoch [18723/20000], Training Loss: 0.4944\n",
            "Epoch [18724/20000], Training Loss: 0.4500\n",
            "Epoch [18725/20000], Training Loss: 0.4989\n",
            "Epoch [18726/20000], Training Loss: 0.5006\n",
            "Epoch [18727/20000], Training Loss: 0.5004\n",
            "Epoch [18728/20000], Training Loss: 0.4861\n",
            "Epoch [18729/20000], Training Loss: 0.5331\n",
            "Epoch [18730/20000], Training Loss: 0.4754\n",
            "Epoch [18731/20000], Training Loss: 0.5088\n",
            "Epoch [18732/20000], Training Loss: 0.4908\n",
            "Epoch [18733/20000], Training Loss: 0.4936\n",
            "Epoch [18734/20000], Training Loss: 0.5013\n",
            "Epoch [18735/20000], Training Loss: 0.5048\n",
            "Epoch [18736/20000], Training Loss: 0.5327\n",
            "Epoch [18737/20000], Training Loss: 0.4708\n",
            "Epoch [18738/20000], Training Loss: 0.5315\n",
            "Epoch [18739/20000], Training Loss: 0.4999\n",
            "Epoch [18740/20000], Training Loss: 0.5102\n",
            "Epoch [18741/20000], Training Loss: 0.5634\n",
            "Epoch [18742/20000], Training Loss: 0.4722\n",
            "Epoch [18743/20000], Training Loss: 0.4603\n",
            "Epoch [18744/20000], Training Loss: 0.5159\n",
            "Epoch [18745/20000], Training Loss: 0.5107\n",
            "Epoch [18746/20000], Training Loss: 0.4238\n",
            "Epoch [18747/20000], Training Loss: 0.4882\n",
            "Epoch [18748/20000], Training Loss: 0.4908\n",
            "Epoch [18749/20000], Training Loss: 0.4516\n",
            "Epoch [18750/20000], Training Loss: 0.5029\n",
            "Epoch [18751/20000], Training Loss: 0.4856\n",
            "Epoch [18752/20000], Training Loss: 0.4425\n",
            "Epoch [18753/20000], Training Loss: 0.5326\n",
            "Epoch [18754/20000], Training Loss: 0.5056\n",
            "Epoch [18755/20000], Training Loss: 0.5245\n",
            "Epoch [18756/20000], Training Loss: 0.5203\n",
            "Epoch [18757/20000], Training Loss: 0.5362\n",
            "Epoch [18758/20000], Training Loss: 0.5233\n",
            "Epoch [18759/20000], Training Loss: 0.5286\n",
            "Epoch [18760/20000], Training Loss: 0.5043\n",
            "Epoch [18761/20000], Training Loss: 0.4742\n",
            "Epoch [18762/20000], Training Loss: 0.5158\n",
            "Epoch [18763/20000], Training Loss: 0.5044\n",
            "Epoch [18764/20000], Training Loss: 0.4805\n",
            "Epoch [18765/20000], Training Loss: 0.4995\n",
            "Epoch [18766/20000], Training Loss: 0.4597\n",
            "Epoch [18767/20000], Training Loss: 0.4643\n",
            "Epoch [18768/20000], Training Loss: 0.4631\n",
            "Epoch [18769/20000], Training Loss: 0.5006\n",
            "Epoch [18770/20000], Training Loss: 0.5015\n",
            "Epoch [18771/20000], Training Loss: 0.4458\n",
            "Epoch [18772/20000], Training Loss: 0.5313\n",
            "Epoch [18773/20000], Training Loss: 0.5096\n",
            "Epoch [18774/20000], Training Loss: 0.4882\n",
            "Epoch [18775/20000], Training Loss: 0.4797\n",
            "Epoch [18776/20000], Training Loss: 0.4674\n",
            "Epoch [18777/20000], Training Loss: 0.4675\n",
            "Epoch [18778/20000], Training Loss: 0.5064\n",
            "Epoch [18779/20000], Training Loss: 0.4811\n",
            "Epoch [18780/20000], Training Loss: 0.4820\n",
            "Epoch [18781/20000], Training Loss: 0.5052\n",
            "Epoch [18782/20000], Training Loss: 0.4825\n",
            "Epoch [18783/20000], Training Loss: 0.5296\n",
            "Epoch [18784/20000], Training Loss: 0.4632\n",
            "Epoch [18785/20000], Training Loss: 0.4889\n",
            "Epoch [18786/20000], Training Loss: 0.4833\n",
            "Epoch [18787/20000], Training Loss: 0.4960\n",
            "Epoch [18788/20000], Training Loss: 0.4863\n",
            "Epoch [18789/20000], Training Loss: 0.5332\n",
            "Epoch [18790/20000], Training Loss: 0.5206\n",
            "Epoch [18791/20000], Training Loss: 0.4432\n",
            "Epoch [18792/20000], Training Loss: 0.5048\n",
            "Epoch [18793/20000], Training Loss: 0.5013\n",
            "Epoch [18794/20000], Training Loss: 0.4701\n",
            "Epoch [18795/20000], Training Loss: 0.5085\n",
            "Epoch [18796/20000], Training Loss: 0.4622\n",
            "Epoch [18797/20000], Training Loss: 0.5203\n",
            "Epoch [18798/20000], Training Loss: 0.5132\n",
            "Epoch [18799/20000], Training Loss: 0.4581\n",
            "Epoch [18800/20000], Training Loss: 0.4661\n",
            "Epoch [18801/20000], Training Loss: 0.4551\n",
            "Epoch [18802/20000], Training Loss: 0.5185\n",
            "Epoch [18803/20000], Training Loss: 0.5281\n",
            "Epoch [18804/20000], Training Loss: 0.5361\n",
            "Epoch [18805/20000], Training Loss: 0.4847\n",
            "Epoch [18806/20000], Training Loss: 0.4769\n",
            "Epoch [18807/20000], Training Loss: 0.5363\n",
            "Epoch [18808/20000], Training Loss: 0.5459\n",
            "Epoch [18809/20000], Training Loss: 0.4634\n",
            "Epoch [18810/20000], Training Loss: 0.4814\n",
            "Epoch [18811/20000], Training Loss: 0.5051\n",
            "Epoch [18812/20000], Training Loss: 0.4620\n",
            "Epoch [18813/20000], Training Loss: 0.4850\n",
            "Epoch [18814/20000], Training Loss: 0.4749\n",
            "Epoch [18815/20000], Training Loss: 0.4772\n",
            "Epoch [18816/20000], Training Loss: 0.5004\n",
            "Epoch [18817/20000], Training Loss: 0.4661\n",
            "Epoch [18818/20000], Training Loss: 0.4795\n",
            "Epoch [18819/20000], Training Loss: 0.5205\n",
            "Epoch [18820/20000], Training Loss: 0.4731\n",
            "Epoch [18821/20000], Training Loss: 0.4960\n",
            "Epoch [18822/20000], Training Loss: 0.4773\n",
            "Epoch [18823/20000], Training Loss: 0.5036\n",
            "Epoch [18824/20000], Training Loss: 0.5031\n",
            "Epoch [18825/20000], Training Loss: 0.4570\n",
            "Epoch [18826/20000], Training Loss: 0.4579\n",
            "Epoch [18827/20000], Training Loss: 0.5260\n",
            "Epoch [18828/20000], Training Loss: 0.4712\n",
            "Epoch [18829/20000], Training Loss: 0.4581\n",
            "Epoch [18830/20000], Training Loss: 0.4870\n",
            "Epoch [18831/20000], Training Loss: 0.4892\n",
            "Epoch [18832/20000], Training Loss: 0.4727\n",
            "Epoch [18833/20000], Training Loss: 0.4880\n",
            "Epoch [18834/20000], Training Loss: 0.4998\n",
            "Epoch [18835/20000], Training Loss: 0.4667\n",
            "Epoch [18836/20000], Training Loss: 0.4803\n",
            "Epoch [18837/20000], Training Loss: 0.4485\n",
            "Epoch [18838/20000], Training Loss: 0.4968\n",
            "Epoch [18839/20000], Training Loss: 0.4979\n",
            "Epoch [18840/20000], Training Loss: 0.4722\n",
            "Epoch [18841/20000], Training Loss: 0.5449\n",
            "Epoch [18842/20000], Training Loss: 0.4934\n",
            "Epoch [18843/20000], Training Loss: 0.4894\n",
            "Epoch [18844/20000], Training Loss: 0.4780\n",
            "Epoch [18845/20000], Training Loss: 0.5043\n",
            "Epoch [18846/20000], Training Loss: 0.5112\n",
            "Epoch [18847/20000], Training Loss: 0.4635\n",
            "Epoch [18848/20000], Training Loss: 0.4831\n",
            "Epoch [18849/20000], Training Loss: 0.4685\n",
            "Epoch [18850/20000], Training Loss: 0.5034\n",
            "Epoch [18851/20000], Training Loss: 0.5196\n",
            "Epoch [18852/20000], Training Loss: 0.4752\n",
            "Epoch [18853/20000], Training Loss: 0.4894\n",
            "Epoch [18854/20000], Training Loss: 0.5736\n",
            "Epoch [18855/20000], Training Loss: 0.4912\n",
            "Epoch [18856/20000], Training Loss: 0.5146\n",
            "Epoch [18857/20000], Training Loss: 0.4629\n",
            "Epoch [18858/20000], Training Loss: 0.4760\n",
            "Epoch [18859/20000], Training Loss: 0.5015\n",
            "Epoch [18860/20000], Training Loss: 0.4825\n",
            "Epoch [18861/20000], Training Loss: 0.5202\n",
            "Epoch [18862/20000], Training Loss: 0.4828\n",
            "Epoch [18863/20000], Training Loss: 0.5315\n",
            "Epoch [18864/20000], Training Loss: 0.4709\n",
            "Epoch [18865/20000], Training Loss: 0.4725\n",
            "Epoch [18866/20000], Training Loss: 0.4850\n",
            "Epoch [18867/20000], Training Loss: 0.5014\n",
            "Epoch [18868/20000], Training Loss: 0.5104\n",
            "Epoch [18869/20000], Training Loss: 0.4782\n",
            "Epoch [18870/20000], Training Loss: 0.4851\n",
            "Epoch [18871/20000], Training Loss: 0.4618\n",
            "Epoch [18872/20000], Training Loss: 0.4777\n",
            "Epoch [18873/20000], Training Loss: 0.4821\n",
            "Epoch [18874/20000], Training Loss: 0.4922\n",
            "Epoch [18875/20000], Training Loss: 0.4793\n",
            "Epoch [18876/20000], Training Loss: 0.4631\n",
            "Epoch [18877/20000], Training Loss: 0.4798\n",
            "Epoch [18878/20000], Training Loss: 0.5495\n",
            "Epoch [18879/20000], Training Loss: 0.4985\n",
            "Epoch [18880/20000], Training Loss: 0.4489\n",
            "Epoch [18881/20000], Training Loss: 0.5504\n",
            "Epoch [18882/20000], Training Loss: 0.4977\n",
            "Epoch [18883/20000], Training Loss: 0.5105\n",
            "Epoch [18884/20000], Training Loss: 0.4837\n",
            "Epoch [18885/20000], Training Loss: 0.4494\n",
            "Epoch [18886/20000], Training Loss: 0.4934\n",
            "Epoch [18887/20000], Training Loss: 0.4870\n",
            "Epoch [18888/20000], Training Loss: 0.4430\n",
            "Epoch [18889/20000], Training Loss: 0.4818\n",
            "Epoch [18890/20000], Training Loss: 0.4595\n",
            "Epoch [18891/20000], Training Loss: 0.5251\n",
            "Epoch [18892/20000], Training Loss: 0.5085\n",
            "Epoch [18893/20000], Training Loss: 0.4691\n",
            "Epoch [18894/20000], Training Loss: 0.5207\n",
            "Epoch [18895/20000], Training Loss: 0.5479\n",
            "Epoch [18896/20000], Training Loss: 0.4780\n",
            "Epoch [18897/20000], Training Loss: 0.4737\n",
            "Epoch [18898/20000], Training Loss: 0.5189\n",
            "Epoch [18899/20000], Training Loss: 0.4971\n",
            "Epoch [18900/20000], Training Loss: 0.4978\n",
            "Epoch [18901/20000], Training Loss: 0.4553\n",
            "Epoch [18902/20000], Training Loss: 0.4840\n",
            "Epoch [18903/20000], Training Loss: 0.4741\n",
            "Epoch [18904/20000], Training Loss: 0.4574\n",
            "Epoch [18905/20000], Training Loss: 0.5221\n",
            "Epoch [18906/20000], Training Loss: 0.5393\n",
            "Epoch [18907/20000], Training Loss: 0.4966\n",
            "Epoch [18908/20000], Training Loss: 0.4762\n",
            "Epoch [18909/20000], Training Loss: 0.4737\n",
            "Epoch [18910/20000], Training Loss: 0.4718\n",
            "Epoch [18911/20000], Training Loss: 0.4864\n",
            "Epoch [18912/20000], Training Loss: 0.4601\n",
            "Epoch [18913/20000], Training Loss: 0.4321\n",
            "Epoch [18914/20000], Training Loss: 0.4945\n",
            "Epoch [18915/20000], Training Loss: 0.5154\n",
            "Epoch [18916/20000], Training Loss: 0.4769\n",
            "Epoch [18917/20000], Training Loss: 0.5381\n",
            "Epoch [18918/20000], Training Loss: 0.5438\n",
            "Epoch [18919/20000], Training Loss: 0.5460\n",
            "Epoch [18920/20000], Training Loss: 0.4942\n",
            "Epoch [18921/20000], Training Loss: 0.5016\n",
            "Epoch [18922/20000], Training Loss: 0.4538\n",
            "Epoch [18923/20000], Training Loss: 0.4378\n",
            "Epoch [18924/20000], Training Loss: 0.4655\n",
            "Epoch [18925/20000], Training Loss: 0.4742\n",
            "Epoch [18926/20000], Training Loss: 0.4988\n",
            "Epoch [18927/20000], Training Loss: 0.4714\n",
            "Epoch [18928/20000], Training Loss: 0.5298\n",
            "Epoch [18929/20000], Training Loss: 0.4695\n",
            "Epoch [18930/20000], Training Loss: 0.4741\n",
            "Epoch [18931/20000], Training Loss: 0.4631\n",
            "Epoch [18932/20000], Training Loss: 0.4596\n",
            "Epoch [18933/20000], Training Loss: 0.4701\n",
            "Epoch [18934/20000], Training Loss: 0.5050\n",
            "Epoch [18935/20000], Training Loss: 0.4594\n",
            "Epoch [18936/20000], Training Loss: 0.4654\n",
            "Epoch [18937/20000], Training Loss: 0.5221\n",
            "Epoch [18938/20000], Training Loss: 0.5048\n",
            "Epoch [18939/20000], Training Loss: 0.4627\n",
            "Epoch [18940/20000], Training Loss: 0.5376\n",
            "Epoch [18941/20000], Training Loss: 0.4876\n",
            "Epoch [18942/20000], Training Loss: 0.4736\n",
            "Epoch [18943/20000], Training Loss: 0.4879\n",
            "Epoch [18944/20000], Training Loss: 0.4648\n",
            "Epoch [18945/20000], Training Loss: 0.5580\n",
            "Epoch [18946/20000], Training Loss: 0.5051\n",
            "Epoch [18947/20000], Training Loss: 0.4691\n",
            "Epoch [18948/20000], Training Loss: 0.5382\n",
            "Epoch [18949/20000], Training Loss: 0.4943\n",
            "Epoch [18950/20000], Training Loss: 0.5267\n",
            "Epoch [18951/20000], Training Loss: 0.5025\n",
            "Epoch [18952/20000], Training Loss: 0.4961\n",
            "Epoch [18953/20000], Training Loss: 0.5177\n",
            "Epoch [18954/20000], Training Loss: 0.5006\n",
            "Epoch [18955/20000], Training Loss: 0.4686\n",
            "Epoch [18956/20000], Training Loss: 0.5142\n",
            "Epoch [18957/20000], Training Loss: 0.4882\n",
            "Epoch [18958/20000], Training Loss: 0.4765\n",
            "Epoch [18959/20000], Training Loss: 0.4633\n",
            "Epoch [18960/20000], Training Loss: 0.4609\n",
            "Epoch [18961/20000], Training Loss: 0.5299\n",
            "Epoch [18962/20000], Training Loss: 0.4581\n",
            "Epoch [18963/20000], Training Loss: 0.5332\n",
            "Epoch [18964/20000], Training Loss: 0.4906\n",
            "Epoch [18965/20000], Training Loss: 0.5013\n",
            "Epoch [18966/20000], Training Loss: 0.5045\n",
            "Epoch [18967/20000], Training Loss: 0.4430\n",
            "Epoch [18968/20000], Training Loss: 0.4592\n",
            "Epoch [18969/20000], Training Loss: 0.5102\n",
            "Epoch [18970/20000], Training Loss: 0.5155\n",
            "Epoch [18971/20000], Training Loss: 0.4279\n",
            "Epoch [18972/20000], Training Loss: 0.5359\n",
            "Epoch [18973/20000], Training Loss: 0.4609\n",
            "Epoch [18974/20000], Training Loss: 0.5398\n",
            "Epoch [18975/20000], Training Loss: 0.4620\n",
            "Epoch [18976/20000], Training Loss: 0.4612\n",
            "Epoch [18977/20000], Training Loss: 0.5095\n",
            "Epoch [18978/20000], Training Loss: 0.4741\n",
            "Epoch [18979/20000], Training Loss: 0.5101\n",
            "Epoch [18980/20000], Training Loss: 0.4949\n",
            "Epoch [18981/20000], Training Loss: 0.4869\n",
            "Epoch [18982/20000], Training Loss: 0.4930\n",
            "Epoch [18983/20000], Training Loss: 0.4928\n",
            "Epoch [18984/20000], Training Loss: 0.4811\n",
            "Epoch [18985/20000], Training Loss: 0.4482\n",
            "Epoch [18986/20000], Training Loss: 0.4839\n",
            "Epoch [18987/20000], Training Loss: 0.5058\n",
            "Epoch [18988/20000], Training Loss: 0.5000\n",
            "Epoch [18989/20000], Training Loss: 0.5297\n",
            "Epoch [18990/20000], Training Loss: 0.4631\n",
            "Epoch [18991/20000], Training Loss: 0.4529\n",
            "Epoch [18992/20000], Training Loss: 0.5018\n",
            "Epoch [18993/20000], Training Loss: 0.4779\n",
            "Epoch [18994/20000], Training Loss: 0.5072\n",
            "Epoch [18995/20000], Training Loss: 0.4645\n",
            "Epoch [18996/20000], Training Loss: 0.5330\n",
            "Epoch [18997/20000], Training Loss: 0.4638\n",
            "Epoch [18998/20000], Training Loss: 0.4528\n",
            "Epoch [18999/20000], Training Loss: 0.4486\n",
            "Epoch [19000/20000], Training Loss: 0.4942\n",
            "Epoch [19001/20000], Training Loss: 0.4892\n",
            "Epoch [19002/20000], Training Loss: 0.4758\n",
            "Epoch [19003/20000], Training Loss: 0.4819\n",
            "Epoch [19004/20000], Training Loss: 0.4546\n",
            "Epoch [19005/20000], Training Loss: 0.5370\n",
            "Epoch [19006/20000], Training Loss: 0.4522\n",
            "Epoch [19007/20000], Training Loss: 0.5482\n",
            "Epoch [19008/20000], Training Loss: 0.4732\n",
            "Epoch [19009/20000], Training Loss: 0.4600\n",
            "Epoch [19010/20000], Training Loss: 0.5212\n",
            "Epoch [19011/20000], Training Loss: 0.5160\n",
            "Epoch [19012/20000], Training Loss: 0.5212\n",
            "Epoch [19013/20000], Training Loss: 0.4763\n",
            "Epoch [19014/20000], Training Loss: 0.4883\n",
            "Epoch [19015/20000], Training Loss: 0.4896\n",
            "Epoch [19016/20000], Training Loss: 0.5423\n",
            "Epoch [19017/20000], Training Loss: 0.5035\n",
            "Epoch [19018/20000], Training Loss: 0.4970\n",
            "Epoch [19019/20000], Training Loss: 0.4726\n",
            "Epoch [19020/20000], Training Loss: 0.4878\n",
            "Epoch [19021/20000], Training Loss: 0.5225\n",
            "Epoch [19022/20000], Training Loss: 0.4507\n",
            "Epoch [19023/20000], Training Loss: 0.4580\n",
            "Epoch [19024/20000], Training Loss: 0.4934\n",
            "Epoch [19025/20000], Training Loss: 0.4853\n",
            "Epoch [19026/20000], Training Loss: 0.5084\n",
            "Epoch [19027/20000], Training Loss: 0.4691\n",
            "Epoch [19028/20000], Training Loss: 0.4860\n",
            "Epoch [19029/20000], Training Loss: 0.5206\n",
            "Epoch [19030/20000], Training Loss: 0.4331\n",
            "Epoch [19031/20000], Training Loss: 0.4756\n",
            "Epoch [19032/20000], Training Loss: 0.4807\n",
            "Epoch [19033/20000], Training Loss: 0.4864\n",
            "Epoch [19034/20000], Training Loss: 0.4643\n",
            "Epoch [19035/20000], Training Loss: 0.4756\n",
            "Epoch [19036/20000], Training Loss: 0.4794\n",
            "Epoch [19037/20000], Training Loss: 0.4927\n",
            "Epoch [19038/20000], Training Loss: 0.5103\n",
            "Epoch [19039/20000], Training Loss: 0.4935\n",
            "Epoch [19040/20000], Training Loss: 0.4831\n",
            "Epoch [19041/20000], Training Loss: 0.5436\n",
            "Epoch [19042/20000], Training Loss: 0.4592\n",
            "Epoch [19043/20000], Training Loss: 0.4697\n",
            "Epoch [19044/20000], Training Loss: 0.4784\n",
            "Epoch [19045/20000], Training Loss: 0.5156\n",
            "Epoch [19046/20000], Training Loss: 0.5052\n",
            "Epoch [19047/20000], Training Loss: 0.5168\n",
            "Epoch [19048/20000], Training Loss: 0.5055\n",
            "Epoch [19049/20000], Training Loss: 0.4951\n",
            "Epoch [19050/20000], Training Loss: 0.4920\n",
            "Epoch [19051/20000], Training Loss: 0.4999\n",
            "Epoch [19052/20000], Training Loss: 0.5142\n",
            "Epoch [19053/20000], Training Loss: 0.4865\n",
            "Epoch [19054/20000], Training Loss: 0.4915\n",
            "Epoch [19055/20000], Training Loss: 0.4616\n",
            "Epoch [19056/20000], Training Loss: 0.5035\n",
            "Epoch [19057/20000], Training Loss: 0.4469\n",
            "Epoch [19058/20000], Training Loss: 0.4873\n",
            "Epoch [19059/20000], Training Loss: 0.4430\n",
            "Epoch [19060/20000], Training Loss: 0.4608\n",
            "Epoch [19061/20000], Training Loss: 0.4634\n",
            "Epoch [19062/20000], Training Loss: 0.4780\n",
            "Epoch [19063/20000], Training Loss: 0.4560\n",
            "Epoch [19064/20000], Training Loss: 0.4675\n",
            "Epoch [19065/20000], Training Loss: 0.5279\n",
            "Epoch [19066/20000], Training Loss: 0.5021\n",
            "Epoch [19067/20000], Training Loss: 0.4555\n",
            "Epoch [19068/20000], Training Loss: 0.5059\n",
            "Epoch [19069/20000], Training Loss: 0.4524\n",
            "Epoch [19070/20000], Training Loss: 0.4562\n",
            "Epoch [19071/20000], Training Loss: 0.4652\n",
            "Epoch [19072/20000], Training Loss: 0.4913\n",
            "Epoch [19073/20000], Training Loss: 0.4751\n",
            "Epoch [19074/20000], Training Loss: 0.5272\n",
            "Epoch [19075/20000], Training Loss: 0.5385\n",
            "Epoch [19076/20000], Training Loss: 0.4531\n",
            "Epoch [19077/20000], Training Loss: 0.4957\n",
            "Epoch [19078/20000], Training Loss: 0.4518\n",
            "Epoch [19079/20000], Training Loss: 0.4897\n",
            "Epoch [19080/20000], Training Loss: 0.5008\n",
            "Epoch [19081/20000], Training Loss: 0.5031\n",
            "Epoch [19082/20000], Training Loss: 0.4509\n",
            "Epoch [19083/20000], Training Loss: 0.4723\n",
            "Epoch [19084/20000], Training Loss: 0.5046\n",
            "Epoch [19085/20000], Training Loss: 0.5108\n",
            "Epoch [19086/20000], Training Loss: 0.4412\n",
            "Epoch [19087/20000], Training Loss: 0.4512\n",
            "Epoch [19088/20000], Training Loss: 0.4668\n",
            "Epoch [19089/20000], Training Loss: 0.4728\n",
            "Epoch [19090/20000], Training Loss: 0.4959\n",
            "Epoch [19091/20000], Training Loss: 0.4684\n",
            "Epoch [19092/20000], Training Loss: 0.4768\n",
            "Epoch [19093/20000], Training Loss: 0.4638\n",
            "Epoch [19094/20000], Training Loss: 0.5105\n",
            "Epoch [19095/20000], Training Loss: 0.5124\n",
            "Epoch [19096/20000], Training Loss: 0.4677\n",
            "Epoch [19097/20000], Training Loss: 0.4992\n",
            "Epoch [19098/20000], Training Loss: 0.5431\n",
            "Epoch [19099/20000], Training Loss: 0.4675\n",
            "Epoch [19100/20000], Training Loss: 0.5563\n",
            "Epoch [19101/20000], Training Loss: 0.4813\n",
            "Epoch [19102/20000], Training Loss: 0.4525\n",
            "Epoch [19103/20000], Training Loss: 0.5083\n",
            "Epoch [19104/20000], Training Loss: 0.4814\n",
            "Epoch [19105/20000], Training Loss: 0.5235\n",
            "Epoch [19106/20000], Training Loss: 0.4904\n",
            "Epoch [19107/20000], Training Loss: 0.4320\n",
            "Epoch [19108/20000], Training Loss: 0.4850\n",
            "Epoch [19109/20000], Training Loss: 0.4761\n",
            "Epoch [19110/20000], Training Loss: 0.4920\n",
            "Epoch [19111/20000], Training Loss: 0.5329\n",
            "Epoch [19112/20000], Training Loss: 0.4600\n",
            "Epoch [19113/20000], Training Loss: 0.4767\n",
            "Epoch [19114/20000], Training Loss: 0.5009\n",
            "Epoch [19115/20000], Training Loss: 0.5507\n",
            "Epoch [19116/20000], Training Loss: 0.4509\n",
            "Epoch [19117/20000], Training Loss: 0.5064\n",
            "Epoch [19118/20000], Training Loss: 0.5186\n",
            "Epoch [19119/20000], Training Loss: 0.4372\n",
            "Epoch [19120/20000], Training Loss: 0.4773\n",
            "Epoch [19121/20000], Training Loss: 0.4775\n",
            "Epoch [19122/20000], Training Loss: 0.4783\n",
            "Epoch [19123/20000], Training Loss: 0.4445\n",
            "Epoch [19124/20000], Training Loss: 0.4934\n",
            "Epoch [19125/20000], Training Loss: 0.5000\n",
            "Epoch [19126/20000], Training Loss: 0.5180\n",
            "Epoch [19127/20000], Training Loss: 0.4793\n",
            "Epoch [19128/20000], Training Loss: 0.4623\n",
            "Epoch [19129/20000], Training Loss: 0.5058\n",
            "Epoch [19130/20000], Training Loss: 0.4590\n",
            "Epoch [19131/20000], Training Loss: 0.5043\n",
            "Epoch [19132/20000], Training Loss: 0.4604\n",
            "Epoch [19133/20000], Training Loss: 0.4834\n",
            "Epoch [19134/20000], Training Loss: 0.4587\n",
            "Epoch [19135/20000], Training Loss: 0.5346\n",
            "Epoch [19136/20000], Training Loss: 0.4693\n",
            "Epoch [19137/20000], Training Loss: 0.4991\n",
            "Epoch [19138/20000], Training Loss: 0.4613\n",
            "Epoch [19139/20000], Training Loss: 0.4377\n",
            "Epoch [19140/20000], Training Loss: 0.5306\n",
            "Epoch [19141/20000], Training Loss: 0.5052\n",
            "Epoch [19142/20000], Training Loss: 0.4695\n",
            "Epoch [19143/20000], Training Loss: 0.4854\n",
            "Epoch [19144/20000], Training Loss: 0.4374\n",
            "Epoch [19145/20000], Training Loss: 0.5115\n",
            "Epoch [19146/20000], Training Loss: 0.5257\n",
            "Epoch [19147/20000], Training Loss: 0.4688\n",
            "Epoch [19148/20000], Training Loss: 0.5199\n",
            "Epoch [19149/20000], Training Loss: 0.5138\n",
            "Epoch [19150/20000], Training Loss: 0.4741\n",
            "Epoch [19151/20000], Training Loss: 0.5438\n",
            "Epoch [19152/20000], Training Loss: 0.4668\n",
            "Epoch [19153/20000], Training Loss: 0.4786\n",
            "Epoch [19154/20000], Training Loss: 0.5198\n",
            "Epoch [19155/20000], Training Loss: 0.5421\n",
            "Epoch [19156/20000], Training Loss: 0.4807\n",
            "Epoch [19157/20000], Training Loss: 0.5416\n",
            "Epoch [19158/20000], Training Loss: 0.5134\n",
            "Epoch [19159/20000], Training Loss: 0.4804\n",
            "Epoch [19160/20000], Training Loss: 0.4951\n",
            "Epoch [19161/20000], Training Loss: 0.5233\n",
            "Epoch [19162/20000], Training Loss: 0.4745\n",
            "Epoch [19163/20000], Training Loss: 0.4586\n",
            "Epoch [19164/20000], Training Loss: 0.4906\n",
            "Epoch [19165/20000], Training Loss: 0.4853\n",
            "Epoch [19166/20000], Training Loss: 0.5018\n",
            "Epoch [19167/20000], Training Loss: 0.4882\n",
            "Epoch [19168/20000], Training Loss: 0.5075\n",
            "Epoch [19169/20000], Training Loss: 0.5279\n",
            "Epoch [19170/20000], Training Loss: 0.4543\n",
            "Epoch [19171/20000], Training Loss: 0.5348\n",
            "Epoch [19172/20000], Training Loss: 0.5140\n",
            "Epoch [19173/20000], Training Loss: 0.4876\n",
            "Epoch [19174/20000], Training Loss: 0.4728\n",
            "Epoch [19175/20000], Training Loss: 0.4726\n",
            "Epoch [19176/20000], Training Loss: 0.4366\n",
            "Epoch [19177/20000], Training Loss: 0.4825\n",
            "Epoch [19178/20000], Training Loss: 0.4925\n",
            "Epoch [19179/20000], Training Loss: 0.4878\n",
            "Epoch [19180/20000], Training Loss: 0.4715\n",
            "Epoch [19181/20000], Training Loss: 0.4851\n",
            "Epoch [19182/20000], Training Loss: 0.4733\n",
            "Epoch [19183/20000], Training Loss: 0.4706\n",
            "Epoch [19184/20000], Training Loss: 0.4726\n",
            "Epoch [19185/20000], Training Loss: 0.4913\n",
            "Epoch [19186/20000], Training Loss: 0.4908\n",
            "Epoch [19187/20000], Training Loss: 0.4529\n",
            "Epoch [19188/20000], Training Loss: 0.5195\n",
            "Epoch [19189/20000], Training Loss: 0.4586\n",
            "Epoch [19190/20000], Training Loss: 0.4920\n",
            "Epoch [19191/20000], Training Loss: 0.4712\n",
            "Epoch [19192/20000], Training Loss: 0.4826\n",
            "Epoch [19193/20000], Training Loss: 0.4871\n",
            "Epoch [19194/20000], Training Loss: 0.4676\n",
            "Epoch [19195/20000], Training Loss: 0.4845\n",
            "Epoch [19196/20000], Training Loss: 0.4556\n",
            "Epoch [19197/20000], Training Loss: 0.5060\n",
            "Epoch [19198/20000], Training Loss: 0.4817\n",
            "Epoch [19199/20000], Training Loss: 0.4985\n",
            "Epoch [19200/20000], Training Loss: 0.5275\n",
            "Epoch [19201/20000], Training Loss: 0.5314\n",
            "Epoch [19202/20000], Training Loss: 0.5067\n",
            "Epoch [19203/20000], Training Loss: 0.4916\n",
            "Epoch [19204/20000], Training Loss: 0.4600\n",
            "Epoch [19205/20000], Training Loss: 0.4931\n",
            "Epoch [19206/20000], Training Loss: 0.4924\n",
            "Epoch [19207/20000], Training Loss: 0.5370\n",
            "Epoch [19208/20000], Training Loss: 0.4940\n",
            "Epoch [19209/20000], Training Loss: 0.4890\n",
            "Epoch [19210/20000], Training Loss: 0.5154\n",
            "Epoch [19211/20000], Training Loss: 0.4702\n",
            "Epoch [19212/20000], Training Loss: 0.5196\n",
            "Epoch [19213/20000], Training Loss: 0.5124\n",
            "Epoch [19214/20000], Training Loss: 0.4791\n",
            "Epoch [19215/20000], Training Loss: 0.4681\n",
            "Epoch [19216/20000], Training Loss: 0.5167\n",
            "Epoch [19217/20000], Training Loss: 0.4690\n",
            "Epoch [19218/20000], Training Loss: 0.4688\n",
            "Epoch [19219/20000], Training Loss: 0.4375\n",
            "Epoch [19220/20000], Training Loss: 0.5104\n",
            "Epoch [19221/20000], Training Loss: 0.4680\n",
            "Epoch [19222/20000], Training Loss: 0.4928\n",
            "Epoch [19223/20000], Training Loss: 0.4567\n",
            "Epoch [19224/20000], Training Loss: 0.4873\n",
            "Epoch [19225/20000], Training Loss: 0.4533\n",
            "Epoch [19226/20000], Training Loss: 0.4514\n",
            "Epoch [19227/20000], Training Loss: 0.5322\n",
            "Epoch [19228/20000], Training Loss: 0.4649\n",
            "Epoch [19229/20000], Training Loss: 0.4651\n",
            "Epoch [19230/20000], Training Loss: 0.5131\n",
            "Epoch [19231/20000], Training Loss: 0.4812\n",
            "Epoch [19232/20000], Training Loss: 0.4537\n",
            "Epoch [19233/20000], Training Loss: 0.4369\n",
            "Epoch [19234/20000], Training Loss: 0.4667\n",
            "Epoch [19235/20000], Training Loss: 0.4675\n",
            "Epoch [19236/20000], Training Loss: 0.5079\n",
            "Epoch [19237/20000], Training Loss: 0.5145\n",
            "Epoch [19238/20000], Training Loss: 0.4800\n",
            "Epoch [19239/20000], Training Loss: 0.4733\n",
            "Epoch [19240/20000], Training Loss: 0.5043\n",
            "Epoch [19241/20000], Training Loss: 0.4886\n",
            "Epoch [19242/20000], Training Loss: 0.4747\n",
            "Epoch [19243/20000], Training Loss: 0.4708\n",
            "Epoch [19244/20000], Training Loss: 0.4963\n",
            "Epoch [19245/20000], Training Loss: 0.5452\n",
            "Epoch [19246/20000], Training Loss: 0.5093\n",
            "Epoch [19247/20000], Training Loss: 0.4988\n",
            "Epoch [19248/20000], Training Loss: 0.4786\n",
            "Epoch [19249/20000], Training Loss: 0.4509\n",
            "Epoch [19250/20000], Training Loss: 0.4612\n",
            "Epoch [19251/20000], Training Loss: 0.4752\n",
            "Epoch [19252/20000], Training Loss: 0.4726\n",
            "Epoch [19253/20000], Training Loss: 0.4540\n",
            "Epoch [19254/20000], Training Loss: 0.4526\n",
            "Epoch [19255/20000], Training Loss: 0.5286\n",
            "Epoch [19256/20000], Training Loss: 0.5246\n",
            "Epoch [19257/20000], Training Loss: 0.4767\n",
            "Epoch [19258/20000], Training Loss: 0.4550\n",
            "Epoch [19259/20000], Training Loss: 0.4675\n",
            "Epoch [19260/20000], Training Loss: 0.4899\n",
            "Epoch [19261/20000], Training Loss: 0.4865\n",
            "Epoch [19262/20000], Training Loss: 0.4615\n",
            "Epoch [19263/20000], Training Loss: 0.4681\n",
            "Epoch [19264/20000], Training Loss: 0.4780\n",
            "Epoch [19265/20000], Training Loss: 0.4972\n",
            "Epoch [19266/20000], Training Loss: 0.5052\n",
            "Epoch [19267/20000], Training Loss: 0.4772\n",
            "Epoch [19268/20000], Training Loss: 0.4699\n",
            "Epoch [19269/20000], Training Loss: 0.5028\n",
            "Epoch [19270/20000], Training Loss: 0.4839\n",
            "Epoch [19271/20000], Training Loss: 0.5249\n",
            "Epoch [19272/20000], Training Loss: 0.4725\n",
            "Epoch [19273/20000], Training Loss: 0.5239\n",
            "Epoch [19274/20000], Training Loss: 0.4780\n",
            "Epoch [19275/20000], Training Loss: 0.4830\n",
            "Epoch [19276/20000], Training Loss: 0.4855\n",
            "Epoch [19277/20000], Training Loss: 0.5095\n",
            "Epoch [19278/20000], Training Loss: 0.4813\n",
            "Epoch [19279/20000], Training Loss: 0.5182\n",
            "Epoch [19280/20000], Training Loss: 0.4895\n",
            "Epoch [19281/20000], Training Loss: 0.4579\n",
            "Epoch [19282/20000], Training Loss: 0.4708\n",
            "Epoch [19283/20000], Training Loss: 0.4983\n",
            "Epoch [19284/20000], Training Loss: 0.5303\n",
            "Epoch [19285/20000], Training Loss: 0.4990\n",
            "Epoch [19286/20000], Training Loss: 0.4775\n",
            "Epoch [19287/20000], Training Loss: 0.4776\n",
            "Epoch [19288/20000], Training Loss: 0.4809\n",
            "Epoch [19289/20000], Training Loss: 0.5213\n",
            "Epoch [19290/20000], Training Loss: 0.4805\n",
            "Epoch [19291/20000], Training Loss: 0.4645\n",
            "Epoch [19292/20000], Training Loss: 0.5361\n",
            "Epoch [19293/20000], Training Loss: 0.4863\n",
            "Epoch [19294/20000], Training Loss: 0.5212\n",
            "Epoch [19295/20000], Training Loss: 0.5186\n",
            "Epoch [19296/20000], Training Loss: 0.5068\n",
            "Epoch [19297/20000], Training Loss: 0.4648\n",
            "Epoch [19298/20000], Training Loss: 0.4803\n",
            "Epoch [19299/20000], Training Loss: 0.5262\n",
            "Epoch [19300/20000], Training Loss: 0.4531\n",
            "Epoch [19301/20000], Training Loss: 0.4651\n",
            "Epoch [19302/20000], Training Loss: 0.4778\n",
            "Epoch [19303/20000], Training Loss: 0.4468\n",
            "Epoch [19304/20000], Training Loss: 0.4811\n",
            "Epoch [19305/20000], Training Loss: 0.4934\n",
            "Epoch [19306/20000], Training Loss: 0.4629\n",
            "Epoch [19307/20000], Training Loss: 0.4958\n",
            "Epoch [19308/20000], Training Loss: 0.4734\n",
            "Epoch [19309/20000], Training Loss: 0.4551\n",
            "Epoch [19310/20000], Training Loss: 0.4656\n",
            "Epoch [19311/20000], Training Loss: 0.5093\n",
            "Epoch [19312/20000], Training Loss: 0.4917\n",
            "Epoch [19313/20000], Training Loss: 0.5092\n",
            "Epoch [19314/20000], Training Loss: 0.5029\n",
            "Epoch [19315/20000], Training Loss: 0.4918\n",
            "Epoch [19316/20000], Training Loss: 0.5364\n",
            "Epoch [19317/20000], Training Loss: 0.4918\n",
            "Epoch [19318/20000], Training Loss: 0.5038\n",
            "Epoch [19319/20000], Training Loss: 0.5281\n",
            "Epoch [19320/20000], Training Loss: 0.4723\n",
            "Epoch [19321/20000], Training Loss: 0.5041\n",
            "Epoch [19322/20000], Training Loss: 0.5329\n",
            "Epoch [19323/20000], Training Loss: 0.5179\n",
            "Epoch [19324/20000], Training Loss: 0.4787\n",
            "Epoch [19325/20000], Training Loss: 0.5101\n",
            "Epoch [19326/20000], Training Loss: 0.4544\n",
            "Epoch [19327/20000], Training Loss: 0.4710\n",
            "Epoch [19328/20000], Training Loss: 0.4692\n",
            "Epoch [19329/20000], Training Loss: 0.4980\n",
            "Epoch [19330/20000], Training Loss: 0.5226\n",
            "Epoch [19331/20000], Training Loss: 0.5039\n",
            "Epoch [19332/20000], Training Loss: 0.4689\n",
            "Epoch [19333/20000], Training Loss: 0.4502\n",
            "Epoch [19334/20000], Training Loss: 0.4725\n",
            "Epoch [19335/20000], Training Loss: 0.5198\n",
            "Epoch [19336/20000], Training Loss: 0.4814\n",
            "Epoch [19337/20000], Training Loss: 0.4472\n",
            "Epoch [19338/20000], Training Loss: 0.4671\n",
            "Epoch [19339/20000], Training Loss: 0.4716\n",
            "Epoch [19340/20000], Training Loss: 0.4679\n",
            "Epoch [19341/20000], Training Loss: 0.5150\n",
            "Epoch [19342/20000], Training Loss: 0.4645\n",
            "Epoch [19343/20000], Training Loss: 0.4750\n",
            "Epoch [19344/20000], Training Loss: 0.5040\n",
            "Epoch [19345/20000], Training Loss: 0.4391\n",
            "Epoch [19346/20000], Training Loss: 0.4654\n",
            "Epoch [19347/20000], Training Loss: 0.4707\n",
            "Epoch [19348/20000], Training Loss: 0.4647\n",
            "Epoch [19349/20000], Training Loss: 0.4456\n",
            "Epoch [19350/20000], Training Loss: 0.5063\n",
            "Epoch [19351/20000], Training Loss: 0.4612\n",
            "Epoch [19352/20000], Training Loss: 0.4524\n",
            "Epoch [19353/20000], Training Loss: 0.4602\n",
            "Epoch [19354/20000], Training Loss: 0.4882\n",
            "Epoch [19355/20000], Training Loss: 0.5125\n",
            "Epoch [19356/20000], Training Loss: 0.5460\n",
            "Epoch [19357/20000], Training Loss: 0.5329\n",
            "Epoch [19358/20000], Training Loss: 0.5019\n",
            "Epoch [19359/20000], Training Loss: 0.5283\n",
            "Epoch [19360/20000], Training Loss: 0.4611\n",
            "Epoch [19361/20000], Training Loss: 0.4829\n",
            "Epoch [19362/20000], Training Loss: 0.5037\n",
            "Epoch [19363/20000], Training Loss: 0.4729\n",
            "Epoch [19364/20000], Training Loss: 0.4786\n",
            "Epoch [19365/20000], Training Loss: 0.4680\n",
            "Epoch [19366/20000], Training Loss: 0.5222\n",
            "Epoch [19367/20000], Training Loss: 0.4702\n",
            "Epoch [19368/20000], Training Loss: 0.4647\n",
            "Epoch [19369/20000], Training Loss: 0.4504\n",
            "Epoch [19370/20000], Training Loss: 0.4942\n",
            "Epoch [19371/20000], Training Loss: 0.4903\n",
            "Epoch [19372/20000], Training Loss: 0.4734\n",
            "Epoch [19373/20000], Training Loss: 0.5021\n",
            "Epoch [19374/20000], Training Loss: 0.4567\n",
            "Epoch [19375/20000], Training Loss: 0.4856\n",
            "Epoch [19376/20000], Training Loss: 0.4876\n",
            "Epoch [19377/20000], Training Loss: 0.4991\n",
            "Epoch [19378/20000], Training Loss: 0.4833\n",
            "Epoch [19379/20000], Training Loss: 0.4578\n",
            "Epoch [19380/20000], Training Loss: 0.5190\n",
            "Epoch [19381/20000], Training Loss: 0.4569\n",
            "Epoch [19382/20000], Training Loss: 0.4713\n",
            "Epoch [19383/20000], Training Loss: 0.5020\n",
            "Epoch [19384/20000], Training Loss: 0.4653\n",
            "Epoch [19385/20000], Training Loss: 0.4662\n",
            "Epoch [19386/20000], Training Loss: 0.5587\n",
            "Epoch [19387/20000], Training Loss: 0.4676\n",
            "Epoch [19388/20000], Training Loss: 0.5080\n",
            "Epoch [19389/20000], Training Loss: 0.5319\n",
            "Epoch [19390/20000], Training Loss: 0.4809\n",
            "Epoch [19391/20000], Training Loss: 0.5015\n",
            "Epoch [19392/20000], Training Loss: 0.4996\n",
            "Epoch [19393/20000], Training Loss: 0.4895\n",
            "Epoch [19394/20000], Training Loss: 0.4520\n",
            "Epoch [19395/20000], Training Loss: 0.4712\n",
            "Epoch [19396/20000], Training Loss: 0.5007\n",
            "Epoch [19397/20000], Training Loss: 0.5096\n",
            "Epoch [19398/20000], Training Loss: 0.4643\n",
            "Epoch [19399/20000], Training Loss: 0.4880\n",
            "Epoch [19400/20000], Training Loss: 0.4574\n",
            "Epoch [19401/20000], Training Loss: 0.4425\n",
            "Epoch [19402/20000], Training Loss: 0.4768\n",
            "Epoch [19403/20000], Training Loss: 0.4894\n",
            "Epoch [19404/20000], Training Loss: 0.4764\n",
            "Epoch [19405/20000], Training Loss: 0.4728\n",
            "Epoch [19406/20000], Training Loss: 0.4390\n",
            "Epoch [19407/20000], Training Loss: 0.5005\n",
            "Epoch [19408/20000], Training Loss: 0.4595\n",
            "Epoch [19409/20000], Training Loss: 0.4978\n",
            "Epoch [19410/20000], Training Loss: 0.4866\n",
            "Epoch [19411/20000], Training Loss: 0.5059\n",
            "Epoch [19412/20000], Training Loss: 0.4995\n",
            "Epoch [19413/20000], Training Loss: 0.5089\n",
            "Epoch [19414/20000], Training Loss: 0.5012\n",
            "Epoch [19415/20000], Training Loss: 0.4821\n",
            "Epoch [19416/20000], Training Loss: 0.4528\n",
            "Epoch [19417/20000], Training Loss: 0.5007\n",
            "Epoch [19418/20000], Training Loss: 0.4829\n",
            "Epoch [19419/20000], Training Loss: 0.4960\n",
            "Epoch [19420/20000], Training Loss: 0.4559\n",
            "Epoch [19421/20000], Training Loss: 0.4754\n",
            "Epoch [19422/20000], Training Loss: 0.4606\n",
            "Epoch [19423/20000], Training Loss: 0.4716\n",
            "Epoch [19424/20000], Training Loss: 0.4758\n",
            "Epoch [19425/20000], Training Loss: 0.4401\n",
            "Epoch [19426/20000], Training Loss: 0.4672\n",
            "Epoch [19427/20000], Training Loss: 0.4587\n",
            "Epoch [19428/20000], Training Loss: 0.4544\n",
            "Epoch [19429/20000], Training Loss: 0.5103\n",
            "Epoch [19430/20000], Training Loss: 0.4804\n",
            "Epoch [19431/20000], Training Loss: 0.5462\n",
            "Epoch [19432/20000], Training Loss: 0.5108\n",
            "Epoch [19433/20000], Training Loss: 0.5030\n",
            "Epoch [19434/20000], Training Loss: 0.4896\n",
            "Epoch [19435/20000], Training Loss: 0.4869\n",
            "Epoch [19436/20000], Training Loss: 0.4925\n",
            "Epoch [19437/20000], Training Loss: 0.4886\n",
            "Epoch [19438/20000], Training Loss: 0.4952\n",
            "Epoch [19439/20000], Training Loss: 0.5149\n",
            "Epoch [19440/20000], Training Loss: 0.4691\n",
            "Epoch [19441/20000], Training Loss: 0.5100\n",
            "Epoch [19442/20000], Training Loss: 0.5092\n",
            "Epoch [19443/20000], Training Loss: 0.4904\n",
            "Epoch [19444/20000], Training Loss: 0.4935\n",
            "Epoch [19445/20000], Training Loss: 0.4451\n",
            "Epoch [19446/20000], Training Loss: 0.4649\n",
            "Epoch [19447/20000], Training Loss: 0.4832\n",
            "Epoch [19448/20000], Training Loss: 0.5340\n",
            "Epoch [19449/20000], Training Loss: 0.4970\n",
            "Epoch [19450/20000], Training Loss: 0.4550\n",
            "Epoch [19451/20000], Training Loss: 0.5026\n",
            "Epoch [19452/20000], Training Loss: 0.4926\n",
            "Epoch [19453/20000], Training Loss: 0.5034\n",
            "Epoch [19454/20000], Training Loss: 0.5446\n",
            "Epoch [19455/20000], Training Loss: 0.4671\n",
            "Epoch [19456/20000], Training Loss: 0.4873\n",
            "Epoch [19457/20000], Training Loss: 0.4828\n",
            "Epoch [19458/20000], Training Loss: 0.4707\n",
            "Epoch [19459/20000], Training Loss: 0.4964\n",
            "Epoch [19460/20000], Training Loss: 0.5195\n",
            "Epoch [19461/20000], Training Loss: 0.5209\n",
            "Epoch [19462/20000], Training Loss: 0.5171\n",
            "Epoch [19463/20000], Training Loss: 0.4614\n",
            "Epoch [19464/20000], Training Loss: 0.4533\n",
            "Epoch [19465/20000], Training Loss: 0.4778\n",
            "Epoch [19466/20000], Training Loss: 0.5322\n",
            "Epoch [19467/20000], Training Loss: 0.5078\n",
            "Epoch [19468/20000], Training Loss: 0.4823\n",
            "Epoch [19469/20000], Training Loss: 0.4583\n",
            "Epoch [19470/20000], Training Loss: 0.5149\n",
            "Epoch [19471/20000], Training Loss: 0.4867\n",
            "Epoch [19472/20000], Training Loss: 0.4628\n",
            "Epoch [19473/20000], Training Loss: 0.4724\n",
            "Epoch [19474/20000], Training Loss: 0.5170\n",
            "Epoch [19475/20000], Training Loss: 0.5334\n",
            "Epoch [19476/20000], Training Loss: 0.4822\n",
            "Epoch [19477/20000], Training Loss: 0.4919\n",
            "Epoch [19478/20000], Training Loss: 0.4835\n",
            "Epoch [19479/20000], Training Loss: 0.5247\n",
            "Epoch [19480/20000], Training Loss: 0.4698\n",
            "Epoch [19481/20000], Training Loss: 0.4993\n",
            "Epoch [19482/20000], Training Loss: 0.5082\n",
            "Epoch [19483/20000], Training Loss: 0.5542\n",
            "Epoch [19484/20000], Training Loss: 0.4816\n",
            "Epoch [19485/20000], Training Loss: 0.5175\n",
            "Epoch [19486/20000], Training Loss: 0.5114\n",
            "Epoch [19487/20000], Training Loss: 0.4821\n",
            "Epoch [19488/20000], Training Loss: 0.5002\n",
            "Epoch [19489/20000], Training Loss: 0.4499\n",
            "Epoch [19490/20000], Training Loss: 0.5054\n",
            "Epoch [19491/20000], Training Loss: 0.5172\n",
            "Epoch [19492/20000], Training Loss: 0.4414\n",
            "Epoch [19493/20000], Training Loss: 0.4802\n",
            "Epoch [19494/20000], Training Loss: 0.4769\n",
            "Epoch [19495/20000], Training Loss: 0.5001\n",
            "Epoch [19496/20000], Training Loss: 0.4958\n",
            "Epoch [19497/20000], Training Loss: 0.4445\n",
            "Epoch [19498/20000], Training Loss: 0.5225\n",
            "Epoch [19499/20000], Training Loss: 0.4657\n",
            "Epoch [19500/20000], Training Loss: 0.4900\n",
            "Epoch [19501/20000], Training Loss: 0.4739\n",
            "Epoch [19502/20000], Training Loss: 0.4571\n",
            "Epoch [19503/20000], Training Loss: 0.4460\n",
            "Epoch [19504/20000], Training Loss: 0.4855\n",
            "Epoch [19505/20000], Training Loss: 0.5087\n",
            "Epoch [19506/20000], Training Loss: 0.5593\n",
            "Epoch [19507/20000], Training Loss: 0.4709\n",
            "Epoch [19508/20000], Training Loss: 0.4944\n",
            "Epoch [19509/20000], Training Loss: 0.4906\n",
            "Epoch [19510/20000], Training Loss: 0.4763\n",
            "Epoch [19511/20000], Training Loss: 0.4832\n",
            "Epoch [19512/20000], Training Loss: 0.5175\n",
            "Epoch [19513/20000], Training Loss: 0.4922\n",
            "Epoch [19514/20000], Training Loss: 0.4927\n",
            "Epoch [19515/20000], Training Loss: 0.4811\n",
            "Epoch [19516/20000], Training Loss: 0.4672\n",
            "Epoch [19517/20000], Training Loss: 0.4766\n",
            "Epoch [19518/20000], Training Loss: 0.4849\n",
            "Epoch [19519/20000], Training Loss: 0.4658\n",
            "Epoch [19520/20000], Training Loss: 0.4928\n",
            "Epoch [19521/20000], Training Loss: 0.5336\n",
            "Epoch [19522/20000], Training Loss: 0.4893\n",
            "Epoch [19523/20000], Training Loss: 0.4806\n",
            "Epoch [19524/20000], Training Loss: 0.4967\n",
            "Epoch [19525/20000], Training Loss: 0.4986\n",
            "Epoch [19526/20000], Training Loss: 0.5036\n",
            "Epoch [19527/20000], Training Loss: 0.5192\n",
            "Epoch [19528/20000], Training Loss: 0.5088\n",
            "Epoch [19529/20000], Training Loss: 0.4779\n",
            "Epoch [19530/20000], Training Loss: 0.5070\n",
            "Epoch [19531/20000], Training Loss: 0.4968\n",
            "Epoch [19532/20000], Training Loss: 0.5119\n",
            "Epoch [19533/20000], Training Loss: 0.4772\n",
            "Epoch [19534/20000], Training Loss: 0.5222\n",
            "Epoch [19535/20000], Training Loss: 0.4799\n",
            "Epoch [19536/20000], Training Loss: 0.4801\n",
            "Epoch [19537/20000], Training Loss: 0.5121\n",
            "Epoch [19538/20000], Training Loss: 0.5037\n",
            "Epoch [19539/20000], Training Loss: 0.5345\n",
            "Epoch [19540/20000], Training Loss: 0.5002\n",
            "Epoch [19541/20000], Training Loss: 0.4526\n",
            "Epoch [19542/20000], Training Loss: 0.4299\n",
            "Epoch [19543/20000], Training Loss: 0.5132\n",
            "Epoch [19544/20000], Training Loss: 0.5015\n",
            "Epoch [19545/20000], Training Loss: 0.4961\n",
            "Epoch [19546/20000], Training Loss: 0.4883\n",
            "Epoch [19547/20000], Training Loss: 0.4347\n",
            "Epoch [19548/20000], Training Loss: 0.4901\n",
            "Epoch [19549/20000], Training Loss: 0.5046\n",
            "Epoch [19550/20000], Training Loss: 0.4798\n",
            "Epoch [19551/20000], Training Loss: 0.4902\n",
            "Epoch [19552/20000], Training Loss: 0.5104\n",
            "Epoch [19553/20000], Training Loss: 0.4846\n",
            "Epoch [19554/20000], Training Loss: 0.4760\n",
            "Epoch [19555/20000], Training Loss: 0.5266\n",
            "Epoch [19556/20000], Training Loss: 0.4990\n",
            "Epoch [19557/20000], Training Loss: 0.4734\n",
            "Epoch [19558/20000], Training Loss: 0.4960\n",
            "Epoch [19559/20000], Training Loss: 0.4870\n",
            "Epoch [19560/20000], Training Loss: 0.5345\n",
            "Epoch [19561/20000], Training Loss: 0.4656\n",
            "Epoch [19562/20000], Training Loss: 0.4610\n",
            "Epoch [19563/20000], Training Loss: 0.5213\n",
            "Epoch [19564/20000], Training Loss: 0.4403\n",
            "Epoch [19565/20000], Training Loss: 0.5096\n",
            "Epoch [19566/20000], Training Loss: 0.4901\n",
            "Epoch [19567/20000], Training Loss: 0.5271\n",
            "Epoch [19568/20000], Training Loss: 0.4581\n",
            "Epoch [19569/20000], Training Loss: 0.4689\n",
            "Epoch [19570/20000], Training Loss: 0.4890\n",
            "Epoch [19571/20000], Training Loss: 0.4654\n",
            "Epoch [19572/20000], Training Loss: 0.4671\n",
            "Epoch [19573/20000], Training Loss: 0.4774\n",
            "Epoch [19574/20000], Training Loss: 0.5164\n",
            "Epoch [19575/20000], Training Loss: 0.5112\n",
            "Epoch [19576/20000], Training Loss: 0.4575\n",
            "Epoch [19577/20000], Training Loss: 0.4737\n",
            "Epoch [19578/20000], Training Loss: 0.4835\n",
            "Epoch [19579/20000], Training Loss: 0.4851\n",
            "Epoch [19580/20000], Training Loss: 0.5019\n",
            "Epoch [19581/20000], Training Loss: 0.4787\n",
            "Epoch [19582/20000], Training Loss: 0.4710\n",
            "Epoch [19583/20000], Training Loss: 0.4936\n",
            "Epoch [19584/20000], Training Loss: 0.5027\n",
            "Epoch [19585/20000], Training Loss: 0.5505\n",
            "Epoch [19586/20000], Training Loss: 0.5243\n",
            "Epoch [19587/20000], Training Loss: 0.5113\n",
            "Epoch [19588/20000], Training Loss: 0.5042\n",
            "Epoch [19589/20000], Training Loss: 0.4753\n",
            "Epoch [19590/20000], Training Loss: 0.5144\n",
            "Epoch [19591/20000], Training Loss: 0.4829\n",
            "Epoch [19592/20000], Training Loss: 0.4707\n",
            "Epoch [19593/20000], Training Loss: 0.4728\n",
            "Epoch [19594/20000], Training Loss: 0.4626\n",
            "Epoch [19595/20000], Training Loss: 0.4844\n",
            "Epoch [19596/20000], Training Loss: 0.4492\n",
            "Epoch [19597/20000], Training Loss: 0.5011\n",
            "Epoch [19598/20000], Training Loss: 0.4704\n",
            "Epoch [19599/20000], Training Loss: 0.5024\n",
            "Epoch [19600/20000], Training Loss: 0.4961\n",
            "Epoch [19601/20000], Training Loss: 0.5396\n",
            "Epoch [19602/20000], Training Loss: 0.4962\n",
            "Epoch [19603/20000], Training Loss: 0.5092\n",
            "Epoch [19604/20000], Training Loss: 0.4911\n",
            "Epoch [19605/20000], Training Loss: 0.4894\n",
            "Epoch [19606/20000], Training Loss: 0.5403\n",
            "Epoch [19607/20000], Training Loss: 0.5109\n",
            "Epoch [19608/20000], Training Loss: 0.5041\n",
            "Epoch [19609/20000], Training Loss: 0.4869\n",
            "Epoch [19610/20000], Training Loss: 0.4843\n",
            "Epoch [19611/20000], Training Loss: 0.5049\n",
            "Epoch [19612/20000], Training Loss: 0.5210\n",
            "Epoch [19613/20000], Training Loss: 0.4704\n",
            "Epoch [19614/20000], Training Loss: 0.4921\n",
            "Epoch [19615/20000], Training Loss: 0.5073\n",
            "Epoch [19616/20000], Training Loss: 0.4580\n",
            "Epoch [19617/20000], Training Loss: 0.5428\n",
            "Epoch [19618/20000], Training Loss: 0.4875\n",
            "Epoch [19619/20000], Training Loss: 0.4631\n",
            "Epoch [19620/20000], Training Loss: 0.4670\n",
            "Epoch [19621/20000], Training Loss: 0.4730\n",
            "Epoch [19622/20000], Training Loss: 0.4818\n",
            "Epoch [19623/20000], Training Loss: 0.4647\n",
            "Epoch [19624/20000], Training Loss: 0.5114\n",
            "Epoch [19625/20000], Training Loss: 0.4980\n",
            "Epoch [19626/20000], Training Loss: 0.5104\n",
            "Epoch [19627/20000], Training Loss: 0.4642\n",
            "Epoch [19628/20000], Training Loss: 0.5068\n",
            "Epoch [19629/20000], Training Loss: 0.4856\n",
            "Epoch [19630/20000], Training Loss: 0.4649\n",
            "Epoch [19631/20000], Training Loss: 0.5216\n",
            "Epoch [19632/20000], Training Loss: 0.4493\n",
            "Epoch [19633/20000], Training Loss: 0.4872\n",
            "Epoch [19634/20000], Training Loss: 0.4564\n",
            "Epoch [19635/20000], Training Loss: 0.5707\n",
            "Epoch [19636/20000], Training Loss: 0.4481\n",
            "Epoch [19637/20000], Training Loss: 0.4882\n",
            "Epoch [19638/20000], Training Loss: 0.5343\n",
            "Epoch [19639/20000], Training Loss: 0.4714\n",
            "Epoch [19640/20000], Training Loss: 0.4736\n",
            "Epoch [19641/20000], Training Loss: 0.4892\n",
            "Epoch [19642/20000], Training Loss: 0.5146\n",
            "Epoch [19643/20000], Training Loss: 0.5010\n",
            "Epoch [19644/20000], Training Loss: 0.4419\n",
            "Epoch [19645/20000], Training Loss: 0.4908\n",
            "Epoch [19646/20000], Training Loss: 0.5006\n",
            "Epoch [19647/20000], Training Loss: 0.4895\n",
            "Epoch [19648/20000], Training Loss: 0.4493\n",
            "Epoch [19649/20000], Training Loss: 0.4902\n",
            "Epoch [19650/20000], Training Loss: 0.4991\n",
            "Epoch [19651/20000], Training Loss: 0.4831\n",
            "Epoch [19652/20000], Training Loss: 0.5149\n",
            "Epoch [19653/20000], Training Loss: 0.4710\n",
            "Epoch [19654/20000], Training Loss: 0.4969\n",
            "Epoch [19655/20000], Training Loss: 0.5022\n",
            "Epoch [19656/20000], Training Loss: 0.4900\n",
            "Epoch [19657/20000], Training Loss: 0.4950\n",
            "Epoch [19658/20000], Training Loss: 0.5335\n",
            "Epoch [19659/20000], Training Loss: 0.5297\n",
            "Epoch [19660/20000], Training Loss: 0.5302\n",
            "Epoch [19661/20000], Training Loss: 0.4713\n",
            "Epoch [19662/20000], Training Loss: 0.5085\n",
            "Epoch [19663/20000], Training Loss: 0.4432\n",
            "Epoch [19664/20000], Training Loss: 0.4727\n",
            "Epoch [19665/20000], Training Loss: 0.5167\n",
            "Epoch [19666/20000], Training Loss: 0.4627\n",
            "Epoch [19667/20000], Training Loss: 0.4496\n",
            "Epoch [19668/20000], Training Loss: 0.5285\n",
            "Epoch [19669/20000], Training Loss: 0.5144\n",
            "Epoch [19670/20000], Training Loss: 0.5172\n",
            "Epoch [19671/20000], Training Loss: 0.5042\n",
            "Epoch [19672/20000], Training Loss: 0.5182\n",
            "Epoch [19673/20000], Training Loss: 0.5170\n",
            "Epoch [19674/20000], Training Loss: 0.4412\n",
            "Epoch [19675/20000], Training Loss: 0.4918\n",
            "Epoch [19676/20000], Training Loss: 0.4603\n",
            "Epoch [19677/20000], Training Loss: 0.4867\n",
            "Epoch [19678/20000], Training Loss: 0.4653\n",
            "Epoch [19679/20000], Training Loss: 0.4718\n",
            "Epoch [19680/20000], Training Loss: 0.4571\n",
            "Epoch [19681/20000], Training Loss: 0.4551\n",
            "Epoch [19682/20000], Training Loss: 0.4692\n",
            "Epoch [19683/20000], Training Loss: 0.4927\n",
            "Epoch [19684/20000], Training Loss: 0.5009\n",
            "Epoch [19685/20000], Training Loss: 0.5241\n",
            "Epoch [19686/20000], Training Loss: 0.4489\n",
            "Epoch [19687/20000], Training Loss: 0.5109\n",
            "Epoch [19688/20000], Training Loss: 0.4771\n",
            "Epoch [19689/20000], Training Loss: 0.4983\n",
            "Epoch [19690/20000], Training Loss: 0.4626\n",
            "Epoch [19691/20000], Training Loss: 0.4793\n",
            "Epoch [19692/20000], Training Loss: 0.5147\n",
            "Epoch [19693/20000], Training Loss: 0.4666\n",
            "Epoch [19694/20000], Training Loss: 0.4781\n",
            "Epoch [19695/20000], Training Loss: 0.5012\n",
            "Epoch [19696/20000], Training Loss: 0.4319\n",
            "Epoch [19697/20000], Training Loss: 0.4838\n",
            "Epoch [19698/20000], Training Loss: 0.5037\n",
            "Epoch [19699/20000], Training Loss: 0.5086\n",
            "Epoch [19700/20000], Training Loss: 0.4863\n",
            "Epoch [19701/20000], Training Loss: 0.5188\n",
            "Epoch [19702/20000], Training Loss: 0.4846\n",
            "Epoch [19703/20000], Training Loss: 0.5108\n",
            "Epoch [19704/20000], Training Loss: 0.5021\n",
            "Epoch [19705/20000], Training Loss: 0.4577\n",
            "Epoch [19706/20000], Training Loss: 0.5008\n",
            "Epoch [19707/20000], Training Loss: 0.4825\n",
            "Epoch [19708/20000], Training Loss: 0.4832\n",
            "Epoch [19709/20000], Training Loss: 0.4861\n",
            "Epoch [19710/20000], Training Loss: 0.4729\n",
            "Epoch [19711/20000], Training Loss: 0.4931\n",
            "Epoch [19712/20000], Training Loss: 0.4891\n",
            "Epoch [19713/20000], Training Loss: 0.5230\n",
            "Epoch [19714/20000], Training Loss: 0.5125\n",
            "Epoch [19715/20000], Training Loss: 0.4893\n",
            "Epoch [19716/20000], Training Loss: 0.4668\n",
            "Epoch [19717/20000], Training Loss: 0.5432\n",
            "Epoch [19718/20000], Training Loss: 0.4327\n",
            "Epoch [19719/20000], Training Loss: 0.4656\n",
            "Epoch [19720/20000], Training Loss: 0.4619\n",
            "Epoch [19721/20000], Training Loss: 0.4721\n",
            "Epoch [19722/20000], Training Loss: 0.5035\n",
            "Epoch [19723/20000], Training Loss: 0.5494\n",
            "Epoch [19724/20000], Training Loss: 0.4870\n",
            "Epoch [19725/20000], Training Loss: 0.4931\n",
            "Epoch [19726/20000], Training Loss: 0.4772\n",
            "Epoch [19727/20000], Training Loss: 0.4817\n",
            "Epoch [19728/20000], Training Loss: 0.4703\n",
            "Epoch [19729/20000], Training Loss: 0.4971\n",
            "Epoch [19730/20000], Training Loss: 0.5167\n",
            "Epoch [19731/20000], Training Loss: 0.4942\n",
            "Epoch [19732/20000], Training Loss: 0.4796\n",
            "Epoch [19733/20000], Training Loss: 0.4600\n",
            "Epoch [19734/20000], Training Loss: 0.5126\n",
            "Epoch [19735/20000], Training Loss: 0.4726\n",
            "Epoch [19736/20000], Training Loss: 0.5035\n",
            "Epoch [19737/20000], Training Loss: 0.4844\n",
            "Epoch [19738/20000], Training Loss: 0.5009\n",
            "Epoch [19739/20000], Training Loss: 0.4685\n",
            "Epoch [19740/20000], Training Loss: 0.4930\n",
            "Epoch [19741/20000], Training Loss: 0.5615\n",
            "Epoch [19742/20000], Training Loss: 0.5008\n",
            "Epoch [19743/20000], Training Loss: 0.4827\n",
            "Epoch [19744/20000], Training Loss: 0.4596\n",
            "Epoch [19745/20000], Training Loss: 0.4669\n",
            "Epoch [19746/20000], Training Loss: 0.4938\n",
            "Epoch [19747/20000], Training Loss: 0.5095\n",
            "Epoch [19748/20000], Training Loss: 0.4875\n",
            "Epoch [19749/20000], Training Loss: 0.4345\n",
            "Epoch [19750/20000], Training Loss: 0.5020\n",
            "Epoch [19751/20000], Training Loss: 0.4875\n",
            "Epoch [19752/20000], Training Loss: 0.4364\n",
            "Epoch [19753/20000], Training Loss: 0.5172\n",
            "Epoch [19754/20000], Training Loss: 0.4998\n",
            "Epoch [19755/20000], Training Loss: 0.4396\n",
            "Epoch [19756/20000], Training Loss: 0.4966\n",
            "Epoch [19757/20000], Training Loss: 0.5435\n",
            "Epoch [19758/20000], Training Loss: 0.4675\n",
            "Epoch [19759/20000], Training Loss: 0.4687\n",
            "Epoch [19760/20000], Training Loss: 0.4571\n",
            "Epoch [19761/20000], Training Loss: 0.4579\n",
            "Epoch [19762/20000], Training Loss: 0.4854\n",
            "Epoch [19763/20000], Training Loss: 0.5124\n",
            "Epoch [19764/20000], Training Loss: 0.4591\n",
            "Epoch [19765/20000], Training Loss: 0.4844\n",
            "Epoch [19766/20000], Training Loss: 0.4933\n",
            "Epoch [19767/20000], Training Loss: 0.4676\n",
            "Epoch [19768/20000], Training Loss: 0.5401\n",
            "Epoch [19769/20000], Training Loss: 0.4845\n",
            "Epoch [19770/20000], Training Loss: 0.4947\n",
            "Epoch [19771/20000], Training Loss: 0.5166\n",
            "Epoch [19772/20000], Training Loss: 0.4915\n",
            "Epoch [19773/20000], Training Loss: 0.5053\n",
            "Epoch [19774/20000], Training Loss: 0.4573\n",
            "Epoch [19775/20000], Training Loss: 0.4647\n",
            "Epoch [19776/20000], Training Loss: 0.5305\n",
            "Epoch [19777/20000], Training Loss: 0.4925\n",
            "Epoch [19778/20000], Training Loss: 0.5345\n",
            "Epoch [19779/20000], Training Loss: 0.5135\n",
            "Epoch [19780/20000], Training Loss: 0.4422\n",
            "Epoch [19781/20000], Training Loss: 0.5093\n",
            "Epoch [19782/20000], Training Loss: 0.4899\n",
            "Epoch [19783/20000], Training Loss: 0.4903\n",
            "Epoch [19784/20000], Training Loss: 0.5095\n",
            "Epoch [19785/20000], Training Loss: 0.4857\n",
            "Epoch [19786/20000], Training Loss: 0.4419\n",
            "Epoch [19787/20000], Training Loss: 0.4873\n",
            "Epoch [19788/20000], Training Loss: 0.4799\n",
            "Epoch [19789/20000], Training Loss: 0.4715\n",
            "Epoch [19790/20000], Training Loss: 0.4871\n",
            "Epoch [19791/20000], Training Loss: 0.4962\n",
            "Epoch [19792/20000], Training Loss: 0.4675\n",
            "Epoch [19793/20000], Training Loss: 0.4394\n",
            "Epoch [19794/20000], Training Loss: 0.5180\n",
            "Epoch [19795/20000], Training Loss: 0.4977\n",
            "Epoch [19796/20000], Training Loss: 0.4852\n",
            "Epoch [19797/20000], Training Loss: 0.4953\n",
            "Epoch [19798/20000], Training Loss: 0.4438\n",
            "Epoch [19799/20000], Training Loss: 0.4937\n",
            "Epoch [19800/20000], Training Loss: 0.4703\n",
            "Epoch [19801/20000], Training Loss: 0.4634\n",
            "Epoch [19802/20000], Training Loss: 0.4391\n",
            "Epoch [19803/20000], Training Loss: 0.4798\n",
            "Epoch [19804/20000], Training Loss: 0.5272\n",
            "Epoch [19805/20000], Training Loss: 0.5005\n",
            "Epoch [19806/20000], Training Loss: 0.4789\n",
            "Epoch [19807/20000], Training Loss: 0.4993\n",
            "Epoch [19808/20000], Training Loss: 0.5041\n",
            "Epoch [19809/20000], Training Loss: 0.4683\n",
            "Epoch [19810/20000], Training Loss: 0.4837\n",
            "Epoch [19811/20000], Training Loss: 0.4666\n",
            "Epoch [19812/20000], Training Loss: 0.5006\n",
            "Epoch [19813/20000], Training Loss: 0.4490\n",
            "Epoch [19814/20000], Training Loss: 0.4776\n",
            "Epoch [19815/20000], Training Loss: 0.4360\n",
            "Epoch [19816/20000], Training Loss: 0.4843\n",
            "Epoch [19817/20000], Training Loss: 0.5265\n",
            "Epoch [19818/20000], Training Loss: 0.5238\n",
            "Epoch [19819/20000], Training Loss: 0.5240\n",
            "Epoch [19820/20000], Training Loss: 0.4805\n",
            "Epoch [19821/20000], Training Loss: 0.4708\n",
            "Epoch [19822/20000], Training Loss: 0.4844\n",
            "Epoch [19823/20000], Training Loss: 0.5009\n",
            "Epoch [19824/20000], Training Loss: 0.5018\n",
            "Epoch [19825/20000], Training Loss: 0.4587\n",
            "Epoch [19826/20000], Training Loss: 0.4496\n",
            "Epoch [19827/20000], Training Loss: 0.4370\n",
            "Epoch [19828/20000], Training Loss: 0.4793\n",
            "Epoch [19829/20000], Training Loss: 0.5360\n",
            "Epoch [19830/20000], Training Loss: 0.4958\n",
            "Epoch [19831/20000], Training Loss: 0.5425\n",
            "Epoch [19832/20000], Training Loss: 0.4922\n",
            "Epoch [19833/20000], Training Loss: 0.4815\n",
            "Epoch [19834/20000], Training Loss: 0.4635\n",
            "Epoch [19835/20000], Training Loss: 0.5168\n",
            "Epoch [19836/20000], Training Loss: 0.4367\n",
            "Epoch [19837/20000], Training Loss: 0.5260\n",
            "Epoch [19838/20000], Training Loss: 0.5195\n",
            "Epoch [19839/20000], Training Loss: 0.5123\n",
            "Epoch [19840/20000], Training Loss: 0.5146\n",
            "Epoch [19841/20000], Training Loss: 0.4691\n",
            "Epoch [19842/20000], Training Loss: 0.5116\n",
            "Epoch [19843/20000], Training Loss: 0.4693\n",
            "Epoch [19844/20000], Training Loss: 0.4683\n",
            "Epoch [19845/20000], Training Loss: 0.4502\n",
            "Epoch [19846/20000], Training Loss: 0.4721\n",
            "Epoch [19847/20000], Training Loss: 0.4676\n",
            "Epoch [19848/20000], Training Loss: 0.4564\n",
            "Epoch [19849/20000], Training Loss: 0.4992\n",
            "Epoch [19850/20000], Training Loss: 0.4770\n",
            "Epoch [19851/20000], Training Loss: 0.5146\n",
            "Epoch [19852/20000], Training Loss: 0.5051\n",
            "Epoch [19853/20000], Training Loss: 0.4565\n",
            "Epoch [19854/20000], Training Loss: 0.5172\n",
            "Epoch [19855/20000], Training Loss: 0.4496\n",
            "Epoch [19856/20000], Training Loss: 0.5075\n",
            "Epoch [19857/20000], Training Loss: 0.4681\n",
            "Epoch [19858/20000], Training Loss: 0.4564\n",
            "Epoch [19859/20000], Training Loss: 0.4703\n",
            "Epoch [19860/20000], Training Loss: 0.5204\n",
            "Epoch [19861/20000], Training Loss: 0.5034\n",
            "Epoch [19862/20000], Training Loss: 0.4780\n",
            "Epoch [19863/20000], Training Loss: 0.4776\n",
            "Epoch [19864/20000], Training Loss: 0.4731\n",
            "Epoch [19865/20000], Training Loss: 0.4893\n",
            "Epoch [19866/20000], Training Loss: 0.5124\n",
            "Epoch [19867/20000], Training Loss: 0.4435\n",
            "Epoch [19868/20000], Training Loss: 0.4719\n",
            "Epoch [19869/20000], Training Loss: 0.4772\n",
            "Epoch [19870/20000], Training Loss: 0.5028\n",
            "Epoch [19871/20000], Training Loss: 0.4997\n",
            "Epoch [19872/20000], Training Loss: 0.4856\n",
            "Epoch [19873/20000], Training Loss: 0.4722\n",
            "Epoch [19874/20000], Training Loss: 0.4701\n",
            "Epoch [19875/20000], Training Loss: 0.4597\n",
            "Epoch [19876/20000], Training Loss: 0.4979\n",
            "Epoch [19877/20000], Training Loss: 0.4928\n",
            "Epoch [19878/20000], Training Loss: 0.4932\n",
            "Epoch [19879/20000], Training Loss: 0.4901\n",
            "Epoch [19880/20000], Training Loss: 0.4808\n",
            "Epoch [19881/20000], Training Loss: 0.5005\n",
            "Epoch [19882/20000], Training Loss: 0.5062\n",
            "Epoch [19883/20000], Training Loss: 0.4495\n",
            "Epoch [19884/20000], Training Loss: 0.4954\n",
            "Epoch [19885/20000], Training Loss: 0.5082\n",
            "Epoch [19886/20000], Training Loss: 0.4515\n",
            "Epoch [19887/20000], Training Loss: 0.4556\n",
            "Epoch [19888/20000], Training Loss: 0.5309\n",
            "Epoch [19889/20000], Training Loss: 0.4446\n",
            "Epoch [19890/20000], Training Loss: 0.5102\n",
            "Epoch [19891/20000], Training Loss: 0.4871\n",
            "Epoch [19892/20000], Training Loss: 0.4552\n",
            "Epoch [19893/20000], Training Loss: 0.4769\n",
            "Epoch [19894/20000], Training Loss: 0.5168\n",
            "Epoch [19895/20000], Training Loss: 0.5118\n",
            "Epoch [19896/20000], Training Loss: 0.4734\n",
            "Epoch [19897/20000], Training Loss: 0.4697\n",
            "Epoch [19898/20000], Training Loss: 0.4525\n",
            "Epoch [19899/20000], Training Loss: 0.4670\n",
            "Epoch [19900/20000], Training Loss: 0.4532\n",
            "Epoch [19901/20000], Training Loss: 0.4488\n",
            "Epoch [19902/20000], Training Loss: 0.5020\n",
            "Epoch [19903/20000], Training Loss: 0.4775\n",
            "Epoch [19904/20000], Training Loss: 0.4915\n",
            "Epoch [19905/20000], Training Loss: 0.4734\n",
            "Epoch [19906/20000], Training Loss: 0.4688\n",
            "Epoch [19907/20000], Training Loss: 0.4601\n",
            "Epoch [19908/20000], Training Loss: 0.4997\n",
            "Epoch [19909/20000], Training Loss: 0.5012\n",
            "Epoch [19910/20000], Training Loss: 0.4770\n",
            "Epoch [19911/20000], Training Loss: 0.5459\n",
            "Epoch [19912/20000], Training Loss: 0.4818\n",
            "Epoch [19913/20000], Training Loss: 0.4859\n",
            "Epoch [19914/20000], Training Loss: 0.4940\n",
            "Epoch [19915/20000], Training Loss: 0.4443\n",
            "Epoch [19916/20000], Training Loss: 0.4539\n",
            "Epoch [19917/20000], Training Loss: 0.4895\n",
            "Epoch [19918/20000], Training Loss: 0.5530\n",
            "Epoch [19919/20000], Training Loss: 0.5384\n",
            "Epoch [19920/20000], Training Loss: 0.4650\n",
            "Epoch [19921/20000], Training Loss: 0.4708\n",
            "Epoch [19922/20000], Training Loss: 0.4780\n",
            "Epoch [19923/20000], Training Loss: 0.5100\n",
            "Epoch [19924/20000], Training Loss: 0.4816\n",
            "Epoch [19925/20000], Training Loss: 0.5123\n",
            "Epoch [19926/20000], Training Loss: 0.5121\n",
            "Epoch [19927/20000], Training Loss: 0.4944\n",
            "Epoch [19928/20000], Training Loss: 0.5032\n",
            "Epoch [19929/20000], Training Loss: 0.4898\n",
            "Epoch [19930/20000], Training Loss: 0.4415\n",
            "Epoch [19931/20000], Training Loss: 0.5019\n",
            "Epoch [19932/20000], Training Loss: 0.4699\n",
            "Epoch [19933/20000], Training Loss: 0.4874\n",
            "Epoch [19934/20000], Training Loss: 0.5082\n",
            "Epoch [19935/20000], Training Loss: 0.4912\n",
            "Epoch [19936/20000], Training Loss: 0.4893\n",
            "Epoch [19937/20000], Training Loss: 0.4755\n",
            "Epoch [19938/20000], Training Loss: 0.4856\n",
            "Epoch [19939/20000], Training Loss: 0.4767\n",
            "Epoch [19940/20000], Training Loss: 0.4532\n",
            "Epoch [19941/20000], Training Loss: 0.4826\n",
            "Epoch [19942/20000], Training Loss: 0.4594\n",
            "Epoch [19943/20000], Training Loss: 0.5034\n",
            "Epoch [19944/20000], Training Loss: 0.4793\n",
            "Epoch [19945/20000], Training Loss: 0.5372\n",
            "Epoch [19946/20000], Training Loss: 0.5385\n",
            "Epoch [19947/20000], Training Loss: 0.4699\n",
            "Epoch [19948/20000], Training Loss: 0.5005\n",
            "Epoch [19949/20000], Training Loss: 0.4758\n",
            "Epoch [19950/20000], Training Loss: 0.4564\n",
            "Epoch [19951/20000], Training Loss: 0.5008\n",
            "Epoch [19952/20000], Training Loss: 0.4567\n",
            "Epoch [19953/20000], Training Loss: 0.4903\n",
            "Epoch [19954/20000], Training Loss: 0.5111\n",
            "Epoch [19955/20000], Training Loss: 0.4879\n",
            "Epoch [19956/20000], Training Loss: 0.5263\n",
            "Epoch [19957/20000], Training Loss: 0.4593\n",
            "Epoch [19958/20000], Training Loss: 0.4736\n",
            "Epoch [19959/20000], Training Loss: 0.4764\n",
            "Epoch [19960/20000], Training Loss: 0.5275\n",
            "Epoch [19961/20000], Training Loss: 0.4740\n",
            "Epoch [19962/20000], Training Loss: 0.4663\n",
            "Epoch [19963/20000], Training Loss: 0.4926\n",
            "Epoch [19964/20000], Training Loss: 0.4411\n",
            "Epoch [19965/20000], Training Loss: 0.4831\n",
            "Epoch [19966/20000], Training Loss: 0.4710\n",
            "Epoch [19967/20000], Training Loss: 0.4515\n",
            "Epoch [19968/20000], Training Loss: 0.5085\n",
            "Epoch [19969/20000], Training Loss: 0.4829\n",
            "Epoch [19970/20000], Training Loss: 0.5200\n",
            "Epoch [19971/20000], Training Loss: 0.5130\n",
            "Epoch [19972/20000], Training Loss: 0.4602\n",
            "Epoch [19973/20000], Training Loss: 0.4915\n",
            "Epoch [19974/20000], Training Loss: 0.4742\n",
            "Epoch [19975/20000], Training Loss: 0.5154\n",
            "Epoch [19976/20000], Training Loss: 0.4674\n",
            "Epoch [19977/20000], Training Loss: 0.4748\n",
            "Epoch [19978/20000], Training Loss: 0.4598\n",
            "Epoch [19979/20000], Training Loss: 0.5028\n",
            "Epoch [19980/20000], Training Loss: 0.4560\n",
            "Epoch [19981/20000], Training Loss: 0.5051\n",
            "Epoch [19982/20000], Training Loss: 0.5024\n",
            "Epoch [19983/20000], Training Loss: 0.5045\n",
            "Epoch [19984/20000], Training Loss: 0.4867\n",
            "Epoch [19985/20000], Training Loss: 0.5357\n",
            "Epoch [19986/20000], Training Loss: 0.5238\n",
            "Epoch [19987/20000], Training Loss: 0.4972\n",
            "Epoch [19988/20000], Training Loss: 0.4525\n",
            "Epoch [19989/20000], Training Loss: 0.4954\n",
            "Epoch [19990/20000], Training Loss: 0.5084\n",
            "Epoch [19991/20000], Training Loss: 0.4603\n",
            "Epoch [19992/20000], Training Loss: 0.5127\n",
            "Epoch [19993/20000], Training Loss: 0.5296\n",
            "Epoch [19994/20000], Training Loss: 0.4928\n",
            "Epoch [19995/20000], Training Loss: 0.5088\n",
            "Epoch [19996/20000], Training Loss: 0.4365\n",
            "Epoch [19997/20000], Training Loss: 0.5185\n",
            "Epoch [19998/20000], Training Loss: 0.4851\n",
            "Epoch [19999/20000], Training Loss: 0.5263\n",
            "Epoch [20000/20000], Training Loss: 0.4707\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Normalize the data\n",
        "train_mean = train_data.mean()\n",
        "train_std = train_data.std()\n",
        "train_data = (train_data - train_mean) / train_std\n",
        "test_data = (test_data - train_mean) / train_std\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Define the hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20000\n",
        "batch_size = 16\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearModel(len(input_cols), len(output_cols))\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the training loop\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    batch_losses = []\n",
        "    # Shuffle the data\n",
        "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "    # Split the data into batches\n",
        "    for i in range(0, len(train_data), batch_size):\n",
        "        batch_data = train_data[i:i+batch_size]\n",
        "        # Extract the input and output data\n",
        "        x_batch = torch.tensor(batch_data[input_cols].values, dtype=torch.float32)\n",
        "        y_batch = torch.tensor(batch_data[output_cols].values, dtype=torch.float32)\n",
        "        # Forward pass\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_losses.append(loss.item())\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}')\n",
        "\n",
        "# Prepare the input data\n",
        "input_data = np.arange(2015, 2026).reshape(-1, 1)\n",
        "input_data = (input_data - train_mean['Year']) / train_std['Year']\n",
        "input_data = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "# Predict the output\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output_data = model(input_data)\n",
        "output_data = output_data.detach().numpy()\n",
        "\n",
        "# Convert the output data back to original scale\n",
        "output_data = (output_data * train_std[output_cols].values) + train_mean[output_cols].values\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Plot the predicted output\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(output_cols):\n",
        "    plt.plot(data['Year'].values, data[col].values, label=col)\n",
        "    plt.plot(np.arange(2015, 2026), output_data[:, i], label=col + ' (predicted)', linestyle='--')\n",
        "plt.legend()\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Self-harm Rates')\n",
        "plt.title('Self-harm Rates on Mental Health and substance use disorders in chosen countries (linear)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ilc4xIgXFuSq"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss over Time Linear')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 3.2280\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_inputs = torch.tensor(test_data[input_cols].values, dtype=torch.float32)\n",
        "    test_outputs = torch.tensor(test_data[output_cols].values, dtype=torch.float32)\n",
        "    test_predictions = model(test_inputs)\n",
        "    test_predictions = test_predictions.squeeze(1)  # remove the extra dimension\n",
        "    test_loss = criterion(test_predictions, test_outputs)\n",
        "print(f'Test Loss: {test_loss:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
