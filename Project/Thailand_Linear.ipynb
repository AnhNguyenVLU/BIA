{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z98MBnlEUsCU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load data and convert to PyTorch tensor\n",
        "data = pd.read_excel('Thailand.xlsx')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "fvG8dBGSWpC9",
        "outputId": "d94092b9-d60c-4024-c040-4bf954f65544"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Entity</th>\n",
              "      <th>Year</th>\n",
              "      <th>Depressive</th>\n",
              "      <th>Anxiety</th>\n",
              "      <th>Bipolar</th>\n",
              "      <th>Eating</th>\n",
              "      <th>Schizophrenia</th>\n",
              "      <th>Attention</th>\n",
              "      <th>Conduct</th>\n",
              "      <th>Diopathic</th>\n",
              "      <th>Autism</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1990</td>\n",
              "      <td>1525578</td>\n",
              "      <td>1841362</td>\n",
              "      <td>183604</td>\n",
              "      <td>63024</td>\n",
              "      <td>158080</td>\n",
              "      <td>1329779</td>\n",
              "      <td>359240</td>\n",
              "      <td>1164077</td>\n",
              "      <td>182157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1991</td>\n",
              "      <td>1568210</td>\n",
              "      <td>1882395</td>\n",
              "      <td>188268</td>\n",
              "      <td>65925</td>\n",
              "      <td>163893</td>\n",
              "      <td>1334272</td>\n",
              "      <td>355965</td>\n",
              "      <td>1128407</td>\n",
              "      <td>184087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1992</td>\n",
              "      <td>1609541</td>\n",
              "      <td>1921084</td>\n",
              "      <td>192672</td>\n",
              "      <td>68419</td>\n",
              "      <td>169437</td>\n",
              "      <td>1337375</td>\n",
              "      <td>352714</td>\n",
              "      <td>1094180</td>\n",
              "      <td>185894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1993</td>\n",
              "      <td>1648774</td>\n",
              "      <td>1956543</td>\n",
              "      <td>196736</td>\n",
              "      <td>70469</td>\n",
              "      <td>174600</td>\n",
              "      <td>1338446</td>\n",
              "      <td>349290</td>\n",
              "      <td>1061855</td>\n",
              "      <td>187504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1994</td>\n",
              "      <td>1685824</td>\n",
              "      <td>1988930</td>\n",
              "      <td>200495</td>\n",
              "      <td>72586</td>\n",
              "      <td>179351</td>\n",
              "      <td>1337897</td>\n",
              "      <td>345634</td>\n",
              "      <td>1032571</td>\n",
              "      <td>188956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1995</td>\n",
              "      <td>1722124</td>\n",
              "      <td>2019086</td>\n",
              "      <td>204051</td>\n",
              "      <td>74092</td>\n",
              "      <td>183706</td>\n",
              "      <td>1336838</td>\n",
              "      <td>341877</td>\n",
              "      <td>1007756</td>\n",
              "      <td>190344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1996</td>\n",
              "      <td>1758093</td>\n",
              "      <td>2047693</td>\n",
              "      <td>207411</td>\n",
              "      <td>75518</td>\n",
              "      <td>187643</td>\n",
              "      <td>1335239</td>\n",
              "      <td>338016</td>\n",
              "      <td>986620</td>\n",
              "      <td>191620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1997</td>\n",
              "      <td>1793742</td>\n",
              "      <td>2075123</td>\n",
              "      <td>210569</td>\n",
              "      <td>76377</td>\n",
              "      <td>191255</td>\n",
              "      <td>1332821</td>\n",
              "      <td>334112</td>\n",
              "      <td>967262</td>\n",
              "      <td>192726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1998</td>\n",
              "      <td>1828824</td>\n",
              "      <td>2101575</td>\n",
              "      <td>213593</td>\n",
              "      <td>77149</td>\n",
              "      <td>194659</td>\n",
              "      <td>1330046</td>\n",
              "      <td>330345</td>\n",
              "      <td>949133</td>\n",
              "      <td>193698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>1999</td>\n",
              "      <td>1864103</td>\n",
              "      <td>2127061</td>\n",
              "      <td>216519</td>\n",
              "      <td>77846</td>\n",
              "      <td>197942</td>\n",
              "      <td>1327407</td>\n",
              "      <td>326922</td>\n",
              "      <td>931693</td>\n",
              "      <td>194571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2000</td>\n",
              "      <td>1897513</td>\n",
              "      <td>2152046</td>\n",
              "      <td>219450</td>\n",
              "      <td>78696</td>\n",
              "      <td>201244</td>\n",
              "      <td>1325183</td>\n",
              "      <td>324003</td>\n",
              "      <td>914387</td>\n",
              "      <td>195408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2001</td>\n",
              "      <td>1932874</td>\n",
              "      <td>2177096</td>\n",
              "      <td>222514</td>\n",
              "      <td>79252</td>\n",
              "      <td>204752</td>\n",
              "      <td>1323121</td>\n",
              "      <td>321605</td>\n",
              "      <td>898796</td>\n",
              "      <td>196249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2002</td>\n",
              "      <td>1969996</td>\n",
              "      <td>2202093</td>\n",
              "      <td>225668</td>\n",
              "      <td>79403</td>\n",
              "      <td>208456</td>\n",
              "      <td>1320665</td>\n",
              "      <td>319465</td>\n",
              "      <td>885613</td>\n",
              "      <td>197051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2003</td>\n",
              "      <td>2008037</td>\n",
              "      <td>2227588</td>\n",
              "      <td>228890</td>\n",
              "      <td>80066</td>\n",
              "      <td>212222</td>\n",
              "      <td>1317720</td>\n",
              "      <td>317532</td>\n",
              "      <td>873017</td>\n",
              "      <td>197821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2004</td>\n",
              "      <td>2046437</td>\n",
              "      <td>2254262</td>\n",
              "      <td>232182</td>\n",
              "      <td>80517</td>\n",
              "      <td>215938</td>\n",
              "      <td>1314198</td>\n",
              "      <td>315716</td>\n",
              "      <td>859118</td>\n",
              "      <td>198571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2005</td>\n",
              "      <td>2082202</td>\n",
              "      <td>2282423</td>\n",
              "      <td>235513</td>\n",
              "      <td>80932</td>\n",
              "      <td>219450</td>\n",
              "      <td>1309699</td>\n",
              "      <td>313802</td>\n",
              "      <td>841781</td>\n",
              "      <td>199276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2006</td>\n",
              "      <td>2139359</td>\n",
              "      <td>2341601</td>\n",
              "      <td>241918</td>\n",
              "      <td>83144</td>\n",
              "      <td>225469</td>\n",
              "      <td>1326804</td>\n",
              "      <td>315569</td>\n",
              "      <td>832380</td>\n",
              "      <td>203055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2007</td>\n",
              "      <td>2217705</td>\n",
              "      <td>2432567</td>\n",
              "      <td>251467</td>\n",
              "      <td>87175</td>\n",
              "      <td>234321</td>\n",
              "      <td>1364901</td>\n",
              "      <td>320410</td>\n",
              "      <td>831158</td>\n",
              "      <td>209947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2008</td>\n",
              "      <td>2254569</td>\n",
              "      <td>2474119</td>\n",
              "      <td>255532</td>\n",
              "      <td>88377</td>\n",
              "      <td>238307</td>\n",
              "      <td>1358399</td>\n",
              "      <td>316894</td>\n",
              "      <td>804692</td>\n",
              "      <td>211133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2009</td>\n",
              "      <td>2248045</td>\n",
              "      <td>2460068</td>\n",
              "      <td>253635</td>\n",
              "      <td>86012</td>\n",
              "      <td>236834</td>\n",
              "      <td>1306249</td>\n",
              "      <td>305570</td>\n",
              "      <td>756261</td>\n",
              "      <td>206271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2010</td>\n",
              "      <td>2260843</td>\n",
              "      <td>2467139</td>\n",
              "      <td>254294</td>\n",
              "      <td>85246</td>\n",
              "      <td>237508</td>\n",
              "      <td>1273260</td>\n",
              "      <td>297546</td>\n",
              "      <td>722018</td>\n",
              "      <td>204095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2011</td>\n",
              "      <td>2294848</td>\n",
              "      <td>2496824</td>\n",
              "      <td>257507</td>\n",
              "      <td>85830</td>\n",
              "      <td>240782</td>\n",
              "      <td>1254615</td>\n",
              "      <td>292419</td>\n",
              "      <td>698756</td>\n",
              "      <td>204567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2012</td>\n",
              "      <td>2327340</td>\n",
              "      <td>2526994</td>\n",
              "      <td>260657</td>\n",
              "      <td>86526</td>\n",
              "      <td>244478</td>\n",
              "      <td>1230494</td>\n",
              "      <td>286848</td>\n",
              "      <td>674313</td>\n",
              "      <td>205044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2013</td>\n",
              "      <td>2358748</td>\n",
              "      <td>2554855</td>\n",
              "      <td>263534</td>\n",
              "      <td>86865</td>\n",
              "      <td>248027</td>\n",
              "      <td>1203664</td>\n",
              "      <td>280486</td>\n",
              "      <td>649866</td>\n",
              "      <td>205320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2014</td>\n",
              "      <td>2387780</td>\n",
              "      <td>2579954</td>\n",
              "      <td>266172</td>\n",
              "      <td>87200</td>\n",
              "      <td>251113</td>\n",
              "      <td>1178780</td>\n",
              "      <td>273660</td>\n",
              "      <td>627497</td>\n",
              "      <td>205410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2015</td>\n",
              "      <td>2419561</td>\n",
              "      <td>2603285</td>\n",
              "      <td>268750</td>\n",
              "      <td>87298</td>\n",
              "      <td>253586</td>\n",
              "      <td>1161631</td>\n",
              "      <td>267048</td>\n",
              "      <td>609816</td>\n",
              "      <td>205481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2016</td>\n",
              "      <td>2447533</td>\n",
              "      <td>2625305</td>\n",
              "      <td>271139</td>\n",
              "      <td>87542</td>\n",
              "      <td>254721</td>\n",
              "      <td>1149216</td>\n",
              "      <td>260725</td>\n",
              "      <td>595539</td>\n",
              "      <td>205428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2017</td>\n",
              "      <td>2472927</td>\n",
              "      <td>2645567</td>\n",
              "      <td>273281</td>\n",
              "      <td>87558</td>\n",
              "      <td>255549</td>\n",
              "      <td>1136296</td>\n",
              "      <td>254729</td>\n",
              "      <td>582054</td>\n",
              "      <td>205203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2018</td>\n",
              "      <td>2496652</td>\n",
              "      <td>2668039</td>\n",
              "      <td>275233</td>\n",
              "      <td>87180</td>\n",
              "      <td>257399</td>\n",
              "      <td>1123078</td>\n",
              "      <td>248926</td>\n",
              "      <td>566125</td>\n",
              "      <td>204882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Thailand</td>\n",
              "      <td>2019</td>\n",
              "      <td>2514313</td>\n",
              "      <td>2694793</td>\n",
              "      <td>277009</td>\n",
              "      <td>87157</td>\n",
              "      <td>259963</td>\n",
              "      <td>1109307</td>\n",
              "      <td>243267</td>\n",
              "      <td>547041</td>\n",
              "      <td>204477</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Entity  Year  Depressive  Anxiety  Bipolar  Eating  Schizophrenia  \\\n",
              "0   Thailand  1990     1525578  1841362   183604   63024         158080   \n",
              "1   Thailand  1991     1568210  1882395   188268   65925         163893   \n",
              "2   Thailand  1992     1609541  1921084   192672   68419         169437   \n",
              "3   Thailand  1993     1648774  1956543   196736   70469         174600   \n",
              "4   Thailand  1994     1685824  1988930   200495   72586         179351   \n",
              "5   Thailand  1995     1722124  2019086   204051   74092         183706   \n",
              "6   Thailand  1996     1758093  2047693   207411   75518         187643   \n",
              "7   Thailand  1997     1793742  2075123   210569   76377         191255   \n",
              "8   Thailand  1998     1828824  2101575   213593   77149         194659   \n",
              "9   Thailand  1999     1864103  2127061   216519   77846         197942   \n",
              "10  Thailand  2000     1897513  2152046   219450   78696         201244   \n",
              "11  Thailand  2001     1932874  2177096   222514   79252         204752   \n",
              "12  Thailand  2002     1969996  2202093   225668   79403         208456   \n",
              "13  Thailand  2003     2008037  2227588   228890   80066         212222   \n",
              "14  Thailand  2004     2046437  2254262   232182   80517         215938   \n",
              "15  Thailand  2005     2082202  2282423   235513   80932         219450   \n",
              "16  Thailand  2006     2139359  2341601   241918   83144         225469   \n",
              "17  Thailand  2007     2217705  2432567   251467   87175         234321   \n",
              "18  Thailand  2008     2254569  2474119   255532   88377         238307   \n",
              "19  Thailand  2009     2248045  2460068   253635   86012         236834   \n",
              "20  Thailand  2010     2260843  2467139   254294   85246         237508   \n",
              "21  Thailand  2011     2294848  2496824   257507   85830         240782   \n",
              "22  Thailand  2012     2327340  2526994   260657   86526         244478   \n",
              "23  Thailand  2013     2358748  2554855   263534   86865         248027   \n",
              "24  Thailand  2014     2387780  2579954   266172   87200         251113   \n",
              "25  Thailand  2015     2419561  2603285   268750   87298         253586   \n",
              "26  Thailand  2016     2447533  2625305   271139   87542         254721   \n",
              "27  Thailand  2017     2472927  2645567   273281   87558         255549   \n",
              "28  Thailand  2018     2496652  2668039   275233   87180         257399   \n",
              "29  Thailand  2019     2514313  2694793   277009   87157         259963   \n",
              "\n",
              "    Attention  Conduct  Diopathic  Autism  \n",
              "0     1329779   359240    1164077  182157  \n",
              "1     1334272   355965    1128407  184087  \n",
              "2     1337375   352714    1094180  185894  \n",
              "3     1338446   349290    1061855  187504  \n",
              "4     1337897   345634    1032571  188956  \n",
              "5     1336838   341877    1007756  190344  \n",
              "6     1335239   338016     986620  191620  \n",
              "7     1332821   334112     967262  192726  \n",
              "8     1330046   330345     949133  193698  \n",
              "9     1327407   326922     931693  194571  \n",
              "10    1325183   324003     914387  195408  \n",
              "11    1323121   321605     898796  196249  \n",
              "12    1320665   319465     885613  197051  \n",
              "13    1317720   317532     873017  197821  \n",
              "14    1314198   315716     859118  198571  \n",
              "15    1309699   313802     841781  199276  \n",
              "16    1326804   315569     832380  203055  \n",
              "17    1364901   320410     831158  209947  \n",
              "18    1358399   316894     804692  211133  \n",
              "19    1306249   305570     756261  206271  \n",
              "20    1273260   297546     722018  204095  \n",
              "21    1254615   292419     698756  204567  \n",
              "22    1230494   286848     674313  205044  \n",
              "23    1203664   280486     649866  205320  \n",
              "24    1178780   273660     627497  205410  \n",
              "25    1161631   267048     609816  205481  \n",
              "26    1149216   260725     595539  205428  \n",
              "27    1136296   254729     582054  205203  \n",
              "28    1123078   248926     566125  204882  \n",
              "29    1109307   243267     547041  204477  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sDa6UXB_5J3z"
      },
      "outputs": [],
      "source": [
        "columns = ['Year', 'Depressive', 'Anxiety', 'Bipolar', 'Eating', 'Schizophrenia', 'Attention', 'Conduct', 'Diopathic', 'Autism']\n",
        "data = data[columns]\n",
        "\n",
        "# Define the input and output columns\n",
        "input_cols = ['Year']\n",
        "output_cols = ['Depressive', 'Anxiety', 'Bipolar', 'Eating', 'Schizophrenia', 'Attention', 'Conduct', 'Diopathic', 'Autism']\n",
        "# Split the data into training and testing sets\n",
        "train_data = data[data['Year'] <= 2017]\n",
        "test_data = data[ data['Year'] >= 2018]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f3ZXO7axb6z1",
        "outputId": "99d3944b-9447-4c63-e67b-3ae2e464b983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20000], Training Loss: 1.5096\n",
            "Epoch [2/20000], Training Loss: 1.5266\n",
            "Epoch [3/20000], Training Loss: 1.4711\n",
            "Epoch [4/20000], Training Loss: 1.5144\n",
            "Epoch [5/20000], Training Loss: 1.5108\n",
            "Epoch [6/20000], Training Loss: 1.5359\n",
            "Epoch [7/20000], Training Loss: 1.5216\n",
            "Epoch [8/20000], Training Loss: 1.4726\n",
            "Epoch [9/20000], Training Loss: 1.4248\n",
            "Epoch [10/20000], Training Loss: 1.3979\n",
            "Epoch [11/20000], Training Loss: 1.4420\n",
            "Epoch [12/20000], Training Loss: 1.4268\n",
            "Epoch [13/20000], Training Loss: 1.4407\n",
            "Epoch [14/20000], Training Loss: 1.4067\n",
            "Epoch [15/20000], Training Loss: 1.4182\n",
            "Epoch [16/20000], Training Loss: 1.3571\n",
            "Epoch [17/20000], Training Loss: 1.3890\n",
            "Epoch [18/20000], Training Loss: 1.3920\n",
            "Epoch [19/20000], Training Loss: 1.3128\n",
            "Epoch [20/20000], Training Loss: 1.3227\n",
            "Epoch [21/20000], Training Loss: 1.2962\n",
            "Epoch [22/20000], Training Loss: 1.2855\n",
            "Epoch [23/20000], Training Loss: 1.3535\n",
            "Epoch [24/20000], Training Loss: 1.3110\n",
            "Epoch [25/20000], Training Loss: 1.3085\n",
            "Epoch [26/20000], Training Loss: 1.2976\n",
            "Epoch [27/20000], Training Loss: 1.2386\n",
            "Epoch [28/20000], Training Loss: 1.2307\n",
            "Epoch [29/20000], Training Loss: 1.2423\n",
            "Epoch [30/20000], Training Loss: 1.1894\n",
            "Epoch [31/20000], Training Loss: 1.2065\n",
            "Epoch [32/20000], Training Loss: 1.1676\n",
            "Epoch [33/20000], Training Loss: 1.2162\n",
            "Epoch [34/20000], Training Loss: 1.1497\n",
            "Epoch [35/20000], Training Loss: 1.1734\n",
            "Epoch [36/20000], Training Loss: 1.2379\n",
            "Epoch [37/20000], Training Loss: 1.1473\n",
            "Epoch [38/20000], Training Loss: 1.1297\n",
            "Epoch [39/20000], Training Loss: 1.1799\n",
            "Epoch [40/20000], Training Loss: 1.1207\n",
            "Epoch [41/20000], Training Loss: 1.1301\n",
            "Epoch [42/20000], Training Loss: 1.0898\n",
            "Epoch [43/20000], Training Loss: 1.1184\n",
            "Epoch [44/20000], Training Loss: 1.0596\n",
            "Epoch [45/20000], Training Loss: 1.0648\n",
            "Epoch [46/20000], Training Loss: 1.0818\n",
            "Epoch [47/20000], Training Loss: 1.0505\n",
            "Epoch [48/20000], Training Loss: 1.0593\n",
            "Epoch [49/20000], Training Loss: 1.0752\n",
            "Epoch [50/20000], Training Loss: 1.0512\n",
            "Epoch [51/20000], Training Loss: 1.0351\n",
            "Epoch [52/20000], Training Loss: 1.0188\n",
            "Epoch [53/20000], Training Loss: 1.0242\n",
            "Epoch [54/20000], Training Loss: 1.0438\n",
            "Epoch [55/20000], Training Loss: 1.0074\n",
            "Epoch [56/20000], Training Loss: 0.9995\n",
            "Epoch [57/20000], Training Loss: 1.0305\n",
            "Epoch [58/20000], Training Loss: 1.0326\n",
            "Epoch [59/20000], Training Loss: 0.9827\n",
            "Epoch [60/20000], Training Loss: 0.9227\n",
            "Epoch [61/20000], Training Loss: 0.9694\n",
            "Epoch [62/20000], Training Loss: 0.9645\n",
            "Epoch [63/20000], Training Loss: 0.9468\n",
            "Epoch [64/20000], Training Loss: 0.9307\n",
            "Epoch [65/20000], Training Loss: 0.9455\n",
            "Epoch [66/20000], Training Loss: 0.9660\n",
            "Epoch [67/20000], Training Loss: 0.9378\n",
            "Epoch [68/20000], Training Loss: 0.9330\n",
            "Epoch [69/20000], Training Loss: 0.9341\n",
            "Epoch [70/20000], Training Loss: 0.9121\n",
            "Epoch [71/20000], Training Loss: 0.8638\n",
            "Epoch [72/20000], Training Loss: 0.8938\n",
            "Epoch [73/20000], Training Loss: 0.8706\n",
            "Epoch [74/20000], Training Loss: 0.8871\n",
            "Epoch [75/20000], Training Loss: 0.8679\n",
            "Epoch [76/20000], Training Loss: 0.8474\n",
            "Epoch [77/20000], Training Loss: 0.8703\n",
            "Epoch [78/20000], Training Loss: 0.8177\n",
            "Epoch [79/20000], Training Loss: 0.8331\n",
            "Epoch [80/20000], Training Loss: 0.8041\n",
            "Epoch [81/20000], Training Loss: 0.8267\n",
            "Epoch [82/20000], Training Loss: 0.8311\n",
            "Epoch [83/20000], Training Loss: 0.8252\n",
            "Epoch [84/20000], Training Loss: 0.7754\n",
            "Epoch [85/20000], Training Loss: 0.7901\n",
            "Epoch [86/20000], Training Loss: 0.7994\n",
            "Epoch [87/20000], Training Loss: 0.7751\n",
            "Epoch [88/20000], Training Loss: 0.8089\n",
            "Epoch [89/20000], Training Loss: 0.7252\n",
            "Epoch [90/20000], Training Loss: 0.7842\n",
            "Epoch [91/20000], Training Loss: 0.7595\n",
            "Epoch [92/20000], Training Loss: 0.7568\n",
            "Epoch [93/20000], Training Loss: 0.7672\n",
            "Epoch [94/20000], Training Loss: 0.7562\n",
            "Epoch [95/20000], Training Loss: 0.7313\n",
            "Epoch [96/20000], Training Loss: 0.7010\n",
            "Epoch [97/20000], Training Loss: 0.7244\n",
            "Epoch [98/20000], Training Loss: 0.7140\n",
            "Epoch [99/20000], Training Loss: 0.7100\n",
            "Epoch [100/20000], Training Loss: 0.7346\n",
            "Epoch [101/20000], Training Loss: 0.7189\n",
            "Epoch [102/20000], Training Loss: 0.6722\n",
            "Epoch [103/20000], Training Loss: 0.6635\n",
            "Epoch [104/20000], Training Loss: 0.6700\n",
            "Epoch [105/20000], Training Loss: 0.6945\n",
            "Epoch [106/20000], Training Loss: 0.6703\n",
            "Epoch [107/20000], Training Loss: 0.6698\n",
            "Epoch [108/20000], Training Loss: 0.6816\n",
            "Epoch [109/20000], Training Loss: 0.6666\n",
            "Epoch [110/20000], Training Loss: 0.6621\n",
            "Epoch [111/20000], Training Loss: 0.6248\n",
            "Epoch [112/20000], Training Loss: 0.6776\n",
            "Epoch [113/20000], Training Loss: 0.6630\n",
            "Epoch [114/20000], Training Loss: 0.6654\n",
            "Epoch [115/20000], Training Loss: 0.6103\n",
            "Epoch [116/20000], Training Loss: 0.6437\n",
            "Epoch [117/20000], Training Loss: 0.6259\n",
            "Epoch [118/20000], Training Loss: 0.6159\n",
            "Epoch [119/20000], Training Loss: 0.6129\n",
            "Epoch [120/20000], Training Loss: 0.6222\n",
            "Epoch [121/20000], Training Loss: 0.6141\n",
            "Epoch [122/20000], Training Loss: 0.5930\n",
            "Epoch [123/20000], Training Loss: 0.5994\n",
            "Epoch [124/20000], Training Loss: 0.6044\n",
            "Epoch [125/20000], Training Loss: 0.5904\n",
            "Epoch [126/20000], Training Loss: 0.5558\n",
            "Epoch [127/20000], Training Loss: 0.5780\n",
            "Epoch [128/20000], Training Loss: 0.5911\n",
            "Epoch [129/20000], Training Loss: 0.5596\n",
            "Epoch [130/20000], Training Loss: 0.5740\n",
            "Epoch [131/20000], Training Loss: 0.5663\n",
            "Epoch [132/20000], Training Loss: 0.5719\n",
            "Epoch [133/20000], Training Loss: 0.5712\n",
            "Epoch [134/20000], Training Loss: 0.5551\n",
            "Epoch [135/20000], Training Loss: 0.5479\n",
            "Epoch [136/20000], Training Loss: 0.5422\n",
            "Epoch [137/20000], Training Loss: 0.5597\n",
            "Epoch [138/20000], Training Loss: 0.5369\n",
            "Epoch [139/20000], Training Loss: 0.5296\n",
            "Epoch [140/20000], Training Loss: 0.5086\n",
            "Epoch [141/20000], Training Loss: 0.5121\n",
            "Epoch [142/20000], Training Loss: 0.5387\n",
            "Epoch [143/20000], Training Loss: 0.5254\n",
            "Epoch [144/20000], Training Loss: 0.5052\n",
            "Epoch [145/20000], Training Loss: 0.4970\n",
            "Epoch [146/20000], Training Loss: 0.4888\n",
            "Epoch [147/20000], Training Loss: 0.4949\n",
            "Epoch [148/20000], Training Loss: 0.4859\n",
            "Epoch [149/20000], Training Loss: 0.5094\n",
            "Epoch [150/20000], Training Loss: 0.4767\n",
            "Epoch [151/20000], Training Loss: 0.4805\n",
            "Epoch [152/20000], Training Loss: 0.4893\n",
            "Epoch [153/20000], Training Loss: 0.4684\n",
            "Epoch [154/20000], Training Loss: 0.4764\n",
            "Epoch [155/20000], Training Loss: 0.4817\n",
            "Epoch [156/20000], Training Loss: 0.4556\n",
            "Epoch [157/20000], Training Loss: 0.4641\n",
            "Epoch [158/20000], Training Loss: 0.4684\n",
            "Epoch [159/20000], Training Loss: 0.4670\n",
            "Epoch [160/20000], Training Loss: 0.4548\n",
            "Epoch [161/20000], Training Loss: 0.4490\n",
            "Epoch [162/20000], Training Loss: 0.4429\n",
            "Epoch [163/20000], Training Loss: 0.4604\n",
            "Epoch [164/20000], Training Loss: 0.4501\n",
            "Epoch [165/20000], Training Loss: 0.4247\n",
            "Epoch [166/20000], Training Loss: 0.4426\n",
            "Epoch [167/20000], Training Loss: 0.4355\n",
            "Epoch [168/20000], Training Loss: 0.4292\n",
            "Epoch [169/20000], Training Loss: 0.4039\n",
            "Epoch [170/20000], Training Loss: 0.4238\n",
            "Epoch [171/20000], Training Loss: 0.4098\n",
            "Epoch [172/20000], Training Loss: 0.4208\n",
            "Epoch [173/20000], Training Loss: 0.4035\n",
            "Epoch [174/20000], Training Loss: 0.3940\n",
            "Epoch [175/20000], Training Loss: 0.3969\n",
            "Epoch [176/20000], Training Loss: 0.4226\n",
            "Epoch [177/20000], Training Loss: 0.4151\n",
            "Epoch [178/20000], Training Loss: 0.4129\n",
            "Epoch [179/20000], Training Loss: 0.4072\n",
            "Epoch [180/20000], Training Loss: 0.3871\n",
            "Epoch [181/20000], Training Loss: 0.3797\n",
            "Epoch [182/20000], Training Loss: 0.3952\n",
            "Epoch [183/20000], Training Loss: 0.3868\n",
            "Epoch [184/20000], Training Loss: 0.3899\n",
            "Epoch [185/20000], Training Loss: 0.3898\n",
            "Epoch [186/20000], Training Loss: 0.3678\n",
            "Epoch [187/20000], Training Loss: 0.3708\n",
            "Epoch [188/20000], Training Loss: 0.3895\n",
            "Epoch [189/20000], Training Loss: 0.3564\n",
            "Epoch [190/20000], Training Loss: 0.3656\n",
            "Epoch [191/20000], Training Loss: 0.3495\n",
            "Epoch [192/20000], Training Loss: 0.3693\n",
            "Epoch [193/20000], Training Loss: 0.3644\n",
            "Epoch [194/20000], Training Loss: 0.3686\n",
            "Epoch [195/20000], Training Loss: 0.3587\n",
            "Epoch [196/20000], Training Loss: 0.3595\n",
            "Epoch [197/20000], Training Loss: 0.3416\n",
            "Epoch [198/20000], Training Loss: 0.3459\n",
            "Epoch [199/20000], Training Loss: 0.3503\n",
            "Epoch [200/20000], Training Loss: 0.3435\n",
            "Epoch [201/20000], Training Loss: 0.3253\n",
            "Epoch [202/20000], Training Loss: 0.3460\n",
            "Epoch [203/20000], Training Loss: 0.3337\n",
            "Epoch [204/20000], Training Loss: 0.3295\n",
            "Epoch [205/20000], Training Loss: 0.3216\n",
            "Epoch [206/20000], Training Loss: 0.3248\n",
            "Epoch [207/20000], Training Loss: 0.3302\n",
            "Epoch [208/20000], Training Loss: 0.3289\n",
            "Epoch [209/20000], Training Loss: 0.3411\n",
            "Epoch [210/20000], Training Loss: 0.3300\n",
            "Epoch [211/20000], Training Loss: 0.3113\n",
            "Epoch [212/20000], Training Loss: 0.3147\n",
            "Epoch [213/20000], Training Loss: 0.3175\n",
            "Epoch [214/20000], Training Loss: 0.3127\n",
            "Epoch [215/20000], Training Loss: 0.3269\n",
            "Epoch [216/20000], Training Loss: 0.3030\n",
            "Epoch [217/20000], Training Loss: 0.3058\n",
            "Epoch [218/20000], Training Loss: 0.2867\n",
            "Epoch [219/20000], Training Loss: 0.2999\n",
            "Epoch [220/20000], Training Loss: 0.3192\n",
            "Epoch [221/20000], Training Loss: 0.2883\n",
            "Epoch [222/20000], Training Loss: 0.2898\n",
            "Epoch [223/20000], Training Loss: 0.2925\n",
            "Epoch [224/20000], Training Loss: 0.3031\n",
            "Epoch [225/20000], Training Loss: 0.3005\n",
            "Epoch [226/20000], Training Loss: 0.2891\n",
            "Epoch [227/20000], Training Loss: 0.2929\n",
            "Epoch [228/20000], Training Loss: 0.2822\n",
            "Epoch [229/20000], Training Loss: 0.3035\n",
            "Epoch [230/20000], Training Loss: 0.2879\n",
            "Epoch [231/20000], Training Loss: 0.2760\n",
            "Epoch [232/20000], Training Loss: 0.2652\n",
            "Epoch [233/20000], Training Loss: 0.2732\n",
            "Epoch [234/20000], Training Loss: 0.2809\n",
            "Epoch [235/20000], Training Loss: 0.2752\n",
            "Epoch [236/20000], Training Loss: 0.2776\n",
            "Epoch [237/20000], Training Loss: 0.2785\n",
            "Epoch [238/20000], Training Loss: 0.2684\n",
            "Epoch [239/20000], Training Loss: 0.2813\n",
            "Epoch [240/20000], Training Loss: 0.2603\n",
            "Epoch [241/20000], Training Loss: 0.2640\n",
            "Epoch [242/20000], Training Loss: 0.2716\n",
            "Epoch [243/20000], Training Loss: 0.2584\n",
            "Epoch [244/20000], Training Loss: 0.2554\n",
            "Epoch [245/20000], Training Loss: 0.2587\n",
            "Epoch [246/20000], Training Loss: 0.2627\n",
            "Epoch [247/20000], Training Loss: 0.2575\n",
            "Epoch [248/20000], Training Loss: 0.2515\n",
            "Epoch [249/20000], Training Loss: 0.2533\n",
            "Epoch [250/20000], Training Loss: 0.2651\n",
            "Epoch [251/20000], Training Loss: 0.2458\n",
            "Epoch [252/20000], Training Loss: 0.2541\n",
            "Epoch [253/20000], Training Loss: 0.2546\n",
            "Epoch [254/20000], Training Loss: 0.2468\n",
            "Epoch [255/20000], Training Loss: 0.2440\n",
            "Epoch [256/20000], Training Loss: 0.2532\n",
            "Epoch [257/20000], Training Loss: 0.2502\n",
            "Epoch [258/20000], Training Loss: 0.2425\n",
            "Epoch [259/20000], Training Loss: 0.2502\n",
            "Epoch [260/20000], Training Loss: 0.2332\n",
            "Epoch [261/20000], Training Loss: 0.2364\n",
            "Epoch [262/20000], Training Loss: 0.2312\n",
            "Epoch [263/20000], Training Loss: 0.2388\n",
            "Epoch [264/20000], Training Loss: 0.2323\n",
            "Epoch [265/20000], Training Loss: 0.2294\n",
            "Epoch [266/20000], Training Loss: 0.2347\n",
            "Epoch [267/20000], Training Loss: 0.2277\n",
            "Epoch [268/20000], Training Loss: 0.2278\n",
            "Epoch [269/20000], Training Loss: 0.2202\n",
            "Epoch [270/20000], Training Loss: 0.2211\n",
            "Epoch [271/20000], Training Loss: 0.2288\n",
            "Epoch [272/20000], Training Loss: 0.2137\n",
            "Epoch [273/20000], Training Loss: 0.2193\n",
            "Epoch [274/20000], Training Loss: 0.2246\n",
            "Epoch [275/20000], Training Loss: 0.2130\n",
            "Epoch [276/20000], Training Loss: 0.2199\n",
            "Epoch [277/20000], Training Loss: 0.2165\n",
            "Epoch [278/20000], Training Loss: 0.2076\n",
            "Epoch [279/20000], Training Loss: 0.2241\n",
            "Epoch [280/20000], Training Loss: 0.2183\n",
            "Epoch [281/20000], Training Loss: 0.2046\n",
            "Epoch [282/20000], Training Loss: 0.2087\n",
            "Epoch [283/20000], Training Loss: 0.2138\n",
            "Epoch [284/20000], Training Loss: 0.2119\n",
            "Epoch [285/20000], Training Loss: 0.2175\n",
            "Epoch [286/20000], Training Loss: 0.2030\n",
            "Epoch [287/20000], Training Loss: 0.2043\n",
            "Epoch [288/20000], Training Loss: 0.2022\n",
            "Epoch [289/20000], Training Loss: 0.2012\n",
            "Epoch [290/20000], Training Loss: 0.2040\n",
            "Epoch [291/20000], Training Loss: 0.1975\n",
            "Epoch [292/20000], Training Loss: 0.2074\n",
            "Epoch [293/20000], Training Loss: 0.2079\n",
            "Epoch [294/20000], Training Loss: 0.1914\n",
            "Epoch [295/20000], Training Loss: 0.2025\n",
            "Epoch [296/20000], Training Loss: 0.1980\n",
            "Epoch [297/20000], Training Loss: 0.1861\n",
            "Epoch [298/20000], Training Loss: 0.1965\n",
            "Epoch [299/20000], Training Loss: 0.1809\n",
            "Epoch [300/20000], Training Loss: 0.1944\n",
            "Epoch [301/20000], Training Loss: 0.1933\n",
            "Epoch [302/20000], Training Loss: 0.1836\n",
            "Epoch [303/20000], Training Loss: 0.1844\n",
            "Epoch [304/20000], Training Loss: 0.1834\n",
            "Epoch [305/20000], Training Loss: 0.1934\n",
            "Epoch [306/20000], Training Loss: 0.1862\n",
            "Epoch [307/20000], Training Loss: 0.1775\n",
            "Epoch [308/20000], Training Loss: 0.1930\n",
            "Epoch [309/20000], Training Loss: 0.1806\n",
            "Epoch [310/20000], Training Loss: 0.1754\n",
            "Epoch [311/20000], Training Loss: 0.1867\n",
            "Epoch [312/20000], Training Loss: 0.1802\n",
            "Epoch [313/20000], Training Loss: 0.1811\n",
            "Epoch [314/20000], Training Loss: 0.1876\n",
            "Epoch [315/20000], Training Loss: 0.1800\n",
            "Epoch [316/20000], Training Loss: 0.1737\n",
            "Epoch [317/20000], Training Loss: 0.1705\n",
            "Epoch [318/20000], Training Loss: 0.1749\n",
            "Epoch [319/20000], Training Loss: 0.1735\n",
            "Epoch [320/20000], Training Loss: 0.1714\n",
            "Epoch [321/20000], Training Loss: 0.1785\n",
            "Epoch [322/20000], Training Loss: 0.1763\n",
            "Epoch [323/20000], Training Loss: 0.1700\n",
            "Epoch [324/20000], Training Loss: 0.1689\n",
            "Epoch [325/20000], Training Loss: 0.1679\n",
            "Epoch [326/20000], Training Loss: 0.1586\n",
            "Epoch [327/20000], Training Loss: 0.1655\n",
            "Epoch [328/20000], Training Loss: 0.1713\n",
            "Epoch [329/20000], Training Loss: 0.1632\n",
            "Epoch [330/20000], Training Loss: 0.1679\n",
            "Epoch [331/20000], Training Loss: 0.1659\n",
            "Epoch [332/20000], Training Loss: 0.1668\n",
            "Epoch [333/20000], Training Loss: 0.1637\n",
            "Epoch [334/20000], Training Loss: 0.1627\n",
            "Epoch [335/20000], Training Loss: 0.1633\n",
            "Epoch [336/20000], Training Loss: 0.1568\n",
            "Epoch [337/20000], Training Loss: 0.1600\n",
            "Epoch [338/20000], Training Loss: 0.1546\n",
            "Epoch [339/20000], Training Loss: 0.1670\n",
            "Epoch [340/20000], Training Loss: 0.1537\n",
            "Epoch [341/20000], Training Loss: 0.1646\n",
            "Epoch [342/20000], Training Loss: 0.1604\n",
            "Epoch [343/20000], Training Loss: 0.1612\n",
            "Epoch [344/20000], Training Loss: 0.1543\n",
            "Epoch [345/20000], Training Loss: 0.1573\n",
            "Epoch [346/20000], Training Loss: 0.1537\n",
            "Epoch [347/20000], Training Loss: 0.1468\n",
            "Epoch [348/20000], Training Loss: 0.1602\n",
            "Epoch [349/20000], Training Loss: 0.1452\n",
            "Epoch [350/20000], Training Loss: 0.1555\n",
            "Epoch [351/20000], Training Loss: 0.1506\n",
            "Epoch [352/20000], Training Loss: 0.1519\n",
            "Epoch [353/20000], Training Loss: 0.1458\n",
            "Epoch [354/20000], Training Loss: 0.1491\n",
            "Epoch [355/20000], Training Loss: 0.1568\n",
            "Epoch [356/20000], Training Loss: 0.1567\n",
            "Epoch [357/20000], Training Loss: 0.1518\n",
            "Epoch [358/20000], Training Loss: 0.1460\n",
            "Epoch [359/20000], Training Loss: 0.1404\n",
            "Epoch [360/20000], Training Loss: 0.1484\n",
            "Epoch [361/20000], Training Loss: 0.1444\n",
            "Epoch [362/20000], Training Loss: 0.1469\n",
            "Epoch [363/20000], Training Loss: 0.1452\n",
            "Epoch [364/20000], Training Loss: 0.1421\n",
            "Epoch [365/20000], Training Loss: 0.1498\n",
            "Epoch [366/20000], Training Loss: 0.1344\n",
            "Epoch [367/20000], Training Loss: 0.1449\n",
            "Epoch [368/20000], Training Loss: 0.1419\n",
            "Epoch [369/20000], Training Loss: 0.1455\n",
            "Epoch [370/20000], Training Loss: 0.1341\n",
            "Epoch [371/20000], Training Loss: 0.1466\n",
            "Epoch [372/20000], Training Loss: 0.1364\n",
            "Epoch [373/20000], Training Loss: 0.1389\n",
            "Epoch [374/20000], Training Loss: 0.1434\n",
            "Epoch [375/20000], Training Loss: 0.1335\n",
            "Epoch [376/20000], Training Loss: 0.1401\n",
            "Epoch [377/20000], Training Loss: 0.1423\n",
            "Epoch [378/20000], Training Loss: 0.1384\n",
            "Epoch [379/20000], Training Loss: 0.1338\n",
            "Epoch [380/20000], Training Loss: 0.1357\n",
            "Epoch [381/20000], Training Loss: 0.1351\n",
            "Epoch [382/20000], Training Loss: 0.1328\n",
            "Epoch [383/20000], Training Loss: 0.1362\n",
            "Epoch [384/20000], Training Loss: 0.1329\n",
            "Epoch [385/20000], Training Loss: 0.1295\n",
            "Epoch [386/20000], Training Loss: 0.1310\n",
            "Epoch [387/20000], Training Loss: 0.1293\n",
            "Epoch [388/20000], Training Loss: 0.1354\n",
            "Epoch [389/20000], Training Loss: 0.1264\n",
            "Epoch [390/20000], Training Loss: 0.1297\n",
            "Epoch [391/20000], Training Loss: 0.1288\n",
            "Epoch [392/20000], Training Loss: 0.1358\n",
            "Epoch [393/20000], Training Loss: 0.1274\n",
            "Epoch [394/20000], Training Loss: 0.1271\n",
            "Epoch [395/20000], Training Loss: 0.1272\n",
            "Epoch [396/20000], Training Loss: 0.1324\n",
            "Epoch [397/20000], Training Loss: 0.1353\n",
            "Epoch [398/20000], Training Loss: 0.1318\n",
            "Epoch [399/20000], Training Loss: 0.1319\n",
            "Epoch [400/20000], Training Loss: 0.1242\n",
            "Epoch [401/20000], Training Loss: 0.1313\n",
            "Epoch [402/20000], Training Loss: 0.1250\n",
            "Epoch [403/20000], Training Loss: 0.1251\n",
            "Epoch [404/20000], Training Loss: 0.1235\n",
            "Epoch [405/20000], Training Loss: 0.1260\n",
            "Epoch [406/20000], Training Loss: 0.1266\n",
            "Epoch [407/20000], Training Loss: 0.1254\n",
            "Epoch [408/20000], Training Loss: 0.1233\n",
            "Epoch [409/20000], Training Loss: 0.1292\n",
            "Epoch [410/20000], Training Loss: 0.1206\n",
            "Epoch [411/20000], Training Loss: 0.1217\n",
            "Epoch [412/20000], Training Loss: 0.1183\n",
            "Epoch [413/20000], Training Loss: 0.1198\n",
            "Epoch [414/20000], Training Loss: 0.1168\n",
            "Epoch [415/20000], Training Loss: 0.1204\n",
            "Epoch [416/20000], Training Loss: 0.1226\n",
            "Epoch [417/20000], Training Loss: 0.1205\n",
            "Epoch [418/20000], Training Loss: 0.1213\n",
            "Epoch [419/20000], Training Loss: 0.1164\n",
            "Epoch [420/20000], Training Loss: 0.1209\n",
            "Epoch [421/20000], Training Loss: 0.1109\n",
            "Epoch [422/20000], Training Loss: 0.1191\n",
            "Epoch [423/20000], Training Loss: 0.1214\n",
            "Epoch [424/20000], Training Loss: 0.1142\n",
            "Epoch [425/20000], Training Loss: 0.1177\n",
            "Epoch [426/20000], Training Loss: 0.1208\n",
            "Epoch [427/20000], Training Loss: 0.1137\n",
            "Epoch [428/20000], Training Loss: 0.1080\n",
            "Epoch [429/20000], Training Loss: 0.1210\n",
            "Epoch [430/20000], Training Loss: 0.1111\n",
            "Epoch [431/20000], Training Loss: 0.1157\n",
            "Epoch [432/20000], Training Loss: 0.1114\n",
            "Epoch [433/20000], Training Loss: 0.1118\n",
            "Epoch [434/20000], Training Loss: 0.1100\n",
            "Epoch [435/20000], Training Loss: 0.1132\n",
            "Epoch [436/20000], Training Loss: 0.1104\n",
            "Epoch [437/20000], Training Loss: 0.1088\n",
            "Epoch [438/20000], Training Loss: 0.1185\n",
            "Epoch [439/20000], Training Loss: 0.1085\n",
            "Epoch [440/20000], Training Loss: 0.1146\n",
            "Epoch [441/20000], Training Loss: 0.1144\n",
            "Epoch [442/20000], Training Loss: 0.1071\n",
            "Epoch [443/20000], Training Loss: 0.1117\n",
            "Epoch [444/20000], Training Loss: 0.1140\n",
            "Epoch [445/20000], Training Loss: 0.1157\n",
            "Epoch [446/20000], Training Loss: 0.1092\n",
            "Epoch [447/20000], Training Loss: 0.1181\n",
            "Epoch [448/20000], Training Loss: 0.1138\n",
            "Epoch [449/20000], Training Loss: 0.1045\n",
            "Epoch [450/20000], Training Loss: 0.1198\n",
            "Epoch [451/20000], Training Loss: 0.1049\n",
            "Epoch [452/20000], Training Loss: 0.1100\n",
            "Epoch [453/20000], Training Loss: 0.1104\n",
            "Epoch [454/20000], Training Loss: 0.1081\n",
            "Epoch [455/20000], Training Loss: 0.1074\n",
            "Epoch [456/20000], Training Loss: 0.1151\n",
            "Epoch [457/20000], Training Loss: 0.1089\n",
            "Epoch [458/20000], Training Loss: 0.1063\n",
            "Epoch [459/20000], Training Loss: 0.1120\n",
            "Epoch [460/20000], Training Loss: 0.1005\n",
            "Epoch [461/20000], Training Loss: 0.1034\n",
            "Epoch [462/20000], Training Loss: 0.1157\n",
            "Epoch [463/20000], Training Loss: 0.1074\n",
            "Epoch [464/20000], Training Loss: 0.1033\n",
            "Epoch [465/20000], Training Loss: 0.1117\n",
            "Epoch [466/20000], Training Loss: 0.1106\n",
            "Epoch [467/20000], Training Loss: 0.0980\n",
            "Epoch [468/20000], Training Loss: 0.1001\n",
            "Epoch [469/20000], Training Loss: 0.1029\n",
            "Epoch [470/20000], Training Loss: 0.1041\n",
            "Epoch [471/20000], Training Loss: 0.1049\n",
            "Epoch [472/20000], Training Loss: 0.0987\n",
            "Epoch [473/20000], Training Loss: 0.1109\n",
            "Epoch [474/20000], Training Loss: 0.1101\n",
            "Epoch [475/20000], Training Loss: 0.1019\n",
            "Epoch [476/20000], Training Loss: 0.0961\n",
            "Epoch [477/20000], Training Loss: 0.1088\n",
            "Epoch [478/20000], Training Loss: 0.1001\n",
            "Epoch [479/20000], Training Loss: 0.0982\n",
            "Epoch [480/20000], Training Loss: 0.1070\n",
            "Epoch [481/20000], Training Loss: 0.1028\n",
            "Epoch [482/20000], Training Loss: 0.1007\n",
            "Epoch [483/20000], Training Loss: 0.1074\n",
            "Epoch [484/20000], Training Loss: 0.0995\n",
            "Epoch [485/20000], Training Loss: 0.1051\n",
            "Epoch [486/20000], Training Loss: 0.1055\n",
            "Epoch [487/20000], Training Loss: 0.1023\n",
            "Epoch [488/20000], Training Loss: 0.1039\n",
            "Epoch [489/20000], Training Loss: 0.1026\n",
            "Epoch [490/20000], Training Loss: 0.1035\n",
            "Epoch [491/20000], Training Loss: 0.1051\n",
            "Epoch [492/20000], Training Loss: 0.1060\n",
            "Epoch [493/20000], Training Loss: 0.0997\n",
            "Epoch [494/20000], Training Loss: 0.1048\n",
            "Epoch [495/20000], Training Loss: 0.1042\n",
            "Epoch [496/20000], Training Loss: 0.1066\n",
            "Epoch [497/20000], Training Loss: 0.0996\n",
            "Epoch [498/20000], Training Loss: 0.0953\n",
            "Epoch [499/20000], Training Loss: 0.1024\n",
            "Epoch [500/20000], Training Loss: 0.0946\n",
            "Epoch [501/20000], Training Loss: 0.1028\n",
            "Epoch [502/20000], Training Loss: 0.1001\n",
            "Epoch [503/20000], Training Loss: 0.1010\n",
            "Epoch [504/20000], Training Loss: 0.1010\n",
            "Epoch [505/20000], Training Loss: 0.1004\n",
            "Epoch [506/20000], Training Loss: 0.1022\n",
            "Epoch [507/20000], Training Loss: 0.0946\n",
            "Epoch [508/20000], Training Loss: 0.0921\n",
            "Epoch [509/20000], Training Loss: 0.0990\n",
            "Epoch [510/20000], Training Loss: 0.1006\n",
            "Epoch [511/20000], Training Loss: 0.0927\n",
            "Epoch [512/20000], Training Loss: 0.1026\n",
            "Epoch [513/20000], Training Loss: 0.0975\n",
            "Epoch [514/20000], Training Loss: 0.0891\n",
            "Epoch [515/20000], Training Loss: 0.0947\n",
            "Epoch [516/20000], Training Loss: 0.0935\n",
            "Epoch [517/20000], Training Loss: 0.0955\n",
            "Epoch [518/20000], Training Loss: 0.0923\n",
            "Epoch [519/20000], Training Loss: 0.1034\n",
            "Epoch [520/20000], Training Loss: 0.0934\n",
            "Epoch [521/20000], Training Loss: 0.0926\n",
            "Epoch [522/20000], Training Loss: 0.1031\n",
            "Epoch [523/20000], Training Loss: 0.0915\n",
            "Epoch [524/20000], Training Loss: 0.0977\n",
            "Epoch [525/20000], Training Loss: 0.0966\n",
            "Epoch [526/20000], Training Loss: 0.0953\n",
            "Epoch [527/20000], Training Loss: 0.0941\n",
            "Epoch [528/20000], Training Loss: 0.0906\n",
            "Epoch [529/20000], Training Loss: 0.1006\n",
            "Epoch [530/20000], Training Loss: 0.0999\n",
            "Epoch [531/20000], Training Loss: 0.0951\n",
            "Epoch [532/20000], Training Loss: 0.0996\n",
            "Epoch [533/20000], Training Loss: 0.1019\n",
            "Epoch [534/20000], Training Loss: 0.0991\n",
            "Epoch [535/20000], Training Loss: 0.0966\n",
            "Epoch [536/20000], Training Loss: 0.0936\n",
            "Epoch [537/20000], Training Loss: 0.0909\n",
            "Epoch [538/20000], Training Loss: 0.0977\n",
            "Epoch [539/20000], Training Loss: 0.0876\n",
            "Epoch [540/20000], Training Loss: 0.0995\n",
            "Epoch [541/20000], Training Loss: 0.0954\n",
            "Epoch [542/20000], Training Loss: 0.0927\n",
            "Epoch [543/20000], Training Loss: 0.0949\n",
            "Epoch [544/20000], Training Loss: 0.0910\n",
            "Epoch [545/20000], Training Loss: 0.0960\n",
            "Epoch [546/20000], Training Loss: 0.0926\n",
            "Epoch [547/20000], Training Loss: 0.0930\n",
            "Epoch [548/20000], Training Loss: 0.0963\n",
            "Epoch [549/20000], Training Loss: 0.0972\n",
            "Epoch [550/20000], Training Loss: 0.0881\n",
            "Epoch [551/20000], Training Loss: 0.0853\n",
            "Epoch [552/20000], Training Loss: 0.0989\n",
            "Epoch [553/20000], Training Loss: 0.0910\n",
            "Epoch [554/20000], Training Loss: 0.0910\n",
            "Epoch [555/20000], Training Loss: 0.0881\n",
            "Epoch [556/20000], Training Loss: 0.0947\n",
            "Epoch [557/20000], Training Loss: 0.0943\n",
            "Epoch [558/20000], Training Loss: 0.0980\n",
            "Epoch [559/20000], Training Loss: 0.0895\n",
            "Epoch [560/20000], Training Loss: 0.0861\n",
            "Epoch [561/20000], Training Loss: 0.0917\n",
            "Epoch [562/20000], Training Loss: 0.0878\n",
            "Epoch [563/20000], Training Loss: 0.0929\n",
            "Epoch [564/20000], Training Loss: 0.0875\n",
            "Epoch [565/20000], Training Loss: 0.0884\n",
            "Epoch [566/20000], Training Loss: 0.0864\n",
            "Epoch [567/20000], Training Loss: 0.0891\n",
            "Epoch [568/20000], Training Loss: 0.0882\n",
            "Epoch [569/20000], Training Loss: 0.0904\n",
            "Epoch [570/20000], Training Loss: 0.0894\n",
            "Epoch [571/20000], Training Loss: 0.0901\n",
            "Epoch [572/20000], Training Loss: 0.0874\n",
            "Epoch [573/20000], Training Loss: 0.0888\n",
            "Epoch [574/20000], Training Loss: 0.0933\n",
            "Epoch [575/20000], Training Loss: 0.0870\n",
            "Epoch [576/20000], Training Loss: 0.0905\n",
            "Epoch [577/20000], Training Loss: 0.0900\n",
            "Epoch [578/20000], Training Loss: 0.0892\n",
            "Epoch [579/20000], Training Loss: 0.0873\n",
            "Epoch [580/20000], Training Loss: 0.0910\n",
            "Epoch [581/20000], Training Loss: 0.0965\n",
            "Epoch [582/20000], Training Loss: 0.0939\n",
            "Epoch [583/20000], Training Loss: 0.0882\n",
            "Epoch [584/20000], Training Loss: 0.0942\n",
            "Epoch [585/20000], Training Loss: 0.0814\n",
            "Epoch [586/20000], Training Loss: 0.0883\n",
            "Epoch [587/20000], Training Loss: 0.0906\n",
            "Epoch [588/20000], Training Loss: 0.0934\n",
            "Epoch [589/20000], Training Loss: 0.0908\n",
            "Epoch [590/20000], Training Loss: 0.0862\n",
            "Epoch [591/20000], Training Loss: 0.0937\n",
            "Epoch [592/20000], Training Loss: 0.0976\n",
            "Epoch [593/20000], Training Loss: 0.0864\n",
            "Epoch [594/20000], Training Loss: 0.0897\n",
            "Epoch [595/20000], Training Loss: 0.0909\n",
            "Epoch [596/20000], Training Loss: 0.0849\n",
            "Epoch [597/20000], Training Loss: 0.0927\n",
            "Epoch [598/20000], Training Loss: 0.0886\n",
            "Epoch [599/20000], Training Loss: 0.0878\n",
            "Epoch [600/20000], Training Loss: 0.0912\n",
            "Epoch [601/20000], Training Loss: 0.0912\n",
            "Epoch [602/20000], Training Loss: 0.0810\n",
            "Epoch [603/20000], Training Loss: 0.0949\n",
            "Epoch [604/20000], Training Loss: 0.0847\n",
            "Epoch [605/20000], Training Loss: 0.0925\n",
            "Epoch [606/20000], Training Loss: 0.0889\n",
            "Epoch [607/20000], Training Loss: 0.0872\n",
            "Epoch [608/20000], Training Loss: 0.0904\n",
            "Epoch [609/20000], Training Loss: 0.0878\n",
            "Epoch [610/20000], Training Loss: 0.0827\n",
            "Epoch [611/20000], Training Loss: 0.0849\n",
            "Epoch [612/20000], Training Loss: 0.0906\n",
            "Epoch [613/20000], Training Loss: 0.0892\n",
            "Epoch [614/20000], Training Loss: 0.0925\n",
            "Epoch [615/20000], Training Loss: 0.0841\n",
            "Epoch [616/20000], Training Loss: 0.0860\n",
            "Epoch [617/20000], Training Loss: 0.0868\n",
            "Epoch [618/20000], Training Loss: 0.0909\n",
            "Epoch [619/20000], Training Loss: 0.0929\n",
            "Epoch [620/20000], Training Loss: 0.0794\n",
            "Epoch [621/20000], Training Loss: 0.0865\n",
            "Epoch [622/20000], Training Loss: 0.0839\n",
            "Epoch [623/20000], Training Loss: 0.0803\n",
            "Epoch [624/20000], Training Loss: 0.0872\n",
            "Epoch [625/20000], Training Loss: 0.0878\n",
            "Epoch [626/20000], Training Loss: 0.0906\n",
            "Epoch [627/20000], Training Loss: 0.0822\n",
            "Epoch [628/20000], Training Loss: 0.0825\n",
            "Epoch [629/20000], Training Loss: 0.0861\n",
            "Epoch [630/20000], Training Loss: 0.0839\n",
            "Epoch [631/20000], Training Loss: 0.0791\n",
            "Epoch [632/20000], Training Loss: 0.0857\n",
            "Epoch [633/20000], Training Loss: 0.0846\n",
            "Epoch [634/20000], Training Loss: 0.0802\n",
            "Epoch [635/20000], Training Loss: 0.0858\n",
            "Epoch [636/20000], Training Loss: 0.0801\n",
            "Epoch [637/20000], Training Loss: 0.0844\n",
            "Epoch [638/20000], Training Loss: 0.0891\n",
            "Epoch [639/20000], Training Loss: 0.0799\n",
            "Epoch [640/20000], Training Loss: 0.0870\n",
            "Epoch [641/20000], Training Loss: 0.0797\n",
            "Epoch [642/20000], Training Loss: 0.0910\n",
            "Epoch [643/20000], Training Loss: 0.0837\n",
            "Epoch [644/20000], Training Loss: 0.0821\n",
            "Epoch [645/20000], Training Loss: 0.0873\n",
            "Epoch [646/20000], Training Loss: 0.0863\n",
            "Epoch [647/20000], Training Loss: 0.0847\n",
            "Epoch [648/20000], Training Loss: 0.0803\n",
            "Epoch [649/20000], Training Loss: 0.0854\n",
            "Epoch [650/20000], Training Loss: 0.0838\n",
            "Epoch [651/20000], Training Loss: 0.0839\n",
            "Epoch [652/20000], Training Loss: 0.0859\n",
            "Epoch [653/20000], Training Loss: 0.0853\n",
            "Epoch [654/20000], Training Loss: 0.0885\n",
            "Epoch [655/20000], Training Loss: 0.0836\n",
            "Epoch [656/20000], Training Loss: 0.0867\n",
            "Epoch [657/20000], Training Loss: 0.0905\n",
            "Epoch [658/20000], Training Loss: 0.0869\n",
            "Epoch [659/20000], Training Loss: 0.0860\n",
            "Epoch [660/20000], Training Loss: 0.0916\n",
            "Epoch [661/20000], Training Loss: 0.0885\n",
            "Epoch [662/20000], Training Loss: 0.0780\n",
            "Epoch [663/20000], Training Loss: 0.0858\n",
            "Epoch [664/20000], Training Loss: 0.0797\n",
            "Epoch [665/20000], Training Loss: 0.0832\n",
            "Epoch [666/20000], Training Loss: 0.0874\n",
            "Epoch [667/20000], Training Loss: 0.0916\n",
            "Epoch [668/20000], Training Loss: 0.0871\n",
            "Epoch [669/20000], Training Loss: 0.0872\n",
            "Epoch [670/20000], Training Loss: 0.0838\n",
            "Epoch [671/20000], Training Loss: 0.0851\n",
            "Epoch [672/20000], Training Loss: 0.0783\n",
            "Epoch [673/20000], Training Loss: 0.0856\n",
            "Epoch [674/20000], Training Loss: 0.0840\n",
            "Epoch [675/20000], Training Loss: 0.0863\n",
            "Epoch [676/20000], Training Loss: 0.0897\n",
            "Epoch [677/20000], Training Loss: 0.0859\n",
            "Epoch [678/20000], Training Loss: 0.0856\n",
            "Epoch [679/20000], Training Loss: 0.0804\n",
            "Epoch [680/20000], Training Loss: 0.0836\n",
            "Epoch [681/20000], Training Loss: 0.0769\n",
            "Epoch [682/20000], Training Loss: 0.0795\n",
            "Epoch [683/20000], Training Loss: 0.0833\n",
            "Epoch [684/20000], Training Loss: 0.0809\n",
            "Epoch [685/20000], Training Loss: 0.0858\n",
            "Epoch [686/20000], Training Loss: 0.0819\n",
            "Epoch [687/20000], Training Loss: 0.0892\n",
            "Epoch [688/20000], Training Loss: 0.0839\n",
            "Epoch [689/20000], Training Loss: 0.0820\n",
            "Epoch [690/20000], Training Loss: 0.0874\n",
            "Epoch [691/20000], Training Loss: 0.0888\n",
            "Epoch [692/20000], Training Loss: 0.0783\n",
            "Epoch [693/20000], Training Loss: 0.0852\n",
            "Epoch [694/20000], Training Loss: 0.0791\n",
            "Epoch [695/20000], Training Loss: 0.0768\n",
            "Epoch [696/20000], Training Loss: 0.0813\n",
            "Epoch [697/20000], Training Loss: 0.0908\n",
            "Epoch [698/20000], Training Loss: 0.0823\n",
            "Epoch [699/20000], Training Loss: 0.0811\n",
            "Epoch [700/20000], Training Loss: 0.0890\n",
            "Epoch [701/20000], Training Loss: 0.0839\n",
            "Epoch [702/20000], Training Loss: 0.0883\n",
            "Epoch [703/20000], Training Loss: 0.0864\n",
            "Epoch [704/20000], Training Loss: 0.0826\n",
            "Epoch [705/20000], Training Loss: 0.0851\n",
            "Epoch [706/20000], Training Loss: 0.0834\n",
            "Epoch [707/20000], Training Loss: 0.0794\n",
            "Epoch [708/20000], Training Loss: 0.0771\n",
            "Epoch [709/20000], Training Loss: 0.0852\n",
            "Epoch [710/20000], Training Loss: 0.0874\n",
            "Epoch [711/20000], Training Loss: 0.0846\n",
            "Epoch [712/20000], Training Loss: 0.0800\n",
            "Epoch [713/20000], Training Loss: 0.0826\n",
            "Epoch [714/20000], Training Loss: 0.0814\n",
            "Epoch [715/20000], Training Loss: 0.0860\n",
            "Epoch [716/20000], Training Loss: 0.0825\n",
            "Epoch [717/20000], Training Loss: 0.0776\n",
            "Epoch [718/20000], Training Loss: 0.0817\n",
            "Epoch [719/20000], Training Loss: 0.0839\n",
            "Epoch [720/20000], Training Loss: 0.0840\n",
            "Epoch [721/20000], Training Loss: 0.0793\n",
            "Epoch [722/20000], Training Loss: 0.0862\n",
            "Epoch [723/20000], Training Loss: 0.0856\n",
            "Epoch [724/20000], Training Loss: 0.0870\n",
            "Epoch [725/20000], Training Loss: 0.0787\n",
            "Epoch [726/20000], Training Loss: 0.0767\n",
            "Epoch [727/20000], Training Loss: 0.0807\n",
            "Epoch [728/20000], Training Loss: 0.0833\n",
            "Epoch [729/20000], Training Loss: 0.0837\n",
            "Epoch [730/20000], Training Loss: 0.0833\n",
            "Epoch [731/20000], Training Loss: 0.0833\n",
            "Epoch [732/20000], Training Loss: 0.0860\n",
            "Epoch [733/20000], Training Loss: 0.0790\n",
            "Epoch [734/20000], Training Loss: 0.0827\n",
            "Epoch [735/20000], Training Loss: 0.0785\n",
            "Epoch [736/20000], Training Loss: 0.0768\n",
            "Epoch [737/20000], Training Loss: 0.0758\n",
            "Epoch [738/20000], Training Loss: 0.0787\n",
            "Epoch [739/20000], Training Loss: 0.0844\n",
            "Epoch [740/20000], Training Loss: 0.0794\n",
            "Epoch [741/20000], Training Loss: 0.0847\n",
            "Epoch [742/20000], Training Loss: 0.0799\n",
            "Epoch [743/20000], Training Loss: 0.0876\n",
            "Epoch [744/20000], Training Loss: 0.0780\n",
            "Epoch [745/20000], Training Loss: 0.0761\n",
            "Epoch [746/20000], Training Loss: 0.0883\n",
            "Epoch [747/20000], Training Loss: 0.0826\n",
            "Epoch [748/20000], Training Loss: 0.0868\n",
            "Epoch [749/20000], Training Loss: 0.0832\n",
            "Epoch [750/20000], Training Loss: 0.0834\n",
            "Epoch [751/20000], Training Loss: 0.0887\n",
            "Epoch [752/20000], Training Loss: 0.0854\n",
            "Epoch [753/20000], Training Loss: 0.0868\n",
            "Epoch [754/20000], Training Loss: 0.0823\n",
            "Epoch [755/20000], Training Loss: 0.0808\n",
            "Epoch [756/20000], Training Loss: 0.0848\n",
            "Epoch [757/20000], Training Loss: 0.0753\n",
            "Epoch [758/20000], Training Loss: 0.0846\n",
            "Epoch [759/20000], Training Loss: 0.0777\n",
            "Epoch [760/20000], Training Loss: 0.0780\n",
            "Epoch [761/20000], Training Loss: 0.0809\n",
            "Epoch [762/20000], Training Loss: 0.0847\n",
            "Epoch [763/20000], Training Loss: 0.0783\n",
            "Epoch [764/20000], Training Loss: 0.0811\n",
            "Epoch [765/20000], Training Loss: 0.0835\n",
            "Epoch [766/20000], Training Loss: 0.0753\n",
            "Epoch [767/20000], Training Loss: 0.0764\n",
            "Epoch [768/20000], Training Loss: 0.0778\n",
            "Epoch [769/20000], Training Loss: 0.0822\n",
            "Epoch [770/20000], Training Loss: 0.0856\n",
            "Epoch [771/20000], Training Loss: 0.0830\n",
            "Epoch [772/20000], Training Loss: 0.0779\n",
            "Epoch [773/20000], Training Loss: 0.0795\n",
            "Epoch [774/20000], Training Loss: 0.0822\n",
            "Epoch [775/20000], Training Loss: 0.0779\n",
            "Epoch [776/20000], Training Loss: 0.0870\n",
            "Epoch [777/20000], Training Loss: 0.0821\n",
            "Epoch [778/20000], Training Loss: 0.0797\n",
            "Epoch [779/20000], Training Loss: 0.0769\n",
            "Epoch [780/20000], Training Loss: 0.0758\n",
            "Epoch [781/20000], Training Loss: 0.0784\n",
            "Epoch [782/20000], Training Loss: 0.0799\n",
            "Epoch [783/20000], Training Loss: 0.0827\n",
            "Epoch [784/20000], Training Loss: 0.0782\n",
            "Epoch [785/20000], Training Loss: 0.0759\n",
            "Epoch [786/20000], Training Loss: 0.0780\n",
            "Epoch [787/20000], Training Loss: 0.0798\n",
            "Epoch [788/20000], Training Loss: 0.0842\n",
            "Epoch [789/20000], Training Loss: 0.0810\n",
            "Epoch [790/20000], Training Loss: 0.0782\n",
            "Epoch [791/20000], Training Loss: 0.0850\n",
            "Epoch [792/20000], Training Loss: 0.0838\n",
            "Epoch [793/20000], Training Loss: 0.0802\n",
            "Epoch [794/20000], Training Loss: 0.0865\n",
            "Epoch [795/20000], Training Loss: 0.0851\n",
            "Epoch [796/20000], Training Loss: 0.0815\n",
            "Epoch [797/20000], Training Loss: 0.0831\n",
            "Epoch [798/20000], Training Loss: 0.0813\n",
            "Epoch [799/20000], Training Loss: 0.0818\n",
            "Epoch [800/20000], Training Loss: 0.0825\n",
            "Epoch [801/20000], Training Loss: 0.0783\n",
            "Epoch [802/20000], Training Loss: 0.0839\n",
            "Epoch [803/20000], Training Loss: 0.0825\n",
            "Epoch [804/20000], Training Loss: 0.0839\n",
            "Epoch [805/20000], Training Loss: 0.0755\n",
            "Epoch [806/20000], Training Loss: 0.0777\n",
            "Epoch [807/20000], Training Loss: 0.0819\n",
            "Epoch [808/20000], Training Loss: 0.0831\n",
            "Epoch [809/20000], Training Loss: 0.0812\n",
            "Epoch [810/20000], Training Loss: 0.0838\n",
            "Epoch [811/20000], Training Loss: 0.0804\n",
            "Epoch [812/20000], Training Loss: 0.0759\n",
            "Epoch [813/20000], Training Loss: 0.0893\n",
            "Epoch [814/20000], Training Loss: 0.0755\n",
            "Epoch [815/20000], Training Loss: 0.0780\n",
            "Epoch [816/20000], Training Loss: 0.0821\n",
            "Epoch [817/20000], Training Loss: 0.0808\n",
            "Epoch [818/20000], Training Loss: 0.0829\n",
            "Epoch [819/20000], Training Loss: 0.0820\n",
            "Epoch [820/20000], Training Loss: 0.0814\n",
            "Epoch [821/20000], Training Loss: 0.0769\n",
            "Epoch [822/20000], Training Loss: 0.0797\n",
            "Epoch [823/20000], Training Loss: 0.0873\n",
            "Epoch [824/20000], Training Loss: 0.0826\n",
            "Epoch [825/20000], Training Loss: 0.0785\n",
            "Epoch [826/20000], Training Loss: 0.0798\n",
            "Epoch [827/20000], Training Loss: 0.0791\n",
            "Epoch [828/20000], Training Loss: 0.0782\n",
            "Epoch [829/20000], Training Loss: 0.0827\n",
            "Epoch [830/20000], Training Loss: 0.0789\n",
            "Epoch [831/20000], Training Loss: 0.0819\n",
            "Epoch [832/20000], Training Loss: 0.0882\n",
            "Epoch [833/20000], Training Loss: 0.0771\n",
            "Epoch [834/20000], Training Loss: 0.0860\n",
            "Epoch [835/20000], Training Loss: 0.0791\n",
            "Epoch [836/20000], Training Loss: 0.0813\n",
            "Epoch [837/20000], Training Loss: 0.0828\n",
            "Epoch [838/20000], Training Loss: 0.0809\n",
            "Epoch [839/20000], Training Loss: 0.0740\n",
            "Epoch [840/20000], Training Loss: 0.0797\n",
            "Epoch [841/20000], Training Loss: 0.0817\n",
            "Epoch [842/20000], Training Loss: 0.0785\n",
            "Epoch [843/20000], Training Loss: 0.0786\n",
            "Epoch [844/20000], Training Loss: 0.0806\n",
            "Epoch [845/20000], Training Loss: 0.0835\n",
            "Epoch [846/20000], Training Loss: 0.0837\n",
            "Epoch [847/20000], Training Loss: 0.0760\n",
            "Epoch [848/20000], Training Loss: 0.0787\n",
            "Epoch [849/20000], Training Loss: 0.0834\n",
            "Epoch [850/20000], Training Loss: 0.0803\n",
            "Epoch [851/20000], Training Loss: 0.0757\n",
            "Epoch [852/20000], Training Loss: 0.0800\n",
            "Epoch [853/20000], Training Loss: 0.0748\n",
            "Epoch [854/20000], Training Loss: 0.0812\n",
            "Epoch [855/20000], Training Loss: 0.0754\n",
            "Epoch [856/20000], Training Loss: 0.0843\n",
            "Epoch [857/20000], Training Loss: 0.0771\n",
            "Epoch [858/20000], Training Loss: 0.0806\n",
            "Epoch [859/20000], Training Loss: 0.0807\n",
            "Epoch [860/20000], Training Loss: 0.0852\n",
            "Epoch [861/20000], Training Loss: 0.0815\n",
            "Epoch [862/20000], Training Loss: 0.0825\n",
            "Epoch [863/20000], Training Loss: 0.0777\n",
            "Epoch [864/20000], Training Loss: 0.0805\n",
            "Epoch [865/20000], Training Loss: 0.0748\n",
            "Epoch [866/20000], Training Loss: 0.0786\n",
            "Epoch [867/20000], Training Loss: 0.0753\n",
            "Epoch [868/20000], Training Loss: 0.0781\n",
            "Epoch [869/20000], Training Loss: 0.0752\n",
            "Epoch [870/20000], Training Loss: 0.0781\n",
            "Epoch [871/20000], Training Loss: 0.0795\n",
            "Epoch [872/20000], Training Loss: 0.0810\n",
            "Epoch [873/20000], Training Loss: 0.0805\n",
            "Epoch [874/20000], Training Loss: 0.0828\n",
            "Epoch [875/20000], Training Loss: 0.0803\n",
            "Epoch [876/20000], Training Loss: 0.0842\n",
            "Epoch [877/20000], Training Loss: 0.0764\n",
            "Epoch [878/20000], Training Loss: 0.0754\n",
            "Epoch [879/20000], Training Loss: 0.0800\n",
            "Epoch [880/20000], Training Loss: 0.0765\n",
            "Epoch [881/20000], Training Loss: 0.0825\n",
            "Epoch [882/20000], Training Loss: 0.0787\n",
            "Epoch [883/20000], Training Loss: 0.0859\n",
            "Epoch [884/20000], Training Loss: 0.0865\n",
            "Epoch [885/20000], Training Loss: 0.0855\n",
            "Epoch [886/20000], Training Loss: 0.0729\n",
            "Epoch [887/20000], Training Loss: 0.0812\n",
            "Epoch [888/20000], Training Loss: 0.0764\n",
            "Epoch [889/20000], Training Loss: 0.0750\n",
            "Epoch [890/20000], Training Loss: 0.0822\n",
            "Epoch [891/20000], Training Loss: 0.0724\n",
            "Epoch [892/20000], Training Loss: 0.0767\n",
            "Epoch [893/20000], Training Loss: 0.0852\n",
            "Epoch [894/20000], Training Loss: 0.0766\n",
            "Epoch [895/20000], Training Loss: 0.0742\n",
            "Epoch [896/20000], Training Loss: 0.0799\n",
            "Epoch [897/20000], Training Loss: 0.0817\n",
            "Epoch [898/20000], Training Loss: 0.0815\n",
            "Epoch [899/20000], Training Loss: 0.0854\n",
            "Epoch [900/20000], Training Loss: 0.0803\n",
            "Epoch [901/20000], Training Loss: 0.0784\n",
            "Epoch [902/20000], Training Loss: 0.0802\n",
            "Epoch [903/20000], Training Loss: 0.0793\n",
            "Epoch [904/20000], Training Loss: 0.0787\n",
            "Epoch [905/20000], Training Loss: 0.0813\n",
            "Epoch [906/20000], Training Loss: 0.0793\n",
            "Epoch [907/20000], Training Loss: 0.0827\n",
            "Epoch [908/20000], Training Loss: 0.0805\n",
            "Epoch [909/20000], Training Loss: 0.0794\n",
            "Epoch [910/20000], Training Loss: 0.0815\n",
            "Epoch [911/20000], Training Loss: 0.0866\n",
            "Epoch [912/20000], Training Loss: 0.0830\n",
            "Epoch [913/20000], Training Loss: 0.0751\n",
            "Epoch [914/20000], Training Loss: 0.0743\n",
            "Epoch [915/20000], Training Loss: 0.0791\n",
            "Epoch [916/20000], Training Loss: 0.0767\n",
            "Epoch [917/20000], Training Loss: 0.0826\n",
            "Epoch [918/20000], Training Loss: 0.0742\n",
            "Epoch [919/20000], Training Loss: 0.0842\n",
            "Epoch [920/20000], Training Loss: 0.0815\n",
            "Epoch [921/20000], Training Loss: 0.0796\n",
            "Epoch [922/20000], Training Loss: 0.0803\n",
            "Epoch [923/20000], Training Loss: 0.0822\n",
            "Epoch [924/20000], Training Loss: 0.0788\n",
            "Epoch [925/20000], Training Loss: 0.0810\n",
            "Epoch [926/20000], Training Loss: 0.0810\n",
            "Epoch [927/20000], Training Loss: 0.0790\n",
            "Epoch [928/20000], Training Loss: 0.0854\n",
            "Epoch [929/20000], Training Loss: 0.0776\n",
            "Epoch [930/20000], Training Loss: 0.0854\n",
            "Epoch [931/20000], Training Loss: 0.0766\n",
            "Epoch [932/20000], Training Loss: 0.0856\n",
            "Epoch [933/20000], Training Loss: 0.0790\n",
            "Epoch [934/20000], Training Loss: 0.0786\n",
            "Epoch [935/20000], Training Loss: 0.0808\n",
            "Epoch [936/20000], Training Loss: 0.0750\n",
            "Epoch [937/20000], Training Loss: 0.0818\n",
            "Epoch [938/20000], Training Loss: 0.0788\n",
            "Epoch [939/20000], Training Loss: 0.0826\n",
            "Epoch [940/20000], Training Loss: 0.0823\n",
            "Epoch [941/20000], Training Loss: 0.0764\n",
            "Epoch [942/20000], Training Loss: 0.0807\n",
            "Epoch [943/20000], Training Loss: 0.0743\n",
            "Epoch [944/20000], Training Loss: 0.0836\n",
            "Epoch [945/20000], Training Loss: 0.0852\n",
            "Epoch [946/20000], Training Loss: 0.0761\n",
            "Epoch [947/20000], Training Loss: 0.0777\n",
            "Epoch [948/20000], Training Loss: 0.0864\n",
            "Epoch [949/20000], Training Loss: 0.0793\n",
            "Epoch [950/20000], Training Loss: 0.0819\n",
            "Epoch [951/20000], Training Loss: 0.0754\n",
            "Epoch [952/20000], Training Loss: 0.0747\n",
            "Epoch [953/20000], Training Loss: 0.0773\n",
            "Epoch [954/20000], Training Loss: 0.0738\n",
            "Epoch [955/20000], Training Loss: 0.0816\n",
            "Epoch [956/20000], Training Loss: 0.0822\n",
            "Epoch [957/20000], Training Loss: 0.0834\n",
            "Epoch [958/20000], Training Loss: 0.0810\n",
            "Epoch [959/20000], Training Loss: 0.0753\n",
            "Epoch [960/20000], Training Loss: 0.0773\n",
            "Epoch [961/20000], Training Loss: 0.0795\n",
            "Epoch [962/20000], Training Loss: 0.0814\n",
            "Epoch [963/20000], Training Loss: 0.0787\n",
            "Epoch [964/20000], Training Loss: 0.0825\n",
            "Epoch [965/20000], Training Loss: 0.0878\n",
            "Epoch [966/20000], Training Loss: 0.0763\n",
            "Epoch [967/20000], Training Loss: 0.0815\n",
            "Epoch [968/20000], Training Loss: 0.0836\n",
            "Epoch [969/20000], Training Loss: 0.0811\n",
            "Epoch [970/20000], Training Loss: 0.0837\n",
            "Epoch [971/20000], Training Loss: 0.0764\n",
            "Epoch [972/20000], Training Loss: 0.0751\n",
            "Epoch [973/20000], Training Loss: 0.0831\n",
            "Epoch [974/20000], Training Loss: 0.0850\n",
            "Epoch [975/20000], Training Loss: 0.0722\n",
            "Epoch [976/20000], Training Loss: 0.0774\n",
            "Epoch [977/20000], Training Loss: 0.0783\n",
            "Epoch [978/20000], Training Loss: 0.0773\n",
            "Epoch [979/20000], Training Loss: 0.0859\n",
            "Epoch [980/20000], Training Loss: 0.0772\n",
            "Epoch [981/20000], Training Loss: 0.0821\n",
            "Epoch [982/20000], Training Loss: 0.0865\n",
            "Epoch [983/20000], Training Loss: 0.0822\n",
            "Epoch [984/20000], Training Loss: 0.0824\n",
            "Epoch [985/20000], Training Loss: 0.0802\n",
            "Epoch [986/20000], Training Loss: 0.0792\n",
            "Epoch [987/20000], Training Loss: 0.0851\n",
            "Epoch [988/20000], Training Loss: 0.0759\n",
            "Epoch [989/20000], Training Loss: 0.0811\n",
            "Epoch [990/20000], Training Loss: 0.0869\n",
            "Epoch [991/20000], Training Loss: 0.0822\n",
            "Epoch [992/20000], Training Loss: 0.0851\n",
            "Epoch [993/20000], Training Loss: 0.0796\n",
            "Epoch [994/20000], Training Loss: 0.0788\n",
            "Epoch [995/20000], Training Loss: 0.0776\n",
            "Epoch [996/20000], Training Loss: 0.0831\n",
            "Epoch [997/20000], Training Loss: 0.0852\n",
            "Epoch [998/20000], Training Loss: 0.0768\n",
            "Epoch [999/20000], Training Loss: 0.0782\n",
            "Epoch [1000/20000], Training Loss: 0.0781\n",
            "Epoch [1001/20000], Training Loss: 0.0755\n",
            "Epoch [1002/20000], Training Loss: 0.0833\n",
            "Epoch [1003/20000], Training Loss: 0.0796\n",
            "Epoch [1004/20000], Training Loss: 0.0786\n",
            "Epoch [1005/20000], Training Loss: 0.0793\n",
            "Epoch [1006/20000], Training Loss: 0.0860\n",
            "Epoch [1007/20000], Training Loss: 0.0812\n",
            "Epoch [1008/20000], Training Loss: 0.0776\n",
            "Epoch [1009/20000], Training Loss: 0.0800\n",
            "Epoch [1010/20000], Training Loss: 0.0841\n",
            "Epoch [1011/20000], Training Loss: 0.0799\n",
            "Epoch [1012/20000], Training Loss: 0.0825\n",
            "Epoch [1013/20000], Training Loss: 0.0854\n",
            "Epoch [1014/20000], Training Loss: 0.0794\n",
            "Epoch [1015/20000], Training Loss: 0.0796\n",
            "Epoch [1016/20000], Training Loss: 0.0779\n",
            "Epoch [1017/20000], Training Loss: 0.0748\n",
            "Epoch [1018/20000], Training Loss: 0.0755\n",
            "Epoch [1019/20000], Training Loss: 0.0832\n",
            "Epoch [1020/20000], Training Loss: 0.0841\n",
            "Epoch [1021/20000], Training Loss: 0.0853\n",
            "Epoch [1022/20000], Training Loss: 0.0860\n",
            "Epoch [1023/20000], Training Loss: 0.0807\n",
            "Epoch [1024/20000], Training Loss: 0.0747\n",
            "Epoch [1025/20000], Training Loss: 0.0828\n",
            "Epoch [1026/20000], Training Loss: 0.0776\n",
            "Epoch [1027/20000], Training Loss: 0.0811\n",
            "Epoch [1028/20000], Training Loss: 0.0846\n",
            "Epoch [1029/20000], Training Loss: 0.0776\n",
            "Epoch [1030/20000], Training Loss: 0.0832\n",
            "Epoch [1031/20000], Training Loss: 0.0828\n",
            "Epoch [1032/20000], Training Loss: 0.0750\n",
            "Epoch [1033/20000], Training Loss: 0.0825\n",
            "Epoch [1034/20000], Training Loss: 0.0825\n",
            "Epoch [1035/20000], Training Loss: 0.0807\n",
            "Epoch [1036/20000], Training Loss: 0.0762\n",
            "Epoch [1037/20000], Training Loss: 0.0786\n",
            "Epoch [1038/20000], Training Loss: 0.0738\n",
            "Epoch [1039/20000], Training Loss: 0.0839\n",
            "Epoch [1040/20000], Training Loss: 0.0803\n",
            "Epoch [1041/20000], Training Loss: 0.0751\n",
            "Epoch [1042/20000], Training Loss: 0.0793\n",
            "Epoch [1043/20000], Training Loss: 0.0744\n",
            "Epoch [1044/20000], Training Loss: 0.0742\n",
            "Epoch [1045/20000], Training Loss: 0.0757\n",
            "Epoch [1046/20000], Training Loss: 0.0783\n",
            "Epoch [1047/20000], Training Loss: 0.0762\n",
            "Epoch [1048/20000], Training Loss: 0.0757\n",
            "Epoch [1049/20000], Training Loss: 0.0822\n",
            "Epoch [1050/20000], Training Loss: 0.0809\n",
            "Epoch [1051/20000], Training Loss: 0.0741\n",
            "Epoch [1052/20000], Training Loss: 0.0801\n",
            "Epoch [1053/20000], Training Loss: 0.0781\n",
            "Epoch [1054/20000], Training Loss: 0.0854\n",
            "Epoch [1055/20000], Training Loss: 0.0752\n",
            "Epoch [1056/20000], Training Loss: 0.0765\n",
            "Epoch [1057/20000], Training Loss: 0.0843\n",
            "Epoch [1058/20000], Training Loss: 0.0796\n",
            "Epoch [1059/20000], Training Loss: 0.0832\n",
            "Epoch [1060/20000], Training Loss: 0.0806\n",
            "Epoch [1061/20000], Training Loss: 0.0831\n",
            "Epoch [1062/20000], Training Loss: 0.0828\n",
            "Epoch [1063/20000], Training Loss: 0.0837\n",
            "Epoch [1064/20000], Training Loss: 0.0820\n",
            "Epoch [1065/20000], Training Loss: 0.0809\n",
            "Epoch [1066/20000], Training Loss: 0.0782\n",
            "Epoch [1067/20000], Training Loss: 0.0763\n",
            "Epoch [1068/20000], Training Loss: 0.0754\n",
            "Epoch [1069/20000], Training Loss: 0.0777\n",
            "Epoch [1070/20000], Training Loss: 0.0807\n",
            "Epoch [1071/20000], Training Loss: 0.0812\n",
            "Epoch [1072/20000], Training Loss: 0.0746\n",
            "Epoch [1073/20000], Training Loss: 0.0750\n",
            "Epoch [1074/20000], Training Loss: 0.0863\n",
            "Epoch [1075/20000], Training Loss: 0.0783\n",
            "Epoch [1076/20000], Training Loss: 0.0765\n",
            "Epoch [1077/20000], Training Loss: 0.0792\n",
            "Epoch [1078/20000], Training Loss: 0.0772\n",
            "Epoch [1079/20000], Training Loss: 0.0790\n",
            "Epoch [1080/20000], Training Loss: 0.0737\n",
            "Epoch [1081/20000], Training Loss: 0.0797\n",
            "Epoch [1082/20000], Training Loss: 0.0745\n",
            "Epoch [1083/20000], Training Loss: 0.0753\n",
            "Epoch [1084/20000], Training Loss: 0.0825\n",
            "Epoch [1085/20000], Training Loss: 0.0793\n",
            "Epoch [1086/20000], Training Loss: 0.0829\n",
            "Epoch [1087/20000], Training Loss: 0.0810\n",
            "Epoch [1088/20000], Training Loss: 0.0855\n",
            "Epoch [1089/20000], Training Loss: 0.0793\n",
            "Epoch [1090/20000], Training Loss: 0.0776\n",
            "Epoch [1091/20000], Training Loss: 0.0818\n",
            "Epoch [1092/20000], Training Loss: 0.0754\n",
            "Epoch [1093/20000], Training Loss: 0.0790\n",
            "Epoch [1094/20000], Training Loss: 0.0753\n",
            "Epoch [1095/20000], Training Loss: 0.0796\n",
            "Epoch [1096/20000], Training Loss: 0.0799\n",
            "Epoch [1097/20000], Training Loss: 0.0833\n",
            "Epoch [1098/20000], Training Loss: 0.0797\n",
            "Epoch [1099/20000], Training Loss: 0.0792\n",
            "Epoch [1100/20000], Training Loss: 0.0752\n",
            "Epoch [1101/20000], Training Loss: 0.0781\n",
            "Epoch [1102/20000], Training Loss: 0.0765\n",
            "Epoch [1103/20000], Training Loss: 0.0766\n",
            "Epoch [1104/20000], Training Loss: 0.0773\n",
            "Epoch [1105/20000], Training Loss: 0.0790\n",
            "Epoch [1106/20000], Training Loss: 0.0734\n",
            "Epoch [1107/20000], Training Loss: 0.0835\n",
            "Epoch [1108/20000], Training Loss: 0.0865\n",
            "Epoch [1109/20000], Training Loss: 0.0815\n",
            "Epoch [1110/20000], Training Loss: 0.0803\n",
            "Epoch [1111/20000], Training Loss: 0.0776\n",
            "Epoch [1112/20000], Training Loss: 0.0793\n",
            "Epoch [1113/20000], Training Loss: 0.0742\n",
            "Epoch [1114/20000], Training Loss: 0.0794\n",
            "Epoch [1115/20000], Training Loss: 0.0759\n",
            "Epoch [1116/20000], Training Loss: 0.0810\n",
            "Epoch [1117/20000], Training Loss: 0.0792\n",
            "Epoch [1118/20000], Training Loss: 0.0804\n",
            "Epoch [1119/20000], Training Loss: 0.0803\n",
            "Epoch [1120/20000], Training Loss: 0.0889\n",
            "Epoch [1121/20000], Training Loss: 0.0828\n",
            "Epoch [1122/20000], Training Loss: 0.0757\n",
            "Epoch [1123/20000], Training Loss: 0.0793\n",
            "Epoch [1124/20000], Training Loss: 0.0789\n",
            "Epoch [1125/20000], Training Loss: 0.0842\n",
            "Epoch [1126/20000], Training Loss: 0.0796\n",
            "Epoch [1127/20000], Training Loss: 0.0790\n",
            "Epoch [1128/20000], Training Loss: 0.0761\n",
            "Epoch [1129/20000], Training Loss: 0.0785\n",
            "Epoch [1130/20000], Training Loss: 0.0831\n",
            "Epoch [1131/20000], Training Loss: 0.0799\n",
            "Epoch [1132/20000], Training Loss: 0.0744\n",
            "Epoch [1133/20000], Training Loss: 0.0784\n",
            "Epoch [1134/20000], Training Loss: 0.0800\n",
            "Epoch [1135/20000], Training Loss: 0.0815\n",
            "Epoch [1136/20000], Training Loss: 0.0810\n",
            "Epoch [1137/20000], Training Loss: 0.0760\n",
            "Epoch [1138/20000], Training Loss: 0.0826\n",
            "Epoch [1139/20000], Training Loss: 0.0757\n",
            "Epoch [1140/20000], Training Loss: 0.0822\n",
            "Epoch [1141/20000], Training Loss: 0.0769\n",
            "Epoch [1142/20000], Training Loss: 0.0776\n",
            "Epoch [1143/20000], Training Loss: 0.0778\n",
            "Epoch [1144/20000], Training Loss: 0.0801\n",
            "Epoch [1145/20000], Training Loss: 0.0831\n",
            "Epoch [1146/20000], Training Loss: 0.0779\n",
            "Epoch [1147/20000], Training Loss: 0.0815\n",
            "Epoch [1148/20000], Training Loss: 0.0795\n",
            "Epoch [1149/20000], Training Loss: 0.0804\n",
            "Epoch [1150/20000], Training Loss: 0.0784\n",
            "Epoch [1151/20000], Training Loss: 0.0822\n",
            "Epoch [1152/20000], Training Loss: 0.0802\n",
            "Epoch [1153/20000], Training Loss: 0.0813\n",
            "Epoch [1154/20000], Training Loss: 0.0842\n",
            "Epoch [1155/20000], Training Loss: 0.0832\n",
            "Epoch [1156/20000], Training Loss: 0.0826\n",
            "Epoch [1157/20000], Training Loss: 0.0795\n",
            "Epoch [1158/20000], Training Loss: 0.0789\n",
            "Epoch [1159/20000], Training Loss: 0.0748\n",
            "Epoch [1160/20000], Training Loss: 0.0850\n",
            "Epoch [1161/20000], Training Loss: 0.0784\n",
            "Epoch [1162/20000], Training Loss: 0.0758\n",
            "Epoch [1163/20000], Training Loss: 0.0806\n",
            "Epoch [1164/20000], Training Loss: 0.0775\n",
            "Epoch [1165/20000], Training Loss: 0.0807\n",
            "Epoch [1166/20000], Training Loss: 0.0732\n",
            "Epoch [1167/20000], Training Loss: 0.0831\n",
            "Epoch [1168/20000], Training Loss: 0.0816\n",
            "Epoch [1169/20000], Training Loss: 0.0794\n",
            "Epoch [1170/20000], Training Loss: 0.0851\n",
            "Epoch [1171/20000], Training Loss: 0.0798\n",
            "Epoch [1172/20000], Training Loss: 0.0810\n",
            "Epoch [1173/20000], Training Loss: 0.0742\n",
            "Epoch [1174/20000], Training Loss: 0.0818\n",
            "Epoch [1175/20000], Training Loss: 0.0754\n",
            "Epoch [1176/20000], Training Loss: 0.0733\n",
            "Epoch [1177/20000], Training Loss: 0.0810\n",
            "Epoch [1178/20000], Training Loss: 0.0775\n",
            "Epoch [1179/20000], Training Loss: 0.0794\n",
            "Epoch [1180/20000], Training Loss: 0.0769\n",
            "Epoch [1181/20000], Training Loss: 0.0818\n",
            "Epoch [1182/20000], Training Loss: 0.0781\n",
            "Epoch [1183/20000], Training Loss: 0.0814\n",
            "Epoch [1184/20000], Training Loss: 0.0748\n",
            "Epoch [1185/20000], Training Loss: 0.0800\n",
            "Epoch [1186/20000], Training Loss: 0.0816\n",
            "Epoch [1187/20000], Training Loss: 0.0737\n",
            "Epoch [1188/20000], Training Loss: 0.0806\n",
            "Epoch [1189/20000], Training Loss: 0.0800\n",
            "Epoch [1190/20000], Training Loss: 0.0753\n",
            "Epoch [1191/20000], Training Loss: 0.0794\n",
            "Epoch [1192/20000], Training Loss: 0.0811\n",
            "Epoch [1193/20000], Training Loss: 0.0808\n",
            "Epoch [1194/20000], Training Loss: 0.0815\n",
            "Epoch [1195/20000], Training Loss: 0.0818\n",
            "Epoch [1196/20000], Training Loss: 0.0891\n",
            "Epoch [1197/20000], Training Loss: 0.0802\n",
            "Epoch [1198/20000], Training Loss: 0.0791\n",
            "Epoch [1199/20000], Training Loss: 0.0776\n",
            "Epoch [1200/20000], Training Loss: 0.0751\n",
            "Epoch [1201/20000], Training Loss: 0.0842\n",
            "Epoch [1202/20000], Training Loss: 0.0818\n",
            "Epoch [1203/20000], Training Loss: 0.0842\n",
            "Epoch [1204/20000], Training Loss: 0.0777\n",
            "Epoch [1205/20000], Training Loss: 0.0847\n",
            "Epoch [1206/20000], Training Loss: 0.0731\n",
            "Epoch [1207/20000], Training Loss: 0.0784\n",
            "Epoch [1208/20000], Training Loss: 0.0799\n",
            "Epoch [1209/20000], Training Loss: 0.0849\n",
            "Epoch [1210/20000], Training Loss: 0.0805\n",
            "Epoch [1211/20000], Training Loss: 0.0786\n",
            "Epoch [1212/20000], Training Loss: 0.0763\n",
            "Epoch [1213/20000], Training Loss: 0.0747\n",
            "Epoch [1214/20000], Training Loss: 0.0843\n",
            "Epoch [1215/20000], Training Loss: 0.0789\n",
            "Epoch [1216/20000], Training Loss: 0.0791\n",
            "Epoch [1217/20000], Training Loss: 0.0791\n",
            "Epoch [1218/20000], Training Loss: 0.0800\n",
            "Epoch [1219/20000], Training Loss: 0.0773\n",
            "Epoch [1220/20000], Training Loss: 0.0799\n",
            "Epoch [1221/20000], Training Loss: 0.0810\n",
            "Epoch [1222/20000], Training Loss: 0.0801\n",
            "Epoch [1223/20000], Training Loss: 0.0748\n",
            "Epoch [1224/20000], Training Loss: 0.0798\n",
            "Epoch [1225/20000], Training Loss: 0.0743\n",
            "Epoch [1226/20000], Training Loss: 0.0770\n",
            "Epoch [1227/20000], Training Loss: 0.0798\n",
            "Epoch [1228/20000], Training Loss: 0.0802\n",
            "Epoch [1229/20000], Training Loss: 0.0747\n",
            "Epoch [1230/20000], Training Loss: 0.0809\n",
            "Epoch [1231/20000], Training Loss: 0.0757\n",
            "Epoch [1232/20000], Training Loss: 0.0795\n",
            "Epoch [1233/20000], Training Loss: 0.0786\n",
            "Epoch [1234/20000], Training Loss: 0.0803\n",
            "Epoch [1235/20000], Training Loss: 0.0822\n",
            "Epoch [1236/20000], Training Loss: 0.0764\n",
            "Epoch [1237/20000], Training Loss: 0.0823\n",
            "Epoch [1238/20000], Training Loss: 0.0796\n",
            "Epoch [1239/20000], Training Loss: 0.0863\n",
            "Epoch [1240/20000], Training Loss: 0.0827\n",
            "Epoch [1241/20000], Training Loss: 0.0775\n",
            "Epoch [1242/20000], Training Loss: 0.0770\n",
            "Epoch [1243/20000], Training Loss: 0.0819\n",
            "Epoch [1244/20000], Training Loss: 0.0813\n",
            "Epoch [1245/20000], Training Loss: 0.0828\n",
            "Epoch [1246/20000], Training Loss: 0.0766\n",
            "Epoch [1247/20000], Training Loss: 0.0786\n",
            "Epoch [1248/20000], Training Loss: 0.0740\n",
            "Epoch [1249/20000], Training Loss: 0.0802\n",
            "Epoch [1250/20000], Training Loss: 0.0843\n",
            "Epoch [1251/20000], Training Loss: 0.0803\n",
            "Epoch [1252/20000], Training Loss: 0.0846\n",
            "Epoch [1253/20000], Training Loss: 0.0824\n",
            "Epoch [1254/20000], Training Loss: 0.0807\n",
            "Epoch [1255/20000], Training Loss: 0.0788\n",
            "Epoch [1256/20000], Training Loss: 0.0782\n",
            "Epoch [1257/20000], Training Loss: 0.0808\n",
            "Epoch [1258/20000], Training Loss: 0.0852\n",
            "Epoch [1259/20000], Training Loss: 0.0807\n",
            "Epoch [1260/20000], Training Loss: 0.0793\n",
            "Epoch [1261/20000], Training Loss: 0.0881\n",
            "Epoch [1262/20000], Training Loss: 0.0759\n",
            "Epoch [1263/20000], Training Loss: 0.0843\n",
            "Epoch [1264/20000], Training Loss: 0.0793\n",
            "Epoch [1265/20000], Training Loss: 0.0861\n",
            "Epoch [1266/20000], Training Loss: 0.0798\n",
            "Epoch [1267/20000], Training Loss: 0.0764\n",
            "Epoch [1268/20000], Training Loss: 0.0810\n",
            "Epoch [1269/20000], Training Loss: 0.0754\n",
            "Epoch [1270/20000], Training Loss: 0.0796\n",
            "Epoch [1271/20000], Training Loss: 0.0796\n",
            "Epoch [1272/20000], Training Loss: 0.0803\n",
            "Epoch [1273/20000], Training Loss: 0.0753\n",
            "Epoch [1274/20000], Training Loss: 0.0782\n",
            "Epoch [1275/20000], Training Loss: 0.0784\n",
            "Epoch [1276/20000], Training Loss: 0.0771\n",
            "Epoch [1277/20000], Training Loss: 0.0845\n",
            "Epoch [1278/20000], Training Loss: 0.0791\n",
            "Epoch [1279/20000], Training Loss: 0.0787\n",
            "Epoch [1280/20000], Training Loss: 0.0820\n",
            "Epoch [1281/20000], Training Loss: 0.0882\n",
            "Epoch [1282/20000], Training Loss: 0.0747\n",
            "Epoch [1283/20000], Training Loss: 0.0748\n",
            "Epoch [1284/20000], Training Loss: 0.0831\n",
            "Epoch [1285/20000], Training Loss: 0.0785\n",
            "Epoch [1286/20000], Training Loss: 0.0820\n",
            "Epoch [1287/20000], Training Loss: 0.0824\n",
            "Epoch [1288/20000], Training Loss: 0.0832\n",
            "Epoch [1289/20000], Training Loss: 0.0762\n",
            "Epoch [1290/20000], Training Loss: 0.0756\n",
            "Epoch [1291/20000], Training Loss: 0.0769\n",
            "Epoch [1292/20000], Training Loss: 0.0813\n",
            "Epoch [1293/20000], Training Loss: 0.0767\n",
            "Epoch [1294/20000], Training Loss: 0.0780\n",
            "Epoch [1295/20000], Training Loss: 0.0790\n",
            "Epoch [1296/20000], Training Loss: 0.0797\n",
            "Epoch [1297/20000], Training Loss: 0.0799\n",
            "Epoch [1298/20000], Training Loss: 0.0811\n",
            "Epoch [1299/20000], Training Loss: 0.0770\n",
            "Epoch [1300/20000], Training Loss: 0.0743\n",
            "Epoch [1301/20000], Training Loss: 0.0842\n",
            "Epoch [1302/20000], Training Loss: 0.0821\n",
            "Epoch [1303/20000], Training Loss: 0.0814\n",
            "Epoch [1304/20000], Training Loss: 0.0796\n",
            "Epoch [1305/20000], Training Loss: 0.0730\n",
            "Epoch [1306/20000], Training Loss: 0.0826\n",
            "Epoch [1307/20000], Training Loss: 0.0830\n",
            "Epoch [1308/20000], Training Loss: 0.0781\n",
            "Epoch [1309/20000], Training Loss: 0.0735\n",
            "Epoch [1310/20000], Training Loss: 0.0815\n",
            "Epoch [1311/20000], Training Loss: 0.0800\n",
            "Epoch [1312/20000], Training Loss: 0.0807\n",
            "Epoch [1313/20000], Training Loss: 0.0758\n",
            "Epoch [1314/20000], Training Loss: 0.0782\n",
            "Epoch [1315/20000], Training Loss: 0.0766\n",
            "Epoch [1316/20000], Training Loss: 0.0867\n",
            "Epoch [1317/20000], Training Loss: 0.0838\n",
            "Epoch [1318/20000], Training Loss: 0.0778\n",
            "Epoch [1319/20000], Training Loss: 0.0800\n",
            "Epoch [1320/20000], Training Loss: 0.0801\n",
            "Epoch [1321/20000], Training Loss: 0.0780\n",
            "Epoch [1322/20000], Training Loss: 0.0796\n",
            "Epoch [1323/20000], Training Loss: 0.0781\n",
            "Epoch [1324/20000], Training Loss: 0.0812\n",
            "Epoch [1325/20000], Training Loss: 0.0750\n",
            "Epoch [1326/20000], Training Loss: 0.0809\n",
            "Epoch [1327/20000], Training Loss: 0.0841\n",
            "Epoch [1328/20000], Training Loss: 0.0794\n",
            "Epoch [1329/20000], Training Loss: 0.0826\n",
            "Epoch [1330/20000], Training Loss: 0.0772\n",
            "Epoch [1331/20000], Training Loss: 0.0777\n",
            "Epoch [1332/20000], Training Loss: 0.0815\n",
            "Epoch [1333/20000], Training Loss: 0.0829\n",
            "Epoch [1334/20000], Training Loss: 0.0730\n",
            "Epoch [1335/20000], Training Loss: 0.0763\n",
            "Epoch [1336/20000], Training Loss: 0.0802\n",
            "Epoch [1337/20000], Training Loss: 0.0810\n",
            "Epoch [1338/20000], Training Loss: 0.0814\n",
            "Epoch [1339/20000], Training Loss: 0.0833\n",
            "Epoch [1340/20000], Training Loss: 0.0806\n",
            "Epoch [1341/20000], Training Loss: 0.0781\n",
            "Epoch [1342/20000], Training Loss: 0.0830\n",
            "Epoch [1343/20000], Training Loss: 0.0886\n",
            "Epoch [1344/20000], Training Loss: 0.0817\n",
            "Epoch [1345/20000], Training Loss: 0.0846\n",
            "Epoch [1346/20000], Training Loss: 0.0806\n",
            "Epoch [1347/20000], Training Loss: 0.0792\n",
            "Epoch [1348/20000], Training Loss: 0.0747\n",
            "Epoch [1349/20000], Training Loss: 0.0821\n",
            "Epoch [1350/20000], Training Loss: 0.0782\n",
            "Epoch [1351/20000], Training Loss: 0.0805\n",
            "Epoch [1352/20000], Training Loss: 0.0802\n",
            "Epoch [1353/20000], Training Loss: 0.0754\n",
            "Epoch [1354/20000], Training Loss: 0.0743\n",
            "Epoch [1355/20000], Training Loss: 0.0785\n",
            "Epoch [1356/20000], Training Loss: 0.0817\n",
            "Epoch [1357/20000], Training Loss: 0.0766\n",
            "Epoch [1358/20000], Training Loss: 0.0763\n",
            "Epoch [1359/20000], Training Loss: 0.0764\n",
            "Epoch [1360/20000], Training Loss: 0.0755\n",
            "Epoch [1361/20000], Training Loss: 0.0824\n",
            "Epoch [1362/20000], Training Loss: 0.0891\n",
            "Epoch [1363/20000], Training Loss: 0.0792\n",
            "Epoch [1364/20000], Training Loss: 0.0808\n",
            "Epoch [1365/20000], Training Loss: 0.0776\n",
            "Epoch [1366/20000], Training Loss: 0.0804\n",
            "Epoch [1367/20000], Training Loss: 0.0820\n",
            "Epoch [1368/20000], Training Loss: 0.0829\n",
            "Epoch [1369/20000], Training Loss: 0.0843\n",
            "Epoch [1370/20000], Training Loss: 0.0764\n",
            "Epoch [1371/20000], Training Loss: 0.0800\n",
            "Epoch [1372/20000], Training Loss: 0.0789\n",
            "Epoch [1373/20000], Training Loss: 0.0754\n",
            "Epoch [1374/20000], Training Loss: 0.0738\n",
            "Epoch [1375/20000], Training Loss: 0.0804\n",
            "Epoch [1376/20000], Training Loss: 0.0782\n",
            "Epoch [1377/20000], Training Loss: 0.0789\n",
            "Epoch [1378/20000], Training Loss: 0.0853\n",
            "Epoch [1379/20000], Training Loss: 0.0749\n",
            "Epoch [1380/20000], Training Loss: 0.0744\n",
            "Epoch [1381/20000], Training Loss: 0.0774\n",
            "Epoch [1382/20000], Training Loss: 0.0773\n",
            "Epoch [1383/20000], Training Loss: 0.0827\n",
            "Epoch [1384/20000], Training Loss: 0.0734\n",
            "Epoch [1385/20000], Training Loss: 0.0790\n",
            "Epoch [1386/20000], Training Loss: 0.0810\n",
            "Epoch [1387/20000], Training Loss: 0.0816\n",
            "Epoch [1388/20000], Training Loss: 0.0780\n",
            "Epoch [1389/20000], Training Loss: 0.0793\n",
            "Epoch [1390/20000], Training Loss: 0.0741\n",
            "Epoch [1391/20000], Training Loss: 0.0749\n",
            "Epoch [1392/20000], Training Loss: 0.0831\n",
            "Epoch [1393/20000], Training Loss: 0.0804\n",
            "Epoch [1394/20000], Training Loss: 0.0782\n",
            "Epoch [1395/20000], Training Loss: 0.0780\n",
            "Epoch [1396/20000], Training Loss: 0.0802\n",
            "Epoch [1397/20000], Training Loss: 0.0790\n",
            "Epoch [1398/20000], Training Loss: 0.0805\n",
            "Epoch [1399/20000], Training Loss: 0.0797\n",
            "Epoch [1400/20000], Training Loss: 0.0843\n",
            "Epoch [1401/20000], Training Loss: 0.0780\n",
            "Epoch [1402/20000], Training Loss: 0.0753\n",
            "Epoch [1403/20000], Training Loss: 0.0769\n",
            "Epoch [1404/20000], Training Loss: 0.0832\n",
            "Epoch [1405/20000], Training Loss: 0.0781\n",
            "Epoch [1406/20000], Training Loss: 0.0801\n",
            "Epoch [1407/20000], Training Loss: 0.0805\n",
            "Epoch [1408/20000], Training Loss: 0.0806\n",
            "Epoch [1409/20000], Training Loss: 0.0800\n",
            "Epoch [1410/20000], Training Loss: 0.0828\n",
            "Epoch [1411/20000], Training Loss: 0.0832\n",
            "Epoch [1412/20000], Training Loss: 0.0744\n",
            "Epoch [1413/20000], Training Loss: 0.0815\n",
            "Epoch [1414/20000], Training Loss: 0.0852\n",
            "Epoch [1415/20000], Training Loss: 0.0790\n",
            "Epoch [1416/20000], Training Loss: 0.0786\n",
            "Epoch [1417/20000], Training Loss: 0.0819\n",
            "Epoch [1418/20000], Training Loss: 0.0775\n",
            "Epoch [1419/20000], Training Loss: 0.0787\n",
            "Epoch [1420/20000], Training Loss: 0.0744\n",
            "Epoch [1421/20000], Training Loss: 0.0788\n",
            "Epoch [1422/20000], Training Loss: 0.0792\n",
            "Epoch [1423/20000], Training Loss: 0.0729\n",
            "Epoch [1424/20000], Training Loss: 0.0735\n",
            "Epoch [1425/20000], Training Loss: 0.0792\n",
            "Epoch [1426/20000], Training Loss: 0.0774\n",
            "Epoch [1427/20000], Training Loss: 0.0791\n",
            "Epoch [1428/20000], Training Loss: 0.0755\n",
            "Epoch [1429/20000], Training Loss: 0.0793\n",
            "Epoch [1430/20000], Training Loss: 0.0782\n",
            "Epoch [1431/20000], Training Loss: 0.0789\n",
            "Epoch [1432/20000], Training Loss: 0.0819\n",
            "Epoch [1433/20000], Training Loss: 0.0803\n",
            "Epoch [1434/20000], Training Loss: 0.0835\n",
            "Epoch [1435/20000], Training Loss: 0.0760\n",
            "Epoch [1436/20000], Training Loss: 0.0797\n",
            "Epoch [1437/20000], Training Loss: 0.0820\n",
            "Epoch [1438/20000], Training Loss: 0.0802\n",
            "Epoch [1439/20000], Training Loss: 0.0777\n",
            "Epoch [1440/20000], Training Loss: 0.0862\n",
            "Epoch [1441/20000], Training Loss: 0.0830\n",
            "Epoch [1442/20000], Training Loss: 0.0731\n",
            "Epoch [1443/20000], Training Loss: 0.0821\n",
            "Epoch [1444/20000], Training Loss: 0.0790\n",
            "Epoch [1445/20000], Training Loss: 0.0764\n",
            "Epoch [1446/20000], Training Loss: 0.0798\n",
            "Epoch [1447/20000], Training Loss: 0.0844\n",
            "Epoch [1448/20000], Training Loss: 0.0789\n",
            "Epoch [1449/20000], Training Loss: 0.0803\n",
            "Epoch [1450/20000], Training Loss: 0.0797\n",
            "Epoch [1451/20000], Training Loss: 0.0831\n",
            "Epoch [1452/20000], Training Loss: 0.0815\n",
            "Epoch [1453/20000], Training Loss: 0.0815\n",
            "Epoch [1454/20000], Training Loss: 0.0864\n",
            "Epoch [1455/20000], Training Loss: 0.0787\n",
            "Epoch [1456/20000], Training Loss: 0.0805\n",
            "Epoch [1457/20000], Training Loss: 0.0759\n",
            "Epoch [1458/20000], Training Loss: 0.0808\n",
            "Epoch [1459/20000], Training Loss: 0.0807\n",
            "Epoch [1460/20000], Training Loss: 0.0865\n",
            "Epoch [1461/20000], Training Loss: 0.0761\n",
            "Epoch [1462/20000], Training Loss: 0.0829\n",
            "Epoch [1463/20000], Training Loss: 0.0754\n",
            "Epoch [1464/20000], Training Loss: 0.0825\n",
            "Epoch [1465/20000], Training Loss: 0.0820\n",
            "Epoch [1466/20000], Training Loss: 0.0837\n",
            "Epoch [1467/20000], Training Loss: 0.0776\n",
            "Epoch [1468/20000], Training Loss: 0.0813\n",
            "Epoch [1469/20000], Training Loss: 0.0871\n",
            "Epoch [1470/20000], Training Loss: 0.0778\n",
            "Epoch [1471/20000], Training Loss: 0.0863\n",
            "Epoch [1472/20000], Training Loss: 0.0729\n",
            "Epoch [1473/20000], Training Loss: 0.0812\n",
            "Epoch [1474/20000], Training Loss: 0.0768\n",
            "Epoch [1475/20000], Training Loss: 0.0821\n",
            "Epoch [1476/20000], Training Loss: 0.0807\n",
            "Epoch [1477/20000], Training Loss: 0.0807\n",
            "Epoch [1478/20000], Training Loss: 0.0821\n",
            "Epoch [1479/20000], Training Loss: 0.0816\n",
            "Epoch [1480/20000], Training Loss: 0.0785\n",
            "Epoch [1481/20000], Training Loss: 0.0837\n",
            "Epoch [1482/20000], Training Loss: 0.0764\n",
            "Epoch [1483/20000], Training Loss: 0.0800\n",
            "Epoch [1484/20000], Training Loss: 0.0804\n",
            "Epoch [1485/20000], Training Loss: 0.0787\n",
            "Epoch [1486/20000], Training Loss: 0.0837\n",
            "Epoch [1487/20000], Training Loss: 0.0857\n",
            "Epoch [1488/20000], Training Loss: 0.0784\n",
            "Epoch [1489/20000], Training Loss: 0.0772\n",
            "Epoch [1490/20000], Training Loss: 0.0771\n",
            "Epoch [1491/20000], Training Loss: 0.0793\n",
            "Epoch [1492/20000], Training Loss: 0.0775\n",
            "Epoch [1493/20000], Training Loss: 0.0786\n",
            "Epoch [1494/20000], Training Loss: 0.0777\n",
            "Epoch [1495/20000], Training Loss: 0.0811\n",
            "Epoch [1496/20000], Training Loss: 0.0753\n",
            "Epoch [1497/20000], Training Loss: 0.0848\n",
            "Epoch [1498/20000], Training Loss: 0.0826\n",
            "Epoch [1499/20000], Training Loss: 0.0820\n",
            "Epoch [1500/20000], Training Loss: 0.0850\n",
            "Epoch [1501/20000], Training Loss: 0.0743\n",
            "Epoch [1502/20000], Training Loss: 0.0770\n",
            "Epoch [1503/20000], Training Loss: 0.0836\n",
            "Epoch [1504/20000], Training Loss: 0.0733\n",
            "Epoch [1505/20000], Training Loss: 0.0783\n",
            "Epoch [1506/20000], Training Loss: 0.0829\n",
            "Epoch [1507/20000], Training Loss: 0.0793\n",
            "Epoch [1508/20000], Training Loss: 0.0814\n",
            "Epoch [1509/20000], Training Loss: 0.0794\n",
            "Epoch [1510/20000], Training Loss: 0.0753\n",
            "Epoch [1511/20000], Training Loss: 0.0859\n",
            "Epoch [1512/20000], Training Loss: 0.0796\n",
            "Epoch [1513/20000], Training Loss: 0.0743\n",
            "Epoch [1514/20000], Training Loss: 0.0823\n",
            "Epoch [1515/20000], Training Loss: 0.0768\n",
            "Epoch [1516/20000], Training Loss: 0.0767\n",
            "Epoch [1517/20000], Training Loss: 0.0738\n",
            "Epoch [1518/20000], Training Loss: 0.0760\n",
            "Epoch [1519/20000], Training Loss: 0.0839\n",
            "Epoch [1520/20000], Training Loss: 0.0786\n",
            "Epoch [1521/20000], Training Loss: 0.0800\n",
            "Epoch [1522/20000], Training Loss: 0.0831\n",
            "Epoch [1523/20000], Training Loss: 0.0772\n",
            "Epoch [1524/20000], Training Loss: 0.0863\n",
            "Epoch [1525/20000], Training Loss: 0.0803\n",
            "Epoch [1526/20000], Training Loss: 0.0816\n",
            "Epoch [1527/20000], Training Loss: 0.0805\n",
            "Epoch [1528/20000], Training Loss: 0.0842\n",
            "Epoch [1529/20000], Training Loss: 0.0867\n",
            "Epoch [1530/20000], Training Loss: 0.0787\n",
            "Epoch [1531/20000], Training Loss: 0.0801\n",
            "Epoch [1532/20000], Training Loss: 0.0747\n",
            "Epoch [1533/20000], Training Loss: 0.0804\n",
            "Epoch [1534/20000], Training Loss: 0.0799\n",
            "Epoch [1535/20000], Training Loss: 0.0796\n",
            "Epoch [1536/20000], Training Loss: 0.0793\n",
            "Epoch [1537/20000], Training Loss: 0.0746\n",
            "Epoch [1538/20000], Training Loss: 0.0785\n",
            "Epoch [1539/20000], Training Loss: 0.0855\n",
            "Epoch [1540/20000], Training Loss: 0.0804\n",
            "Epoch [1541/20000], Training Loss: 0.0810\n",
            "Epoch [1542/20000], Training Loss: 0.0779\n",
            "Epoch [1543/20000], Training Loss: 0.0861\n",
            "Epoch [1544/20000], Training Loss: 0.0789\n",
            "Epoch [1545/20000], Training Loss: 0.0753\n",
            "Epoch [1546/20000], Training Loss: 0.0807\n",
            "Epoch [1547/20000], Training Loss: 0.0797\n",
            "Epoch [1548/20000], Training Loss: 0.0785\n",
            "Epoch [1549/20000], Training Loss: 0.0786\n",
            "Epoch [1550/20000], Training Loss: 0.0781\n",
            "Epoch [1551/20000], Training Loss: 0.0799\n",
            "Epoch [1552/20000], Training Loss: 0.0825\n",
            "Epoch [1553/20000], Training Loss: 0.0788\n",
            "Epoch [1554/20000], Training Loss: 0.0868\n",
            "Epoch [1555/20000], Training Loss: 0.0791\n",
            "Epoch [1556/20000], Training Loss: 0.0776\n",
            "Epoch [1557/20000], Training Loss: 0.0775\n",
            "Epoch [1558/20000], Training Loss: 0.0779\n",
            "Epoch [1559/20000], Training Loss: 0.0816\n",
            "Epoch [1560/20000], Training Loss: 0.0777\n",
            "Epoch [1561/20000], Training Loss: 0.0741\n",
            "Epoch [1562/20000], Training Loss: 0.0796\n",
            "Epoch [1563/20000], Training Loss: 0.0867\n",
            "Epoch [1564/20000], Training Loss: 0.0829\n",
            "Epoch [1565/20000], Training Loss: 0.0763\n",
            "Epoch [1566/20000], Training Loss: 0.0845\n",
            "Epoch [1567/20000], Training Loss: 0.0755\n",
            "Epoch [1568/20000], Training Loss: 0.0745\n",
            "Epoch [1569/20000], Training Loss: 0.0840\n",
            "Epoch [1570/20000], Training Loss: 0.0872\n",
            "Epoch [1571/20000], Training Loss: 0.0796\n",
            "Epoch [1572/20000], Training Loss: 0.0802\n",
            "Epoch [1573/20000], Training Loss: 0.0748\n",
            "Epoch [1574/20000], Training Loss: 0.0782\n",
            "Epoch [1575/20000], Training Loss: 0.0749\n",
            "Epoch [1576/20000], Training Loss: 0.0799\n",
            "Epoch [1577/20000], Training Loss: 0.0783\n",
            "Epoch [1578/20000], Training Loss: 0.0757\n",
            "Epoch [1579/20000], Training Loss: 0.0795\n",
            "Epoch [1580/20000], Training Loss: 0.0758\n",
            "Epoch [1581/20000], Training Loss: 0.0803\n",
            "Epoch [1582/20000], Training Loss: 0.0748\n",
            "Epoch [1583/20000], Training Loss: 0.0770\n",
            "Epoch [1584/20000], Training Loss: 0.0794\n",
            "Epoch [1585/20000], Training Loss: 0.0787\n",
            "Epoch [1586/20000], Training Loss: 0.0770\n",
            "Epoch [1587/20000], Training Loss: 0.0731\n",
            "Epoch [1588/20000], Training Loss: 0.0783\n",
            "Epoch [1589/20000], Training Loss: 0.0823\n",
            "Epoch [1590/20000], Training Loss: 0.0846\n",
            "Epoch [1591/20000], Training Loss: 0.0827\n",
            "Epoch [1592/20000], Training Loss: 0.0727\n",
            "Epoch [1593/20000], Training Loss: 0.0801\n",
            "Epoch [1594/20000], Training Loss: 0.0850\n",
            "Epoch [1595/20000], Training Loss: 0.0815\n",
            "Epoch [1596/20000], Training Loss: 0.0802\n",
            "Epoch [1597/20000], Training Loss: 0.0763\n",
            "Epoch [1598/20000], Training Loss: 0.0812\n",
            "Epoch [1599/20000], Training Loss: 0.0793\n",
            "Epoch [1600/20000], Training Loss: 0.0834\n",
            "Epoch [1601/20000], Training Loss: 0.0799\n",
            "Epoch [1602/20000], Training Loss: 0.0805\n",
            "Epoch [1603/20000], Training Loss: 0.0781\n",
            "Epoch [1604/20000], Training Loss: 0.0732\n",
            "Epoch [1605/20000], Training Loss: 0.0788\n",
            "Epoch [1606/20000], Training Loss: 0.0803\n",
            "Epoch [1607/20000], Training Loss: 0.0786\n",
            "Epoch [1608/20000], Training Loss: 0.0734\n",
            "Epoch [1609/20000], Training Loss: 0.0784\n",
            "Epoch [1610/20000], Training Loss: 0.0730\n",
            "Epoch [1611/20000], Training Loss: 0.0790\n",
            "Epoch [1612/20000], Training Loss: 0.0776\n",
            "Epoch [1613/20000], Training Loss: 0.0844\n",
            "Epoch [1614/20000], Training Loss: 0.0809\n",
            "Epoch [1615/20000], Training Loss: 0.0844\n",
            "Epoch [1616/20000], Training Loss: 0.0812\n",
            "Epoch [1617/20000], Training Loss: 0.0813\n",
            "Epoch [1618/20000], Training Loss: 0.0797\n",
            "Epoch [1619/20000], Training Loss: 0.0841\n",
            "Epoch [1620/20000], Training Loss: 0.0795\n",
            "Epoch [1621/20000], Training Loss: 0.0758\n",
            "Epoch [1622/20000], Training Loss: 0.0808\n",
            "Epoch [1623/20000], Training Loss: 0.0781\n",
            "Epoch [1624/20000], Training Loss: 0.0805\n",
            "Epoch [1625/20000], Training Loss: 0.0790\n",
            "Epoch [1626/20000], Training Loss: 0.0764\n",
            "Epoch [1627/20000], Training Loss: 0.0810\n",
            "Epoch [1628/20000], Training Loss: 0.0739\n",
            "Epoch [1629/20000], Training Loss: 0.0831\n",
            "Epoch [1630/20000], Training Loss: 0.0793\n",
            "Epoch [1631/20000], Training Loss: 0.0841\n",
            "Epoch [1632/20000], Training Loss: 0.0796\n",
            "Epoch [1633/20000], Training Loss: 0.0832\n",
            "Epoch [1634/20000], Training Loss: 0.0788\n",
            "Epoch [1635/20000], Training Loss: 0.0872\n",
            "Epoch [1636/20000], Training Loss: 0.0827\n",
            "Epoch [1637/20000], Training Loss: 0.0780\n",
            "Epoch [1638/20000], Training Loss: 0.0744\n",
            "Epoch [1639/20000], Training Loss: 0.0806\n",
            "Epoch [1640/20000], Training Loss: 0.0799\n",
            "Epoch [1641/20000], Training Loss: 0.0853\n",
            "Epoch [1642/20000], Training Loss: 0.0880\n",
            "Epoch [1643/20000], Training Loss: 0.0782\n",
            "Epoch [1644/20000], Training Loss: 0.0858\n",
            "Epoch [1645/20000], Training Loss: 0.0736\n",
            "Epoch [1646/20000], Training Loss: 0.0814\n",
            "Epoch [1647/20000], Training Loss: 0.0788\n",
            "Epoch [1648/20000], Training Loss: 0.0747\n",
            "Epoch [1649/20000], Training Loss: 0.0802\n",
            "Epoch [1650/20000], Training Loss: 0.0853\n",
            "Epoch [1651/20000], Training Loss: 0.0778\n",
            "Epoch [1652/20000], Training Loss: 0.0827\n",
            "Epoch [1653/20000], Training Loss: 0.0745\n",
            "Epoch [1654/20000], Training Loss: 0.0778\n",
            "Epoch [1655/20000], Training Loss: 0.0803\n",
            "Epoch [1656/20000], Training Loss: 0.0793\n",
            "Epoch [1657/20000], Training Loss: 0.0809\n",
            "Epoch [1658/20000], Training Loss: 0.0788\n",
            "Epoch [1659/20000], Training Loss: 0.0792\n",
            "Epoch [1660/20000], Training Loss: 0.0818\n",
            "Epoch [1661/20000], Training Loss: 0.0856\n",
            "Epoch [1662/20000], Training Loss: 0.0811\n",
            "Epoch [1663/20000], Training Loss: 0.0771\n",
            "Epoch [1664/20000], Training Loss: 0.0754\n",
            "Epoch [1665/20000], Training Loss: 0.0780\n",
            "Epoch [1666/20000], Training Loss: 0.0828\n",
            "Epoch [1667/20000], Training Loss: 0.0743\n",
            "Epoch [1668/20000], Training Loss: 0.0736\n",
            "Epoch [1669/20000], Training Loss: 0.0785\n",
            "Epoch [1670/20000], Training Loss: 0.0761\n",
            "Epoch [1671/20000], Training Loss: 0.0811\n",
            "Epoch [1672/20000], Training Loss: 0.0745\n",
            "Epoch [1673/20000], Training Loss: 0.0788\n",
            "Epoch [1674/20000], Training Loss: 0.0819\n",
            "Epoch [1675/20000], Training Loss: 0.0763\n",
            "Epoch [1676/20000], Training Loss: 0.0829\n",
            "Epoch [1677/20000], Training Loss: 0.0767\n",
            "Epoch [1678/20000], Training Loss: 0.0796\n",
            "Epoch [1679/20000], Training Loss: 0.0854\n",
            "Epoch [1680/20000], Training Loss: 0.0825\n",
            "Epoch [1681/20000], Training Loss: 0.0742\n",
            "Epoch [1682/20000], Training Loss: 0.0760\n",
            "Epoch [1683/20000], Training Loss: 0.0767\n",
            "Epoch [1684/20000], Training Loss: 0.0784\n",
            "Epoch [1685/20000], Training Loss: 0.0752\n",
            "Epoch [1686/20000], Training Loss: 0.0854\n",
            "Epoch [1687/20000], Training Loss: 0.0826\n",
            "Epoch [1688/20000], Training Loss: 0.0769\n",
            "Epoch [1689/20000], Training Loss: 0.0749\n",
            "Epoch [1690/20000], Training Loss: 0.0809\n",
            "Epoch [1691/20000], Training Loss: 0.0776\n",
            "Epoch [1692/20000], Training Loss: 0.0732\n",
            "Epoch [1693/20000], Training Loss: 0.0767\n",
            "Epoch [1694/20000], Training Loss: 0.0787\n",
            "Epoch [1695/20000], Training Loss: 0.0737\n",
            "Epoch [1696/20000], Training Loss: 0.0824\n",
            "Epoch [1697/20000], Training Loss: 0.0793\n",
            "Epoch [1698/20000], Training Loss: 0.0768\n",
            "Epoch [1699/20000], Training Loss: 0.0784\n",
            "Epoch [1700/20000], Training Loss: 0.0757\n",
            "Epoch [1701/20000], Training Loss: 0.0840\n",
            "Epoch [1702/20000], Training Loss: 0.0806\n",
            "Epoch [1703/20000], Training Loss: 0.0881\n",
            "Epoch [1704/20000], Training Loss: 0.0780\n",
            "Epoch [1705/20000], Training Loss: 0.0788\n",
            "Epoch [1706/20000], Training Loss: 0.0753\n",
            "Epoch [1707/20000], Training Loss: 0.0800\n",
            "Epoch [1708/20000], Training Loss: 0.0762\n",
            "Epoch [1709/20000], Training Loss: 0.0811\n",
            "Epoch [1710/20000], Training Loss: 0.0817\n",
            "Epoch [1711/20000], Training Loss: 0.0778\n",
            "Epoch [1712/20000], Training Loss: 0.0810\n",
            "Epoch [1713/20000], Training Loss: 0.0751\n",
            "Epoch [1714/20000], Training Loss: 0.0776\n",
            "Epoch [1715/20000], Training Loss: 0.0852\n",
            "Epoch [1716/20000], Training Loss: 0.0813\n",
            "Epoch [1717/20000], Training Loss: 0.0805\n",
            "Epoch [1718/20000], Training Loss: 0.0778\n",
            "Epoch [1719/20000], Training Loss: 0.0768\n",
            "Epoch [1720/20000], Training Loss: 0.0812\n",
            "Epoch [1721/20000], Training Loss: 0.0762\n",
            "Epoch [1722/20000], Training Loss: 0.0771\n",
            "Epoch [1723/20000], Training Loss: 0.0817\n",
            "Epoch [1724/20000], Training Loss: 0.0761\n",
            "Epoch [1725/20000], Training Loss: 0.0743\n",
            "Epoch [1726/20000], Training Loss: 0.0734\n",
            "Epoch [1727/20000], Training Loss: 0.0790\n",
            "Epoch [1728/20000], Training Loss: 0.0854\n",
            "Epoch [1729/20000], Training Loss: 0.0777\n",
            "Epoch [1730/20000], Training Loss: 0.0814\n",
            "Epoch [1731/20000], Training Loss: 0.0786\n",
            "Epoch [1732/20000], Training Loss: 0.0807\n",
            "Epoch [1733/20000], Training Loss: 0.0751\n",
            "Epoch [1734/20000], Training Loss: 0.0823\n",
            "Epoch [1735/20000], Training Loss: 0.0824\n",
            "Epoch [1736/20000], Training Loss: 0.0774\n",
            "Epoch [1737/20000], Training Loss: 0.0855\n",
            "Epoch [1738/20000], Training Loss: 0.0810\n",
            "Epoch [1739/20000], Training Loss: 0.0839\n",
            "Epoch [1740/20000], Training Loss: 0.0775\n",
            "Epoch [1741/20000], Training Loss: 0.0745\n",
            "Epoch [1742/20000], Training Loss: 0.0840\n",
            "Epoch [1743/20000], Training Loss: 0.0855\n",
            "Epoch [1744/20000], Training Loss: 0.0846\n",
            "Epoch [1745/20000], Training Loss: 0.0761\n",
            "Epoch [1746/20000], Training Loss: 0.0720\n",
            "Epoch [1747/20000], Training Loss: 0.0823\n",
            "Epoch [1748/20000], Training Loss: 0.0812\n",
            "Epoch [1749/20000], Training Loss: 0.0802\n",
            "Epoch [1750/20000], Training Loss: 0.0828\n",
            "Epoch [1751/20000], Training Loss: 0.0795\n",
            "Epoch [1752/20000], Training Loss: 0.0826\n",
            "Epoch [1753/20000], Training Loss: 0.0777\n",
            "Epoch [1754/20000], Training Loss: 0.0778\n",
            "Epoch [1755/20000], Training Loss: 0.0781\n",
            "Epoch [1756/20000], Training Loss: 0.0821\n",
            "Epoch [1757/20000], Training Loss: 0.0804\n",
            "Epoch [1758/20000], Training Loss: 0.0802\n",
            "Epoch [1759/20000], Training Loss: 0.0853\n",
            "Epoch [1760/20000], Training Loss: 0.0770\n",
            "Epoch [1761/20000], Training Loss: 0.0871\n",
            "Epoch [1762/20000], Training Loss: 0.0751\n",
            "Epoch [1763/20000], Training Loss: 0.0802\n",
            "Epoch [1764/20000], Training Loss: 0.0816\n",
            "Epoch [1765/20000], Training Loss: 0.0751\n",
            "Epoch [1766/20000], Training Loss: 0.0777\n",
            "Epoch [1767/20000], Training Loss: 0.0805\n",
            "Epoch [1768/20000], Training Loss: 0.0812\n",
            "Epoch [1769/20000], Training Loss: 0.0791\n",
            "Epoch [1770/20000], Training Loss: 0.0793\n",
            "Epoch [1771/20000], Training Loss: 0.0838\n",
            "Epoch [1772/20000], Training Loss: 0.0860\n",
            "Epoch [1773/20000], Training Loss: 0.0868\n",
            "Epoch [1774/20000], Training Loss: 0.0798\n",
            "Epoch [1775/20000], Training Loss: 0.0794\n",
            "Epoch [1776/20000], Training Loss: 0.0843\n",
            "Epoch [1777/20000], Training Loss: 0.0847\n",
            "Epoch [1778/20000], Training Loss: 0.0766\n",
            "Epoch [1779/20000], Training Loss: 0.0819\n",
            "Epoch [1780/20000], Training Loss: 0.0813\n",
            "Epoch [1781/20000], Training Loss: 0.0764\n",
            "Epoch [1782/20000], Training Loss: 0.0777\n",
            "Epoch [1783/20000], Training Loss: 0.0805\n",
            "Epoch [1784/20000], Training Loss: 0.0813\n",
            "Epoch [1785/20000], Training Loss: 0.0777\n",
            "Epoch [1786/20000], Training Loss: 0.0800\n",
            "Epoch [1787/20000], Training Loss: 0.0858\n",
            "Epoch [1788/20000], Training Loss: 0.0792\n",
            "Epoch [1789/20000], Training Loss: 0.0802\n",
            "Epoch [1790/20000], Training Loss: 0.0796\n",
            "Epoch [1791/20000], Training Loss: 0.0782\n",
            "Epoch [1792/20000], Training Loss: 0.0761\n",
            "Epoch [1793/20000], Training Loss: 0.0836\n",
            "Epoch [1794/20000], Training Loss: 0.0753\n",
            "Epoch [1795/20000], Training Loss: 0.0823\n",
            "Epoch [1796/20000], Training Loss: 0.0824\n",
            "Epoch [1797/20000], Training Loss: 0.0823\n",
            "Epoch [1798/20000], Training Loss: 0.0782\n",
            "Epoch [1799/20000], Training Loss: 0.0785\n",
            "Epoch [1800/20000], Training Loss: 0.0800\n",
            "Epoch [1801/20000], Training Loss: 0.0774\n",
            "Epoch [1802/20000], Training Loss: 0.0806\n",
            "Epoch [1803/20000], Training Loss: 0.0803\n",
            "Epoch [1804/20000], Training Loss: 0.0832\n",
            "Epoch [1805/20000], Training Loss: 0.0810\n",
            "Epoch [1806/20000], Training Loss: 0.0749\n",
            "Epoch [1807/20000], Training Loss: 0.0803\n",
            "Epoch [1808/20000], Training Loss: 0.0793\n",
            "Epoch [1809/20000], Training Loss: 0.0831\n",
            "Epoch [1810/20000], Training Loss: 0.0780\n",
            "Epoch [1811/20000], Training Loss: 0.0753\n",
            "Epoch [1812/20000], Training Loss: 0.0811\n",
            "Epoch [1813/20000], Training Loss: 0.0771\n",
            "Epoch [1814/20000], Training Loss: 0.0774\n",
            "Epoch [1815/20000], Training Loss: 0.0809\n",
            "Epoch [1816/20000], Training Loss: 0.0766\n",
            "Epoch [1817/20000], Training Loss: 0.0778\n",
            "Epoch [1818/20000], Training Loss: 0.0859\n",
            "Epoch [1819/20000], Training Loss: 0.0797\n",
            "Epoch [1820/20000], Training Loss: 0.0794\n",
            "Epoch [1821/20000], Training Loss: 0.0810\n",
            "Epoch [1822/20000], Training Loss: 0.0775\n",
            "Epoch [1823/20000], Training Loss: 0.0777\n",
            "Epoch [1824/20000], Training Loss: 0.0858\n",
            "Epoch [1825/20000], Training Loss: 0.0808\n",
            "Epoch [1826/20000], Training Loss: 0.0807\n",
            "Epoch [1827/20000], Training Loss: 0.0795\n",
            "Epoch [1828/20000], Training Loss: 0.0786\n",
            "Epoch [1829/20000], Training Loss: 0.0774\n",
            "Epoch [1830/20000], Training Loss: 0.0877\n",
            "Epoch [1831/20000], Training Loss: 0.0828\n",
            "Epoch [1832/20000], Training Loss: 0.0806\n",
            "Epoch [1833/20000], Training Loss: 0.0772\n",
            "Epoch [1834/20000], Training Loss: 0.0815\n",
            "Epoch [1835/20000], Training Loss: 0.0730\n",
            "Epoch [1836/20000], Training Loss: 0.0801\n",
            "Epoch [1837/20000], Training Loss: 0.0847\n",
            "Epoch [1838/20000], Training Loss: 0.0812\n",
            "Epoch [1839/20000], Training Loss: 0.0752\n",
            "Epoch [1840/20000], Training Loss: 0.0760\n",
            "Epoch [1841/20000], Training Loss: 0.0736\n",
            "Epoch [1842/20000], Training Loss: 0.0745\n",
            "Epoch [1843/20000], Training Loss: 0.0746\n",
            "Epoch [1844/20000], Training Loss: 0.0761\n",
            "Epoch [1845/20000], Training Loss: 0.0801\n",
            "Epoch [1846/20000], Training Loss: 0.0807\n",
            "Epoch [1847/20000], Training Loss: 0.0845\n",
            "Epoch [1848/20000], Training Loss: 0.0777\n",
            "Epoch [1849/20000], Training Loss: 0.0801\n",
            "Epoch [1850/20000], Training Loss: 0.0869\n",
            "Epoch [1851/20000], Training Loss: 0.0775\n",
            "Epoch [1852/20000], Training Loss: 0.0794\n",
            "Epoch [1853/20000], Training Loss: 0.0808\n",
            "Epoch [1854/20000], Training Loss: 0.0766\n",
            "Epoch [1855/20000], Training Loss: 0.0787\n",
            "Epoch [1856/20000], Training Loss: 0.0829\n",
            "Epoch [1857/20000], Training Loss: 0.0776\n",
            "Epoch [1858/20000], Training Loss: 0.0809\n",
            "Epoch [1859/20000], Training Loss: 0.0821\n",
            "Epoch [1860/20000], Training Loss: 0.0797\n",
            "Epoch [1861/20000], Training Loss: 0.0778\n",
            "Epoch [1862/20000], Training Loss: 0.0815\n",
            "Epoch [1863/20000], Training Loss: 0.0776\n",
            "Epoch [1864/20000], Training Loss: 0.0758\n",
            "Epoch [1865/20000], Training Loss: 0.0816\n",
            "Epoch [1866/20000], Training Loss: 0.0845\n",
            "Epoch [1867/20000], Training Loss: 0.0855\n",
            "Epoch [1868/20000], Training Loss: 0.0779\n",
            "Epoch [1869/20000], Training Loss: 0.0758\n",
            "Epoch [1870/20000], Training Loss: 0.0783\n",
            "Epoch [1871/20000], Training Loss: 0.0796\n",
            "Epoch [1872/20000], Training Loss: 0.0819\n",
            "Epoch [1873/20000], Training Loss: 0.0838\n",
            "Epoch [1874/20000], Training Loss: 0.0755\n",
            "Epoch [1875/20000], Training Loss: 0.0789\n",
            "Epoch [1876/20000], Training Loss: 0.0812\n",
            "Epoch [1877/20000], Training Loss: 0.0753\n",
            "Epoch [1878/20000], Training Loss: 0.0800\n",
            "Epoch [1879/20000], Training Loss: 0.0800\n",
            "Epoch [1880/20000], Training Loss: 0.0767\n",
            "Epoch [1881/20000], Training Loss: 0.0730\n",
            "Epoch [1882/20000], Training Loss: 0.0802\n",
            "Epoch [1883/20000], Training Loss: 0.0815\n",
            "Epoch [1884/20000], Training Loss: 0.0758\n",
            "Epoch [1885/20000], Training Loss: 0.0754\n",
            "Epoch [1886/20000], Training Loss: 0.0822\n",
            "Epoch [1887/20000], Training Loss: 0.0743\n",
            "Epoch [1888/20000], Training Loss: 0.0751\n",
            "Epoch [1889/20000], Training Loss: 0.0853\n",
            "Epoch [1890/20000], Training Loss: 0.0817\n",
            "Epoch [1891/20000], Training Loss: 0.0833\n",
            "Epoch [1892/20000], Training Loss: 0.0745\n",
            "Epoch [1893/20000], Training Loss: 0.0772\n",
            "Epoch [1894/20000], Training Loss: 0.0824\n",
            "Epoch [1895/20000], Training Loss: 0.0841\n",
            "Epoch [1896/20000], Training Loss: 0.0794\n",
            "Epoch [1897/20000], Training Loss: 0.0811\n",
            "Epoch [1898/20000], Training Loss: 0.0745\n",
            "Epoch [1899/20000], Training Loss: 0.0762\n",
            "Epoch [1900/20000], Training Loss: 0.0726\n",
            "Epoch [1901/20000], Training Loss: 0.0796\n",
            "Epoch [1902/20000], Training Loss: 0.0826\n",
            "Epoch [1903/20000], Training Loss: 0.0743\n",
            "Epoch [1904/20000], Training Loss: 0.0823\n",
            "Epoch [1905/20000], Training Loss: 0.0755\n",
            "Epoch [1906/20000], Training Loss: 0.0852\n",
            "Epoch [1907/20000], Training Loss: 0.0786\n",
            "Epoch [1908/20000], Training Loss: 0.0745\n",
            "Epoch [1909/20000], Training Loss: 0.0789\n",
            "Epoch [1910/20000], Training Loss: 0.0773\n",
            "Epoch [1911/20000], Training Loss: 0.0831\n",
            "Epoch [1912/20000], Training Loss: 0.0798\n",
            "Epoch [1913/20000], Training Loss: 0.0818\n",
            "Epoch [1914/20000], Training Loss: 0.0762\n",
            "Epoch [1915/20000], Training Loss: 0.0814\n",
            "Epoch [1916/20000], Training Loss: 0.0774\n",
            "Epoch [1917/20000], Training Loss: 0.0748\n",
            "Epoch [1918/20000], Training Loss: 0.0817\n",
            "Epoch [1919/20000], Training Loss: 0.0804\n",
            "Epoch [1920/20000], Training Loss: 0.0796\n",
            "Epoch [1921/20000], Training Loss: 0.0852\n",
            "Epoch [1922/20000], Training Loss: 0.0810\n",
            "Epoch [1923/20000], Training Loss: 0.0786\n",
            "Epoch [1924/20000], Training Loss: 0.0758\n",
            "Epoch [1925/20000], Training Loss: 0.0792\n",
            "Epoch [1926/20000], Training Loss: 0.0775\n",
            "Epoch [1927/20000], Training Loss: 0.0837\n",
            "Epoch [1928/20000], Training Loss: 0.0781\n",
            "Epoch [1929/20000], Training Loss: 0.0827\n",
            "Epoch [1930/20000], Training Loss: 0.0799\n",
            "Epoch [1931/20000], Training Loss: 0.0835\n",
            "Epoch [1932/20000], Training Loss: 0.0799\n",
            "Epoch [1933/20000], Training Loss: 0.0873\n",
            "Epoch [1934/20000], Training Loss: 0.0817\n",
            "Epoch [1935/20000], Training Loss: 0.0838\n",
            "Epoch [1936/20000], Training Loss: 0.0770\n",
            "Epoch [1937/20000], Training Loss: 0.0841\n",
            "Epoch [1938/20000], Training Loss: 0.0759\n",
            "Epoch [1939/20000], Training Loss: 0.0768\n",
            "Epoch [1940/20000], Training Loss: 0.0795\n",
            "Epoch [1941/20000], Training Loss: 0.0846\n",
            "Epoch [1942/20000], Training Loss: 0.0794\n",
            "Epoch [1943/20000], Training Loss: 0.0735\n",
            "Epoch [1944/20000], Training Loss: 0.0801\n",
            "Epoch [1945/20000], Training Loss: 0.0745\n",
            "Epoch [1946/20000], Training Loss: 0.0750\n",
            "Epoch [1947/20000], Training Loss: 0.0848\n",
            "Epoch [1948/20000], Training Loss: 0.0850\n",
            "Epoch [1949/20000], Training Loss: 0.0799\n",
            "Epoch [1950/20000], Training Loss: 0.0831\n",
            "Epoch [1951/20000], Training Loss: 0.0741\n",
            "Epoch [1952/20000], Training Loss: 0.0808\n",
            "Epoch [1953/20000], Training Loss: 0.0741\n",
            "Epoch [1954/20000], Training Loss: 0.0849\n",
            "Epoch [1955/20000], Training Loss: 0.0844\n",
            "Epoch [1956/20000], Training Loss: 0.0807\n",
            "Epoch [1957/20000], Training Loss: 0.0806\n",
            "Epoch [1958/20000], Training Loss: 0.0757\n",
            "Epoch [1959/20000], Training Loss: 0.0851\n",
            "Epoch [1960/20000], Training Loss: 0.0801\n",
            "Epoch [1961/20000], Training Loss: 0.0775\n",
            "Epoch [1962/20000], Training Loss: 0.0769\n",
            "Epoch [1963/20000], Training Loss: 0.0795\n",
            "Epoch [1964/20000], Training Loss: 0.0786\n",
            "Epoch [1965/20000], Training Loss: 0.0796\n",
            "Epoch [1966/20000], Training Loss: 0.0737\n",
            "Epoch [1967/20000], Training Loss: 0.0783\n",
            "Epoch [1968/20000], Training Loss: 0.0755\n",
            "Epoch [1969/20000], Training Loss: 0.0786\n",
            "Epoch [1970/20000], Training Loss: 0.0799\n",
            "Epoch [1971/20000], Training Loss: 0.0843\n",
            "Epoch [1972/20000], Training Loss: 0.0777\n",
            "Epoch [1973/20000], Training Loss: 0.0819\n",
            "Epoch [1974/20000], Training Loss: 0.0762\n",
            "Epoch [1975/20000], Training Loss: 0.0787\n",
            "Epoch [1976/20000], Training Loss: 0.0882\n",
            "Epoch [1977/20000], Training Loss: 0.0761\n",
            "Epoch [1978/20000], Training Loss: 0.0794\n",
            "Epoch [1979/20000], Training Loss: 0.0837\n",
            "Epoch [1980/20000], Training Loss: 0.0729\n",
            "Epoch [1981/20000], Training Loss: 0.0745\n",
            "Epoch [1982/20000], Training Loss: 0.0768\n",
            "Epoch [1983/20000], Training Loss: 0.0732\n",
            "Epoch [1984/20000], Training Loss: 0.0783\n",
            "Epoch [1985/20000], Training Loss: 0.0759\n",
            "Epoch [1986/20000], Training Loss: 0.0846\n",
            "Epoch [1987/20000], Training Loss: 0.0796\n",
            "Epoch [1988/20000], Training Loss: 0.0770\n",
            "Epoch [1989/20000], Training Loss: 0.0813\n",
            "Epoch [1990/20000], Training Loss: 0.0830\n",
            "Epoch [1991/20000], Training Loss: 0.0816\n",
            "Epoch [1992/20000], Training Loss: 0.0796\n",
            "Epoch [1993/20000], Training Loss: 0.0793\n",
            "Epoch [1994/20000], Training Loss: 0.0744\n",
            "Epoch [1995/20000], Training Loss: 0.0811\n",
            "Epoch [1996/20000], Training Loss: 0.0830\n",
            "Epoch [1997/20000], Training Loss: 0.0771\n",
            "Epoch [1998/20000], Training Loss: 0.0828\n",
            "Epoch [1999/20000], Training Loss: 0.0764\n",
            "Epoch [2000/20000], Training Loss: 0.0821\n",
            "Epoch [2001/20000], Training Loss: 0.0771\n",
            "Epoch [2002/20000], Training Loss: 0.0825\n",
            "Epoch [2003/20000], Training Loss: 0.0806\n",
            "Epoch [2004/20000], Training Loss: 0.0814\n",
            "Epoch [2005/20000], Training Loss: 0.0750\n",
            "Epoch [2006/20000], Training Loss: 0.0858\n",
            "Epoch [2007/20000], Training Loss: 0.0810\n",
            "Epoch [2008/20000], Training Loss: 0.0818\n",
            "Epoch [2009/20000], Training Loss: 0.0765\n",
            "Epoch [2010/20000], Training Loss: 0.0752\n",
            "Epoch [2011/20000], Training Loss: 0.0796\n",
            "Epoch [2012/20000], Training Loss: 0.0775\n",
            "Epoch [2013/20000], Training Loss: 0.0798\n",
            "Epoch [2014/20000], Training Loss: 0.0780\n",
            "Epoch [2015/20000], Training Loss: 0.0799\n",
            "Epoch [2016/20000], Training Loss: 0.0812\n",
            "Epoch [2017/20000], Training Loss: 0.0849\n",
            "Epoch [2018/20000], Training Loss: 0.0839\n",
            "Epoch [2019/20000], Training Loss: 0.0787\n",
            "Epoch [2020/20000], Training Loss: 0.0758\n",
            "Epoch [2021/20000], Training Loss: 0.0832\n",
            "Epoch [2022/20000], Training Loss: 0.0746\n",
            "Epoch [2023/20000], Training Loss: 0.0790\n",
            "Epoch [2024/20000], Training Loss: 0.0751\n",
            "Epoch [2025/20000], Training Loss: 0.0759\n",
            "Epoch [2026/20000], Training Loss: 0.0799\n",
            "Epoch [2027/20000], Training Loss: 0.0795\n",
            "Epoch [2028/20000], Training Loss: 0.0812\n",
            "Epoch [2029/20000], Training Loss: 0.0825\n",
            "Epoch [2030/20000], Training Loss: 0.0779\n",
            "Epoch [2031/20000], Training Loss: 0.0762\n",
            "Epoch [2032/20000], Training Loss: 0.0756\n",
            "Epoch [2033/20000], Training Loss: 0.0754\n",
            "Epoch [2034/20000], Training Loss: 0.0792\n",
            "Epoch [2035/20000], Training Loss: 0.0757\n",
            "Epoch [2036/20000], Training Loss: 0.0804\n",
            "Epoch [2037/20000], Training Loss: 0.0803\n",
            "Epoch [2038/20000], Training Loss: 0.0762\n",
            "Epoch [2039/20000], Training Loss: 0.0761\n",
            "Epoch [2040/20000], Training Loss: 0.0802\n",
            "Epoch [2041/20000], Training Loss: 0.0790\n",
            "Epoch [2042/20000], Training Loss: 0.0788\n",
            "Epoch [2043/20000], Training Loss: 0.0738\n",
            "Epoch [2044/20000], Training Loss: 0.0857\n",
            "Epoch [2045/20000], Training Loss: 0.0814\n",
            "Epoch [2046/20000], Training Loss: 0.0824\n",
            "Epoch [2047/20000], Training Loss: 0.0818\n",
            "Epoch [2048/20000], Training Loss: 0.0799\n",
            "Epoch [2049/20000], Training Loss: 0.0785\n",
            "Epoch [2050/20000], Training Loss: 0.0761\n",
            "Epoch [2051/20000], Training Loss: 0.0745\n",
            "Epoch [2052/20000], Training Loss: 0.0760\n",
            "Epoch [2053/20000], Training Loss: 0.0793\n",
            "Epoch [2054/20000], Training Loss: 0.0807\n",
            "Epoch [2055/20000], Training Loss: 0.0745\n",
            "Epoch [2056/20000], Training Loss: 0.0765\n",
            "Epoch [2057/20000], Training Loss: 0.0792\n",
            "Epoch [2058/20000], Training Loss: 0.0735\n",
            "Epoch [2059/20000], Training Loss: 0.0755\n",
            "Epoch [2060/20000], Training Loss: 0.0844\n",
            "Epoch [2061/20000], Training Loss: 0.0801\n",
            "Epoch [2062/20000], Training Loss: 0.0765\n",
            "Epoch [2063/20000], Training Loss: 0.0761\n",
            "Epoch [2064/20000], Training Loss: 0.0754\n",
            "Epoch [2065/20000], Training Loss: 0.0794\n",
            "Epoch [2066/20000], Training Loss: 0.0763\n",
            "Epoch [2067/20000], Training Loss: 0.0793\n",
            "Epoch [2068/20000], Training Loss: 0.0818\n",
            "Epoch [2069/20000], Training Loss: 0.0783\n",
            "Epoch [2070/20000], Training Loss: 0.0721\n",
            "Epoch [2071/20000], Training Loss: 0.0856\n",
            "Epoch [2072/20000], Training Loss: 0.0736\n",
            "Epoch [2073/20000], Training Loss: 0.0825\n",
            "Epoch [2074/20000], Training Loss: 0.0814\n",
            "Epoch [2075/20000], Training Loss: 0.0784\n",
            "Epoch [2076/20000], Training Loss: 0.0863\n",
            "Epoch [2077/20000], Training Loss: 0.0841\n",
            "Epoch [2078/20000], Training Loss: 0.0757\n",
            "Epoch [2079/20000], Training Loss: 0.0795\n",
            "Epoch [2080/20000], Training Loss: 0.0784\n",
            "Epoch [2081/20000], Training Loss: 0.0805\n",
            "Epoch [2082/20000], Training Loss: 0.0791\n",
            "Epoch [2083/20000], Training Loss: 0.0818\n",
            "Epoch [2084/20000], Training Loss: 0.0738\n",
            "Epoch [2085/20000], Training Loss: 0.0803\n",
            "Epoch [2086/20000], Training Loss: 0.0811\n",
            "Epoch [2087/20000], Training Loss: 0.0741\n",
            "Epoch [2088/20000], Training Loss: 0.0802\n",
            "Epoch [2089/20000], Training Loss: 0.0760\n",
            "Epoch [2090/20000], Training Loss: 0.0773\n",
            "Epoch [2091/20000], Training Loss: 0.0770\n",
            "Epoch [2092/20000], Training Loss: 0.0795\n",
            "Epoch [2093/20000], Training Loss: 0.0834\n",
            "Epoch [2094/20000], Training Loss: 0.0803\n",
            "Epoch [2095/20000], Training Loss: 0.0823\n",
            "Epoch [2096/20000], Training Loss: 0.0750\n",
            "Epoch [2097/20000], Training Loss: 0.0813\n",
            "Epoch [2098/20000], Training Loss: 0.0769\n",
            "Epoch [2099/20000], Training Loss: 0.0753\n",
            "Epoch [2100/20000], Training Loss: 0.0778\n",
            "Epoch [2101/20000], Training Loss: 0.0860\n",
            "Epoch [2102/20000], Training Loss: 0.0782\n",
            "Epoch [2103/20000], Training Loss: 0.0783\n",
            "Epoch [2104/20000], Training Loss: 0.0787\n",
            "Epoch [2105/20000], Training Loss: 0.0750\n",
            "Epoch [2106/20000], Training Loss: 0.0743\n",
            "Epoch [2107/20000], Training Loss: 0.0843\n",
            "Epoch [2108/20000], Training Loss: 0.0803\n",
            "Epoch [2109/20000], Training Loss: 0.0784\n",
            "Epoch [2110/20000], Training Loss: 0.0839\n",
            "Epoch [2111/20000], Training Loss: 0.0778\n",
            "Epoch [2112/20000], Training Loss: 0.0804\n",
            "Epoch [2113/20000], Training Loss: 0.0770\n",
            "Epoch [2114/20000], Training Loss: 0.0827\n",
            "Epoch [2115/20000], Training Loss: 0.0839\n",
            "Epoch [2116/20000], Training Loss: 0.0834\n",
            "Epoch [2117/20000], Training Loss: 0.0834\n",
            "Epoch [2118/20000], Training Loss: 0.0881\n",
            "Epoch [2119/20000], Training Loss: 0.0762\n",
            "Epoch [2120/20000], Training Loss: 0.0858\n",
            "Epoch [2121/20000], Training Loss: 0.0772\n",
            "Epoch [2122/20000], Training Loss: 0.0812\n",
            "Epoch [2123/20000], Training Loss: 0.0804\n",
            "Epoch [2124/20000], Training Loss: 0.0746\n",
            "Epoch [2125/20000], Training Loss: 0.0816\n",
            "Epoch [2126/20000], Training Loss: 0.0793\n",
            "Epoch [2127/20000], Training Loss: 0.0785\n",
            "Epoch [2128/20000], Training Loss: 0.0834\n",
            "Epoch [2129/20000], Training Loss: 0.0800\n",
            "Epoch [2130/20000], Training Loss: 0.0801\n",
            "Epoch [2131/20000], Training Loss: 0.0826\n",
            "Epoch [2132/20000], Training Loss: 0.0862\n",
            "Epoch [2133/20000], Training Loss: 0.0810\n",
            "Epoch [2134/20000], Training Loss: 0.0855\n",
            "Epoch [2135/20000], Training Loss: 0.0847\n",
            "Epoch [2136/20000], Training Loss: 0.0793\n",
            "Epoch [2137/20000], Training Loss: 0.0812\n",
            "Epoch [2138/20000], Training Loss: 0.0732\n",
            "Epoch [2139/20000], Training Loss: 0.0746\n",
            "Epoch [2140/20000], Training Loss: 0.0814\n",
            "Epoch [2141/20000], Training Loss: 0.0862\n",
            "Epoch [2142/20000], Training Loss: 0.0795\n",
            "Epoch [2143/20000], Training Loss: 0.0816\n",
            "Epoch [2144/20000], Training Loss: 0.0762\n",
            "Epoch [2145/20000], Training Loss: 0.0795\n",
            "Epoch [2146/20000], Training Loss: 0.0807\n",
            "Epoch [2147/20000], Training Loss: 0.0755\n",
            "Epoch [2148/20000], Training Loss: 0.0845\n",
            "Epoch [2149/20000], Training Loss: 0.0860\n",
            "Epoch [2150/20000], Training Loss: 0.0809\n",
            "Epoch [2151/20000], Training Loss: 0.0802\n",
            "Epoch [2152/20000], Training Loss: 0.0838\n",
            "Epoch [2153/20000], Training Loss: 0.0825\n",
            "Epoch [2154/20000], Training Loss: 0.0749\n",
            "Epoch [2155/20000], Training Loss: 0.0792\n",
            "Epoch [2156/20000], Training Loss: 0.0799\n",
            "Epoch [2157/20000], Training Loss: 0.0811\n",
            "Epoch [2158/20000], Training Loss: 0.0770\n",
            "Epoch [2159/20000], Training Loss: 0.0786\n",
            "Epoch [2160/20000], Training Loss: 0.0857\n",
            "Epoch [2161/20000], Training Loss: 0.0859\n",
            "Epoch [2162/20000], Training Loss: 0.0802\n",
            "Epoch [2163/20000], Training Loss: 0.0793\n",
            "Epoch [2164/20000], Training Loss: 0.0769\n",
            "Epoch [2165/20000], Training Loss: 0.0786\n",
            "Epoch [2166/20000], Training Loss: 0.0819\n",
            "Epoch [2167/20000], Training Loss: 0.0801\n",
            "Epoch [2168/20000], Training Loss: 0.0800\n",
            "Epoch [2169/20000], Training Loss: 0.0792\n",
            "Epoch [2170/20000], Training Loss: 0.0735\n",
            "Epoch [2171/20000], Training Loss: 0.0827\n",
            "Epoch [2172/20000], Training Loss: 0.0793\n",
            "Epoch [2173/20000], Training Loss: 0.0804\n",
            "Epoch [2174/20000], Training Loss: 0.0827\n",
            "Epoch [2175/20000], Training Loss: 0.0816\n",
            "Epoch [2176/20000], Training Loss: 0.0811\n",
            "Epoch [2177/20000], Training Loss: 0.0811\n",
            "Epoch [2178/20000], Training Loss: 0.0761\n",
            "Epoch [2179/20000], Training Loss: 0.0820\n",
            "Epoch [2180/20000], Training Loss: 0.0779\n",
            "Epoch [2181/20000], Training Loss: 0.0800\n",
            "Epoch [2182/20000], Training Loss: 0.0747\n",
            "Epoch [2183/20000], Training Loss: 0.0789\n",
            "Epoch [2184/20000], Training Loss: 0.0832\n",
            "Epoch [2185/20000], Training Loss: 0.0788\n",
            "Epoch [2186/20000], Training Loss: 0.0824\n",
            "Epoch [2187/20000], Training Loss: 0.0812\n",
            "Epoch [2188/20000], Training Loss: 0.0784\n",
            "Epoch [2189/20000], Training Loss: 0.0810\n",
            "Epoch [2190/20000], Training Loss: 0.0866\n",
            "Epoch [2191/20000], Training Loss: 0.0769\n",
            "Epoch [2192/20000], Training Loss: 0.0802\n",
            "Epoch [2193/20000], Training Loss: 0.0842\n",
            "Epoch [2194/20000], Training Loss: 0.0806\n",
            "Epoch [2195/20000], Training Loss: 0.0797\n",
            "Epoch [2196/20000], Training Loss: 0.0826\n",
            "Epoch [2197/20000], Training Loss: 0.0806\n",
            "Epoch [2198/20000], Training Loss: 0.0801\n",
            "Epoch [2199/20000], Training Loss: 0.0793\n",
            "Epoch [2200/20000], Training Loss: 0.0858\n",
            "Epoch [2201/20000], Training Loss: 0.0713\n",
            "Epoch [2202/20000], Training Loss: 0.0762\n",
            "Epoch [2203/20000], Training Loss: 0.0765\n",
            "Epoch [2204/20000], Training Loss: 0.0856\n",
            "Epoch [2205/20000], Training Loss: 0.0798\n",
            "Epoch [2206/20000], Training Loss: 0.0793\n",
            "Epoch [2207/20000], Training Loss: 0.0765\n",
            "Epoch [2208/20000], Training Loss: 0.0782\n",
            "Epoch [2209/20000], Training Loss: 0.0807\n",
            "Epoch [2210/20000], Training Loss: 0.0830\n",
            "Epoch [2211/20000], Training Loss: 0.0777\n",
            "Epoch [2212/20000], Training Loss: 0.0814\n",
            "Epoch [2213/20000], Training Loss: 0.0839\n",
            "Epoch [2214/20000], Training Loss: 0.0767\n",
            "Epoch [2215/20000], Training Loss: 0.0793\n",
            "Epoch [2216/20000], Training Loss: 0.0860\n",
            "Epoch [2217/20000], Training Loss: 0.0836\n",
            "Epoch [2218/20000], Training Loss: 0.0735\n",
            "Epoch [2219/20000], Training Loss: 0.0823\n",
            "Epoch [2220/20000], Training Loss: 0.0796\n",
            "Epoch [2221/20000], Training Loss: 0.0796\n",
            "Epoch [2222/20000], Training Loss: 0.0775\n",
            "Epoch [2223/20000], Training Loss: 0.0798\n",
            "Epoch [2224/20000], Training Loss: 0.0867\n",
            "Epoch [2225/20000], Training Loss: 0.0768\n",
            "Epoch [2226/20000], Training Loss: 0.0799\n",
            "Epoch [2227/20000], Training Loss: 0.0819\n",
            "Epoch [2228/20000], Training Loss: 0.0744\n",
            "Epoch [2229/20000], Training Loss: 0.0857\n",
            "Epoch [2230/20000], Training Loss: 0.0768\n",
            "Epoch [2231/20000], Training Loss: 0.0835\n",
            "Epoch [2232/20000], Training Loss: 0.0821\n",
            "Epoch [2233/20000], Training Loss: 0.0782\n",
            "Epoch [2234/20000], Training Loss: 0.0826\n",
            "Epoch [2235/20000], Training Loss: 0.0755\n",
            "Epoch [2236/20000], Training Loss: 0.0870\n",
            "Epoch [2237/20000], Training Loss: 0.0824\n",
            "Epoch [2238/20000], Training Loss: 0.0871\n",
            "Epoch [2239/20000], Training Loss: 0.0766\n",
            "Epoch [2240/20000], Training Loss: 0.0780\n",
            "Epoch [2241/20000], Training Loss: 0.0779\n",
            "Epoch [2242/20000], Training Loss: 0.0797\n",
            "Epoch [2243/20000], Training Loss: 0.0803\n",
            "Epoch [2244/20000], Training Loss: 0.0835\n",
            "Epoch [2245/20000], Training Loss: 0.0781\n",
            "Epoch [2246/20000], Training Loss: 0.0788\n",
            "Epoch [2247/20000], Training Loss: 0.0800\n",
            "Epoch [2248/20000], Training Loss: 0.0777\n",
            "Epoch [2249/20000], Training Loss: 0.0809\n",
            "Epoch [2250/20000], Training Loss: 0.0816\n",
            "Epoch [2251/20000], Training Loss: 0.0738\n",
            "Epoch [2252/20000], Training Loss: 0.0799\n",
            "Epoch [2253/20000], Training Loss: 0.0793\n",
            "Epoch [2254/20000], Training Loss: 0.0803\n",
            "Epoch [2255/20000], Training Loss: 0.0738\n",
            "Epoch [2256/20000], Training Loss: 0.0864\n",
            "Epoch [2257/20000], Training Loss: 0.0784\n",
            "Epoch [2258/20000], Training Loss: 0.0802\n",
            "Epoch [2259/20000], Training Loss: 0.0815\n",
            "Epoch [2260/20000], Training Loss: 0.0783\n",
            "Epoch [2261/20000], Training Loss: 0.0836\n",
            "Epoch [2262/20000], Training Loss: 0.0782\n",
            "Epoch [2263/20000], Training Loss: 0.0806\n",
            "Epoch [2264/20000], Training Loss: 0.0760\n",
            "Epoch [2265/20000], Training Loss: 0.0770\n",
            "Epoch [2266/20000], Training Loss: 0.0852\n",
            "Epoch [2267/20000], Training Loss: 0.0826\n",
            "Epoch [2268/20000], Training Loss: 0.0773\n",
            "Epoch [2269/20000], Training Loss: 0.0777\n",
            "Epoch [2270/20000], Training Loss: 0.0812\n",
            "Epoch [2271/20000], Training Loss: 0.0807\n",
            "Epoch [2272/20000], Training Loss: 0.0878\n",
            "Epoch [2273/20000], Training Loss: 0.0826\n",
            "Epoch [2274/20000], Training Loss: 0.0776\n",
            "Epoch [2275/20000], Training Loss: 0.0744\n",
            "Epoch [2276/20000], Training Loss: 0.0789\n",
            "Epoch [2277/20000], Training Loss: 0.0781\n",
            "Epoch [2278/20000], Training Loss: 0.0778\n",
            "Epoch [2279/20000], Training Loss: 0.0781\n",
            "Epoch [2280/20000], Training Loss: 0.0789\n",
            "Epoch [2281/20000], Training Loss: 0.0835\n",
            "Epoch [2282/20000], Training Loss: 0.0782\n",
            "Epoch [2283/20000], Training Loss: 0.0755\n",
            "Epoch [2284/20000], Training Loss: 0.0778\n",
            "Epoch [2285/20000], Training Loss: 0.0808\n",
            "Epoch [2286/20000], Training Loss: 0.0750\n",
            "Epoch [2287/20000], Training Loss: 0.0789\n",
            "Epoch [2288/20000], Training Loss: 0.0801\n",
            "Epoch [2289/20000], Training Loss: 0.0774\n",
            "Epoch [2290/20000], Training Loss: 0.0834\n",
            "Epoch [2291/20000], Training Loss: 0.0817\n",
            "Epoch [2292/20000], Training Loss: 0.0835\n",
            "Epoch [2293/20000], Training Loss: 0.0826\n",
            "Epoch [2294/20000], Training Loss: 0.0777\n",
            "Epoch [2295/20000], Training Loss: 0.0762\n",
            "Epoch [2296/20000], Training Loss: 0.0792\n",
            "Epoch [2297/20000], Training Loss: 0.0827\n",
            "Epoch [2298/20000], Training Loss: 0.0833\n",
            "Epoch [2299/20000], Training Loss: 0.0864\n",
            "Epoch [2300/20000], Training Loss: 0.0857\n",
            "Epoch [2301/20000], Training Loss: 0.0770\n",
            "Epoch [2302/20000], Training Loss: 0.0859\n",
            "Epoch [2303/20000], Training Loss: 0.0750\n",
            "Epoch [2304/20000], Training Loss: 0.0833\n",
            "Epoch [2305/20000], Training Loss: 0.0776\n",
            "Epoch [2306/20000], Training Loss: 0.0807\n",
            "Epoch [2307/20000], Training Loss: 0.0759\n",
            "Epoch [2308/20000], Training Loss: 0.0777\n",
            "Epoch [2309/20000], Training Loss: 0.0815\n",
            "Epoch [2310/20000], Training Loss: 0.0808\n",
            "Epoch [2311/20000], Training Loss: 0.0782\n",
            "Epoch [2312/20000], Training Loss: 0.0800\n",
            "Epoch [2313/20000], Training Loss: 0.0802\n",
            "Epoch [2314/20000], Training Loss: 0.0760\n",
            "Epoch [2315/20000], Training Loss: 0.0769\n",
            "Epoch [2316/20000], Training Loss: 0.0780\n",
            "Epoch [2317/20000], Training Loss: 0.0851\n",
            "Epoch [2318/20000], Training Loss: 0.0755\n",
            "Epoch [2319/20000], Training Loss: 0.0811\n",
            "Epoch [2320/20000], Training Loss: 0.0862\n",
            "Epoch [2321/20000], Training Loss: 0.0760\n",
            "Epoch [2322/20000], Training Loss: 0.0772\n",
            "Epoch [2323/20000], Training Loss: 0.0855\n",
            "Epoch [2324/20000], Training Loss: 0.0723\n",
            "Epoch [2325/20000], Training Loss: 0.0805\n",
            "Epoch [2326/20000], Training Loss: 0.0744\n",
            "Epoch [2327/20000], Training Loss: 0.0750\n",
            "Epoch [2328/20000], Training Loss: 0.0795\n",
            "Epoch [2329/20000], Training Loss: 0.0749\n",
            "Epoch [2330/20000], Training Loss: 0.0800\n",
            "Epoch [2331/20000], Training Loss: 0.0742\n",
            "Epoch [2332/20000], Training Loss: 0.0791\n",
            "Epoch [2333/20000], Training Loss: 0.0826\n",
            "Epoch [2334/20000], Training Loss: 0.0790\n",
            "Epoch [2335/20000], Training Loss: 0.0774\n",
            "Epoch [2336/20000], Training Loss: 0.0722\n",
            "Epoch [2337/20000], Training Loss: 0.0844\n",
            "Epoch [2338/20000], Training Loss: 0.0798\n",
            "Epoch [2339/20000], Training Loss: 0.0759\n",
            "Epoch [2340/20000], Training Loss: 0.0776\n",
            "Epoch [2341/20000], Training Loss: 0.0787\n",
            "Epoch [2342/20000], Training Loss: 0.0807\n",
            "Epoch [2343/20000], Training Loss: 0.0815\n",
            "Epoch [2344/20000], Training Loss: 0.0752\n",
            "Epoch [2345/20000], Training Loss: 0.0839\n",
            "Epoch [2346/20000], Training Loss: 0.0791\n",
            "Epoch [2347/20000], Training Loss: 0.0754\n",
            "Epoch [2348/20000], Training Loss: 0.0809\n",
            "Epoch [2349/20000], Training Loss: 0.0825\n",
            "Epoch [2350/20000], Training Loss: 0.0837\n",
            "Epoch [2351/20000], Training Loss: 0.0827\n",
            "Epoch [2352/20000], Training Loss: 0.0823\n",
            "Epoch [2353/20000], Training Loss: 0.0719\n",
            "Epoch [2354/20000], Training Loss: 0.0816\n",
            "Epoch [2355/20000], Training Loss: 0.0769\n",
            "Epoch [2356/20000], Training Loss: 0.0833\n",
            "Epoch [2357/20000], Training Loss: 0.0766\n",
            "Epoch [2358/20000], Training Loss: 0.0813\n",
            "Epoch [2359/20000], Training Loss: 0.0756\n",
            "Epoch [2360/20000], Training Loss: 0.0811\n",
            "Epoch [2361/20000], Training Loss: 0.0782\n",
            "Epoch [2362/20000], Training Loss: 0.0796\n",
            "Epoch [2363/20000], Training Loss: 0.0777\n",
            "Epoch [2364/20000], Training Loss: 0.0840\n",
            "Epoch [2365/20000], Training Loss: 0.0805\n",
            "Epoch [2366/20000], Training Loss: 0.0794\n",
            "Epoch [2367/20000], Training Loss: 0.0784\n",
            "Epoch [2368/20000], Training Loss: 0.0852\n",
            "Epoch [2369/20000], Training Loss: 0.0766\n",
            "Epoch [2370/20000], Training Loss: 0.0758\n",
            "Epoch [2371/20000], Training Loss: 0.0820\n",
            "Epoch [2372/20000], Training Loss: 0.0745\n",
            "Epoch [2373/20000], Training Loss: 0.0781\n",
            "Epoch [2374/20000], Training Loss: 0.0828\n",
            "Epoch [2375/20000], Training Loss: 0.0735\n",
            "Epoch [2376/20000], Training Loss: 0.0737\n",
            "Epoch [2377/20000], Training Loss: 0.0819\n",
            "Epoch [2378/20000], Training Loss: 0.0839\n",
            "Epoch [2379/20000], Training Loss: 0.0775\n",
            "Epoch [2380/20000], Training Loss: 0.0781\n",
            "Epoch [2381/20000], Training Loss: 0.0763\n",
            "Epoch [2382/20000], Training Loss: 0.0790\n",
            "Epoch [2383/20000], Training Loss: 0.0765\n",
            "Epoch [2384/20000], Training Loss: 0.0768\n",
            "Epoch [2385/20000], Training Loss: 0.0771\n",
            "Epoch [2386/20000], Training Loss: 0.0773\n",
            "Epoch [2387/20000], Training Loss: 0.0817\n",
            "Epoch [2388/20000], Training Loss: 0.0827\n",
            "Epoch [2389/20000], Training Loss: 0.0753\n",
            "Epoch [2390/20000], Training Loss: 0.0845\n",
            "Epoch [2391/20000], Training Loss: 0.0817\n",
            "Epoch [2392/20000], Training Loss: 0.0811\n",
            "Epoch [2393/20000], Training Loss: 0.0758\n",
            "Epoch [2394/20000], Training Loss: 0.0797\n",
            "Epoch [2395/20000], Training Loss: 0.0746\n",
            "Epoch [2396/20000], Training Loss: 0.0796\n",
            "Epoch [2397/20000], Training Loss: 0.0797\n",
            "Epoch [2398/20000], Training Loss: 0.0812\n",
            "Epoch [2399/20000], Training Loss: 0.0802\n",
            "Epoch [2400/20000], Training Loss: 0.0746\n",
            "Epoch [2401/20000], Training Loss: 0.0759\n",
            "Epoch [2402/20000], Training Loss: 0.0781\n",
            "Epoch [2403/20000], Training Loss: 0.0861\n",
            "Epoch [2404/20000], Training Loss: 0.0784\n",
            "Epoch [2405/20000], Training Loss: 0.0841\n",
            "Epoch [2406/20000], Training Loss: 0.0810\n",
            "Epoch [2407/20000], Training Loss: 0.0747\n",
            "Epoch [2408/20000], Training Loss: 0.0760\n",
            "Epoch [2409/20000], Training Loss: 0.0752\n",
            "Epoch [2410/20000], Training Loss: 0.0810\n",
            "Epoch [2411/20000], Training Loss: 0.0768\n",
            "Epoch [2412/20000], Training Loss: 0.0802\n",
            "Epoch [2413/20000], Training Loss: 0.0836\n",
            "Epoch [2414/20000], Training Loss: 0.0766\n",
            "Epoch [2415/20000], Training Loss: 0.0778\n",
            "Epoch [2416/20000], Training Loss: 0.0800\n",
            "Epoch [2417/20000], Training Loss: 0.0758\n",
            "Epoch [2418/20000], Training Loss: 0.0837\n",
            "Epoch [2419/20000], Training Loss: 0.0773\n",
            "Epoch [2420/20000], Training Loss: 0.0828\n",
            "Epoch [2421/20000], Training Loss: 0.0861\n",
            "Epoch [2422/20000], Training Loss: 0.0820\n",
            "Epoch [2423/20000], Training Loss: 0.0817\n",
            "Epoch [2424/20000], Training Loss: 0.0831\n",
            "Epoch [2425/20000], Training Loss: 0.0864\n",
            "Epoch [2426/20000], Training Loss: 0.0810\n",
            "Epoch [2427/20000], Training Loss: 0.0810\n",
            "Epoch [2428/20000], Training Loss: 0.0812\n",
            "Epoch [2429/20000], Training Loss: 0.0793\n",
            "Epoch [2430/20000], Training Loss: 0.0810\n",
            "Epoch [2431/20000], Training Loss: 0.0789\n",
            "Epoch [2432/20000], Training Loss: 0.0859\n",
            "Epoch [2433/20000], Training Loss: 0.0799\n",
            "Epoch [2434/20000], Training Loss: 0.0836\n",
            "Epoch [2435/20000], Training Loss: 0.0784\n",
            "Epoch [2436/20000], Training Loss: 0.0799\n",
            "Epoch [2437/20000], Training Loss: 0.0839\n",
            "Epoch [2438/20000], Training Loss: 0.0806\n",
            "Epoch [2439/20000], Training Loss: 0.0797\n",
            "Epoch [2440/20000], Training Loss: 0.0764\n",
            "Epoch [2441/20000], Training Loss: 0.0748\n",
            "Epoch [2442/20000], Training Loss: 0.0771\n",
            "Epoch [2443/20000], Training Loss: 0.0769\n",
            "Epoch [2444/20000], Training Loss: 0.0818\n",
            "Epoch [2445/20000], Training Loss: 0.0780\n",
            "Epoch [2446/20000], Training Loss: 0.0737\n",
            "Epoch [2447/20000], Training Loss: 0.0785\n",
            "Epoch [2448/20000], Training Loss: 0.0816\n",
            "Epoch [2449/20000], Training Loss: 0.0838\n",
            "Epoch [2450/20000], Training Loss: 0.0752\n",
            "Epoch [2451/20000], Training Loss: 0.0785\n",
            "Epoch [2452/20000], Training Loss: 0.0811\n",
            "Epoch [2453/20000], Training Loss: 0.0820\n",
            "Epoch [2454/20000], Training Loss: 0.0815\n",
            "Epoch [2455/20000], Training Loss: 0.0759\n",
            "Epoch [2456/20000], Training Loss: 0.0799\n",
            "Epoch [2457/20000], Training Loss: 0.0844\n",
            "Epoch [2458/20000], Training Loss: 0.0806\n",
            "Epoch [2459/20000], Training Loss: 0.0821\n",
            "Epoch [2460/20000], Training Loss: 0.0742\n",
            "Epoch [2461/20000], Training Loss: 0.0792\n",
            "Epoch [2462/20000], Training Loss: 0.0790\n",
            "Epoch [2463/20000], Training Loss: 0.0842\n",
            "Epoch [2464/20000], Training Loss: 0.0811\n",
            "Epoch [2465/20000], Training Loss: 0.0861\n",
            "Epoch [2466/20000], Training Loss: 0.0784\n",
            "Epoch [2467/20000], Training Loss: 0.0747\n",
            "Epoch [2468/20000], Training Loss: 0.0770\n",
            "Epoch [2469/20000], Training Loss: 0.0878\n",
            "Epoch [2470/20000], Training Loss: 0.0786\n",
            "Epoch [2471/20000], Training Loss: 0.0787\n",
            "Epoch [2472/20000], Training Loss: 0.0746\n",
            "Epoch [2473/20000], Training Loss: 0.0876\n",
            "Epoch [2474/20000], Training Loss: 0.0800\n",
            "Epoch [2475/20000], Training Loss: 0.0830\n",
            "Epoch [2476/20000], Training Loss: 0.0785\n",
            "Epoch [2477/20000], Training Loss: 0.0740\n",
            "Epoch [2478/20000], Training Loss: 0.0860\n",
            "Epoch [2479/20000], Training Loss: 0.0856\n",
            "Epoch [2480/20000], Training Loss: 0.0826\n",
            "Epoch [2481/20000], Training Loss: 0.0799\n",
            "Epoch [2482/20000], Training Loss: 0.0797\n",
            "Epoch [2483/20000], Training Loss: 0.0849\n",
            "Epoch [2484/20000], Training Loss: 0.0786\n",
            "Epoch [2485/20000], Training Loss: 0.0748\n",
            "Epoch [2486/20000], Training Loss: 0.0826\n",
            "Epoch [2487/20000], Training Loss: 0.0806\n",
            "Epoch [2488/20000], Training Loss: 0.0748\n",
            "Epoch [2489/20000], Training Loss: 0.0848\n",
            "Epoch [2490/20000], Training Loss: 0.0755\n",
            "Epoch [2491/20000], Training Loss: 0.0797\n",
            "Epoch [2492/20000], Training Loss: 0.0795\n",
            "Epoch [2493/20000], Training Loss: 0.0714\n",
            "Epoch [2494/20000], Training Loss: 0.0785\n",
            "Epoch [2495/20000], Training Loss: 0.0815\n",
            "Epoch [2496/20000], Training Loss: 0.0775\n",
            "Epoch [2497/20000], Training Loss: 0.0802\n",
            "Epoch [2498/20000], Training Loss: 0.0796\n",
            "Epoch [2499/20000], Training Loss: 0.0755\n",
            "Epoch [2500/20000], Training Loss: 0.0808\n",
            "Epoch [2501/20000], Training Loss: 0.0860\n",
            "Epoch [2502/20000], Training Loss: 0.0862\n",
            "Epoch [2503/20000], Training Loss: 0.0808\n",
            "Epoch [2504/20000], Training Loss: 0.0838\n",
            "Epoch [2505/20000], Training Loss: 0.0810\n",
            "Epoch [2506/20000], Training Loss: 0.0864\n",
            "Epoch [2507/20000], Training Loss: 0.0807\n",
            "Epoch [2508/20000], Training Loss: 0.0767\n",
            "Epoch [2509/20000], Training Loss: 0.0743\n",
            "Epoch [2510/20000], Training Loss: 0.0861\n",
            "Epoch [2511/20000], Training Loss: 0.0817\n",
            "Epoch [2512/20000], Training Loss: 0.0798\n",
            "Epoch [2513/20000], Training Loss: 0.0797\n",
            "Epoch [2514/20000], Training Loss: 0.0778\n",
            "Epoch [2515/20000], Training Loss: 0.0787\n",
            "Epoch [2516/20000], Training Loss: 0.0834\n",
            "Epoch [2517/20000], Training Loss: 0.0854\n",
            "Epoch [2518/20000], Training Loss: 0.0758\n",
            "Epoch [2519/20000], Training Loss: 0.0799\n",
            "Epoch [2520/20000], Training Loss: 0.0815\n",
            "Epoch [2521/20000], Training Loss: 0.0819\n",
            "Epoch [2522/20000], Training Loss: 0.0760\n",
            "Epoch [2523/20000], Training Loss: 0.0752\n",
            "Epoch [2524/20000], Training Loss: 0.0809\n",
            "Epoch [2525/20000], Training Loss: 0.0790\n",
            "Epoch [2526/20000], Training Loss: 0.0810\n",
            "Epoch [2527/20000], Training Loss: 0.0765\n",
            "Epoch [2528/20000], Training Loss: 0.0790\n",
            "Epoch [2529/20000], Training Loss: 0.0793\n",
            "Epoch [2530/20000], Training Loss: 0.0799\n",
            "Epoch [2531/20000], Training Loss: 0.0795\n",
            "Epoch [2532/20000], Training Loss: 0.0737\n",
            "Epoch [2533/20000], Training Loss: 0.0784\n",
            "Epoch [2534/20000], Training Loss: 0.0768\n",
            "Epoch [2535/20000], Training Loss: 0.0756\n",
            "Epoch [2536/20000], Training Loss: 0.0839\n",
            "Epoch [2537/20000], Training Loss: 0.0761\n",
            "Epoch [2538/20000], Training Loss: 0.0748\n",
            "Epoch [2539/20000], Training Loss: 0.0749\n",
            "Epoch [2540/20000], Training Loss: 0.0797\n",
            "Epoch [2541/20000], Training Loss: 0.0759\n",
            "Epoch [2542/20000], Training Loss: 0.0755\n",
            "Epoch [2543/20000], Training Loss: 0.0817\n",
            "Epoch [2544/20000], Training Loss: 0.0759\n",
            "Epoch [2545/20000], Training Loss: 0.0800\n",
            "Epoch [2546/20000], Training Loss: 0.0818\n",
            "Epoch [2547/20000], Training Loss: 0.0759\n",
            "Epoch [2548/20000], Training Loss: 0.0760\n",
            "Epoch [2549/20000], Training Loss: 0.0770\n",
            "Epoch [2550/20000], Training Loss: 0.0759\n",
            "Epoch [2551/20000], Training Loss: 0.0846\n",
            "Epoch [2552/20000], Training Loss: 0.0822\n",
            "Epoch [2553/20000], Training Loss: 0.0824\n",
            "Epoch [2554/20000], Training Loss: 0.0816\n",
            "Epoch [2555/20000], Training Loss: 0.0764\n",
            "Epoch [2556/20000], Training Loss: 0.0782\n",
            "Epoch [2557/20000], Training Loss: 0.0837\n",
            "Epoch [2558/20000], Training Loss: 0.0817\n",
            "Epoch [2559/20000], Training Loss: 0.0823\n",
            "Epoch [2560/20000], Training Loss: 0.0841\n",
            "Epoch [2561/20000], Training Loss: 0.0769\n",
            "Epoch [2562/20000], Training Loss: 0.0833\n",
            "Epoch [2563/20000], Training Loss: 0.0776\n",
            "Epoch [2564/20000], Training Loss: 0.0801\n",
            "Epoch [2565/20000], Training Loss: 0.0778\n",
            "Epoch [2566/20000], Training Loss: 0.0823\n",
            "Epoch [2567/20000], Training Loss: 0.0849\n",
            "Epoch [2568/20000], Training Loss: 0.0786\n",
            "Epoch [2569/20000], Training Loss: 0.0792\n",
            "Epoch [2570/20000], Training Loss: 0.0834\n",
            "Epoch [2571/20000], Training Loss: 0.0767\n",
            "Epoch [2572/20000], Training Loss: 0.0737\n",
            "Epoch [2573/20000], Training Loss: 0.0784\n",
            "Epoch [2574/20000], Training Loss: 0.0850\n",
            "Epoch [2575/20000], Training Loss: 0.0769\n",
            "Epoch [2576/20000], Training Loss: 0.0792\n",
            "Epoch [2577/20000], Training Loss: 0.0815\n",
            "Epoch [2578/20000], Training Loss: 0.0787\n",
            "Epoch [2579/20000], Training Loss: 0.0795\n",
            "Epoch [2580/20000], Training Loss: 0.0752\n",
            "Epoch [2581/20000], Training Loss: 0.0764\n",
            "Epoch [2582/20000], Training Loss: 0.0809\n",
            "Epoch [2583/20000], Training Loss: 0.0740\n",
            "Epoch [2584/20000], Training Loss: 0.0779\n",
            "Epoch [2585/20000], Training Loss: 0.0806\n",
            "Epoch [2586/20000], Training Loss: 0.0788\n",
            "Epoch [2587/20000], Training Loss: 0.0810\n",
            "Epoch [2588/20000], Training Loss: 0.0808\n",
            "Epoch [2589/20000], Training Loss: 0.0789\n",
            "Epoch [2590/20000], Training Loss: 0.0767\n",
            "Epoch [2591/20000], Training Loss: 0.0841\n",
            "Epoch [2592/20000], Training Loss: 0.0778\n",
            "Epoch [2593/20000], Training Loss: 0.0862\n",
            "Epoch [2594/20000], Training Loss: 0.0789\n",
            "Epoch [2595/20000], Training Loss: 0.0767\n",
            "Epoch [2596/20000], Training Loss: 0.0816\n",
            "Epoch [2597/20000], Training Loss: 0.0731\n",
            "Epoch [2598/20000], Training Loss: 0.0780\n",
            "Epoch [2599/20000], Training Loss: 0.0857\n",
            "Epoch [2600/20000], Training Loss: 0.0837\n",
            "Epoch [2601/20000], Training Loss: 0.0769\n",
            "Epoch [2602/20000], Training Loss: 0.0799\n",
            "Epoch [2603/20000], Training Loss: 0.0754\n",
            "Epoch [2604/20000], Training Loss: 0.0863\n",
            "Epoch [2605/20000], Training Loss: 0.0822\n",
            "Epoch [2606/20000], Training Loss: 0.0772\n",
            "Epoch [2607/20000], Training Loss: 0.0787\n",
            "Epoch [2608/20000], Training Loss: 0.0808\n",
            "Epoch [2609/20000], Training Loss: 0.0824\n",
            "Epoch [2610/20000], Training Loss: 0.0792\n",
            "Epoch [2611/20000], Training Loss: 0.0773\n",
            "Epoch [2612/20000], Training Loss: 0.0776\n",
            "Epoch [2613/20000], Training Loss: 0.0844\n",
            "Epoch [2614/20000], Training Loss: 0.0780\n",
            "Epoch [2615/20000], Training Loss: 0.0811\n",
            "Epoch [2616/20000], Training Loss: 0.0774\n",
            "Epoch [2617/20000], Training Loss: 0.0801\n",
            "Epoch [2618/20000], Training Loss: 0.0812\n",
            "Epoch [2619/20000], Training Loss: 0.0810\n",
            "Epoch [2620/20000], Training Loss: 0.0868\n",
            "Epoch [2621/20000], Training Loss: 0.0823\n",
            "Epoch [2622/20000], Training Loss: 0.0813\n",
            "Epoch [2623/20000], Training Loss: 0.0884\n",
            "Epoch [2624/20000], Training Loss: 0.0759\n",
            "Epoch [2625/20000], Training Loss: 0.0811\n",
            "Epoch [2626/20000], Training Loss: 0.0763\n",
            "Epoch [2627/20000], Training Loss: 0.0835\n",
            "Epoch [2628/20000], Training Loss: 0.0784\n",
            "Epoch [2629/20000], Training Loss: 0.0848\n",
            "Epoch [2630/20000], Training Loss: 0.0761\n",
            "Epoch [2631/20000], Training Loss: 0.0807\n",
            "Epoch [2632/20000], Training Loss: 0.0806\n",
            "Epoch [2633/20000], Training Loss: 0.0768\n",
            "Epoch [2634/20000], Training Loss: 0.0748\n",
            "Epoch [2635/20000], Training Loss: 0.0851\n",
            "Epoch [2636/20000], Training Loss: 0.0799\n",
            "Epoch [2637/20000], Training Loss: 0.0761\n",
            "Epoch [2638/20000], Training Loss: 0.0790\n",
            "Epoch [2639/20000], Training Loss: 0.0827\n",
            "Epoch [2640/20000], Training Loss: 0.0805\n",
            "Epoch [2641/20000], Training Loss: 0.0830\n",
            "Epoch [2642/20000], Training Loss: 0.0802\n",
            "Epoch [2643/20000], Training Loss: 0.0796\n",
            "Epoch [2644/20000], Training Loss: 0.0736\n",
            "Epoch [2645/20000], Training Loss: 0.0810\n",
            "Epoch [2646/20000], Training Loss: 0.0805\n",
            "Epoch [2647/20000], Training Loss: 0.0826\n",
            "Epoch [2648/20000], Training Loss: 0.0749\n",
            "Epoch [2649/20000], Training Loss: 0.0827\n",
            "Epoch [2650/20000], Training Loss: 0.0769\n",
            "Epoch [2651/20000], Training Loss: 0.0809\n",
            "Epoch [2652/20000], Training Loss: 0.0779\n",
            "Epoch [2653/20000], Training Loss: 0.0816\n",
            "Epoch [2654/20000], Training Loss: 0.0811\n",
            "Epoch [2655/20000], Training Loss: 0.0780\n",
            "Epoch [2656/20000], Training Loss: 0.0776\n",
            "Epoch [2657/20000], Training Loss: 0.0797\n",
            "Epoch [2658/20000], Training Loss: 0.0819\n",
            "Epoch [2659/20000], Training Loss: 0.0779\n",
            "Epoch [2660/20000], Training Loss: 0.0835\n",
            "Epoch [2661/20000], Training Loss: 0.0762\n",
            "Epoch [2662/20000], Training Loss: 0.0826\n",
            "Epoch [2663/20000], Training Loss: 0.0770\n",
            "Epoch [2664/20000], Training Loss: 0.0756\n",
            "Epoch [2665/20000], Training Loss: 0.0763\n",
            "Epoch [2666/20000], Training Loss: 0.0748\n",
            "Epoch [2667/20000], Training Loss: 0.0732\n",
            "Epoch [2668/20000], Training Loss: 0.0813\n",
            "Epoch [2669/20000], Training Loss: 0.0825\n",
            "Epoch [2670/20000], Training Loss: 0.0800\n",
            "Epoch [2671/20000], Training Loss: 0.0791\n",
            "Epoch [2672/20000], Training Loss: 0.0834\n",
            "Epoch [2673/20000], Training Loss: 0.0811\n",
            "Epoch [2674/20000], Training Loss: 0.0790\n",
            "Epoch [2675/20000], Training Loss: 0.0821\n",
            "Epoch [2676/20000], Training Loss: 0.0721\n",
            "Epoch [2677/20000], Training Loss: 0.0808\n",
            "Epoch [2678/20000], Training Loss: 0.0801\n",
            "Epoch [2679/20000], Training Loss: 0.0808\n",
            "Epoch [2680/20000], Training Loss: 0.0794\n",
            "Epoch [2681/20000], Training Loss: 0.0781\n",
            "Epoch [2682/20000], Training Loss: 0.0784\n",
            "Epoch [2683/20000], Training Loss: 0.0785\n",
            "Epoch [2684/20000], Training Loss: 0.0812\n",
            "Epoch [2685/20000], Training Loss: 0.0873\n",
            "Epoch [2686/20000], Training Loss: 0.0814\n",
            "Epoch [2687/20000], Training Loss: 0.0767\n",
            "Epoch [2688/20000], Training Loss: 0.0842\n",
            "Epoch [2689/20000], Training Loss: 0.0813\n",
            "Epoch [2690/20000], Training Loss: 0.0759\n",
            "Epoch [2691/20000], Training Loss: 0.0792\n",
            "Epoch [2692/20000], Training Loss: 0.0782\n",
            "Epoch [2693/20000], Training Loss: 0.0783\n",
            "Epoch [2694/20000], Training Loss: 0.0739\n",
            "Epoch [2695/20000], Training Loss: 0.0856\n",
            "Epoch [2696/20000], Training Loss: 0.0828\n",
            "Epoch [2697/20000], Training Loss: 0.0725\n",
            "Epoch [2698/20000], Training Loss: 0.0811\n",
            "Epoch [2699/20000], Training Loss: 0.0769\n",
            "Epoch [2700/20000], Training Loss: 0.0785\n",
            "Epoch [2701/20000], Training Loss: 0.0786\n",
            "Epoch [2702/20000], Training Loss: 0.0777\n",
            "Epoch [2703/20000], Training Loss: 0.0818\n",
            "Epoch [2704/20000], Training Loss: 0.0773\n",
            "Epoch [2705/20000], Training Loss: 0.0787\n",
            "Epoch [2706/20000], Training Loss: 0.0823\n",
            "Epoch [2707/20000], Training Loss: 0.0782\n",
            "Epoch [2708/20000], Training Loss: 0.0788\n",
            "Epoch [2709/20000], Training Loss: 0.0823\n",
            "Epoch [2710/20000], Training Loss: 0.0800\n",
            "Epoch [2711/20000], Training Loss: 0.0810\n",
            "Epoch [2712/20000], Training Loss: 0.0759\n",
            "Epoch [2713/20000], Training Loss: 0.0819\n",
            "Epoch [2714/20000], Training Loss: 0.0814\n",
            "Epoch [2715/20000], Training Loss: 0.0790\n",
            "Epoch [2716/20000], Training Loss: 0.0810\n",
            "Epoch [2717/20000], Training Loss: 0.0769\n",
            "Epoch [2718/20000], Training Loss: 0.0743\n",
            "Epoch [2719/20000], Training Loss: 0.0765\n",
            "Epoch [2720/20000], Training Loss: 0.0746\n",
            "Epoch [2721/20000], Training Loss: 0.0752\n",
            "Epoch [2722/20000], Training Loss: 0.0840\n",
            "Epoch [2723/20000], Training Loss: 0.0837\n",
            "Epoch [2724/20000], Training Loss: 0.0789\n",
            "Epoch [2725/20000], Training Loss: 0.0761\n",
            "Epoch [2726/20000], Training Loss: 0.0761\n",
            "Epoch [2727/20000], Training Loss: 0.0833\n",
            "Epoch [2728/20000], Training Loss: 0.0758\n",
            "Epoch [2729/20000], Training Loss: 0.0821\n",
            "Epoch [2730/20000], Training Loss: 0.0804\n",
            "Epoch [2731/20000], Training Loss: 0.0838\n",
            "Epoch [2732/20000], Training Loss: 0.0865\n",
            "Epoch [2733/20000], Training Loss: 0.0819\n",
            "Epoch [2734/20000], Training Loss: 0.0781\n",
            "Epoch [2735/20000], Training Loss: 0.0751\n",
            "Epoch [2736/20000], Training Loss: 0.0860\n",
            "Epoch [2737/20000], Training Loss: 0.0810\n",
            "Epoch [2738/20000], Training Loss: 0.0852\n",
            "Epoch [2739/20000], Training Loss: 0.0740\n",
            "Epoch [2740/20000], Training Loss: 0.0719\n",
            "Epoch [2741/20000], Training Loss: 0.0796\n",
            "Epoch [2742/20000], Training Loss: 0.0764\n",
            "Epoch [2743/20000], Training Loss: 0.0840\n",
            "Epoch [2744/20000], Training Loss: 0.0809\n",
            "Epoch [2745/20000], Training Loss: 0.0761\n",
            "Epoch [2746/20000], Training Loss: 0.0798\n",
            "Epoch [2747/20000], Training Loss: 0.0795\n",
            "Epoch [2748/20000], Training Loss: 0.0776\n",
            "Epoch [2749/20000], Training Loss: 0.0799\n",
            "Epoch [2750/20000], Training Loss: 0.0764\n",
            "Epoch [2751/20000], Training Loss: 0.0821\n",
            "Epoch [2752/20000], Training Loss: 0.0776\n",
            "Epoch [2753/20000], Training Loss: 0.0809\n",
            "Epoch [2754/20000], Training Loss: 0.0801\n",
            "Epoch [2755/20000], Training Loss: 0.0773\n",
            "Epoch [2756/20000], Training Loss: 0.0803\n",
            "Epoch [2757/20000], Training Loss: 0.0744\n",
            "Epoch [2758/20000], Training Loss: 0.0783\n",
            "Epoch [2759/20000], Training Loss: 0.0741\n",
            "Epoch [2760/20000], Training Loss: 0.0857\n",
            "Epoch [2761/20000], Training Loss: 0.0785\n",
            "Epoch [2762/20000], Training Loss: 0.0777\n",
            "Epoch [2763/20000], Training Loss: 0.0802\n",
            "Epoch [2764/20000], Training Loss: 0.0831\n",
            "Epoch [2765/20000], Training Loss: 0.0819\n",
            "Epoch [2766/20000], Training Loss: 0.0872\n",
            "Epoch [2767/20000], Training Loss: 0.0832\n",
            "Epoch [2768/20000], Training Loss: 0.0802\n",
            "Epoch [2769/20000], Training Loss: 0.0790\n",
            "Epoch [2770/20000], Training Loss: 0.0859\n",
            "Epoch [2771/20000], Training Loss: 0.0764\n",
            "Epoch [2772/20000], Training Loss: 0.0852\n",
            "Epoch [2773/20000], Training Loss: 0.0796\n",
            "Epoch [2774/20000], Training Loss: 0.0734\n",
            "Epoch [2775/20000], Training Loss: 0.0738\n",
            "Epoch [2776/20000], Training Loss: 0.0844\n",
            "Epoch [2777/20000], Training Loss: 0.0851\n",
            "Epoch [2778/20000], Training Loss: 0.0770\n",
            "Epoch [2779/20000], Training Loss: 0.0774\n",
            "Epoch [2780/20000], Training Loss: 0.0793\n",
            "Epoch [2781/20000], Training Loss: 0.0805\n",
            "Epoch [2782/20000], Training Loss: 0.0812\n",
            "Epoch [2783/20000], Training Loss: 0.0789\n",
            "Epoch [2784/20000], Training Loss: 0.0749\n",
            "Epoch [2785/20000], Training Loss: 0.0796\n",
            "Epoch [2786/20000], Training Loss: 0.0808\n",
            "Epoch [2787/20000], Training Loss: 0.0799\n",
            "Epoch [2788/20000], Training Loss: 0.0784\n",
            "Epoch [2789/20000], Training Loss: 0.0782\n",
            "Epoch [2790/20000], Training Loss: 0.0850\n",
            "Epoch [2791/20000], Training Loss: 0.0804\n",
            "Epoch [2792/20000], Training Loss: 0.0849\n",
            "Epoch [2793/20000], Training Loss: 0.0822\n",
            "Epoch [2794/20000], Training Loss: 0.0749\n",
            "Epoch [2795/20000], Training Loss: 0.0777\n",
            "Epoch [2796/20000], Training Loss: 0.0853\n",
            "Epoch [2797/20000], Training Loss: 0.0851\n",
            "Epoch [2798/20000], Training Loss: 0.0762\n",
            "Epoch [2799/20000], Training Loss: 0.0809\n",
            "Epoch [2800/20000], Training Loss: 0.0801\n",
            "Epoch [2801/20000], Training Loss: 0.0761\n",
            "Epoch [2802/20000], Training Loss: 0.0832\n",
            "Epoch [2803/20000], Training Loss: 0.0792\n",
            "Epoch [2804/20000], Training Loss: 0.0856\n",
            "Epoch [2805/20000], Training Loss: 0.0876\n",
            "Epoch [2806/20000], Training Loss: 0.0794\n",
            "Epoch [2807/20000], Training Loss: 0.0773\n",
            "Epoch [2808/20000], Training Loss: 0.0745\n",
            "Epoch [2809/20000], Training Loss: 0.0787\n",
            "Epoch [2810/20000], Training Loss: 0.0798\n",
            "Epoch [2811/20000], Training Loss: 0.0746\n",
            "Epoch [2812/20000], Training Loss: 0.0753\n",
            "Epoch [2813/20000], Training Loss: 0.0870\n",
            "Epoch [2814/20000], Training Loss: 0.0770\n",
            "Epoch [2815/20000], Training Loss: 0.0742\n",
            "Epoch [2816/20000], Training Loss: 0.0870\n",
            "Epoch [2817/20000], Training Loss: 0.0789\n",
            "Epoch [2818/20000], Training Loss: 0.0732\n",
            "Epoch [2819/20000], Training Loss: 0.0756\n",
            "Epoch [2820/20000], Training Loss: 0.0786\n",
            "Epoch [2821/20000], Training Loss: 0.0761\n",
            "Epoch [2822/20000], Training Loss: 0.0851\n",
            "Epoch [2823/20000], Training Loss: 0.0726\n",
            "Epoch [2824/20000], Training Loss: 0.0825\n",
            "Epoch [2825/20000], Training Loss: 0.0881\n",
            "Epoch [2826/20000], Training Loss: 0.0798\n",
            "Epoch [2827/20000], Training Loss: 0.0804\n",
            "Epoch [2828/20000], Training Loss: 0.0763\n",
            "Epoch [2829/20000], Training Loss: 0.0764\n",
            "Epoch [2830/20000], Training Loss: 0.0820\n",
            "Epoch [2831/20000], Training Loss: 0.0792\n",
            "Epoch [2832/20000], Training Loss: 0.0816\n",
            "Epoch [2833/20000], Training Loss: 0.0812\n",
            "Epoch [2834/20000], Training Loss: 0.0825\n",
            "Epoch [2835/20000], Training Loss: 0.0770\n",
            "Epoch [2836/20000], Training Loss: 0.0811\n",
            "Epoch [2837/20000], Training Loss: 0.0742\n",
            "Epoch [2838/20000], Training Loss: 0.0830\n",
            "Epoch [2839/20000], Training Loss: 0.0810\n",
            "Epoch [2840/20000], Training Loss: 0.0807\n",
            "Epoch [2841/20000], Training Loss: 0.0805\n",
            "Epoch [2842/20000], Training Loss: 0.0798\n",
            "Epoch [2843/20000], Training Loss: 0.0783\n",
            "Epoch [2844/20000], Training Loss: 0.0781\n",
            "Epoch [2845/20000], Training Loss: 0.0811\n",
            "Epoch [2846/20000], Training Loss: 0.0828\n",
            "Epoch [2847/20000], Training Loss: 0.0841\n",
            "Epoch [2848/20000], Training Loss: 0.0756\n",
            "Epoch [2849/20000], Training Loss: 0.0816\n",
            "Epoch [2850/20000], Training Loss: 0.0795\n",
            "Epoch [2851/20000], Training Loss: 0.0815\n",
            "Epoch [2852/20000], Training Loss: 0.0816\n",
            "Epoch [2853/20000], Training Loss: 0.0848\n",
            "Epoch [2854/20000], Training Loss: 0.0763\n",
            "Epoch [2855/20000], Training Loss: 0.0793\n",
            "Epoch [2856/20000], Training Loss: 0.0800\n",
            "Epoch [2857/20000], Training Loss: 0.0769\n",
            "Epoch [2858/20000], Training Loss: 0.0814\n",
            "Epoch [2859/20000], Training Loss: 0.0808\n",
            "Epoch [2860/20000], Training Loss: 0.0778\n",
            "Epoch [2861/20000], Training Loss: 0.0802\n",
            "Epoch [2862/20000], Training Loss: 0.0785\n",
            "Epoch [2863/20000], Training Loss: 0.0852\n",
            "Epoch [2864/20000], Training Loss: 0.0826\n",
            "Epoch [2865/20000], Training Loss: 0.0844\n",
            "Epoch [2866/20000], Training Loss: 0.0805\n",
            "Epoch [2867/20000], Training Loss: 0.0845\n",
            "Epoch [2868/20000], Training Loss: 0.0809\n",
            "Epoch [2869/20000], Training Loss: 0.0852\n",
            "Epoch [2870/20000], Training Loss: 0.0780\n",
            "Epoch [2871/20000], Training Loss: 0.0805\n",
            "Epoch [2872/20000], Training Loss: 0.0865\n",
            "Epoch [2873/20000], Training Loss: 0.0802\n",
            "Epoch [2874/20000], Training Loss: 0.0784\n",
            "Epoch [2875/20000], Training Loss: 0.0797\n",
            "Epoch [2876/20000], Training Loss: 0.0759\n",
            "Epoch [2877/20000], Training Loss: 0.0844\n",
            "Epoch [2878/20000], Training Loss: 0.0745\n",
            "Epoch [2879/20000], Training Loss: 0.0754\n",
            "Epoch [2880/20000], Training Loss: 0.0803\n",
            "Epoch [2881/20000], Training Loss: 0.0779\n",
            "Epoch [2882/20000], Training Loss: 0.0754\n",
            "Epoch [2883/20000], Training Loss: 0.0810\n",
            "Epoch [2884/20000], Training Loss: 0.0815\n",
            "Epoch [2885/20000], Training Loss: 0.0783\n",
            "Epoch [2886/20000], Training Loss: 0.0873\n",
            "Epoch [2887/20000], Training Loss: 0.0801\n",
            "Epoch [2888/20000], Training Loss: 0.0738\n",
            "Epoch [2889/20000], Training Loss: 0.0834\n",
            "Epoch [2890/20000], Training Loss: 0.0779\n",
            "Epoch [2891/20000], Training Loss: 0.0871\n",
            "Epoch [2892/20000], Training Loss: 0.0758\n",
            "Epoch [2893/20000], Training Loss: 0.0802\n",
            "Epoch [2894/20000], Training Loss: 0.0816\n",
            "Epoch [2895/20000], Training Loss: 0.0813\n",
            "Epoch [2896/20000], Training Loss: 0.0765\n",
            "Epoch [2897/20000], Training Loss: 0.0846\n",
            "Epoch [2898/20000], Training Loss: 0.0738\n",
            "Epoch [2899/20000], Training Loss: 0.0760\n",
            "Epoch [2900/20000], Training Loss: 0.0795\n",
            "Epoch [2901/20000], Training Loss: 0.0805\n",
            "Epoch [2902/20000], Training Loss: 0.0780\n",
            "Epoch [2903/20000], Training Loss: 0.0789\n",
            "Epoch [2904/20000], Training Loss: 0.0744\n",
            "Epoch [2905/20000], Training Loss: 0.0837\n",
            "Epoch [2906/20000], Training Loss: 0.0786\n",
            "Epoch [2907/20000], Training Loss: 0.0800\n",
            "Epoch [2908/20000], Training Loss: 0.0839\n",
            "Epoch [2909/20000], Training Loss: 0.0842\n",
            "Epoch [2910/20000], Training Loss: 0.0778\n",
            "Epoch [2911/20000], Training Loss: 0.0844\n",
            "Epoch [2912/20000], Training Loss: 0.0725\n",
            "Epoch [2913/20000], Training Loss: 0.0803\n",
            "Epoch [2914/20000], Training Loss: 0.0888\n",
            "Epoch [2915/20000], Training Loss: 0.0813\n",
            "Epoch [2916/20000], Training Loss: 0.0736\n",
            "Epoch [2917/20000], Training Loss: 0.0764\n",
            "Epoch [2918/20000], Training Loss: 0.0785\n",
            "Epoch [2919/20000], Training Loss: 0.0834\n",
            "Epoch [2920/20000], Training Loss: 0.0862\n",
            "Epoch [2921/20000], Training Loss: 0.0795\n",
            "Epoch [2922/20000], Training Loss: 0.0767\n",
            "Epoch [2923/20000], Training Loss: 0.0800\n",
            "Epoch [2924/20000], Training Loss: 0.0797\n",
            "Epoch [2925/20000], Training Loss: 0.0800\n",
            "Epoch [2926/20000], Training Loss: 0.0814\n",
            "Epoch [2927/20000], Training Loss: 0.0787\n",
            "Epoch [2928/20000], Training Loss: 0.0825\n",
            "Epoch [2929/20000], Training Loss: 0.0844\n",
            "Epoch [2930/20000], Training Loss: 0.0828\n",
            "Epoch [2931/20000], Training Loss: 0.0868\n",
            "Epoch [2932/20000], Training Loss: 0.0820\n",
            "Epoch [2933/20000], Training Loss: 0.0781\n",
            "Epoch [2934/20000], Training Loss: 0.0848\n",
            "Epoch [2935/20000], Training Loss: 0.0736\n",
            "Epoch [2936/20000], Training Loss: 0.0779\n",
            "Epoch [2937/20000], Training Loss: 0.0802\n",
            "Epoch [2938/20000], Training Loss: 0.0728\n",
            "Epoch [2939/20000], Training Loss: 0.0812\n",
            "Epoch [2940/20000], Training Loss: 0.0820\n",
            "Epoch [2941/20000], Training Loss: 0.0744\n",
            "Epoch [2942/20000], Training Loss: 0.0754\n",
            "Epoch [2943/20000], Training Loss: 0.0826\n",
            "Epoch [2944/20000], Training Loss: 0.0769\n",
            "Epoch [2945/20000], Training Loss: 0.0789\n",
            "Epoch [2946/20000], Training Loss: 0.0807\n",
            "Epoch [2947/20000], Training Loss: 0.0757\n",
            "Epoch [2948/20000], Training Loss: 0.0806\n",
            "Epoch [2949/20000], Training Loss: 0.0800\n",
            "Epoch [2950/20000], Training Loss: 0.0732\n",
            "Epoch [2951/20000], Training Loss: 0.0837\n",
            "Epoch [2952/20000], Training Loss: 0.0741\n",
            "Epoch [2953/20000], Training Loss: 0.0797\n",
            "Epoch [2954/20000], Training Loss: 0.0722\n",
            "Epoch [2955/20000], Training Loss: 0.0846\n",
            "Epoch [2956/20000], Training Loss: 0.0821\n",
            "Epoch [2957/20000], Training Loss: 0.0736\n",
            "Epoch [2958/20000], Training Loss: 0.0791\n",
            "Epoch [2959/20000], Training Loss: 0.0828\n",
            "Epoch [2960/20000], Training Loss: 0.0830\n",
            "Epoch [2961/20000], Training Loss: 0.0815\n",
            "Epoch [2962/20000], Training Loss: 0.0770\n",
            "Epoch [2963/20000], Training Loss: 0.0795\n",
            "Epoch [2964/20000], Training Loss: 0.0860\n",
            "Epoch [2965/20000], Training Loss: 0.0815\n",
            "Epoch [2966/20000], Training Loss: 0.0805\n",
            "Epoch [2967/20000], Training Loss: 0.0771\n",
            "Epoch [2968/20000], Training Loss: 0.0794\n",
            "Epoch [2969/20000], Training Loss: 0.0853\n",
            "Epoch [2970/20000], Training Loss: 0.0752\n",
            "Epoch [2971/20000], Training Loss: 0.0772\n",
            "Epoch [2972/20000], Training Loss: 0.0733\n",
            "Epoch [2973/20000], Training Loss: 0.0807\n",
            "Epoch [2974/20000], Training Loss: 0.0802\n",
            "Epoch [2975/20000], Training Loss: 0.0812\n",
            "Epoch [2976/20000], Training Loss: 0.0753\n",
            "Epoch [2977/20000], Training Loss: 0.0784\n",
            "Epoch [2978/20000], Training Loss: 0.0780\n",
            "Epoch [2979/20000], Training Loss: 0.0847\n",
            "Epoch [2980/20000], Training Loss: 0.0785\n",
            "Epoch [2981/20000], Training Loss: 0.0835\n",
            "Epoch [2982/20000], Training Loss: 0.0807\n",
            "Epoch [2983/20000], Training Loss: 0.0740\n",
            "Epoch [2984/20000], Training Loss: 0.0839\n",
            "Epoch [2985/20000], Training Loss: 0.0833\n",
            "Epoch [2986/20000], Training Loss: 0.0757\n",
            "Epoch [2987/20000], Training Loss: 0.0757\n",
            "Epoch [2988/20000], Training Loss: 0.0809\n",
            "Epoch [2989/20000], Training Loss: 0.0772\n",
            "Epoch [2990/20000], Training Loss: 0.0826\n",
            "Epoch [2991/20000], Training Loss: 0.0801\n",
            "Epoch [2992/20000], Training Loss: 0.0753\n",
            "Epoch [2993/20000], Training Loss: 0.0803\n",
            "Epoch [2994/20000], Training Loss: 0.0879\n",
            "Epoch [2995/20000], Training Loss: 0.0744\n",
            "Epoch [2996/20000], Training Loss: 0.0773\n",
            "Epoch [2997/20000], Training Loss: 0.0755\n",
            "Epoch [2998/20000], Training Loss: 0.0755\n",
            "Epoch [2999/20000], Training Loss: 0.0834\n",
            "Epoch [3000/20000], Training Loss: 0.0744\n",
            "Epoch [3001/20000], Training Loss: 0.0834\n",
            "Epoch [3002/20000], Training Loss: 0.0765\n",
            "Epoch [3003/20000], Training Loss: 0.0784\n",
            "Epoch [3004/20000], Training Loss: 0.0759\n",
            "Epoch [3005/20000], Training Loss: 0.0813\n",
            "Epoch [3006/20000], Training Loss: 0.0748\n",
            "Epoch [3007/20000], Training Loss: 0.0737\n",
            "Epoch [3008/20000], Training Loss: 0.0732\n",
            "Epoch [3009/20000], Training Loss: 0.0777\n",
            "Epoch [3010/20000], Training Loss: 0.0841\n",
            "Epoch [3011/20000], Training Loss: 0.0836\n",
            "Epoch [3012/20000], Training Loss: 0.0782\n",
            "Epoch [3013/20000], Training Loss: 0.0786\n",
            "Epoch [3014/20000], Training Loss: 0.0772\n",
            "Epoch [3015/20000], Training Loss: 0.0837\n",
            "Epoch [3016/20000], Training Loss: 0.0815\n",
            "Epoch [3017/20000], Training Loss: 0.0817\n",
            "Epoch [3018/20000], Training Loss: 0.0855\n",
            "Epoch [3019/20000], Training Loss: 0.0775\n",
            "Epoch [3020/20000], Training Loss: 0.0764\n",
            "Epoch [3021/20000], Training Loss: 0.0883\n",
            "Epoch [3022/20000], Training Loss: 0.0835\n",
            "Epoch [3023/20000], Training Loss: 0.0762\n",
            "Epoch [3024/20000], Training Loss: 0.0848\n",
            "Epoch [3025/20000], Training Loss: 0.0818\n",
            "Epoch [3026/20000], Training Loss: 0.0835\n",
            "Epoch [3027/20000], Training Loss: 0.0806\n",
            "Epoch [3028/20000], Training Loss: 0.0813\n",
            "Epoch [3029/20000], Training Loss: 0.0803\n",
            "Epoch [3030/20000], Training Loss: 0.0783\n",
            "Epoch [3031/20000], Training Loss: 0.0868\n",
            "Epoch [3032/20000], Training Loss: 0.0874\n",
            "Epoch [3033/20000], Training Loss: 0.0728\n",
            "Epoch [3034/20000], Training Loss: 0.0863\n",
            "Epoch [3035/20000], Training Loss: 0.0816\n",
            "Epoch [3036/20000], Training Loss: 0.0797\n",
            "Epoch [3037/20000], Training Loss: 0.0873\n",
            "Epoch [3038/20000], Training Loss: 0.0795\n",
            "Epoch [3039/20000], Training Loss: 0.0800\n",
            "Epoch [3040/20000], Training Loss: 0.0728\n",
            "Epoch [3041/20000], Training Loss: 0.0769\n",
            "Epoch [3042/20000], Training Loss: 0.0872\n",
            "Epoch [3043/20000], Training Loss: 0.0794\n",
            "Epoch [3044/20000], Training Loss: 0.0808\n",
            "Epoch [3045/20000], Training Loss: 0.0852\n",
            "Epoch [3046/20000], Training Loss: 0.0778\n",
            "Epoch [3047/20000], Training Loss: 0.0813\n",
            "Epoch [3048/20000], Training Loss: 0.0836\n",
            "Epoch [3049/20000], Training Loss: 0.0773\n",
            "Epoch [3050/20000], Training Loss: 0.0805\n",
            "Epoch [3051/20000], Training Loss: 0.0852\n",
            "Epoch [3052/20000], Training Loss: 0.0782\n",
            "Epoch [3053/20000], Training Loss: 0.0792\n",
            "Epoch [3054/20000], Training Loss: 0.0780\n",
            "Epoch [3055/20000], Training Loss: 0.0824\n",
            "Epoch [3056/20000], Training Loss: 0.0807\n",
            "Epoch [3057/20000], Training Loss: 0.0783\n",
            "Epoch [3058/20000], Training Loss: 0.0741\n",
            "Epoch [3059/20000], Training Loss: 0.0830\n",
            "Epoch [3060/20000], Training Loss: 0.0796\n",
            "Epoch [3061/20000], Training Loss: 0.0789\n",
            "Epoch [3062/20000], Training Loss: 0.0824\n",
            "Epoch [3063/20000], Training Loss: 0.0780\n",
            "Epoch [3064/20000], Training Loss: 0.0758\n",
            "Epoch [3065/20000], Training Loss: 0.0816\n",
            "Epoch [3066/20000], Training Loss: 0.0854\n",
            "Epoch [3067/20000], Training Loss: 0.0777\n",
            "Epoch [3068/20000], Training Loss: 0.0765\n",
            "Epoch [3069/20000], Training Loss: 0.0763\n",
            "Epoch [3070/20000], Training Loss: 0.0816\n",
            "Epoch [3071/20000], Training Loss: 0.0815\n",
            "Epoch [3072/20000], Training Loss: 0.0791\n",
            "Epoch [3073/20000], Training Loss: 0.0867\n",
            "Epoch [3074/20000], Training Loss: 0.0785\n",
            "Epoch [3075/20000], Training Loss: 0.0796\n",
            "Epoch [3076/20000], Training Loss: 0.0728\n",
            "Epoch [3077/20000], Training Loss: 0.0783\n",
            "Epoch [3078/20000], Training Loss: 0.0773\n",
            "Epoch [3079/20000], Training Loss: 0.0748\n",
            "Epoch [3080/20000], Training Loss: 0.0780\n",
            "Epoch [3081/20000], Training Loss: 0.0832\n",
            "Epoch [3082/20000], Training Loss: 0.0876\n",
            "Epoch [3083/20000], Training Loss: 0.0797\n",
            "Epoch [3084/20000], Training Loss: 0.0809\n",
            "Epoch [3085/20000], Training Loss: 0.0751\n",
            "Epoch [3086/20000], Training Loss: 0.0810\n",
            "Epoch [3087/20000], Training Loss: 0.0779\n",
            "Epoch [3088/20000], Training Loss: 0.0844\n",
            "Epoch [3089/20000], Training Loss: 0.0802\n",
            "Epoch [3090/20000], Training Loss: 0.0766\n",
            "Epoch [3091/20000], Training Loss: 0.0796\n",
            "Epoch [3092/20000], Training Loss: 0.0823\n",
            "Epoch [3093/20000], Training Loss: 0.0850\n",
            "Epoch [3094/20000], Training Loss: 0.0790\n",
            "Epoch [3095/20000], Training Loss: 0.0796\n",
            "Epoch [3096/20000], Training Loss: 0.0760\n",
            "Epoch [3097/20000], Training Loss: 0.0794\n",
            "Epoch [3098/20000], Training Loss: 0.0829\n",
            "Epoch [3099/20000], Training Loss: 0.0766\n",
            "Epoch [3100/20000], Training Loss: 0.0761\n",
            "Epoch [3101/20000], Training Loss: 0.0755\n",
            "Epoch [3102/20000], Training Loss: 0.0827\n",
            "Epoch [3103/20000], Training Loss: 0.0809\n",
            "Epoch [3104/20000], Training Loss: 0.0749\n",
            "Epoch [3105/20000], Training Loss: 0.0837\n",
            "Epoch [3106/20000], Training Loss: 0.0791\n",
            "Epoch [3107/20000], Training Loss: 0.0853\n",
            "Epoch [3108/20000], Training Loss: 0.0767\n",
            "Epoch [3109/20000], Training Loss: 0.0790\n",
            "Epoch [3110/20000], Training Loss: 0.0795\n",
            "Epoch [3111/20000], Training Loss: 0.0776\n",
            "Epoch [3112/20000], Training Loss: 0.0786\n",
            "Epoch [3113/20000], Training Loss: 0.0736\n",
            "Epoch [3114/20000], Training Loss: 0.0851\n",
            "Epoch [3115/20000], Training Loss: 0.0753\n",
            "Epoch [3116/20000], Training Loss: 0.0736\n",
            "Epoch [3117/20000], Training Loss: 0.0774\n",
            "Epoch [3118/20000], Training Loss: 0.0739\n",
            "Epoch [3119/20000], Training Loss: 0.0767\n",
            "Epoch [3120/20000], Training Loss: 0.0765\n",
            "Epoch [3121/20000], Training Loss: 0.0832\n",
            "Epoch [3122/20000], Training Loss: 0.0752\n",
            "Epoch [3123/20000], Training Loss: 0.0747\n",
            "Epoch [3124/20000], Training Loss: 0.0869\n",
            "Epoch [3125/20000], Training Loss: 0.0880\n",
            "Epoch [3126/20000], Training Loss: 0.0789\n",
            "Epoch [3127/20000], Training Loss: 0.0849\n",
            "Epoch [3128/20000], Training Loss: 0.0791\n",
            "Epoch [3129/20000], Training Loss: 0.0818\n",
            "Epoch [3130/20000], Training Loss: 0.0772\n",
            "Epoch [3131/20000], Training Loss: 0.0740\n",
            "Epoch [3132/20000], Training Loss: 0.0802\n",
            "Epoch [3133/20000], Training Loss: 0.0816\n",
            "Epoch [3134/20000], Training Loss: 0.0784\n",
            "Epoch [3135/20000], Training Loss: 0.0791\n",
            "Epoch [3136/20000], Training Loss: 0.0828\n",
            "Epoch [3137/20000], Training Loss: 0.0816\n",
            "Epoch [3138/20000], Training Loss: 0.0790\n",
            "Epoch [3139/20000], Training Loss: 0.0788\n",
            "Epoch [3140/20000], Training Loss: 0.0768\n",
            "Epoch [3141/20000], Training Loss: 0.0829\n",
            "Epoch [3142/20000], Training Loss: 0.0877\n",
            "Epoch [3143/20000], Training Loss: 0.0778\n",
            "Epoch [3144/20000], Training Loss: 0.0783\n",
            "Epoch [3145/20000], Training Loss: 0.0749\n",
            "Epoch [3146/20000], Training Loss: 0.0818\n",
            "Epoch [3147/20000], Training Loss: 0.0802\n",
            "Epoch [3148/20000], Training Loss: 0.0811\n",
            "Epoch [3149/20000], Training Loss: 0.0800\n",
            "Epoch [3150/20000], Training Loss: 0.0754\n",
            "Epoch [3151/20000], Training Loss: 0.0783\n",
            "Epoch [3152/20000], Training Loss: 0.0785\n",
            "Epoch [3153/20000], Training Loss: 0.0772\n",
            "Epoch [3154/20000], Training Loss: 0.0751\n",
            "Epoch [3155/20000], Training Loss: 0.0862\n",
            "Epoch [3156/20000], Training Loss: 0.0761\n",
            "Epoch [3157/20000], Training Loss: 0.0783\n",
            "Epoch [3158/20000], Training Loss: 0.0753\n",
            "Epoch [3159/20000], Training Loss: 0.0806\n",
            "Epoch [3160/20000], Training Loss: 0.0827\n",
            "Epoch [3161/20000], Training Loss: 0.0804\n",
            "Epoch [3162/20000], Training Loss: 0.0847\n",
            "Epoch [3163/20000], Training Loss: 0.0777\n",
            "Epoch [3164/20000], Training Loss: 0.0795\n",
            "Epoch [3165/20000], Training Loss: 0.0768\n",
            "Epoch [3166/20000], Training Loss: 0.0739\n",
            "Epoch [3167/20000], Training Loss: 0.0771\n",
            "Epoch [3168/20000], Training Loss: 0.0788\n",
            "Epoch [3169/20000], Training Loss: 0.0794\n",
            "Epoch [3170/20000], Training Loss: 0.0759\n",
            "Epoch [3171/20000], Training Loss: 0.0777\n",
            "Epoch [3172/20000], Training Loss: 0.0809\n",
            "Epoch [3173/20000], Training Loss: 0.0807\n",
            "Epoch [3174/20000], Training Loss: 0.0748\n",
            "Epoch [3175/20000], Training Loss: 0.0810\n",
            "Epoch [3176/20000], Training Loss: 0.0863\n",
            "Epoch [3177/20000], Training Loss: 0.0769\n",
            "Epoch [3178/20000], Training Loss: 0.0792\n",
            "Epoch [3179/20000], Training Loss: 0.0803\n",
            "Epoch [3180/20000], Training Loss: 0.0802\n",
            "Epoch [3181/20000], Training Loss: 0.0765\n",
            "Epoch [3182/20000], Training Loss: 0.0770\n",
            "Epoch [3183/20000], Training Loss: 0.0839\n",
            "Epoch [3184/20000], Training Loss: 0.0752\n",
            "Epoch [3185/20000], Training Loss: 0.0773\n",
            "Epoch [3186/20000], Training Loss: 0.0817\n",
            "Epoch [3187/20000], Training Loss: 0.0762\n",
            "Epoch [3188/20000], Training Loss: 0.0790\n",
            "Epoch [3189/20000], Training Loss: 0.0809\n",
            "Epoch [3190/20000], Training Loss: 0.0833\n",
            "Epoch [3191/20000], Training Loss: 0.0797\n",
            "Epoch [3192/20000], Training Loss: 0.0808\n",
            "Epoch [3193/20000], Training Loss: 0.0809\n",
            "Epoch [3194/20000], Training Loss: 0.0836\n",
            "Epoch [3195/20000], Training Loss: 0.0833\n",
            "Epoch [3196/20000], Training Loss: 0.0797\n",
            "Epoch [3197/20000], Training Loss: 0.0791\n",
            "Epoch [3198/20000], Training Loss: 0.0846\n",
            "Epoch [3199/20000], Training Loss: 0.0804\n",
            "Epoch [3200/20000], Training Loss: 0.0779\n",
            "Epoch [3201/20000], Training Loss: 0.0810\n",
            "Epoch [3202/20000], Training Loss: 0.0833\n",
            "Epoch [3203/20000], Training Loss: 0.0800\n",
            "Epoch [3204/20000], Training Loss: 0.0788\n",
            "Epoch [3205/20000], Training Loss: 0.0820\n",
            "Epoch [3206/20000], Training Loss: 0.0812\n",
            "Epoch [3207/20000], Training Loss: 0.0777\n",
            "Epoch [3208/20000], Training Loss: 0.0742\n",
            "Epoch [3209/20000], Training Loss: 0.0805\n",
            "Epoch [3210/20000], Training Loss: 0.0761\n",
            "Epoch [3211/20000], Training Loss: 0.0845\n",
            "Epoch [3212/20000], Training Loss: 0.0783\n",
            "Epoch [3213/20000], Training Loss: 0.0802\n",
            "Epoch [3214/20000], Training Loss: 0.0797\n",
            "Epoch [3215/20000], Training Loss: 0.0803\n",
            "Epoch [3216/20000], Training Loss: 0.0785\n",
            "Epoch [3217/20000], Training Loss: 0.0780\n",
            "Epoch [3218/20000], Training Loss: 0.0794\n",
            "Epoch [3219/20000], Training Loss: 0.0736\n",
            "Epoch [3220/20000], Training Loss: 0.0871\n",
            "Epoch [3221/20000], Training Loss: 0.0806\n",
            "Epoch [3222/20000], Training Loss: 0.0831\n",
            "Epoch [3223/20000], Training Loss: 0.0823\n",
            "Epoch [3224/20000], Training Loss: 0.0828\n",
            "Epoch [3225/20000], Training Loss: 0.0801\n",
            "Epoch [3226/20000], Training Loss: 0.0830\n",
            "Epoch [3227/20000], Training Loss: 0.0776\n",
            "Epoch [3228/20000], Training Loss: 0.0785\n",
            "Epoch [3229/20000], Training Loss: 0.0827\n",
            "Epoch [3230/20000], Training Loss: 0.0849\n",
            "Epoch [3231/20000], Training Loss: 0.0810\n",
            "Epoch [3232/20000], Training Loss: 0.0824\n",
            "Epoch [3233/20000], Training Loss: 0.0791\n",
            "Epoch [3234/20000], Training Loss: 0.0794\n",
            "Epoch [3235/20000], Training Loss: 0.0808\n",
            "Epoch [3236/20000], Training Loss: 0.0852\n",
            "Epoch [3237/20000], Training Loss: 0.0810\n",
            "Epoch [3238/20000], Training Loss: 0.0788\n",
            "Epoch [3239/20000], Training Loss: 0.0809\n",
            "Epoch [3240/20000], Training Loss: 0.0880\n",
            "Epoch [3241/20000], Training Loss: 0.0783\n",
            "Epoch [3242/20000], Training Loss: 0.0780\n",
            "Epoch [3243/20000], Training Loss: 0.0824\n",
            "Epoch [3244/20000], Training Loss: 0.0812\n",
            "Epoch [3245/20000], Training Loss: 0.0748\n",
            "Epoch [3246/20000], Training Loss: 0.0792\n",
            "Epoch [3247/20000], Training Loss: 0.0779\n",
            "Epoch [3248/20000], Training Loss: 0.0815\n",
            "Epoch [3249/20000], Training Loss: 0.0814\n",
            "Epoch [3250/20000], Training Loss: 0.0791\n",
            "Epoch [3251/20000], Training Loss: 0.0829\n",
            "Epoch [3252/20000], Training Loss: 0.0806\n",
            "Epoch [3253/20000], Training Loss: 0.0793\n",
            "Epoch [3254/20000], Training Loss: 0.0853\n",
            "Epoch [3255/20000], Training Loss: 0.0740\n",
            "Epoch [3256/20000], Training Loss: 0.0739\n",
            "Epoch [3257/20000], Training Loss: 0.0855\n",
            "Epoch [3258/20000], Training Loss: 0.0847\n",
            "Epoch [3259/20000], Training Loss: 0.0741\n",
            "Epoch [3260/20000], Training Loss: 0.0797\n",
            "Epoch [3261/20000], Training Loss: 0.0836\n",
            "Epoch [3262/20000], Training Loss: 0.0839\n",
            "Epoch [3263/20000], Training Loss: 0.0810\n",
            "Epoch [3264/20000], Training Loss: 0.0825\n",
            "Epoch [3265/20000], Training Loss: 0.0842\n",
            "Epoch [3266/20000], Training Loss: 0.0790\n",
            "Epoch [3267/20000], Training Loss: 0.0759\n",
            "Epoch [3268/20000], Training Loss: 0.0863\n",
            "Epoch [3269/20000], Training Loss: 0.0833\n",
            "Epoch [3270/20000], Training Loss: 0.0784\n",
            "Epoch [3271/20000], Training Loss: 0.0860\n",
            "Epoch [3272/20000], Training Loss: 0.0812\n",
            "Epoch [3273/20000], Training Loss: 0.0752\n",
            "Epoch [3274/20000], Training Loss: 0.0819\n",
            "Epoch [3275/20000], Training Loss: 0.0767\n",
            "Epoch [3276/20000], Training Loss: 0.0826\n",
            "Epoch [3277/20000], Training Loss: 0.0773\n",
            "Epoch [3278/20000], Training Loss: 0.0780\n",
            "Epoch [3279/20000], Training Loss: 0.0751\n",
            "Epoch [3280/20000], Training Loss: 0.0757\n",
            "Epoch [3281/20000], Training Loss: 0.0823\n",
            "Epoch [3282/20000], Training Loss: 0.0745\n",
            "Epoch [3283/20000], Training Loss: 0.0752\n",
            "Epoch [3284/20000], Training Loss: 0.0803\n",
            "Epoch [3285/20000], Training Loss: 0.0821\n",
            "Epoch [3286/20000], Training Loss: 0.0755\n",
            "Epoch [3287/20000], Training Loss: 0.0819\n",
            "Epoch [3288/20000], Training Loss: 0.0789\n",
            "Epoch [3289/20000], Training Loss: 0.0789\n",
            "Epoch [3290/20000], Training Loss: 0.0819\n",
            "Epoch [3291/20000], Training Loss: 0.0804\n",
            "Epoch [3292/20000], Training Loss: 0.0793\n",
            "Epoch [3293/20000], Training Loss: 0.0793\n",
            "Epoch [3294/20000], Training Loss: 0.0768\n",
            "Epoch [3295/20000], Training Loss: 0.0834\n",
            "Epoch [3296/20000], Training Loss: 0.0817\n",
            "Epoch [3297/20000], Training Loss: 0.0821\n",
            "Epoch [3298/20000], Training Loss: 0.0874\n",
            "Epoch [3299/20000], Training Loss: 0.0808\n",
            "Epoch [3300/20000], Training Loss: 0.0799\n",
            "Epoch [3301/20000], Training Loss: 0.0813\n",
            "Epoch [3302/20000], Training Loss: 0.0797\n",
            "Epoch [3303/20000], Training Loss: 0.0779\n",
            "Epoch [3304/20000], Training Loss: 0.0774\n",
            "Epoch [3305/20000], Training Loss: 0.0765\n",
            "Epoch [3306/20000], Training Loss: 0.0851\n",
            "Epoch [3307/20000], Training Loss: 0.0730\n",
            "Epoch [3308/20000], Training Loss: 0.0770\n",
            "Epoch [3309/20000], Training Loss: 0.0767\n",
            "Epoch [3310/20000], Training Loss: 0.0852\n",
            "Epoch [3311/20000], Training Loss: 0.0734\n",
            "Epoch [3312/20000], Training Loss: 0.0838\n",
            "Epoch [3313/20000], Training Loss: 0.0807\n",
            "Epoch [3314/20000], Training Loss: 0.0850\n",
            "Epoch [3315/20000], Training Loss: 0.0775\n",
            "Epoch [3316/20000], Training Loss: 0.0816\n",
            "Epoch [3317/20000], Training Loss: 0.0832\n",
            "Epoch [3318/20000], Training Loss: 0.0784\n",
            "Epoch [3319/20000], Training Loss: 0.0825\n",
            "Epoch [3320/20000], Training Loss: 0.0781\n",
            "Epoch [3321/20000], Training Loss: 0.0729\n",
            "Epoch [3322/20000], Training Loss: 0.0893\n",
            "Epoch [3323/20000], Training Loss: 0.0863\n",
            "Epoch [3324/20000], Training Loss: 0.0857\n",
            "Epoch [3325/20000], Training Loss: 0.0730\n",
            "Epoch [3326/20000], Training Loss: 0.0841\n",
            "Epoch [3327/20000], Training Loss: 0.0784\n",
            "Epoch [3328/20000], Training Loss: 0.0846\n",
            "Epoch [3329/20000], Training Loss: 0.0771\n",
            "Epoch [3330/20000], Training Loss: 0.0725\n",
            "Epoch [3331/20000], Training Loss: 0.0793\n",
            "Epoch [3332/20000], Training Loss: 0.0718\n",
            "Epoch [3333/20000], Training Loss: 0.0788\n",
            "Epoch [3334/20000], Training Loss: 0.0810\n",
            "Epoch [3335/20000], Training Loss: 0.0780\n",
            "Epoch [3336/20000], Training Loss: 0.0826\n",
            "Epoch [3337/20000], Training Loss: 0.0835\n",
            "Epoch [3338/20000], Training Loss: 0.0803\n",
            "Epoch [3339/20000], Training Loss: 0.0761\n",
            "Epoch [3340/20000], Training Loss: 0.0804\n",
            "Epoch [3341/20000], Training Loss: 0.0820\n",
            "Epoch [3342/20000], Training Loss: 0.0805\n",
            "Epoch [3343/20000], Training Loss: 0.0817\n",
            "Epoch [3344/20000], Training Loss: 0.0799\n",
            "Epoch [3345/20000], Training Loss: 0.0770\n",
            "Epoch [3346/20000], Training Loss: 0.0800\n",
            "Epoch [3347/20000], Training Loss: 0.0821\n",
            "Epoch [3348/20000], Training Loss: 0.0778\n",
            "Epoch [3349/20000], Training Loss: 0.0752\n",
            "Epoch [3350/20000], Training Loss: 0.0842\n",
            "Epoch [3351/20000], Training Loss: 0.0768\n",
            "Epoch [3352/20000], Training Loss: 0.0843\n",
            "Epoch [3353/20000], Training Loss: 0.0813\n",
            "Epoch [3354/20000], Training Loss: 0.0800\n",
            "Epoch [3355/20000], Training Loss: 0.0829\n",
            "Epoch [3356/20000], Training Loss: 0.0756\n",
            "Epoch [3357/20000], Training Loss: 0.0810\n",
            "Epoch [3358/20000], Training Loss: 0.0797\n",
            "Epoch [3359/20000], Training Loss: 0.0762\n",
            "Epoch [3360/20000], Training Loss: 0.0748\n",
            "Epoch [3361/20000], Training Loss: 0.0798\n",
            "Epoch [3362/20000], Training Loss: 0.0799\n",
            "Epoch [3363/20000], Training Loss: 0.0809\n",
            "Epoch [3364/20000], Training Loss: 0.0772\n",
            "Epoch [3365/20000], Training Loss: 0.0742\n",
            "Epoch [3366/20000], Training Loss: 0.0762\n",
            "Epoch [3367/20000], Training Loss: 0.0763\n",
            "Epoch [3368/20000], Training Loss: 0.0756\n",
            "Epoch [3369/20000], Training Loss: 0.0791\n",
            "Epoch [3370/20000], Training Loss: 0.0816\n",
            "Epoch [3371/20000], Training Loss: 0.0853\n",
            "Epoch [3372/20000], Training Loss: 0.0797\n",
            "Epoch [3373/20000], Training Loss: 0.0811\n",
            "Epoch [3374/20000], Training Loss: 0.0792\n",
            "Epoch [3375/20000], Training Loss: 0.0752\n",
            "Epoch [3376/20000], Training Loss: 0.0832\n",
            "Epoch [3377/20000], Training Loss: 0.0787\n",
            "Epoch [3378/20000], Training Loss: 0.0800\n",
            "Epoch [3379/20000], Training Loss: 0.0790\n",
            "Epoch [3380/20000], Training Loss: 0.0794\n",
            "Epoch [3381/20000], Training Loss: 0.0835\n",
            "Epoch [3382/20000], Training Loss: 0.0815\n",
            "Epoch [3383/20000], Training Loss: 0.0800\n",
            "Epoch [3384/20000], Training Loss: 0.0788\n",
            "Epoch [3385/20000], Training Loss: 0.0745\n",
            "Epoch [3386/20000], Training Loss: 0.0782\n",
            "Epoch [3387/20000], Training Loss: 0.0788\n",
            "Epoch [3388/20000], Training Loss: 0.0807\n",
            "Epoch [3389/20000], Training Loss: 0.0761\n",
            "Epoch [3390/20000], Training Loss: 0.0749\n",
            "Epoch [3391/20000], Training Loss: 0.0804\n",
            "Epoch [3392/20000], Training Loss: 0.0818\n",
            "Epoch [3393/20000], Training Loss: 0.0858\n",
            "Epoch [3394/20000], Training Loss: 0.0833\n",
            "Epoch [3395/20000], Training Loss: 0.0796\n",
            "Epoch [3396/20000], Training Loss: 0.0772\n",
            "Epoch [3397/20000], Training Loss: 0.0776\n",
            "Epoch [3398/20000], Training Loss: 0.0792\n",
            "Epoch [3399/20000], Training Loss: 0.0845\n",
            "Epoch [3400/20000], Training Loss: 0.0762\n",
            "Epoch [3401/20000], Training Loss: 0.0858\n",
            "Epoch [3402/20000], Training Loss: 0.0831\n",
            "Epoch [3403/20000], Training Loss: 0.0737\n",
            "Epoch [3404/20000], Training Loss: 0.0849\n",
            "Epoch [3405/20000], Training Loss: 0.0778\n",
            "Epoch [3406/20000], Training Loss: 0.0742\n",
            "Epoch [3407/20000], Training Loss: 0.0714\n",
            "Epoch [3408/20000], Training Loss: 0.0787\n",
            "Epoch [3409/20000], Training Loss: 0.0765\n",
            "Epoch [3410/20000], Training Loss: 0.0858\n",
            "Epoch [3411/20000], Training Loss: 0.0795\n",
            "Epoch [3412/20000], Training Loss: 0.0830\n",
            "Epoch [3413/20000], Training Loss: 0.0759\n",
            "Epoch [3414/20000], Training Loss: 0.0812\n",
            "Epoch [3415/20000], Training Loss: 0.0873\n",
            "Epoch [3416/20000], Training Loss: 0.0792\n",
            "Epoch [3417/20000], Training Loss: 0.0837\n",
            "Epoch [3418/20000], Training Loss: 0.0789\n",
            "Epoch [3419/20000], Training Loss: 0.0802\n",
            "Epoch [3420/20000], Training Loss: 0.0796\n",
            "Epoch [3421/20000], Training Loss: 0.0806\n",
            "Epoch [3422/20000], Training Loss: 0.0768\n",
            "Epoch [3423/20000], Training Loss: 0.0736\n",
            "Epoch [3424/20000], Training Loss: 0.0783\n",
            "Epoch [3425/20000], Training Loss: 0.0835\n",
            "Epoch [3426/20000], Training Loss: 0.0795\n",
            "Epoch [3427/20000], Training Loss: 0.0836\n",
            "Epoch [3428/20000], Training Loss: 0.0803\n",
            "Epoch [3429/20000], Training Loss: 0.0871\n",
            "Epoch [3430/20000], Training Loss: 0.0799\n",
            "Epoch [3431/20000], Training Loss: 0.0808\n",
            "Epoch [3432/20000], Training Loss: 0.0754\n",
            "Epoch [3433/20000], Training Loss: 0.0784\n",
            "Epoch [3434/20000], Training Loss: 0.0755\n",
            "Epoch [3435/20000], Training Loss: 0.0768\n",
            "Epoch [3436/20000], Training Loss: 0.0816\n",
            "Epoch [3437/20000], Training Loss: 0.0760\n",
            "Epoch [3438/20000], Training Loss: 0.0807\n",
            "Epoch [3439/20000], Training Loss: 0.0837\n",
            "Epoch [3440/20000], Training Loss: 0.0748\n",
            "Epoch [3441/20000], Training Loss: 0.0793\n",
            "Epoch [3442/20000], Training Loss: 0.0754\n",
            "Epoch [3443/20000], Training Loss: 0.0785\n",
            "Epoch [3444/20000], Training Loss: 0.0798\n",
            "Epoch [3445/20000], Training Loss: 0.0736\n",
            "Epoch [3446/20000], Training Loss: 0.0761\n",
            "Epoch [3447/20000], Training Loss: 0.0725\n",
            "Epoch [3448/20000], Training Loss: 0.0826\n",
            "Epoch [3449/20000], Training Loss: 0.0867\n",
            "Epoch [3450/20000], Training Loss: 0.0809\n",
            "Epoch [3451/20000], Training Loss: 0.0789\n",
            "Epoch [3452/20000], Training Loss: 0.0743\n",
            "Epoch [3453/20000], Training Loss: 0.0775\n",
            "Epoch [3454/20000], Training Loss: 0.0810\n",
            "Epoch [3455/20000], Training Loss: 0.0799\n",
            "Epoch [3456/20000], Training Loss: 0.0800\n",
            "Epoch [3457/20000], Training Loss: 0.0750\n",
            "Epoch [3458/20000], Training Loss: 0.0837\n",
            "Epoch [3459/20000], Training Loss: 0.0757\n",
            "Epoch [3460/20000], Training Loss: 0.0817\n",
            "Epoch [3461/20000], Training Loss: 0.0809\n",
            "Epoch [3462/20000], Training Loss: 0.0799\n",
            "Epoch [3463/20000], Training Loss: 0.0753\n",
            "Epoch [3464/20000], Training Loss: 0.0771\n",
            "Epoch [3465/20000], Training Loss: 0.0806\n",
            "Epoch [3466/20000], Training Loss: 0.0804\n",
            "Epoch [3467/20000], Training Loss: 0.0821\n",
            "Epoch [3468/20000], Training Loss: 0.0803\n",
            "Epoch [3469/20000], Training Loss: 0.0750\n",
            "Epoch [3470/20000], Training Loss: 0.0864\n",
            "Epoch [3471/20000], Training Loss: 0.0772\n",
            "Epoch [3472/20000], Training Loss: 0.0850\n",
            "Epoch [3473/20000], Training Loss: 0.0749\n",
            "Epoch [3474/20000], Training Loss: 0.0736\n",
            "Epoch [3475/20000], Training Loss: 0.0786\n",
            "Epoch [3476/20000], Training Loss: 0.0799\n",
            "Epoch [3477/20000], Training Loss: 0.0803\n",
            "Epoch [3478/20000], Training Loss: 0.0784\n",
            "Epoch [3479/20000], Training Loss: 0.0760\n",
            "Epoch [3480/20000], Training Loss: 0.0833\n",
            "Epoch [3481/20000], Training Loss: 0.0831\n",
            "Epoch [3482/20000], Training Loss: 0.0800\n",
            "Epoch [3483/20000], Training Loss: 0.0798\n",
            "Epoch [3484/20000], Training Loss: 0.0803\n",
            "Epoch [3485/20000], Training Loss: 0.0820\n",
            "Epoch [3486/20000], Training Loss: 0.0798\n",
            "Epoch [3487/20000], Training Loss: 0.0792\n",
            "Epoch [3488/20000], Training Loss: 0.0790\n",
            "Epoch [3489/20000], Training Loss: 0.0786\n",
            "Epoch [3490/20000], Training Loss: 0.0792\n",
            "Epoch [3491/20000], Training Loss: 0.0777\n",
            "Epoch [3492/20000], Training Loss: 0.0791\n",
            "Epoch [3493/20000], Training Loss: 0.0861\n",
            "Epoch [3494/20000], Training Loss: 0.0781\n",
            "Epoch [3495/20000], Training Loss: 0.0839\n",
            "Epoch [3496/20000], Training Loss: 0.0790\n",
            "Epoch [3497/20000], Training Loss: 0.0804\n",
            "Epoch [3498/20000], Training Loss: 0.0776\n",
            "Epoch [3499/20000], Training Loss: 0.0757\n",
            "Epoch [3500/20000], Training Loss: 0.0748\n",
            "Epoch [3501/20000], Training Loss: 0.0796\n",
            "Epoch [3502/20000], Training Loss: 0.0755\n",
            "Epoch [3503/20000], Training Loss: 0.0797\n",
            "Epoch [3504/20000], Training Loss: 0.0770\n",
            "Epoch [3505/20000], Training Loss: 0.0798\n",
            "Epoch [3506/20000], Training Loss: 0.0796\n",
            "Epoch [3507/20000], Training Loss: 0.0838\n",
            "Epoch [3508/20000], Training Loss: 0.0732\n",
            "Epoch [3509/20000], Training Loss: 0.0812\n",
            "Epoch [3510/20000], Training Loss: 0.0773\n",
            "Epoch [3511/20000], Training Loss: 0.0782\n",
            "Epoch [3512/20000], Training Loss: 0.0782\n",
            "Epoch [3513/20000], Training Loss: 0.0858\n",
            "Epoch [3514/20000], Training Loss: 0.0809\n",
            "Epoch [3515/20000], Training Loss: 0.0804\n",
            "Epoch [3516/20000], Training Loss: 0.0816\n",
            "Epoch [3517/20000], Training Loss: 0.0779\n",
            "Epoch [3518/20000], Training Loss: 0.0791\n",
            "Epoch [3519/20000], Training Loss: 0.0781\n",
            "Epoch [3520/20000], Training Loss: 0.0832\n",
            "Epoch [3521/20000], Training Loss: 0.0773\n",
            "Epoch [3522/20000], Training Loss: 0.0750\n",
            "Epoch [3523/20000], Training Loss: 0.0813\n",
            "Epoch [3524/20000], Training Loss: 0.0751\n",
            "Epoch [3525/20000], Training Loss: 0.0725\n",
            "Epoch [3526/20000], Training Loss: 0.0804\n",
            "Epoch [3527/20000], Training Loss: 0.0804\n",
            "Epoch [3528/20000], Training Loss: 0.0811\n",
            "Epoch [3529/20000], Training Loss: 0.0781\n",
            "Epoch [3530/20000], Training Loss: 0.0795\n",
            "Epoch [3531/20000], Training Loss: 0.0791\n",
            "Epoch [3532/20000], Training Loss: 0.0898\n",
            "Epoch [3533/20000], Training Loss: 0.0746\n",
            "Epoch [3534/20000], Training Loss: 0.0762\n",
            "Epoch [3535/20000], Training Loss: 0.0732\n",
            "Epoch [3536/20000], Training Loss: 0.0799\n",
            "Epoch [3537/20000], Training Loss: 0.0825\n",
            "Epoch [3538/20000], Training Loss: 0.0809\n",
            "Epoch [3539/20000], Training Loss: 0.0765\n",
            "Epoch [3540/20000], Training Loss: 0.0783\n",
            "Epoch [3541/20000], Training Loss: 0.0793\n",
            "Epoch [3542/20000], Training Loss: 0.0818\n",
            "Epoch [3543/20000], Training Loss: 0.0740\n",
            "Epoch [3544/20000], Training Loss: 0.0820\n",
            "Epoch [3545/20000], Training Loss: 0.0811\n",
            "Epoch [3546/20000], Training Loss: 0.0812\n",
            "Epoch [3547/20000], Training Loss: 0.0727\n",
            "Epoch [3548/20000], Training Loss: 0.0802\n",
            "Epoch [3549/20000], Training Loss: 0.0761\n",
            "Epoch [3550/20000], Training Loss: 0.0880\n",
            "Epoch [3551/20000], Training Loss: 0.0769\n",
            "Epoch [3552/20000], Training Loss: 0.0793\n",
            "Epoch [3553/20000], Training Loss: 0.0799\n",
            "Epoch [3554/20000], Training Loss: 0.0741\n",
            "Epoch [3555/20000], Training Loss: 0.0745\n",
            "Epoch [3556/20000], Training Loss: 0.0853\n",
            "Epoch [3557/20000], Training Loss: 0.0811\n",
            "Epoch [3558/20000], Training Loss: 0.0743\n",
            "Epoch [3559/20000], Training Loss: 0.0772\n",
            "Epoch [3560/20000], Training Loss: 0.0824\n",
            "Epoch [3561/20000], Training Loss: 0.0862\n",
            "Epoch [3562/20000], Training Loss: 0.0763\n",
            "Epoch [3563/20000], Training Loss: 0.0754\n",
            "Epoch [3564/20000], Training Loss: 0.0820\n",
            "Epoch [3565/20000], Training Loss: 0.0729\n",
            "Epoch [3566/20000], Training Loss: 0.0831\n",
            "Epoch [3567/20000], Training Loss: 0.0732\n",
            "Epoch [3568/20000], Training Loss: 0.0803\n",
            "Epoch [3569/20000], Training Loss: 0.0837\n",
            "Epoch [3570/20000], Training Loss: 0.0758\n",
            "Epoch [3571/20000], Training Loss: 0.0809\n",
            "Epoch [3572/20000], Training Loss: 0.0819\n",
            "Epoch [3573/20000], Training Loss: 0.0790\n",
            "Epoch [3574/20000], Training Loss: 0.0756\n",
            "Epoch [3575/20000], Training Loss: 0.0796\n",
            "Epoch [3576/20000], Training Loss: 0.0793\n",
            "Epoch [3577/20000], Training Loss: 0.0738\n",
            "Epoch [3578/20000], Training Loss: 0.0794\n",
            "Epoch [3579/20000], Training Loss: 0.0835\n",
            "Epoch [3580/20000], Training Loss: 0.0816\n",
            "Epoch [3581/20000], Training Loss: 0.0744\n",
            "Epoch [3582/20000], Training Loss: 0.0802\n",
            "Epoch [3583/20000], Training Loss: 0.0780\n",
            "Epoch [3584/20000], Training Loss: 0.0782\n",
            "Epoch [3585/20000], Training Loss: 0.0755\n",
            "Epoch [3586/20000], Training Loss: 0.0747\n",
            "Epoch [3587/20000], Training Loss: 0.0806\n",
            "Epoch [3588/20000], Training Loss: 0.0766\n",
            "Epoch [3589/20000], Training Loss: 0.0835\n",
            "Epoch [3590/20000], Training Loss: 0.0758\n",
            "Epoch [3591/20000], Training Loss: 0.0812\n",
            "Epoch [3592/20000], Training Loss: 0.0802\n",
            "Epoch [3593/20000], Training Loss: 0.0775\n",
            "Epoch [3594/20000], Training Loss: 0.0729\n",
            "Epoch [3595/20000], Training Loss: 0.0811\n",
            "Epoch [3596/20000], Training Loss: 0.0753\n",
            "Epoch [3597/20000], Training Loss: 0.0772\n",
            "Epoch [3598/20000], Training Loss: 0.0780\n",
            "Epoch [3599/20000], Training Loss: 0.0774\n",
            "Epoch [3600/20000], Training Loss: 0.0831\n",
            "Epoch [3601/20000], Training Loss: 0.0812\n",
            "Epoch [3602/20000], Training Loss: 0.0808\n",
            "Epoch [3603/20000], Training Loss: 0.0798\n",
            "Epoch [3604/20000], Training Loss: 0.0797\n",
            "Epoch [3605/20000], Training Loss: 0.0812\n",
            "Epoch [3606/20000], Training Loss: 0.0856\n",
            "Epoch [3607/20000], Training Loss: 0.0764\n",
            "Epoch [3608/20000], Training Loss: 0.0830\n",
            "Epoch [3609/20000], Training Loss: 0.0816\n",
            "Epoch [3610/20000], Training Loss: 0.0755\n",
            "Epoch [3611/20000], Training Loss: 0.0813\n",
            "Epoch [3612/20000], Training Loss: 0.0829\n",
            "Epoch [3613/20000], Training Loss: 0.0817\n",
            "Epoch [3614/20000], Training Loss: 0.0824\n",
            "Epoch [3615/20000], Training Loss: 0.0837\n",
            "Epoch [3616/20000], Training Loss: 0.0849\n",
            "Epoch [3617/20000], Training Loss: 0.0746\n",
            "Epoch [3618/20000], Training Loss: 0.0828\n",
            "Epoch [3619/20000], Training Loss: 0.0736\n",
            "Epoch [3620/20000], Training Loss: 0.0812\n",
            "Epoch [3621/20000], Training Loss: 0.0823\n",
            "Epoch [3622/20000], Training Loss: 0.0750\n",
            "Epoch [3623/20000], Training Loss: 0.0847\n",
            "Epoch [3624/20000], Training Loss: 0.0795\n",
            "Epoch [3625/20000], Training Loss: 0.0795\n",
            "Epoch [3626/20000], Training Loss: 0.0730\n",
            "Epoch [3627/20000], Training Loss: 0.0790\n",
            "Epoch [3628/20000], Training Loss: 0.0747\n",
            "Epoch [3629/20000], Training Loss: 0.0832\n",
            "Epoch [3630/20000], Training Loss: 0.0798\n",
            "Epoch [3631/20000], Training Loss: 0.0785\n",
            "Epoch [3632/20000], Training Loss: 0.0734\n",
            "Epoch [3633/20000], Training Loss: 0.0774\n",
            "Epoch [3634/20000], Training Loss: 0.0802\n",
            "Epoch [3635/20000], Training Loss: 0.0847\n",
            "Epoch [3636/20000], Training Loss: 0.0759\n",
            "Epoch [3637/20000], Training Loss: 0.0799\n",
            "Epoch [3638/20000], Training Loss: 0.0787\n",
            "Epoch [3639/20000], Training Loss: 0.0807\n",
            "Epoch [3640/20000], Training Loss: 0.0794\n",
            "Epoch [3641/20000], Training Loss: 0.0833\n",
            "Epoch [3642/20000], Training Loss: 0.0833\n",
            "Epoch [3643/20000], Training Loss: 0.0799\n",
            "Epoch [3644/20000], Training Loss: 0.0729\n",
            "Epoch [3645/20000], Training Loss: 0.0748\n",
            "Epoch [3646/20000], Training Loss: 0.0767\n",
            "Epoch [3647/20000], Training Loss: 0.0816\n",
            "Epoch [3648/20000], Training Loss: 0.0810\n",
            "Epoch [3649/20000], Training Loss: 0.0778\n",
            "Epoch [3650/20000], Training Loss: 0.0825\n",
            "Epoch [3651/20000], Training Loss: 0.0739\n",
            "Epoch [3652/20000], Training Loss: 0.0866\n",
            "Epoch [3653/20000], Training Loss: 0.0808\n",
            "Epoch [3654/20000], Training Loss: 0.0823\n",
            "Epoch [3655/20000], Training Loss: 0.0800\n",
            "Epoch [3656/20000], Training Loss: 0.0856\n",
            "Epoch [3657/20000], Training Loss: 0.0779\n",
            "Epoch [3658/20000], Training Loss: 0.0848\n",
            "Epoch [3659/20000], Training Loss: 0.0816\n",
            "Epoch [3660/20000], Training Loss: 0.0823\n",
            "Epoch [3661/20000], Training Loss: 0.0789\n",
            "Epoch [3662/20000], Training Loss: 0.0794\n",
            "Epoch [3663/20000], Training Loss: 0.0791\n",
            "Epoch [3664/20000], Training Loss: 0.0750\n",
            "Epoch [3665/20000], Training Loss: 0.0842\n",
            "Epoch [3666/20000], Training Loss: 0.0824\n",
            "Epoch [3667/20000], Training Loss: 0.0822\n",
            "Epoch [3668/20000], Training Loss: 0.0745\n",
            "Epoch [3669/20000], Training Loss: 0.0840\n",
            "Epoch [3670/20000], Training Loss: 0.0738\n",
            "Epoch [3671/20000], Training Loss: 0.0878\n",
            "Epoch [3672/20000], Training Loss: 0.0789\n",
            "Epoch [3673/20000], Training Loss: 0.0835\n",
            "Epoch [3674/20000], Training Loss: 0.0782\n",
            "Epoch [3675/20000], Training Loss: 0.0793\n",
            "Epoch [3676/20000], Training Loss: 0.0769\n",
            "Epoch [3677/20000], Training Loss: 0.0854\n",
            "Epoch [3678/20000], Training Loss: 0.0837\n",
            "Epoch [3679/20000], Training Loss: 0.0761\n",
            "Epoch [3680/20000], Training Loss: 0.0768\n",
            "Epoch [3681/20000], Training Loss: 0.0831\n",
            "Epoch [3682/20000], Training Loss: 0.0765\n",
            "Epoch [3683/20000], Training Loss: 0.0807\n",
            "Epoch [3684/20000], Training Loss: 0.0880\n",
            "Epoch [3685/20000], Training Loss: 0.0821\n",
            "Epoch [3686/20000], Training Loss: 0.0762\n",
            "Epoch [3687/20000], Training Loss: 0.0791\n",
            "Epoch [3688/20000], Training Loss: 0.0779\n",
            "Epoch [3689/20000], Training Loss: 0.0757\n",
            "Epoch [3690/20000], Training Loss: 0.0746\n",
            "Epoch [3691/20000], Training Loss: 0.0813\n",
            "Epoch [3692/20000], Training Loss: 0.0816\n",
            "Epoch [3693/20000], Training Loss: 0.0776\n",
            "Epoch [3694/20000], Training Loss: 0.0837\n",
            "Epoch [3695/20000], Training Loss: 0.0826\n",
            "Epoch [3696/20000], Training Loss: 0.0793\n",
            "Epoch [3697/20000], Training Loss: 0.0861\n",
            "Epoch [3698/20000], Training Loss: 0.0774\n",
            "Epoch [3699/20000], Training Loss: 0.0799\n",
            "Epoch [3700/20000], Training Loss: 0.0757\n",
            "Epoch [3701/20000], Training Loss: 0.0778\n",
            "Epoch [3702/20000], Training Loss: 0.0801\n",
            "Epoch [3703/20000], Training Loss: 0.0843\n",
            "Epoch [3704/20000], Training Loss: 0.0802\n",
            "Epoch [3705/20000], Training Loss: 0.0812\n",
            "Epoch [3706/20000], Training Loss: 0.0781\n",
            "Epoch [3707/20000], Training Loss: 0.0826\n",
            "Epoch [3708/20000], Training Loss: 0.0771\n",
            "Epoch [3709/20000], Training Loss: 0.0746\n",
            "Epoch [3710/20000], Training Loss: 0.0734\n",
            "Epoch [3711/20000], Training Loss: 0.0807\n",
            "Epoch [3712/20000], Training Loss: 0.0812\n",
            "Epoch [3713/20000], Training Loss: 0.0792\n",
            "Epoch [3714/20000], Training Loss: 0.0845\n",
            "Epoch [3715/20000], Training Loss: 0.0829\n",
            "Epoch [3716/20000], Training Loss: 0.0843\n",
            "Epoch [3717/20000], Training Loss: 0.0779\n",
            "Epoch [3718/20000], Training Loss: 0.0783\n",
            "Epoch [3719/20000], Training Loss: 0.0751\n",
            "Epoch [3720/20000], Training Loss: 0.0785\n",
            "Epoch [3721/20000], Training Loss: 0.0770\n",
            "Epoch [3722/20000], Training Loss: 0.0796\n",
            "Epoch [3723/20000], Training Loss: 0.0841\n",
            "Epoch [3724/20000], Training Loss: 0.0779\n",
            "Epoch [3725/20000], Training Loss: 0.0748\n",
            "Epoch [3726/20000], Training Loss: 0.0816\n",
            "Epoch [3727/20000], Training Loss: 0.0812\n",
            "Epoch [3728/20000], Training Loss: 0.0762\n",
            "Epoch [3729/20000], Training Loss: 0.0789\n",
            "Epoch [3730/20000], Training Loss: 0.0845\n",
            "Epoch [3731/20000], Training Loss: 0.0801\n",
            "Epoch [3732/20000], Training Loss: 0.0776\n",
            "Epoch [3733/20000], Training Loss: 0.0799\n",
            "Epoch [3734/20000], Training Loss: 0.0861\n",
            "Epoch [3735/20000], Training Loss: 0.0839\n",
            "Epoch [3736/20000], Training Loss: 0.0776\n",
            "Epoch [3737/20000], Training Loss: 0.0792\n",
            "Epoch [3738/20000], Training Loss: 0.0786\n",
            "Epoch [3739/20000], Training Loss: 0.0770\n",
            "Epoch [3740/20000], Training Loss: 0.0802\n",
            "Epoch [3741/20000], Training Loss: 0.0750\n",
            "Epoch [3742/20000], Training Loss: 0.0808\n",
            "Epoch [3743/20000], Training Loss: 0.0749\n",
            "Epoch [3744/20000], Training Loss: 0.0799\n",
            "Epoch [3745/20000], Training Loss: 0.0856\n",
            "Epoch [3746/20000], Training Loss: 0.0858\n",
            "Epoch [3747/20000], Training Loss: 0.0735\n",
            "Epoch [3748/20000], Training Loss: 0.0780\n",
            "Epoch [3749/20000], Training Loss: 0.0856\n",
            "Epoch [3750/20000], Training Loss: 0.0772\n",
            "Epoch [3751/20000], Training Loss: 0.0796\n",
            "Epoch [3752/20000], Training Loss: 0.0755\n",
            "Epoch [3753/20000], Training Loss: 0.0736\n",
            "Epoch [3754/20000], Training Loss: 0.0767\n",
            "Epoch [3755/20000], Training Loss: 0.0754\n",
            "Epoch [3756/20000], Training Loss: 0.0801\n",
            "Epoch [3757/20000], Training Loss: 0.0801\n",
            "Epoch [3758/20000], Training Loss: 0.0775\n",
            "Epoch [3759/20000], Training Loss: 0.0825\n",
            "Epoch [3760/20000], Training Loss: 0.0801\n",
            "Epoch [3761/20000], Training Loss: 0.0740\n",
            "Epoch [3762/20000], Training Loss: 0.0834\n",
            "Epoch [3763/20000], Training Loss: 0.0875\n",
            "Epoch [3764/20000], Training Loss: 0.0799\n",
            "Epoch [3765/20000], Training Loss: 0.0775\n",
            "Epoch [3766/20000], Training Loss: 0.0801\n",
            "Epoch [3767/20000], Training Loss: 0.0792\n",
            "Epoch [3768/20000], Training Loss: 0.0816\n",
            "Epoch [3769/20000], Training Loss: 0.0856\n",
            "Epoch [3770/20000], Training Loss: 0.0814\n",
            "Epoch [3771/20000], Training Loss: 0.0855\n",
            "Epoch [3772/20000], Training Loss: 0.0821\n",
            "Epoch [3773/20000], Training Loss: 0.0792\n",
            "Epoch [3774/20000], Training Loss: 0.0784\n",
            "Epoch [3775/20000], Training Loss: 0.0820\n",
            "Epoch [3776/20000], Training Loss: 0.0805\n",
            "Epoch [3777/20000], Training Loss: 0.0743\n",
            "Epoch [3778/20000], Training Loss: 0.0742\n",
            "Epoch [3779/20000], Training Loss: 0.0838\n",
            "Epoch [3780/20000], Training Loss: 0.0785\n",
            "Epoch [3781/20000], Training Loss: 0.0797\n",
            "Epoch [3782/20000], Training Loss: 0.0742\n",
            "Epoch [3783/20000], Training Loss: 0.0792\n",
            "Epoch [3784/20000], Training Loss: 0.0810\n",
            "Epoch [3785/20000], Training Loss: 0.0774\n",
            "Epoch [3786/20000], Training Loss: 0.0787\n",
            "Epoch [3787/20000], Training Loss: 0.0771\n",
            "Epoch [3788/20000], Training Loss: 0.0802\n",
            "Epoch [3789/20000], Training Loss: 0.0775\n",
            "Epoch [3790/20000], Training Loss: 0.0803\n",
            "Epoch [3791/20000], Training Loss: 0.0762\n",
            "Epoch [3792/20000], Training Loss: 0.0736\n",
            "Epoch [3793/20000], Training Loss: 0.0783\n",
            "Epoch [3794/20000], Training Loss: 0.0763\n",
            "Epoch [3795/20000], Training Loss: 0.0714\n",
            "Epoch [3796/20000], Training Loss: 0.0796\n",
            "Epoch [3797/20000], Training Loss: 0.0858\n",
            "Epoch [3798/20000], Training Loss: 0.0829\n",
            "Epoch [3799/20000], Training Loss: 0.0852\n",
            "Epoch [3800/20000], Training Loss: 0.0816\n",
            "Epoch [3801/20000], Training Loss: 0.0741\n",
            "Epoch [3802/20000], Training Loss: 0.0755\n",
            "Epoch [3803/20000], Training Loss: 0.0822\n",
            "Epoch [3804/20000], Training Loss: 0.0850\n",
            "Epoch [3805/20000], Training Loss: 0.0794\n",
            "Epoch [3806/20000], Training Loss: 0.0767\n",
            "Epoch [3807/20000], Training Loss: 0.0820\n",
            "Epoch [3808/20000], Training Loss: 0.0767\n",
            "Epoch [3809/20000], Training Loss: 0.0811\n",
            "Epoch [3810/20000], Training Loss: 0.0770\n",
            "Epoch [3811/20000], Training Loss: 0.0830\n",
            "Epoch [3812/20000], Training Loss: 0.0816\n",
            "Epoch [3813/20000], Training Loss: 0.0757\n",
            "Epoch [3814/20000], Training Loss: 0.0854\n",
            "Epoch [3815/20000], Training Loss: 0.0819\n",
            "Epoch [3816/20000], Training Loss: 0.0775\n",
            "Epoch [3817/20000], Training Loss: 0.0772\n",
            "Epoch [3818/20000], Training Loss: 0.0818\n",
            "Epoch [3819/20000], Training Loss: 0.0808\n",
            "Epoch [3820/20000], Training Loss: 0.0803\n",
            "Epoch [3821/20000], Training Loss: 0.0776\n",
            "Epoch [3822/20000], Training Loss: 0.0778\n",
            "Epoch [3823/20000], Training Loss: 0.0769\n",
            "Epoch [3824/20000], Training Loss: 0.0770\n",
            "Epoch [3825/20000], Training Loss: 0.0813\n",
            "Epoch [3826/20000], Training Loss: 0.0790\n",
            "Epoch [3827/20000], Training Loss: 0.0866\n",
            "Epoch [3828/20000], Training Loss: 0.0787\n",
            "Epoch [3829/20000], Training Loss: 0.0804\n",
            "Epoch [3830/20000], Training Loss: 0.0738\n",
            "Epoch [3831/20000], Training Loss: 0.0739\n",
            "Epoch [3832/20000], Training Loss: 0.0796\n",
            "Epoch [3833/20000], Training Loss: 0.0750\n",
            "Epoch [3834/20000], Training Loss: 0.0807\n",
            "Epoch [3835/20000], Training Loss: 0.0750\n",
            "Epoch [3836/20000], Training Loss: 0.0760\n",
            "Epoch [3837/20000], Training Loss: 0.0859\n",
            "Epoch [3838/20000], Training Loss: 0.0822\n",
            "Epoch [3839/20000], Training Loss: 0.0795\n",
            "Epoch [3840/20000], Training Loss: 0.0807\n",
            "Epoch [3841/20000], Training Loss: 0.0875\n",
            "Epoch [3842/20000], Training Loss: 0.0770\n",
            "Epoch [3843/20000], Training Loss: 0.0827\n",
            "Epoch [3844/20000], Training Loss: 0.0789\n",
            "Epoch [3845/20000], Training Loss: 0.0749\n",
            "Epoch [3846/20000], Training Loss: 0.0734\n",
            "Epoch [3847/20000], Training Loss: 0.0804\n",
            "Epoch [3848/20000], Training Loss: 0.0819\n",
            "Epoch [3849/20000], Training Loss: 0.0812\n",
            "Epoch [3850/20000], Training Loss: 0.0731\n",
            "Epoch [3851/20000], Training Loss: 0.0775\n",
            "Epoch [3852/20000], Training Loss: 0.0763\n",
            "Epoch [3853/20000], Training Loss: 0.0738\n",
            "Epoch [3854/20000], Training Loss: 0.0759\n",
            "Epoch [3855/20000], Training Loss: 0.0730\n",
            "Epoch [3856/20000], Training Loss: 0.0737\n",
            "Epoch [3857/20000], Training Loss: 0.0786\n",
            "Epoch [3858/20000], Training Loss: 0.0844\n",
            "Epoch [3859/20000], Training Loss: 0.0754\n",
            "Epoch [3860/20000], Training Loss: 0.0752\n",
            "Epoch [3861/20000], Training Loss: 0.0862\n",
            "Epoch [3862/20000], Training Loss: 0.0734\n",
            "Epoch [3863/20000], Training Loss: 0.0755\n",
            "Epoch [3864/20000], Training Loss: 0.0738\n",
            "Epoch [3865/20000], Training Loss: 0.0792\n",
            "Epoch [3866/20000], Training Loss: 0.0750\n",
            "Epoch [3867/20000], Training Loss: 0.0785\n",
            "Epoch [3868/20000], Training Loss: 0.0811\n",
            "Epoch [3869/20000], Training Loss: 0.0818\n",
            "Epoch [3870/20000], Training Loss: 0.0811\n",
            "Epoch [3871/20000], Training Loss: 0.0760\n",
            "Epoch [3872/20000], Training Loss: 0.0858\n",
            "Epoch [3873/20000], Training Loss: 0.0774\n",
            "Epoch [3874/20000], Training Loss: 0.0786\n",
            "Epoch [3875/20000], Training Loss: 0.0745\n",
            "Epoch [3876/20000], Training Loss: 0.0872\n",
            "Epoch [3877/20000], Training Loss: 0.0861\n",
            "Epoch [3878/20000], Training Loss: 0.0815\n",
            "Epoch [3879/20000], Training Loss: 0.0827\n",
            "Epoch [3880/20000], Training Loss: 0.0817\n",
            "Epoch [3881/20000], Training Loss: 0.0744\n",
            "Epoch [3882/20000], Training Loss: 0.0810\n",
            "Epoch [3883/20000], Training Loss: 0.0747\n",
            "Epoch [3884/20000], Training Loss: 0.0787\n",
            "Epoch [3885/20000], Training Loss: 0.0768\n",
            "Epoch [3886/20000], Training Loss: 0.0810\n",
            "Epoch [3887/20000], Training Loss: 0.0849\n",
            "Epoch [3888/20000], Training Loss: 0.0826\n",
            "Epoch [3889/20000], Training Loss: 0.0810\n",
            "Epoch [3890/20000], Training Loss: 0.0844\n",
            "Epoch [3891/20000], Training Loss: 0.0751\n",
            "Epoch [3892/20000], Training Loss: 0.0788\n",
            "Epoch [3893/20000], Training Loss: 0.0765\n",
            "Epoch [3894/20000], Training Loss: 0.0830\n",
            "Epoch [3895/20000], Training Loss: 0.0826\n",
            "Epoch [3896/20000], Training Loss: 0.0838\n",
            "Epoch [3897/20000], Training Loss: 0.0764\n",
            "Epoch [3898/20000], Training Loss: 0.0804\n",
            "Epoch [3899/20000], Training Loss: 0.0843\n",
            "Epoch [3900/20000], Training Loss: 0.0804\n",
            "Epoch [3901/20000], Training Loss: 0.0803\n",
            "Epoch [3902/20000], Training Loss: 0.0734\n",
            "Epoch [3903/20000], Training Loss: 0.0744\n",
            "Epoch [3904/20000], Training Loss: 0.0864\n",
            "Epoch [3905/20000], Training Loss: 0.0858\n",
            "Epoch [3906/20000], Training Loss: 0.0771\n",
            "Epoch [3907/20000], Training Loss: 0.0809\n",
            "Epoch [3908/20000], Training Loss: 0.0860\n",
            "Epoch [3909/20000], Training Loss: 0.0776\n",
            "Epoch [3910/20000], Training Loss: 0.0798\n",
            "Epoch [3911/20000], Training Loss: 0.0790\n",
            "Epoch [3912/20000], Training Loss: 0.0795\n",
            "Epoch [3913/20000], Training Loss: 0.0823\n",
            "Epoch [3914/20000], Training Loss: 0.0747\n",
            "Epoch [3915/20000], Training Loss: 0.0805\n",
            "Epoch [3916/20000], Training Loss: 0.0846\n",
            "Epoch [3917/20000], Training Loss: 0.0745\n",
            "Epoch [3918/20000], Training Loss: 0.0818\n",
            "Epoch [3919/20000], Training Loss: 0.0763\n",
            "Epoch [3920/20000], Training Loss: 0.0791\n",
            "Epoch [3921/20000], Training Loss: 0.0843\n",
            "Epoch [3922/20000], Training Loss: 0.0838\n",
            "Epoch [3923/20000], Training Loss: 0.0753\n",
            "Epoch [3924/20000], Training Loss: 0.0836\n",
            "Epoch [3925/20000], Training Loss: 0.0795\n",
            "Epoch [3926/20000], Training Loss: 0.0795\n",
            "Epoch [3927/20000], Training Loss: 0.0772\n",
            "Epoch [3928/20000], Training Loss: 0.0741\n",
            "Epoch [3929/20000], Training Loss: 0.0754\n",
            "Epoch [3930/20000], Training Loss: 0.0793\n",
            "Epoch [3931/20000], Training Loss: 0.0774\n",
            "Epoch [3932/20000], Training Loss: 0.0791\n",
            "Epoch [3933/20000], Training Loss: 0.0776\n",
            "Epoch [3934/20000], Training Loss: 0.0814\n",
            "Epoch [3935/20000], Training Loss: 0.0822\n",
            "Epoch [3936/20000], Training Loss: 0.0822\n",
            "Epoch [3937/20000], Training Loss: 0.0751\n",
            "Epoch [3938/20000], Training Loss: 0.0799\n",
            "Epoch [3939/20000], Training Loss: 0.0836\n",
            "Epoch [3940/20000], Training Loss: 0.0811\n",
            "Epoch [3941/20000], Training Loss: 0.0843\n",
            "Epoch [3942/20000], Training Loss: 0.0764\n",
            "Epoch [3943/20000], Training Loss: 0.0818\n",
            "Epoch [3944/20000], Training Loss: 0.0796\n",
            "Epoch [3945/20000], Training Loss: 0.0834\n",
            "Epoch [3946/20000], Training Loss: 0.0836\n",
            "Epoch [3947/20000], Training Loss: 0.0805\n",
            "Epoch [3948/20000], Training Loss: 0.0818\n",
            "Epoch [3949/20000], Training Loss: 0.0780\n",
            "Epoch [3950/20000], Training Loss: 0.0807\n",
            "Epoch [3951/20000], Training Loss: 0.0800\n",
            "Epoch [3952/20000], Training Loss: 0.0789\n",
            "Epoch [3953/20000], Training Loss: 0.0784\n",
            "Epoch [3954/20000], Training Loss: 0.0827\n",
            "Epoch [3955/20000], Training Loss: 0.0811\n",
            "Epoch [3956/20000], Training Loss: 0.0826\n",
            "Epoch [3957/20000], Training Loss: 0.0809\n",
            "Epoch [3958/20000], Training Loss: 0.0838\n",
            "Epoch [3959/20000], Training Loss: 0.0746\n",
            "Epoch [3960/20000], Training Loss: 0.0811\n",
            "Epoch [3961/20000], Training Loss: 0.0769\n",
            "Epoch [3962/20000], Training Loss: 0.0861\n",
            "Epoch [3963/20000], Training Loss: 0.0785\n",
            "Epoch [3964/20000], Training Loss: 0.0816\n",
            "Epoch [3965/20000], Training Loss: 0.0784\n",
            "Epoch [3966/20000], Training Loss: 0.0737\n",
            "Epoch [3967/20000], Training Loss: 0.0755\n",
            "Epoch [3968/20000], Training Loss: 0.0748\n",
            "Epoch [3969/20000], Training Loss: 0.0785\n",
            "Epoch [3970/20000], Training Loss: 0.0740\n",
            "Epoch [3971/20000], Training Loss: 0.0831\n",
            "Epoch [3972/20000], Training Loss: 0.0728\n",
            "Epoch [3973/20000], Training Loss: 0.0746\n",
            "Epoch [3974/20000], Training Loss: 0.0755\n",
            "Epoch [3975/20000], Training Loss: 0.0813\n",
            "Epoch [3976/20000], Training Loss: 0.0753\n",
            "Epoch [3977/20000], Training Loss: 0.0838\n",
            "Epoch [3978/20000], Training Loss: 0.0776\n",
            "Epoch [3979/20000], Training Loss: 0.0784\n",
            "Epoch [3980/20000], Training Loss: 0.0761\n",
            "Epoch [3981/20000], Training Loss: 0.0836\n",
            "Epoch [3982/20000], Training Loss: 0.0824\n",
            "Epoch [3983/20000], Training Loss: 0.0792\n",
            "Epoch [3984/20000], Training Loss: 0.0768\n",
            "Epoch [3985/20000], Training Loss: 0.0863\n",
            "Epoch [3986/20000], Training Loss: 0.0737\n",
            "Epoch [3987/20000], Training Loss: 0.0825\n",
            "Epoch [3988/20000], Training Loss: 0.0757\n",
            "Epoch [3989/20000], Training Loss: 0.0737\n",
            "Epoch [3990/20000], Training Loss: 0.0778\n",
            "Epoch [3991/20000], Training Loss: 0.0802\n",
            "Epoch [3992/20000], Training Loss: 0.0830\n",
            "Epoch [3993/20000], Training Loss: 0.0759\n",
            "Epoch [3994/20000], Training Loss: 0.0839\n",
            "Epoch [3995/20000], Training Loss: 0.0777\n",
            "Epoch [3996/20000], Training Loss: 0.0748\n",
            "Epoch [3997/20000], Training Loss: 0.0804\n",
            "Epoch [3998/20000], Training Loss: 0.0743\n",
            "Epoch [3999/20000], Training Loss: 0.0852\n",
            "Epoch [4000/20000], Training Loss: 0.0795\n",
            "Epoch [4001/20000], Training Loss: 0.0796\n",
            "Epoch [4002/20000], Training Loss: 0.0856\n",
            "Epoch [4003/20000], Training Loss: 0.0758\n",
            "Epoch [4004/20000], Training Loss: 0.0769\n",
            "Epoch [4005/20000], Training Loss: 0.0824\n",
            "Epoch [4006/20000], Training Loss: 0.0804\n",
            "Epoch [4007/20000], Training Loss: 0.0772\n",
            "Epoch [4008/20000], Training Loss: 0.0846\n",
            "Epoch [4009/20000], Training Loss: 0.0783\n",
            "Epoch [4010/20000], Training Loss: 0.0803\n",
            "Epoch [4011/20000], Training Loss: 0.0728\n",
            "Epoch [4012/20000], Training Loss: 0.0829\n",
            "Epoch [4013/20000], Training Loss: 0.0873\n",
            "Epoch [4014/20000], Training Loss: 0.0764\n",
            "Epoch [4015/20000], Training Loss: 0.0750\n",
            "Epoch [4016/20000], Training Loss: 0.0817\n",
            "Epoch [4017/20000], Training Loss: 0.0794\n",
            "Epoch [4018/20000], Training Loss: 0.0791\n",
            "Epoch [4019/20000], Training Loss: 0.0797\n",
            "Epoch [4020/20000], Training Loss: 0.0777\n",
            "Epoch [4021/20000], Training Loss: 0.0846\n",
            "Epoch [4022/20000], Training Loss: 0.0762\n",
            "Epoch [4023/20000], Training Loss: 0.0846\n",
            "Epoch [4024/20000], Training Loss: 0.0826\n",
            "Epoch [4025/20000], Training Loss: 0.0747\n",
            "Epoch [4026/20000], Training Loss: 0.0820\n",
            "Epoch [4027/20000], Training Loss: 0.0739\n",
            "Epoch [4028/20000], Training Loss: 0.0793\n",
            "Epoch [4029/20000], Training Loss: 0.0803\n",
            "Epoch [4030/20000], Training Loss: 0.0827\n",
            "Epoch [4031/20000], Training Loss: 0.0781\n",
            "Epoch [4032/20000], Training Loss: 0.0755\n",
            "Epoch [4033/20000], Training Loss: 0.0780\n",
            "Epoch [4034/20000], Training Loss: 0.0871\n",
            "Epoch [4035/20000], Training Loss: 0.0790\n",
            "Epoch [4036/20000], Training Loss: 0.0797\n",
            "Epoch [4037/20000], Training Loss: 0.0824\n",
            "Epoch [4038/20000], Training Loss: 0.0823\n",
            "Epoch [4039/20000], Training Loss: 0.0818\n",
            "Epoch [4040/20000], Training Loss: 0.0871\n",
            "Epoch [4041/20000], Training Loss: 0.0744\n",
            "Epoch [4042/20000], Training Loss: 0.0775\n",
            "Epoch [4043/20000], Training Loss: 0.0742\n",
            "Epoch [4044/20000], Training Loss: 0.0735\n",
            "Epoch [4045/20000], Training Loss: 0.0814\n",
            "Epoch [4046/20000], Training Loss: 0.0774\n",
            "Epoch [4047/20000], Training Loss: 0.0770\n",
            "Epoch [4048/20000], Training Loss: 0.0770\n",
            "Epoch [4049/20000], Training Loss: 0.0823\n",
            "Epoch [4050/20000], Training Loss: 0.0782\n",
            "Epoch [4051/20000], Training Loss: 0.0738\n",
            "Epoch [4052/20000], Training Loss: 0.0739\n",
            "Epoch [4053/20000], Training Loss: 0.0749\n",
            "Epoch [4054/20000], Training Loss: 0.0814\n",
            "Epoch [4055/20000], Training Loss: 0.0826\n",
            "Epoch [4056/20000], Training Loss: 0.0810\n",
            "Epoch [4057/20000], Training Loss: 0.0762\n",
            "Epoch [4058/20000], Training Loss: 0.0833\n",
            "Epoch [4059/20000], Training Loss: 0.0803\n",
            "Epoch [4060/20000], Training Loss: 0.0821\n",
            "Epoch [4061/20000], Training Loss: 0.0768\n",
            "Epoch [4062/20000], Training Loss: 0.0819\n",
            "Epoch [4063/20000], Training Loss: 0.0809\n",
            "Epoch [4064/20000], Training Loss: 0.0771\n",
            "Epoch [4065/20000], Training Loss: 0.0824\n",
            "Epoch [4066/20000], Training Loss: 0.0730\n",
            "Epoch [4067/20000], Training Loss: 0.0811\n",
            "Epoch [4068/20000], Training Loss: 0.0837\n",
            "Epoch [4069/20000], Training Loss: 0.0849\n",
            "Epoch [4070/20000], Training Loss: 0.0797\n",
            "Epoch [4071/20000], Training Loss: 0.0785\n",
            "Epoch [4072/20000], Training Loss: 0.0850\n",
            "Epoch [4073/20000], Training Loss: 0.0801\n",
            "Epoch [4074/20000], Training Loss: 0.0781\n",
            "Epoch [4075/20000], Training Loss: 0.0751\n",
            "Epoch [4076/20000], Training Loss: 0.0803\n",
            "Epoch [4077/20000], Training Loss: 0.0751\n",
            "Epoch [4078/20000], Training Loss: 0.0780\n",
            "Epoch [4079/20000], Training Loss: 0.0783\n",
            "Epoch [4080/20000], Training Loss: 0.0758\n",
            "Epoch [4081/20000], Training Loss: 0.0817\n",
            "Epoch [4082/20000], Training Loss: 0.0858\n",
            "Epoch [4083/20000], Training Loss: 0.0829\n",
            "Epoch [4084/20000], Training Loss: 0.0873\n",
            "Epoch [4085/20000], Training Loss: 0.0786\n",
            "Epoch [4086/20000], Training Loss: 0.0817\n",
            "Epoch [4087/20000], Training Loss: 0.0751\n",
            "Epoch [4088/20000], Training Loss: 0.0818\n",
            "Epoch [4089/20000], Training Loss: 0.0810\n",
            "Epoch [4090/20000], Training Loss: 0.0774\n",
            "Epoch [4091/20000], Training Loss: 0.0820\n",
            "Epoch [4092/20000], Training Loss: 0.0793\n",
            "Epoch [4093/20000], Training Loss: 0.0757\n",
            "Epoch [4094/20000], Training Loss: 0.0812\n",
            "Epoch [4095/20000], Training Loss: 0.0798\n",
            "Epoch [4096/20000], Training Loss: 0.0848\n",
            "Epoch [4097/20000], Training Loss: 0.0809\n",
            "Epoch [4098/20000], Training Loss: 0.0881\n",
            "Epoch [4099/20000], Training Loss: 0.0844\n",
            "Epoch [4100/20000], Training Loss: 0.0763\n",
            "Epoch [4101/20000], Training Loss: 0.0788\n",
            "Epoch [4102/20000], Training Loss: 0.0806\n",
            "Epoch [4103/20000], Training Loss: 0.0842\n",
            "Epoch [4104/20000], Training Loss: 0.0809\n",
            "Epoch [4105/20000], Training Loss: 0.0782\n",
            "Epoch [4106/20000], Training Loss: 0.0774\n",
            "Epoch [4107/20000], Training Loss: 0.0859\n",
            "Epoch [4108/20000], Training Loss: 0.0767\n",
            "Epoch [4109/20000], Training Loss: 0.0733\n",
            "Epoch [4110/20000], Training Loss: 0.0791\n",
            "Epoch [4111/20000], Training Loss: 0.0819\n",
            "Epoch [4112/20000], Training Loss: 0.0780\n",
            "Epoch [4113/20000], Training Loss: 0.0783\n",
            "Epoch [4114/20000], Training Loss: 0.0794\n",
            "Epoch [4115/20000], Training Loss: 0.0755\n",
            "Epoch [4116/20000], Training Loss: 0.0825\n",
            "Epoch [4117/20000], Training Loss: 0.0794\n",
            "Epoch [4118/20000], Training Loss: 0.0853\n",
            "Epoch [4119/20000], Training Loss: 0.0850\n",
            "Epoch [4120/20000], Training Loss: 0.0749\n",
            "Epoch [4121/20000], Training Loss: 0.0802\n",
            "Epoch [4122/20000], Training Loss: 0.0849\n",
            "Epoch [4123/20000], Training Loss: 0.0772\n",
            "Epoch [4124/20000], Training Loss: 0.0784\n",
            "Epoch [4125/20000], Training Loss: 0.0797\n",
            "Epoch [4126/20000], Training Loss: 0.0818\n",
            "Epoch [4127/20000], Training Loss: 0.0785\n",
            "Epoch [4128/20000], Training Loss: 0.0872\n",
            "Epoch [4129/20000], Training Loss: 0.0773\n",
            "Epoch [4130/20000], Training Loss: 0.0855\n",
            "Epoch [4131/20000], Training Loss: 0.0792\n",
            "Epoch [4132/20000], Training Loss: 0.0789\n",
            "Epoch [4133/20000], Training Loss: 0.0758\n",
            "Epoch [4134/20000], Training Loss: 0.0754\n",
            "Epoch [4135/20000], Training Loss: 0.0828\n",
            "Epoch [4136/20000], Training Loss: 0.0815\n",
            "Epoch [4137/20000], Training Loss: 0.0813\n",
            "Epoch [4138/20000], Training Loss: 0.0825\n",
            "Epoch [4139/20000], Training Loss: 0.0810\n",
            "Epoch [4140/20000], Training Loss: 0.0791\n",
            "Epoch [4141/20000], Training Loss: 0.0788\n",
            "Epoch [4142/20000], Training Loss: 0.0845\n",
            "Epoch [4143/20000], Training Loss: 0.0742\n",
            "Epoch [4144/20000], Training Loss: 0.0850\n",
            "Epoch [4145/20000], Training Loss: 0.0864\n",
            "Epoch [4146/20000], Training Loss: 0.0765\n",
            "Epoch [4147/20000], Training Loss: 0.0744\n",
            "Epoch [4148/20000], Training Loss: 0.0812\n",
            "Epoch [4149/20000], Training Loss: 0.0742\n",
            "Epoch [4150/20000], Training Loss: 0.0814\n",
            "Epoch [4151/20000], Training Loss: 0.0786\n",
            "Epoch [4152/20000], Training Loss: 0.0771\n",
            "Epoch [4153/20000], Training Loss: 0.0759\n",
            "Epoch [4154/20000], Training Loss: 0.0863\n",
            "Epoch [4155/20000], Training Loss: 0.0809\n",
            "Epoch [4156/20000], Training Loss: 0.0833\n",
            "Epoch [4157/20000], Training Loss: 0.0828\n",
            "Epoch [4158/20000], Training Loss: 0.0787\n",
            "Epoch [4159/20000], Training Loss: 0.0779\n",
            "Epoch [4160/20000], Training Loss: 0.0831\n",
            "Epoch [4161/20000], Training Loss: 0.0774\n",
            "Epoch [4162/20000], Training Loss: 0.0752\n",
            "Epoch [4163/20000], Training Loss: 0.0779\n",
            "Epoch [4164/20000], Training Loss: 0.0764\n",
            "Epoch [4165/20000], Training Loss: 0.0834\n",
            "Epoch [4166/20000], Training Loss: 0.0745\n",
            "Epoch [4167/20000], Training Loss: 0.0815\n",
            "Epoch [4168/20000], Training Loss: 0.0768\n",
            "Epoch [4169/20000], Training Loss: 0.0827\n",
            "Epoch [4170/20000], Training Loss: 0.0755\n",
            "Epoch [4171/20000], Training Loss: 0.0795\n",
            "Epoch [4172/20000], Training Loss: 0.0798\n",
            "Epoch [4173/20000], Training Loss: 0.0794\n",
            "Epoch [4174/20000], Training Loss: 0.0762\n",
            "Epoch [4175/20000], Training Loss: 0.0808\n",
            "Epoch [4176/20000], Training Loss: 0.0830\n",
            "Epoch [4177/20000], Training Loss: 0.0777\n",
            "Epoch [4178/20000], Training Loss: 0.0817\n",
            "Epoch [4179/20000], Training Loss: 0.0800\n",
            "Epoch [4180/20000], Training Loss: 0.0782\n",
            "Epoch [4181/20000], Training Loss: 0.0854\n",
            "Epoch [4182/20000], Training Loss: 0.0831\n",
            "Epoch [4183/20000], Training Loss: 0.0784\n",
            "Epoch [4184/20000], Training Loss: 0.0795\n",
            "Epoch [4185/20000], Training Loss: 0.0786\n",
            "Epoch [4186/20000], Training Loss: 0.0762\n",
            "Epoch [4187/20000], Training Loss: 0.0806\n",
            "Epoch [4188/20000], Training Loss: 0.0780\n",
            "Epoch [4189/20000], Training Loss: 0.0826\n",
            "Epoch [4190/20000], Training Loss: 0.0790\n",
            "Epoch [4191/20000], Training Loss: 0.0783\n",
            "Epoch [4192/20000], Training Loss: 0.0827\n",
            "Epoch [4193/20000], Training Loss: 0.0732\n",
            "Epoch [4194/20000], Training Loss: 0.0766\n",
            "Epoch [4195/20000], Training Loss: 0.0850\n",
            "Epoch [4196/20000], Training Loss: 0.0782\n",
            "Epoch [4197/20000], Training Loss: 0.0739\n",
            "Epoch [4198/20000], Training Loss: 0.0765\n",
            "Epoch [4199/20000], Training Loss: 0.0769\n",
            "Epoch [4200/20000], Training Loss: 0.0815\n",
            "Epoch [4201/20000], Training Loss: 0.0742\n",
            "Epoch [4202/20000], Training Loss: 0.0794\n",
            "Epoch [4203/20000], Training Loss: 0.0810\n",
            "Epoch [4204/20000], Training Loss: 0.0802\n",
            "Epoch [4205/20000], Training Loss: 0.0761\n",
            "Epoch [4206/20000], Training Loss: 0.0778\n",
            "Epoch [4207/20000], Training Loss: 0.0768\n",
            "Epoch [4208/20000], Training Loss: 0.0834\n",
            "Epoch [4209/20000], Training Loss: 0.0778\n",
            "Epoch [4210/20000], Training Loss: 0.0814\n",
            "Epoch [4211/20000], Training Loss: 0.0801\n",
            "Epoch [4212/20000], Training Loss: 0.0768\n",
            "Epoch [4213/20000], Training Loss: 0.0746\n",
            "Epoch [4214/20000], Training Loss: 0.0806\n",
            "Epoch [4215/20000], Training Loss: 0.0784\n",
            "Epoch [4216/20000], Training Loss: 0.0807\n",
            "Epoch [4217/20000], Training Loss: 0.0793\n",
            "Epoch [4218/20000], Training Loss: 0.0799\n",
            "Epoch [4219/20000], Training Loss: 0.0846\n",
            "Epoch [4220/20000], Training Loss: 0.0789\n",
            "Epoch [4221/20000], Training Loss: 0.0783\n",
            "Epoch [4222/20000], Training Loss: 0.0818\n",
            "Epoch [4223/20000], Training Loss: 0.0837\n",
            "Epoch [4224/20000], Training Loss: 0.0747\n",
            "Epoch [4225/20000], Training Loss: 0.0797\n",
            "Epoch [4226/20000], Training Loss: 0.0803\n",
            "Epoch [4227/20000], Training Loss: 0.0807\n",
            "Epoch [4228/20000], Training Loss: 0.0794\n",
            "Epoch [4229/20000], Training Loss: 0.0826\n",
            "Epoch [4230/20000], Training Loss: 0.0823\n",
            "Epoch [4231/20000], Training Loss: 0.0850\n",
            "Epoch [4232/20000], Training Loss: 0.0798\n",
            "Epoch [4233/20000], Training Loss: 0.0754\n",
            "Epoch [4234/20000], Training Loss: 0.0791\n",
            "Epoch [4235/20000], Training Loss: 0.0795\n",
            "Epoch [4236/20000], Training Loss: 0.0848\n",
            "Epoch [4237/20000], Training Loss: 0.0863\n",
            "Epoch [4238/20000], Training Loss: 0.0797\n",
            "Epoch [4239/20000], Training Loss: 0.0814\n",
            "Epoch [4240/20000], Training Loss: 0.0749\n",
            "Epoch [4241/20000], Training Loss: 0.0864\n",
            "Epoch [4242/20000], Training Loss: 0.0802\n",
            "Epoch [4243/20000], Training Loss: 0.0778\n",
            "Epoch [4244/20000], Training Loss: 0.0787\n",
            "Epoch [4245/20000], Training Loss: 0.0752\n",
            "Epoch [4246/20000], Training Loss: 0.0782\n",
            "Epoch [4247/20000], Training Loss: 0.0825\n",
            "Epoch [4248/20000], Training Loss: 0.0749\n",
            "Epoch [4249/20000], Training Loss: 0.0779\n",
            "Epoch [4250/20000], Training Loss: 0.0819\n",
            "Epoch [4251/20000], Training Loss: 0.0798\n",
            "Epoch [4252/20000], Training Loss: 0.0734\n",
            "Epoch [4253/20000], Training Loss: 0.0760\n",
            "Epoch [4254/20000], Training Loss: 0.0820\n",
            "Epoch [4255/20000], Training Loss: 0.0728\n",
            "Epoch [4256/20000], Training Loss: 0.0755\n",
            "Epoch [4257/20000], Training Loss: 0.0779\n",
            "Epoch [4258/20000], Training Loss: 0.0770\n",
            "Epoch [4259/20000], Training Loss: 0.0860\n",
            "Epoch [4260/20000], Training Loss: 0.0792\n",
            "Epoch [4261/20000], Training Loss: 0.0796\n",
            "Epoch [4262/20000], Training Loss: 0.0823\n",
            "Epoch [4263/20000], Training Loss: 0.0830\n",
            "Epoch [4264/20000], Training Loss: 0.0764\n",
            "Epoch [4265/20000], Training Loss: 0.0782\n",
            "Epoch [4266/20000], Training Loss: 0.0779\n",
            "Epoch [4267/20000], Training Loss: 0.0815\n",
            "Epoch [4268/20000], Training Loss: 0.0748\n",
            "Epoch [4269/20000], Training Loss: 0.0824\n",
            "Epoch [4270/20000], Training Loss: 0.0799\n",
            "Epoch [4271/20000], Training Loss: 0.0781\n",
            "Epoch [4272/20000], Training Loss: 0.0830\n",
            "Epoch [4273/20000], Training Loss: 0.0825\n",
            "Epoch [4274/20000], Training Loss: 0.0792\n",
            "Epoch [4275/20000], Training Loss: 0.0813\n",
            "Epoch [4276/20000], Training Loss: 0.0760\n",
            "Epoch [4277/20000], Training Loss: 0.0793\n",
            "Epoch [4278/20000], Training Loss: 0.0737\n",
            "Epoch [4279/20000], Training Loss: 0.0790\n",
            "Epoch [4280/20000], Training Loss: 0.0750\n",
            "Epoch [4281/20000], Training Loss: 0.0770\n",
            "Epoch [4282/20000], Training Loss: 0.0780\n",
            "Epoch [4283/20000], Training Loss: 0.0807\n",
            "Epoch [4284/20000], Training Loss: 0.0800\n",
            "Epoch [4285/20000], Training Loss: 0.0797\n",
            "Epoch [4286/20000], Training Loss: 0.0807\n",
            "Epoch [4287/20000], Training Loss: 0.0797\n",
            "Epoch [4288/20000], Training Loss: 0.0777\n",
            "Epoch [4289/20000], Training Loss: 0.0748\n",
            "Epoch [4290/20000], Training Loss: 0.0849\n",
            "Epoch [4291/20000], Training Loss: 0.0797\n",
            "Epoch [4292/20000], Training Loss: 0.0823\n",
            "Epoch [4293/20000], Training Loss: 0.0792\n",
            "Epoch [4294/20000], Training Loss: 0.0821\n",
            "Epoch [4295/20000], Training Loss: 0.0842\n",
            "Epoch [4296/20000], Training Loss: 0.0776\n",
            "Epoch [4297/20000], Training Loss: 0.0799\n",
            "Epoch [4298/20000], Training Loss: 0.0748\n",
            "Epoch [4299/20000], Training Loss: 0.0823\n",
            "Epoch [4300/20000], Training Loss: 0.0792\n",
            "Epoch [4301/20000], Training Loss: 0.0807\n",
            "Epoch [4302/20000], Training Loss: 0.0729\n",
            "Epoch [4303/20000], Training Loss: 0.0810\n",
            "Epoch [4304/20000], Training Loss: 0.0747\n",
            "Epoch [4305/20000], Training Loss: 0.0766\n",
            "Epoch [4306/20000], Training Loss: 0.0803\n",
            "Epoch [4307/20000], Training Loss: 0.0803\n",
            "Epoch [4308/20000], Training Loss: 0.0812\n",
            "Epoch [4309/20000], Training Loss: 0.0760\n",
            "Epoch [4310/20000], Training Loss: 0.0762\n",
            "Epoch [4311/20000], Training Loss: 0.0734\n",
            "Epoch [4312/20000], Training Loss: 0.0804\n",
            "Epoch [4313/20000], Training Loss: 0.0857\n",
            "Epoch [4314/20000], Training Loss: 0.0762\n",
            "Epoch [4315/20000], Training Loss: 0.0760\n",
            "Epoch [4316/20000], Training Loss: 0.0822\n",
            "Epoch [4317/20000], Training Loss: 0.0838\n",
            "Epoch [4318/20000], Training Loss: 0.0737\n",
            "Epoch [4319/20000], Training Loss: 0.0830\n",
            "Epoch [4320/20000], Training Loss: 0.0767\n",
            "Epoch [4321/20000], Training Loss: 0.0795\n",
            "Epoch [4322/20000], Training Loss: 0.0740\n",
            "Epoch [4323/20000], Training Loss: 0.0790\n",
            "Epoch [4324/20000], Training Loss: 0.0752\n",
            "Epoch [4325/20000], Training Loss: 0.0800\n",
            "Epoch [4326/20000], Training Loss: 0.0856\n",
            "Epoch [4327/20000], Training Loss: 0.0803\n",
            "Epoch [4328/20000], Training Loss: 0.0789\n",
            "Epoch [4329/20000], Training Loss: 0.0745\n",
            "Epoch [4330/20000], Training Loss: 0.0760\n",
            "Epoch [4331/20000], Training Loss: 0.0754\n",
            "Epoch [4332/20000], Training Loss: 0.0807\n",
            "Epoch [4333/20000], Training Loss: 0.0770\n",
            "Epoch [4334/20000], Training Loss: 0.0777\n",
            "Epoch [4335/20000], Training Loss: 0.0776\n",
            "Epoch [4336/20000], Training Loss: 0.0784\n",
            "Epoch [4337/20000], Training Loss: 0.0771\n",
            "Epoch [4338/20000], Training Loss: 0.0780\n",
            "Epoch [4339/20000], Training Loss: 0.0826\n",
            "Epoch [4340/20000], Training Loss: 0.0742\n",
            "Epoch [4341/20000], Training Loss: 0.0818\n",
            "Epoch [4342/20000], Training Loss: 0.0822\n",
            "Epoch [4343/20000], Training Loss: 0.0805\n",
            "Epoch [4344/20000], Training Loss: 0.0806\n",
            "Epoch [4345/20000], Training Loss: 0.0744\n",
            "Epoch [4346/20000], Training Loss: 0.0856\n",
            "Epoch [4347/20000], Training Loss: 0.0839\n",
            "Epoch [4348/20000], Training Loss: 0.0795\n",
            "Epoch [4349/20000], Training Loss: 0.0771\n",
            "Epoch [4350/20000], Training Loss: 0.0783\n",
            "Epoch [4351/20000], Training Loss: 0.0811\n",
            "Epoch [4352/20000], Training Loss: 0.0747\n",
            "Epoch [4353/20000], Training Loss: 0.0780\n",
            "Epoch [4354/20000], Training Loss: 0.0780\n",
            "Epoch [4355/20000], Training Loss: 0.0852\n",
            "Epoch [4356/20000], Training Loss: 0.0803\n",
            "Epoch [4357/20000], Training Loss: 0.0727\n",
            "Epoch [4358/20000], Training Loss: 0.0759\n",
            "Epoch [4359/20000], Training Loss: 0.0859\n",
            "Epoch [4360/20000], Training Loss: 0.0821\n",
            "Epoch [4361/20000], Training Loss: 0.0851\n",
            "Epoch [4362/20000], Training Loss: 0.0758\n",
            "Epoch [4363/20000], Training Loss: 0.0744\n",
            "Epoch [4364/20000], Training Loss: 0.0796\n",
            "Epoch [4365/20000], Training Loss: 0.0799\n",
            "Epoch [4366/20000], Training Loss: 0.0824\n",
            "Epoch [4367/20000], Training Loss: 0.0724\n",
            "Epoch [4368/20000], Training Loss: 0.0807\n",
            "Epoch [4369/20000], Training Loss: 0.0806\n",
            "Epoch [4370/20000], Training Loss: 0.0882\n",
            "Epoch [4371/20000], Training Loss: 0.0742\n",
            "Epoch [4372/20000], Training Loss: 0.0734\n",
            "Epoch [4373/20000], Training Loss: 0.0793\n",
            "Epoch [4374/20000], Training Loss: 0.0779\n",
            "Epoch [4375/20000], Training Loss: 0.0799\n",
            "Epoch [4376/20000], Training Loss: 0.0837\n",
            "Epoch [4377/20000], Training Loss: 0.0778\n",
            "Epoch [4378/20000], Training Loss: 0.0736\n",
            "Epoch [4379/20000], Training Loss: 0.0869\n",
            "Epoch [4380/20000], Training Loss: 0.0765\n",
            "Epoch [4381/20000], Training Loss: 0.0754\n",
            "Epoch [4382/20000], Training Loss: 0.0843\n",
            "Epoch [4383/20000], Training Loss: 0.0787\n",
            "Epoch [4384/20000], Training Loss: 0.0802\n",
            "Epoch [4385/20000], Training Loss: 0.0835\n",
            "Epoch [4386/20000], Training Loss: 0.0743\n",
            "Epoch [4387/20000], Training Loss: 0.0827\n",
            "Epoch [4388/20000], Training Loss: 0.0852\n",
            "Epoch [4389/20000], Training Loss: 0.0741\n",
            "Epoch [4390/20000], Training Loss: 0.0748\n",
            "Epoch [4391/20000], Training Loss: 0.0770\n",
            "Epoch [4392/20000], Training Loss: 0.0806\n",
            "Epoch [4393/20000], Training Loss: 0.0815\n",
            "Epoch [4394/20000], Training Loss: 0.0741\n",
            "Epoch [4395/20000], Training Loss: 0.0823\n",
            "Epoch [4396/20000], Training Loss: 0.0844\n",
            "Epoch [4397/20000], Training Loss: 0.0831\n",
            "Epoch [4398/20000], Training Loss: 0.0856\n",
            "Epoch [4399/20000], Training Loss: 0.0836\n",
            "Epoch [4400/20000], Training Loss: 0.0833\n",
            "Epoch [4401/20000], Training Loss: 0.0804\n",
            "Epoch [4402/20000], Training Loss: 0.0777\n",
            "Epoch [4403/20000], Training Loss: 0.0813\n",
            "Epoch [4404/20000], Training Loss: 0.0786\n",
            "Epoch [4405/20000], Training Loss: 0.0763\n",
            "Epoch [4406/20000], Training Loss: 0.0778\n",
            "Epoch [4407/20000], Training Loss: 0.0820\n",
            "Epoch [4408/20000], Training Loss: 0.0827\n",
            "Epoch [4409/20000], Training Loss: 0.0807\n",
            "Epoch [4410/20000], Training Loss: 0.0765\n",
            "Epoch [4411/20000], Training Loss: 0.0821\n",
            "Epoch [4412/20000], Training Loss: 0.0793\n",
            "Epoch [4413/20000], Training Loss: 0.0813\n",
            "Epoch [4414/20000], Training Loss: 0.0778\n",
            "Epoch [4415/20000], Training Loss: 0.0858\n",
            "Epoch [4416/20000], Training Loss: 0.0761\n",
            "Epoch [4417/20000], Training Loss: 0.0842\n",
            "Epoch [4418/20000], Training Loss: 0.0825\n",
            "Epoch [4419/20000], Training Loss: 0.0770\n",
            "Epoch [4420/20000], Training Loss: 0.0774\n",
            "Epoch [4421/20000], Training Loss: 0.0828\n",
            "Epoch [4422/20000], Training Loss: 0.0868\n",
            "Epoch [4423/20000], Training Loss: 0.0792\n",
            "Epoch [4424/20000], Training Loss: 0.0859\n",
            "Epoch [4425/20000], Training Loss: 0.0877\n",
            "Epoch [4426/20000], Training Loss: 0.0727\n",
            "Epoch [4427/20000], Training Loss: 0.0735\n",
            "Epoch [4428/20000], Training Loss: 0.0793\n",
            "Epoch [4429/20000], Training Loss: 0.0844\n",
            "Epoch [4430/20000], Training Loss: 0.0799\n",
            "Epoch [4431/20000], Training Loss: 0.0848\n",
            "Epoch [4432/20000], Training Loss: 0.0789\n",
            "Epoch [4433/20000], Training Loss: 0.0759\n",
            "Epoch [4434/20000], Training Loss: 0.0799\n",
            "Epoch [4435/20000], Training Loss: 0.0878\n",
            "Epoch [4436/20000], Training Loss: 0.0780\n",
            "Epoch [4437/20000], Training Loss: 0.0756\n",
            "Epoch [4438/20000], Training Loss: 0.0851\n",
            "Epoch [4439/20000], Training Loss: 0.0866\n",
            "Epoch [4440/20000], Training Loss: 0.0867\n",
            "Epoch [4441/20000], Training Loss: 0.0803\n",
            "Epoch [4442/20000], Training Loss: 0.0753\n",
            "Epoch [4443/20000], Training Loss: 0.0734\n",
            "Epoch [4444/20000], Training Loss: 0.0840\n",
            "Epoch [4445/20000], Training Loss: 0.0854\n",
            "Epoch [4446/20000], Training Loss: 0.0801\n",
            "Epoch [4447/20000], Training Loss: 0.0744\n",
            "Epoch [4448/20000], Training Loss: 0.0800\n",
            "Epoch [4449/20000], Training Loss: 0.0774\n",
            "Epoch [4450/20000], Training Loss: 0.0820\n",
            "Epoch [4451/20000], Training Loss: 0.0769\n",
            "Epoch [4452/20000], Training Loss: 0.0799\n",
            "Epoch [4453/20000], Training Loss: 0.0817\n",
            "Epoch [4454/20000], Training Loss: 0.0769\n",
            "Epoch [4455/20000], Training Loss: 0.0803\n",
            "Epoch [4456/20000], Training Loss: 0.0821\n",
            "Epoch [4457/20000], Training Loss: 0.0823\n",
            "Epoch [4458/20000], Training Loss: 0.0853\n",
            "Epoch [4459/20000], Training Loss: 0.0781\n",
            "Epoch [4460/20000], Training Loss: 0.0855\n",
            "Epoch [4461/20000], Training Loss: 0.0792\n",
            "Epoch [4462/20000], Training Loss: 0.0773\n",
            "Epoch [4463/20000], Training Loss: 0.0796\n",
            "Epoch [4464/20000], Training Loss: 0.0852\n",
            "Epoch [4465/20000], Training Loss: 0.0823\n",
            "Epoch [4466/20000], Training Loss: 0.0838\n",
            "Epoch [4467/20000], Training Loss: 0.0821\n",
            "Epoch [4468/20000], Training Loss: 0.0790\n",
            "Epoch [4469/20000], Training Loss: 0.0818\n",
            "Epoch [4470/20000], Training Loss: 0.0860\n",
            "Epoch [4471/20000], Training Loss: 0.0744\n",
            "Epoch [4472/20000], Training Loss: 0.0728\n",
            "Epoch [4473/20000], Training Loss: 0.0838\n",
            "Epoch [4474/20000], Training Loss: 0.0785\n",
            "Epoch [4475/20000], Training Loss: 0.0779\n",
            "Epoch [4476/20000], Training Loss: 0.0779\n",
            "Epoch [4477/20000], Training Loss: 0.0791\n",
            "Epoch [4478/20000], Training Loss: 0.0777\n",
            "Epoch [4479/20000], Training Loss: 0.0824\n",
            "Epoch [4480/20000], Training Loss: 0.0777\n",
            "Epoch [4481/20000], Training Loss: 0.0849\n",
            "Epoch [4482/20000], Training Loss: 0.0812\n",
            "Epoch [4483/20000], Training Loss: 0.0852\n",
            "Epoch [4484/20000], Training Loss: 0.0812\n",
            "Epoch [4485/20000], Training Loss: 0.0799\n",
            "Epoch [4486/20000], Training Loss: 0.0762\n",
            "Epoch [4487/20000], Training Loss: 0.0826\n",
            "Epoch [4488/20000], Training Loss: 0.0794\n",
            "Epoch [4489/20000], Training Loss: 0.0845\n",
            "Epoch [4490/20000], Training Loss: 0.0744\n",
            "Epoch [4491/20000], Training Loss: 0.0768\n",
            "Epoch [4492/20000], Training Loss: 0.0795\n",
            "Epoch [4493/20000], Training Loss: 0.0766\n",
            "Epoch [4494/20000], Training Loss: 0.0839\n",
            "Epoch [4495/20000], Training Loss: 0.0899\n",
            "Epoch [4496/20000], Training Loss: 0.0790\n",
            "Epoch [4497/20000], Training Loss: 0.0746\n",
            "Epoch [4498/20000], Training Loss: 0.0742\n",
            "Epoch [4499/20000], Training Loss: 0.0811\n",
            "Epoch [4500/20000], Training Loss: 0.0825\n",
            "Epoch [4501/20000], Training Loss: 0.0780\n",
            "Epoch [4502/20000], Training Loss: 0.0727\n",
            "Epoch [4503/20000], Training Loss: 0.0807\n",
            "Epoch [4504/20000], Training Loss: 0.0736\n",
            "Epoch [4505/20000], Training Loss: 0.0797\n",
            "Epoch [4506/20000], Training Loss: 0.0793\n",
            "Epoch [4507/20000], Training Loss: 0.0759\n",
            "Epoch [4508/20000], Training Loss: 0.0806\n",
            "Epoch [4509/20000], Training Loss: 0.0784\n",
            "Epoch [4510/20000], Training Loss: 0.0812\n",
            "Epoch [4511/20000], Training Loss: 0.0834\n",
            "Epoch [4512/20000], Training Loss: 0.0817\n",
            "Epoch [4513/20000], Training Loss: 0.0769\n",
            "Epoch [4514/20000], Training Loss: 0.0791\n",
            "Epoch [4515/20000], Training Loss: 0.0798\n",
            "Epoch [4516/20000], Training Loss: 0.0780\n",
            "Epoch [4517/20000], Training Loss: 0.0851\n",
            "Epoch [4518/20000], Training Loss: 0.0821\n",
            "Epoch [4519/20000], Training Loss: 0.0830\n",
            "Epoch [4520/20000], Training Loss: 0.0840\n",
            "Epoch [4521/20000], Training Loss: 0.0750\n",
            "Epoch [4522/20000], Training Loss: 0.0848\n",
            "Epoch [4523/20000], Training Loss: 0.0856\n",
            "Epoch [4524/20000], Training Loss: 0.0762\n",
            "Epoch [4525/20000], Training Loss: 0.0783\n",
            "Epoch [4526/20000], Training Loss: 0.0834\n",
            "Epoch [4527/20000], Training Loss: 0.0765\n",
            "Epoch [4528/20000], Training Loss: 0.0776\n",
            "Epoch [4529/20000], Training Loss: 0.0739\n",
            "Epoch [4530/20000], Training Loss: 0.0856\n",
            "Epoch [4531/20000], Training Loss: 0.0751\n",
            "Epoch [4532/20000], Training Loss: 0.0785\n",
            "Epoch [4533/20000], Training Loss: 0.0796\n",
            "Epoch [4534/20000], Training Loss: 0.0794\n",
            "Epoch [4535/20000], Training Loss: 0.0796\n",
            "Epoch [4536/20000], Training Loss: 0.0806\n",
            "Epoch [4537/20000], Training Loss: 0.0853\n",
            "Epoch [4538/20000], Training Loss: 0.0733\n",
            "Epoch [4539/20000], Training Loss: 0.0794\n",
            "Epoch [4540/20000], Training Loss: 0.0873\n",
            "Epoch [4541/20000], Training Loss: 0.0758\n",
            "Epoch [4542/20000], Training Loss: 0.0840\n",
            "Epoch [4543/20000], Training Loss: 0.0796\n",
            "Epoch [4544/20000], Training Loss: 0.0738\n",
            "Epoch [4545/20000], Training Loss: 0.0807\n",
            "Epoch [4546/20000], Training Loss: 0.0794\n",
            "Epoch [4547/20000], Training Loss: 0.0764\n",
            "Epoch [4548/20000], Training Loss: 0.0826\n",
            "Epoch [4549/20000], Training Loss: 0.0835\n",
            "Epoch [4550/20000], Training Loss: 0.0798\n",
            "Epoch [4551/20000], Training Loss: 0.0837\n",
            "Epoch [4552/20000], Training Loss: 0.0759\n",
            "Epoch [4553/20000], Training Loss: 0.0757\n",
            "Epoch [4554/20000], Training Loss: 0.0843\n",
            "Epoch [4555/20000], Training Loss: 0.0791\n",
            "Epoch [4556/20000], Training Loss: 0.0799\n",
            "Epoch [4557/20000], Training Loss: 0.0818\n",
            "Epoch [4558/20000], Training Loss: 0.0743\n",
            "Epoch [4559/20000], Training Loss: 0.0793\n",
            "Epoch [4560/20000], Training Loss: 0.0844\n",
            "Epoch [4561/20000], Training Loss: 0.0805\n",
            "Epoch [4562/20000], Training Loss: 0.0789\n",
            "Epoch [4563/20000], Training Loss: 0.0837\n",
            "Epoch [4564/20000], Training Loss: 0.0870\n",
            "Epoch [4565/20000], Training Loss: 0.0774\n",
            "Epoch [4566/20000], Training Loss: 0.0751\n",
            "Epoch [4567/20000], Training Loss: 0.0761\n",
            "Epoch [4568/20000], Training Loss: 0.0778\n",
            "Epoch [4569/20000], Training Loss: 0.0801\n",
            "Epoch [4570/20000], Training Loss: 0.0795\n",
            "Epoch [4571/20000], Training Loss: 0.0805\n",
            "Epoch [4572/20000], Training Loss: 0.0824\n",
            "Epoch [4573/20000], Training Loss: 0.0755\n",
            "Epoch [4574/20000], Training Loss: 0.0850\n",
            "Epoch [4575/20000], Training Loss: 0.0754\n",
            "Epoch [4576/20000], Training Loss: 0.0774\n",
            "Epoch [4577/20000], Training Loss: 0.0761\n",
            "Epoch [4578/20000], Training Loss: 0.0799\n",
            "Epoch [4579/20000], Training Loss: 0.0773\n",
            "Epoch [4580/20000], Training Loss: 0.0804\n",
            "Epoch [4581/20000], Training Loss: 0.0803\n",
            "Epoch [4582/20000], Training Loss: 0.0803\n",
            "Epoch [4583/20000], Training Loss: 0.0768\n",
            "Epoch [4584/20000], Training Loss: 0.0823\n",
            "Epoch [4585/20000], Training Loss: 0.0823\n",
            "Epoch [4586/20000], Training Loss: 0.0776\n",
            "Epoch [4587/20000], Training Loss: 0.0766\n",
            "Epoch [4588/20000], Training Loss: 0.0775\n",
            "Epoch [4589/20000], Training Loss: 0.0732\n",
            "Epoch [4590/20000], Training Loss: 0.0792\n",
            "Epoch [4591/20000], Training Loss: 0.0791\n",
            "Epoch [4592/20000], Training Loss: 0.0776\n",
            "Epoch [4593/20000], Training Loss: 0.0737\n",
            "Epoch [4594/20000], Training Loss: 0.0820\n",
            "Epoch [4595/20000], Training Loss: 0.0804\n",
            "Epoch [4596/20000], Training Loss: 0.0754\n",
            "Epoch [4597/20000], Training Loss: 0.0797\n",
            "Epoch [4598/20000], Training Loss: 0.0788\n",
            "Epoch [4599/20000], Training Loss: 0.0817\n",
            "Epoch [4600/20000], Training Loss: 0.0789\n",
            "Epoch [4601/20000], Training Loss: 0.0785\n",
            "Epoch [4602/20000], Training Loss: 0.0841\n",
            "Epoch [4603/20000], Training Loss: 0.0807\n",
            "Epoch [4604/20000], Training Loss: 0.0733\n",
            "Epoch [4605/20000], Training Loss: 0.0832\n",
            "Epoch [4606/20000], Training Loss: 0.0757\n",
            "Epoch [4607/20000], Training Loss: 0.0778\n",
            "Epoch [4608/20000], Training Loss: 0.0773\n",
            "Epoch [4609/20000], Training Loss: 0.0808\n",
            "Epoch [4610/20000], Training Loss: 0.0801\n",
            "Epoch [4611/20000], Training Loss: 0.0781\n",
            "Epoch [4612/20000], Training Loss: 0.0748\n",
            "Epoch [4613/20000], Training Loss: 0.0797\n",
            "Epoch [4614/20000], Training Loss: 0.0811\n",
            "Epoch [4615/20000], Training Loss: 0.0800\n",
            "Epoch [4616/20000], Training Loss: 0.0802\n",
            "Epoch [4617/20000], Training Loss: 0.0853\n",
            "Epoch [4618/20000], Training Loss: 0.0840\n",
            "Epoch [4619/20000], Training Loss: 0.0801\n",
            "Epoch [4620/20000], Training Loss: 0.0744\n",
            "Epoch [4621/20000], Training Loss: 0.0729\n",
            "Epoch [4622/20000], Training Loss: 0.0800\n",
            "Epoch [4623/20000], Training Loss: 0.0782\n",
            "Epoch [4624/20000], Training Loss: 0.0830\n",
            "Epoch [4625/20000], Training Loss: 0.0811\n",
            "Epoch [4626/20000], Training Loss: 0.0757\n",
            "Epoch [4627/20000], Training Loss: 0.0712\n",
            "Epoch [4628/20000], Training Loss: 0.0800\n",
            "Epoch [4629/20000], Training Loss: 0.0820\n",
            "Epoch [4630/20000], Training Loss: 0.0834\n",
            "Epoch [4631/20000], Training Loss: 0.0824\n",
            "Epoch [4632/20000], Training Loss: 0.0800\n",
            "Epoch [4633/20000], Training Loss: 0.0757\n",
            "Epoch [4634/20000], Training Loss: 0.0744\n",
            "Epoch [4635/20000], Training Loss: 0.0759\n",
            "Epoch [4636/20000], Training Loss: 0.0776\n",
            "Epoch [4637/20000], Training Loss: 0.0824\n",
            "Epoch [4638/20000], Training Loss: 0.0805\n",
            "Epoch [4639/20000], Training Loss: 0.0752\n",
            "Epoch [4640/20000], Training Loss: 0.0753\n",
            "Epoch [4641/20000], Training Loss: 0.0813\n",
            "Epoch [4642/20000], Training Loss: 0.0838\n",
            "Epoch [4643/20000], Training Loss: 0.0805\n",
            "Epoch [4644/20000], Training Loss: 0.0775\n",
            "Epoch [4645/20000], Training Loss: 0.0770\n",
            "Epoch [4646/20000], Training Loss: 0.0819\n",
            "Epoch [4647/20000], Training Loss: 0.0786\n",
            "Epoch [4648/20000], Training Loss: 0.0813\n",
            "Epoch [4649/20000], Training Loss: 0.0836\n",
            "Epoch [4650/20000], Training Loss: 0.0840\n",
            "Epoch [4651/20000], Training Loss: 0.0721\n",
            "Epoch [4652/20000], Training Loss: 0.0821\n",
            "Epoch [4653/20000], Training Loss: 0.0803\n",
            "Epoch [4654/20000], Training Loss: 0.0781\n",
            "Epoch [4655/20000], Training Loss: 0.0746\n",
            "Epoch [4656/20000], Training Loss: 0.0847\n",
            "Epoch [4657/20000], Training Loss: 0.0833\n",
            "Epoch [4658/20000], Training Loss: 0.0858\n",
            "Epoch [4659/20000], Training Loss: 0.0774\n",
            "Epoch [4660/20000], Training Loss: 0.0792\n",
            "Epoch [4661/20000], Training Loss: 0.0778\n",
            "Epoch [4662/20000], Training Loss: 0.0819\n",
            "Epoch [4663/20000], Training Loss: 0.0752\n",
            "Epoch [4664/20000], Training Loss: 0.0839\n",
            "Epoch [4665/20000], Training Loss: 0.0786\n",
            "Epoch [4666/20000], Training Loss: 0.0733\n",
            "Epoch [4667/20000], Training Loss: 0.0785\n",
            "Epoch [4668/20000], Training Loss: 0.0768\n",
            "Epoch [4669/20000], Training Loss: 0.0803\n",
            "Epoch [4670/20000], Training Loss: 0.0767\n",
            "Epoch [4671/20000], Training Loss: 0.0779\n",
            "Epoch [4672/20000], Training Loss: 0.0732\n",
            "Epoch [4673/20000], Training Loss: 0.0829\n",
            "Epoch [4674/20000], Training Loss: 0.0736\n",
            "Epoch [4675/20000], Training Loss: 0.0807\n",
            "Epoch [4676/20000], Training Loss: 0.0769\n",
            "Epoch [4677/20000], Training Loss: 0.0752\n",
            "Epoch [4678/20000], Training Loss: 0.0826\n",
            "Epoch [4679/20000], Training Loss: 0.0784\n",
            "Epoch [4680/20000], Training Loss: 0.0815\n",
            "Epoch [4681/20000], Training Loss: 0.0737\n",
            "Epoch [4682/20000], Training Loss: 0.0764\n",
            "Epoch [4683/20000], Training Loss: 0.0816\n",
            "Epoch [4684/20000], Training Loss: 0.0769\n",
            "Epoch [4685/20000], Training Loss: 0.0778\n",
            "Epoch [4686/20000], Training Loss: 0.0828\n",
            "Epoch [4687/20000], Training Loss: 0.0789\n",
            "Epoch [4688/20000], Training Loss: 0.0859\n",
            "Epoch [4689/20000], Training Loss: 0.0757\n",
            "Epoch [4690/20000], Training Loss: 0.0786\n",
            "Epoch [4691/20000], Training Loss: 0.0754\n",
            "Epoch [4692/20000], Training Loss: 0.0841\n",
            "Epoch [4693/20000], Training Loss: 0.0814\n",
            "Epoch [4694/20000], Training Loss: 0.0784\n",
            "Epoch [4695/20000], Training Loss: 0.0780\n",
            "Epoch [4696/20000], Training Loss: 0.0758\n",
            "Epoch [4697/20000], Training Loss: 0.0801\n",
            "Epoch [4698/20000], Training Loss: 0.0860\n",
            "Epoch [4699/20000], Training Loss: 0.0840\n",
            "Epoch [4700/20000], Training Loss: 0.0748\n",
            "Epoch [4701/20000], Training Loss: 0.0800\n",
            "Epoch [4702/20000], Training Loss: 0.0796\n",
            "Epoch [4703/20000], Training Loss: 0.0799\n",
            "Epoch [4704/20000], Training Loss: 0.0792\n",
            "Epoch [4705/20000], Training Loss: 0.0811\n",
            "Epoch [4706/20000], Training Loss: 0.0790\n",
            "Epoch [4707/20000], Training Loss: 0.0870\n",
            "Epoch [4708/20000], Training Loss: 0.0818\n",
            "Epoch [4709/20000], Training Loss: 0.0796\n",
            "Epoch [4710/20000], Training Loss: 0.0735\n",
            "Epoch [4711/20000], Training Loss: 0.0818\n",
            "Epoch [4712/20000], Training Loss: 0.0802\n",
            "Epoch [4713/20000], Training Loss: 0.0735\n",
            "Epoch [4714/20000], Training Loss: 0.0793\n",
            "Epoch [4715/20000], Training Loss: 0.0801\n",
            "Epoch [4716/20000], Training Loss: 0.0795\n",
            "Epoch [4717/20000], Training Loss: 0.0795\n",
            "Epoch [4718/20000], Training Loss: 0.0772\n",
            "Epoch [4719/20000], Training Loss: 0.0778\n",
            "Epoch [4720/20000], Training Loss: 0.0770\n",
            "Epoch [4721/20000], Training Loss: 0.0742\n",
            "Epoch [4722/20000], Training Loss: 0.0804\n",
            "Epoch [4723/20000], Training Loss: 0.0794\n",
            "Epoch [4724/20000], Training Loss: 0.0801\n",
            "Epoch [4725/20000], Training Loss: 0.0785\n",
            "Epoch [4726/20000], Training Loss: 0.0796\n",
            "Epoch [4727/20000], Training Loss: 0.0808\n",
            "Epoch [4728/20000], Training Loss: 0.0835\n",
            "Epoch [4729/20000], Training Loss: 0.0796\n",
            "Epoch [4730/20000], Training Loss: 0.0843\n",
            "Epoch [4731/20000], Training Loss: 0.0803\n",
            "Epoch [4732/20000], Training Loss: 0.0760\n",
            "Epoch [4733/20000], Training Loss: 0.0849\n",
            "Epoch [4734/20000], Training Loss: 0.0796\n",
            "Epoch [4735/20000], Training Loss: 0.0798\n",
            "Epoch [4736/20000], Training Loss: 0.0764\n",
            "Epoch [4737/20000], Training Loss: 0.0806\n",
            "Epoch [4738/20000], Training Loss: 0.0804\n",
            "Epoch [4739/20000], Training Loss: 0.0736\n",
            "Epoch [4740/20000], Training Loss: 0.0766\n",
            "Epoch [4741/20000], Training Loss: 0.0790\n",
            "Epoch [4742/20000], Training Loss: 0.0770\n",
            "Epoch [4743/20000], Training Loss: 0.0762\n",
            "Epoch [4744/20000], Training Loss: 0.0786\n",
            "Epoch [4745/20000], Training Loss: 0.0767\n",
            "Epoch [4746/20000], Training Loss: 0.0805\n",
            "Epoch [4747/20000], Training Loss: 0.0836\n",
            "Epoch [4748/20000], Training Loss: 0.0772\n",
            "Epoch [4749/20000], Training Loss: 0.0831\n",
            "Epoch [4750/20000], Training Loss: 0.0814\n",
            "Epoch [4751/20000], Training Loss: 0.0770\n",
            "Epoch [4752/20000], Training Loss: 0.0818\n",
            "Epoch [4753/20000], Training Loss: 0.0797\n",
            "Epoch [4754/20000], Training Loss: 0.0795\n",
            "Epoch [4755/20000], Training Loss: 0.0748\n",
            "Epoch [4756/20000], Training Loss: 0.0816\n",
            "Epoch [4757/20000], Training Loss: 0.0750\n",
            "Epoch [4758/20000], Training Loss: 0.0828\n",
            "Epoch [4759/20000], Training Loss: 0.0810\n",
            "Epoch [4760/20000], Training Loss: 0.0770\n",
            "Epoch [4761/20000], Training Loss: 0.0824\n",
            "Epoch [4762/20000], Training Loss: 0.0785\n",
            "Epoch [4763/20000], Training Loss: 0.0808\n",
            "Epoch [4764/20000], Training Loss: 0.0756\n",
            "Epoch [4765/20000], Training Loss: 0.0754\n",
            "Epoch [4766/20000], Training Loss: 0.0739\n",
            "Epoch [4767/20000], Training Loss: 0.0828\n",
            "Epoch [4768/20000], Training Loss: 0.0815\n",
            "Epoch [4769/20000], Training Loss: 0.0790\n",
            "Epoch [4770/20000], Training Loss: 0.0820\n",
            "Epoch [4771/20000], Training Loss: 0.0748\n",
            "Epoch [4772/20000], Training Loss: 0.0787\n",
            "Epoch [4773/20000], Training Loss: 0.0800\n",
            "Epoch [4774/20000], Training Loss: 0.0835\n",
            "Epoch [4775/20000], Training Loss: 0.0778\n",
            "Epoch [4776/20000], Training Loss: 0.0801\n",
            "Epoch [4777/20000], Training Loss: 0.0768\n",
            "Epoch [4778/20000], Training Loss: 0.0764\n",
            "Epoch [4779/20000], Training Loss: 0.0835\n",
            "Epoch [4780/20000], Training Loss: 0.0858\n",
            "Epoch [4781/20000], Training Loss: 0.0825\n",
            "Epoch [4782/20000], Training Loss: 0.0759\n",
            "Epoch [4783/20000], Training Loss: 0.0778\n",
            "Epoch [4784/20000], Training Loss: 0.0774\n",
            "Epoch [4785/20000], Training Loss: 0.0816\n",
            "Epoch [4786/20000], Training Loss: 0.0799\n",
            "Epoch [4787/20000], Training Loss: 0.0818\n",
            "Epoch [4788/20000], Training Loss: 0.0777\n",
            "Epoch [4789/20000], Training Loss: 0.0780\n",
            "Epoch [4790/20000], Training Loss: 0.0786\n",
            "Epoch [4791/20000], Training Loss: 0.0790\n",
            "Epoch [4792/20000], Training Loss: 0.0892\n",
            "Epoch [4793/20000], Training Loss: 0.0787\n",
            "Epoch [4794/20000], Training Loss: 0.0861\n",
            "Epoch [4795/20000], Training Loss: 0.0814\n",
            "Epoch [4796/20000], Training Loss: 0.0825\n",
            "Epoch [4797/20000], Training Loss: 0.0803\n",
            "Epoch [4798/20000], Training Loss: 0.0805\n",
            "Epoch [4799/20000], Training Loss: 0.0801\n",
            "Epoch [4800/20000], Training Loss: 0.0802\n",
            "Epoch [4801/20000], Training Loss: 0.0758\n",
            "Epoch [4802/20000], Training Loss: 0.0785\n",
            "Epoch [4803/20000], Training Loss: 0.0859\n",
            "Epoch [4804/20000], Training Loss: 0.0724\n",
            "Epoch [4805/20000], Training Loss: 0.0813\n",
            "Epoch [4806/20000], Training Loss: 0.0771\n",
            "Epoch [4807/20000], Training Loss: 0.0821\n",
            "Epoch [4808/20000], Training Loss: 0.0767\n",
            "Epoch [4809/20000], Training Loss: 0.0745\n",
            "Epoch [4810/20000], Training Loss: 0.0792\n",
            "Epoch [4811/20000], Training Loss: 0.0848\n",
            "Epoch [4812/20000], Training Loss: 0.0794\n",
            "Epoch [4813/20000], Training Loss: 0.0745\n",
            "Epoch [4814/20000], Training Loss: 0.0792\n",
            "Epoch [4815/20000], Training Loss: 0.0745\n",
            "Epoch [4816/20000], Training Loss: 0.0854\n",
            "Epoch [4817/20000], Training Loss: 0.0764\n",
            "Epoch [4818/20000], Training Loss: 0.0787\n",
            "Epoch [4819/20000], Training Loss: 0.0803\n",
            "Epoch [4820/20000], Training Loss: 0.0829\n",
            "Epoch [4821/20000], Training Loss: 0.0733\n",
            "Epoch [4822/20000], Training Loss: 0.0815\n",
            "Epoch [4823/20000], Training Loss: 0.0810\n",
            "Epoch [4824/20000], Training Loss: 0.0825\n",
            "Epoch [4825/20000], Training Loss: 0.0765\n",
            "Epoch [4826/20000], Training Loss: 0.0750\n",
            "Epoch [4827/20000], Training Loss: 0.0810\n",
            "Epoch [4828/20000], Training Loss: 0.0800\n",
            "Epoch [4829/20000], Training Loss: 0.0787\n",
            "Epoch [4830/20000], Training Loss: 0.0744\n",
            "Epoch [4831/20000], Training Loss: 0.0796\n",
            "Epoch [4832/20000], Training Loss: 0.0775\n",
            "Epoch [4833/20000], Training Loss: 0.0793\n",
            "Epoch [4834/20000], Training Loss: 0.0865\n",
            "Epoch [4835/20000], Training Loss: 0.0817\n",
            "Epoch [4836/20000], Training Loss: 0.0778\n",
            "Epoch [4837/20000], Training Loss: 0.0826\n",
            "Epoch [4838/20000], Training Loss: 0.0800\n",
            "Epoch [4839/20000], Training Loss: 0.0792\n",
            "Epoch [4840/20000], Training Loss: 0.0797\n",
            "Epoch [4841/20000], Training Loss: 0.0809\n",
            "Epoch [4842/20000], Training Loss: 0.0787\n",
            "Epoch [4843/20000], Training Loss: 0.0790\n",
            "Epoch [4844/20000], Training Loss: 0.0820\n",
            "Epoch [4845/20000], Training Loss: 0.0803\n",
            "Epoch [4846/20000], Training Loss: 0.0756\n",
            "Epoch [4847/20000], Training Loss: 0.0772\n",
            "Epoch [4848/20000], Training Loss: 0.0841\n",
            "Epoch [4849/20000], Training Loss: 0.0766\n",
            "Epoch [4850/20000], Training Loss: 0.0801\n",
            "Epoch [4851/20000], Training Loss: 0.0768\n",
            "Epoch [4852/20000], Training Loss: 0.0811\n",
            "Epoch [4853/20000], Training Loss: 0.0799\n",
            "Epoch [4854/20000], Training Loss: 0.0820\n",
            "Epoch [4855/20000], Training Loss: 0.0783\n",
            "Epoch [4856/20000], Training Loss: 0.0785\n",
            "Epoch [4857/20000], Training Loss: 0.0823\n",
            "Epoch [4858/20000], Training Loss: 0.0868\n",
            "Epoch [4859/20000], Training Loss: 0.0800\n",
            "Epoch [4860/20000], Training Loss: 0.0776\n",
            "Epoch [4861/20000], Training Loss: 0.0758\n",
            "Epoch [4862/20000], Training Loss: 0.0784\n",
            "Epoch [4863/20000], Training Loss: 0.0744\n",
            "Epoch [4864/20000], Training Loss: 0.0760\n",
            "Epoch [4865/20000], Training Loss: 0.0782\n",
            "Epoch [4866/20000], Training Loss: 0.0855\n",
            "Epoch [4867/20000], Training Loss: 0.0856\n",
            "Epoch [4868/20000], Training Loss: 0.0765\n",
            "Epoch [4869/20000], Training Loss: 0.0829\n",
            "Epoch [4870/20000], Training Loss: 0.0765\n",
            "Epoch [4871/20000], Training Loss: 0.0847\n",
            "Epoch [4872/20000], Training Loss: 0.0776\n",
            "Epoch [4873/20000], Training Loss: 0.0783\n",
            "Epoch [4874/20000], Training Loss: 0.0765\n",
            "Epoch [4875/20000], Training Loss: 0.0791\n",
            "Epoch [4876/20000], Training Loss: 0.0804\n",
            "Epoch [4877/20000], Training Loss: 0.0811\n",
            "Epoch [4878/20000], Training Loss: 0.0804\n",
            "Epoch [4879/20000], Training Loss: 0.0781\n",
            "Epoch [4880/20000], Training Loss: 0.0856\n",
            "Epoch [4881/20000], Training Loss: 0.0744\n",
            "Epoch [4882/20000], Training Loss: 0.0831\n",
            "Epoch [4883/20000], Training Loss: 0.0837\n",
            "Epoch [4884/20000], Training Loss: 0.0821\n",
            "Epoch [4885/20000], Training Loss: 0.0818\n",
            "Epoch [4886/20000], Training Loss: 0.0806\n",
            "Epoch [4887/20000], Training Loss: 0.0776\n",
            "Epoch [4888/20000], Training Loss: 0.0828\n",
            "Epoch [4889/20000], Training Loss: 0.0800\n",
            "Epoch [4890/20000], Training Loss: 0.0776\n",
            "Epoch [4891/20000], Training Loss: 0.0816\n",
            "Epoch [4892/20000], Training Loss: 0.0826\n",
            "Epoch [4893/20000], Training Loss: 0.0790\n",
            "Epoch [4894/20000], Training Loss: 0.0769\n",
            "Epoch [4895/20000], Training Loss: 0.0817\n",
            "Epoch [4896/20000], Training Loss: 0.0804\n",
            "Epoch [4897/20000], Training Loss: 0.0787\n",
            "Epoch [4898/20000], Training Loss: 0.0745\n",
            "Epoch [4899/20000], Training Loss: 0.0825\n",
            "Epoch [4900/20000], Training Loss: 0.0838\n",
            "Epoch [4901/20000], Training Loss: 0.0835\n",
            "Epoch [4902/20000], Training Loss: 0.0769\n",
            "Epoch [4903/20000], Training Loss: 0.0848\n",
            "Epoch [4904/20000], Training Loss: 0.0793\n",
            "Epoch [4905/20000], Training Loss: 0.0826\n",
            "Epoch [4906/20000], Training Loss: 0.0798\n",
            "Epoch [4907/20000], Training Loss: 0.0766\n",
            "Epoch [4908/20000], Training Loss: 0.0796\n",
            "Epoch [4909/20000], Training Loss: 0.0750\n",
            "Epoch [4910/20000], Training Loss: 0.0773\n",
            "Epoch [4911/20000], Training Loss: 0.0819\n",
            "Epoch [4912/20000], Training Loss: 0.0742\n",
            "Epoch [4913/20000], Training Loss: 0.0760\n",
            "Epoch [4914/20000], Training Loss: 0.0789\n",
            "Epoch [4915/20000], Training Loss: 0.0786\n",
            "Epoch [4916/20000], Training Loss: 0.0761\n",
            "Epoch [4917/20000], Training Loss: 0.0782\n",
            "Epoch [4918/20000], Training Loss: 0.0807\n",
            "Epoch [4919/20000], Training Loss: 0.0795\n",
            "Epoch [4920/20000], Training Loss: 0.0836\n",
            "Epoch [4921/20000], Training Loss: 0.0815\n",
            "Epoch [4922/20000], Training Loss: 0.0852\n",
            "Epoch [4923/20000], Training Loss: 0.0803\n",
            "Epoch [4924/20000], Training Loss: 0.0797\n",
            "Epoch [4925/20000], Training Loss: 0.0830\n",
            "Epoch [4926/20000], Training Loss: 0.0764\n",
            "Epoch [4927/20000], Training Loss: 0.0799\n",
            "Epoch [4928/20000], Training Loss: 0.0741\n",
            "Epoch [4929/20000], Training Loss: 0.0843\n",
            "Epoch [4930/20000], Training Loss: 0.0795\n",
            "Epoch [4931/20000], Training Loss: 0.0803\n",
            "Epoch [4932/20000], Training Loss: 0.0798\n",
            "Epoch [4933/20000], Training Loss: 0.0815\n",
            "Epoch [4934/20000], Training Loss: 0.0774\n",
            "Epoch [4935/20000], Training Loss: 0.0732\n",
            "Epoch [4936/20000], Training Loss: 0.0825\n",
            "Epoch [4937/20000], Training Loss: 0.0808\n",
            "Epoch [4938/20000], Training Loss: 0.0751\n",
            "Epoch [4939/20000], Training Loss: 0.0803\n",
            "Epoch [4940/20000], Training Loss: 0.0768\n",
            "Epoch [4941/20000], Training Loss: 0.0817\n",
            "Epoch [4942/20000], Training Loss: 0.0741\n",
            "Epoch [4943/20000], Training Loss: 0.0800\n",
            "Epoch [4944/20000], Training Loss: 0.0758\n",
            "Epoch [4945/20000], Training Loss: 0.0797\n",
            "Epoch [4946/20000], Training Loss: 0.0773\n",
            "Epoch [4947/20000], Training Loss: 0.0808\n",
            "Epoch [4948/20000], Training Loss: 0.0728\n",
            "Epoch [4949/20000], Training Loss: 0.0831\n",
            "Epoch [4950/20000], Training Loss: 0.0810\n",
            "Epoch [4951/20000], Training Loss: 0.0823\n",
            "Epoch [4952/20000], Training Loss: 0.0753\n",
            "Epoch [4953/20000], Training Loss: 0.0827\n",
            "Epoch [4954/20000], Training Loss: 0.0797\n",
            "Epoch [4955/20000], Training Loss: 0.0791\n",
            "Epoch [4956/20000], Training Loss: 0.0806\n",
            "Epoch [4957/20000], Training Loss: 0.0860\n",
            "Epoch [4958/20000], Training Loss: 0.0794\n",
            "Epoch [4959/20000], Training Loss: 0.0829\n",
            "Epoch [4960/20000], Training Loss: 0.0766\n",
            "Epoch [4961/20000], Training Loss: 0.0763\n",
            "Epoch [4962/20000], Training Loss: 0.0767\n",
            "Epoch [4963/20000], Training Loss: 0.0849\n",
            "Epoch [4964/20000], Training Loss: 0.0857\n",
            "Epoch [4965/20000], Training Loss: 0.0765\n",
            "Epoch [4966/20000], Training Loss: 0.0791\n",
            "Epoch [4967/20000], Training Loss: 0.0777\n",
            "Epoch [4968/20000], Training Loss: 0.0820\n",
            "Epoch [4969/20000], Training Loss: 0.0729\n",
            "Epoch [4970/20000], Training Loss: 0.0802\n",
            "Epoch [4971/20000], Training Loss: 0.0799\n",
            "Epoch [4972/20000], Training Loss: 0.0781\n",
            "Epoch [4973/20000], Training Loss: 0.0845\n",
            "Epoch [4974/20000], Training Loss: 0.0783\n",
            "Epoch [4975/20000], Training Loss: 0.0773\n",
            "Epoch [4976/20000], Training Loss: 0.0801\n",
            "Epoch [4977/20000], Training Loss: 0.0808\n",
            "Epoch [4978/20000], Training Loss: 0.0778\n",
            "Epoch [4979/20000], Training Loss: 0.0826\n",
            "Epoch [4980/20000], Training Loss: 0.0821\n",
            "Epoch [4981/20000], Training Loss: 0.0794\n",
            "Epoch [4982/20000], Training Loss: 0.0812\n",
            "Epoch [4983/20000], Training Loss: 0.0750\n",
            "Epoch [4984/20000], Training Loss: 0.0802\n",
            "Epoch [4985/20000], Training Loss: 0.0769\n",
            "Epoch [4986/20000], Training Loss: 0.0830\n",
            "Epoch [4987/20000], Training Loss: 0.0834\n",
            "Epoch [4988/20000], Training Loss: 0.0793\n",
            "Epoch [4989/20000], Training Loss: 0.0765\n",
            "Epoch [4990/20000], Training Loss: 0.0826\n",
            "Epoch [4991/20000], Training Loss: 0.0779\n",
            "Epoch [4992/20000], Training Loss: 0.0775\n",
            "Epoch [4993/20000], Training Loss: 0.0848\n",
            "Epoch [4994/20000], Training Loss: 0.0774\n",
            "Epoch [4995/20000], Training Loss: 0.0795\n",
            "Epoch [4996/20000], Training Loss: 0.0793\n",
            "Epoch [4997/20000], Training Loss: 0.0840\n",
            "Epoch [4998/20000], Training Loss: 0.0816\n",
            "Epoch [4999/20000], Training Loss: 0.0837\n",
            "Epoch [5000/20000], Training Loss: 0.0837\n",
            "Epoch [5001/20000], Training Loss: 0.0817\n",
            "Epoch [5002/20000], Training Loss: 0.0858\n",
            "Epoch [5003/20000], Training Loss: 0.0817\n",
            "Epoch [5004/20000], Training Loss: 0.0757\n",
            "Epoch [5005/20000], Training Loss: 0.0806\n",
            "Epoch [5006/20000], Training Loss: 0.0825\n",
            "Epoch [5007/20000], Training Loss: 0.0751\n",
            "Epoch [5008/20000], Training Loss: 0.0855\n",
            "Epoch [5009/20000], Training Loss: 0.0831\n",
            "Epoch [5010/20000], Training Loss: 0.0769\n",
            "Epoch [5011/20000], Training Loss: 0.0745\n",
            "Epoch [5012/20000], Training Loss: 0.0747\n",
            "Epoch [5013/20000], Training Loss: 0.0828\n",
            "Epoch [5014/20000], Training Loss: 0.0809\n",
            "Epoch [5015/20000], Training Loss: 0.0783\n",
            "Epoch [5016/20000], Training Loss: 0.0811\n",
            "Epoch [5017/20000], Training Loss: 0.0811\n",
            "Epoch [5018/20000], Training Loss: 0.0794\n",
            "Epoch [5019/20000], Training Loss: 0.0836\n",
            "Epoch [5020/20000], Training Loss: 0.0831\n",
            "Epoch [5021/20000], Training Loss: 0.0734\n",
            "Epoch [5022/20000], Training Loss: 0.0730\n",
            "Epoch [5023/20000], Training Loss: 0.0722\n",
            "Epoch [5024/20000], Training Loss: 0.0763\n",
            "Epoch [5025/20000], Training Loss: 0.0781\n",
            "Epoch [5026/20000], Training Loss: 0.0773\n",
            "Epoch [5027/20000], Training Loss: 0.0811\n",
            "Epoch [5028/20000], Training Loss: 0.0802\n",
            "Epoch [5029/20000], Training Loss: 0.0827\n",
            "Epoch [5030/20000], Training Loss: 0.0787\n",
            "Epoch [5031/20000], Training Loss: 0.0823\n",
            "Epoch [5032/20000], Training Loss: 0.0807\n",
            "Epoch [5033/20000], Training Loss: 0.0814\n",
            "Epoch [5034/20000], Training Loss: 0.0811\n",
            "Epoch [5035/20000], Training Loss: 0.0811\n",
            "Epoch [5036/20000], Training Loss: 0.0798\n",
            "Epoch [5037/20000], Training Loss: 0.0804\n",
            "Epoch [5038/20000], Training Loss: 0.0816\n",
            "Epoch [5039/20000], Training Loss: 0.0778\n",
            "Epoch [5040/20000], Training Loss: 0.0762\n",
            "Epoch [5041/20000], Training Loss: 0.0822\n",
            "Epoch [5042/20000], Training Loss: 0.0797\n",
            "Epoch [5043/20000], Training Loss: 0.0840\n",
            "Epoch [5044/20000], Training Loss: 0.0742\n",
            "Epoch [5045/20000], Training Loss: 0.0747\n",
            "Epoch [5046/20000], Training Loss: 0.0778\n",
            "Epoch [5047/20000], Training Loss: 0.0750\n",
            "Epoch [5048/20000], Training Loss: 0.0801\n",
            "Epoch [5049/20000], Training Loss: 0.0867\n",
            "Epoch [5050/20000], Training Loss: 0.0792\n",
            "Epoch [5051/20000], Training Loss: 0.0793\n",
            "Epoch [5052/20000], Training Loss: 0.0793\n",
            "Epoch [5053/20000], Training Loss: 0.0786\n",
            "Epoch [5054/20000], Training Loss: 0.0804\n",
            "Epoch [5055/20000], Training Loss: 0.0756\n",
            "Epoch [5056/20000], Training Loss: 0.0794\n",
            "Epoch [5057/20000], Training Loss: 0.0797\n",
            "Epoch [5058/20000], Training Loss: 0.0825\n",
            "Epoch [5059/20000], Training Loss: 0.0811\n",
            "Epoch [5060/20000], Training Loss: 0.0830\n",
            "Epoch [5061/20000], Training Loss: 0.0854\n",
            "Epoch [5062/20000], Training Loss: 0.0837\n",
            "Epoch [5063/20000], Training Loss: 0.0803\n",
            "Epoch [5064/20000], Training Loss: 0.0745\n",
            "Epoch [5065/20000], Training Loss: 0.0821\n",
            "Epoch [5066/20000], Training Loss: 0.0777\n",
            "Epoch [5067/20000], Training Loss: 0.0790\n",
            "Epoch [5068/20000], Training Loss: 0.0781\n",
            "Epoch [5069/20000], Training Loss: 0.0842\n",
            "Epoch [5070/20000], Training Loss: 0.0755\n",
            "Epoch [5071/20000], Training Loss: 0.0743\n",
            "Epoch [5072/20000], Training Loss: 0.0850\n",
            "Epoch [5073/20000], Training Loss: 0.0813\n",
            "Epoch [5074/20000], Training Loss: 0.0825\n",
            "Epoch [5075/20000], Training Loss: 0.0823\n",
            "Epoch [5076/20000], Training Loss: 0.0774\n",
            "Epoch [5077/20000], Training Loss: 0.0761\n",
            "Epoch [5078/20000], Training Loss: 0.0785\n",
            "Epoch [5079/20000], Training Loss: 0.0859\n",
            "Epoch [5080/20000], Training Loss: 0.0801\n",
            "Epoch [5081/20000], Training Loss: 0.0843\n",
            "Epoch [5082/20000], Training Loss: 0.0825\n",
            "Epoch [5083/20000], Training Loss: 0.0783\n",
            "Epoch [5084/20000], Training Loss: 0.0793\n",
            "Epoch [5085/20000], Training Loss: 0.0748\n",
            "Epoch [5086/20000], Training Loss: 0.0800\n",
            "Epoch [5087/20000], Training Loss: 0.0868\n",
            "Epoch [5088/20000], Training Loss: 0.0832\n",
            "Epoch [5089/20000], Training Loss: 0.0878\n",
            "Epoch [5090/20000], Training Loss: 0.0806\n",
            "Epoch [5091/20000], Training Loss: 0.0812\n",
            "Epoch [5092/20000], Training Loss: 0.0725\n",
            "Epoch [5093/20000], Training Loss: 0.0844\n",
            "Epoch [5094/20000], Training Loss: 0.0851\n",
            "Epoch [5095/20000], Training Loss: 0.0751\n",
            "Epoch [5096/20000], Training Loss: 0.0752\n",
            "Epoch [5097/20000], Training Loss: 0.0785\n",
            "Epoch [5098/20000], Training Loss: 0.0842\n",
            "Epoch [5099/20000], Training Loss: 0.0808\n",
            "Epoch [5100/20000], Training Loss: 0.0793\n",
            "Epoch [5101/20000], Training Loss: 0.0796\n",
            "Epoch [5102/20000], Training Loss: 0.0802\n",
            "Epoch [5103/20000], Training Loss: 0.0807\n",
            "Epoch [5104/20000], Training Loss: 0.0725\n",
            "Epoch [5105/20000], Training Loss: 0.0809\n",
            "Epoch [5106/20000], Training Loss: 0.0761\n",
            "Epoch [5107/20000], Training Loss: 0.0828\n",
            "Epoch [5108/20000], Training Loss: 0.0776\n",
            "Epoch [5109/20000], Training Loss: 0.0744\n",
            "Epoch [5110/20000], Training Loss: 0.0745\n",
            "Epoch [5111/20000], Training Loss: 0.0847\n",
            "Epoch [5112/20000], Training Loss: 0.0811\n",
            "Epoch [5113/20000], Training Loss: 0.0789\n",
            "Epoch [5114/20000], Training Loss: 0.0790\n",
            "Epoch [5115/20000], Training Loss: 0.0808\n",
            "Epoch [5116/20000], Training Loss: 0.0861\n",
            "Epoch [5117/20000], Training Loss: 0.0804\n",
            "Epoch [5118/20000], Training Loss: 0.0816\n",
            "Epoch [5119/20000], Training Loss: 0.0801\n",
            "Epoch [5120/20000], Training Loss: 0.0804\n",
            "Epoch [5121/20000], Training Loss: 0.0807\n",
            "Epoch [5122/20000], Training Loss: 0.0792\n",
            "Epoch [5123/20000], Training Loss: 0.0768\n",
            "Epoch [5124/20000], Training Loss: 0.0804\n",
            "Epoch [5125/20000], Training Loss: 0.0771\n",
            "Epoch [5126/20000], Training Loss: 0.0748\n",
            "Epoch [5127/20000], Training Loss: 0.0812\n",
            "Epoch [5128/20000], Training Loss: 0.0802\n",
            "Epoch [5129/20000], Training Loss: 0.0828\n",
            "Epoch [5130/20000], Training Loss: 0.0795\n",
            "Epoch [5131/20000], Training Loss: 0.0854\n",
            "Epoch [5132/20000], Training Loss: 0.0798\n",
            "Epoch [5133/20000], Training Loss: 0.0730\n",
            "Epoch [5134/20000], Training Loss: 0.0823\n",
            "Epoch [5135/20000], Training Loss: 0.0739\n",
            "Epoch [5136/20000], Training Loss: 0.0816\n",
            "Epoch [5137/20000], Training Loss: 0.0788\n",
            "Epoch [5138/20000], Training Loss: 0.0818\n",
            "Epoch [5139/20000], Training Loss: 0.0755\n",
            "Epoch [5140/20000], Training Loss: 0.0789\n",
            "Epoch [5141/20000], Training Loss: 0.0772\n",
            "Epoch [5142/20000], Training Loss: 0.0738\n",
            "Epoch [5143/20000], Training Loss: 0.0776\n",
            "Epoch [5144/20000], Training Loss: 0.0783\n",
            "Epoch [5145/20000], Training Loss: 0.0792\n",
            "Epoch [5146/20000], Training Loss: 0.0769\n",
            "Epoch [5147/20000], Training Loss: 0.0749\n",
            "Epoch [5148/20000], Training Loss: 0.0828\n",
            "Epoch [5149/20000], Training Loss: 0.0824\n",
            "Epoch [5150/20000], Training Loss: 0.0853\n",
            "Epoch [5151/20000], Training Loss: 0.0715\n",
            "Epoch [5152/20000], Training Loss: 0.0767\n",
            "Epoch [5153/20000], Training Loss: 0.0858\n",
            "Epoch [5154/20000], Training Loss: 0.0819\n",
            "Epoch [5155/20000], Training Loss: 0.0831\n",
            "Epoch [5156/20000], Training Loss: 0.0760\n",
            "Epoch [5157/20000], Training Loss: 0.0741\n",
            "Epoch [5158/20000], Training Loss: 0.0748\n",
            "Epoch [5159/20000], Training Loss: 0.0787\n",
            "Epoch [5160/20000], Training Loss: 0.0790\n",
            "Epoch [5161/20000], Training Loss: 0.0829\n",
            "Epoch [5162/20000], Training Loss: 0.0777\n",
            "Epoch [5163/20000], Training Loss: 0.0801\n",
            "Epoch [5164/20000], Training Loss: 0.0780\n",
            "Epoch [5165/20000], Training Loss: 0.0755\n",
            "Epoch [5166/20000], Training Loss: 0.0859\n",
            "Epoch [5167/20000], Training Loss: 0.0812\n",
            "Epoch [5168/20000], Training Loss: 0.0776\n",
            "Epoch [5169/20000], Training Loss: 0.0778\n",
            "Epoch [5170/20000], Training Loss: 0.0826\n",
            "Epoch [5171/20000], Training Loss: 0.0746\n",
            "Epoch [5172/20000], Training Loss: 0.0799\n",
            "Epoch [5173/20000], Training Loss: 0.0852\n",
            "Epoch [5174/20000], Training Loss: 0.0807\n",
            "Epoch [5175/20000], Training Loss: 0.0765\n",
            "Epoch [5176/20000], Training Loss: 0.0768\n",
            "Epoch [5177/20000], Training Loss: 0.0874\n",
            "Epoch [5178/20000], Training Loss: 0.0748\n",
            "Epoch [5179/20000], Training Loss: 0.0750\n",
            "Epoch [5180/20000], Training Loss: 0.0816\n",
            "Epoch [5181/20000], Training Loss: 0.0812\n",
            "Epoch [5182/20000], Training Loss: 0.0780\n",
            "Epoch [5183/20000], Training Loss: 0.0819\n",
            "Epoch [5184/20000], Training Loss: 0.0768\n",
            "Epoch [5185/20000], Training Loss: 0.0756\n",
            "Epoch [5186/20000], Training Loss: 0.0837\n",
            "Epoch [5187/20000], Training Loss: 0.0795\n",
            "Epoch [5188/20000], Training Loss: 0.0726\n",
            "Epoch [5189/20000], Training Loss: 0.0769\n",
            "Epoch [5190/20000], Training Loss: 0.0800\n",
            "Epoch [5191/20000], Training Loss: 0.0804\n",
            "Epoch [5192/20000], Training Loss: 0.0753\n",
            "Epoch [5193/20000], Training Loss: 0.0817\n",
            "Epoch [5194/20000], Training Loss: 0.0867\n",
            "Epoch [5195/20000], Training Loss: 0.0846\n",
            "Epoch [5196/20000], Training Loss: 0.0802\n",
            "Epoch [5197/20000], Training Loss: 0.0793\n",
            "Epoch [5198/20000], Training Loss: 0.0859\n",
            "Epoch [5199/20000], Training Loss: 0.0747\n",
            "Epoch [5200/20000], Training Loss: 0.0751\n",
            "Epoch [5201/20000], Training Loss: 0.0857\n",
            "Epoch [5202/20000], Training Loss: 0.0843\n",
            "Epoch [5203/20000], Training Loss: 0.0765\n",
            "Epoch [5204/20000], Training Loss: 0.0770\n",
            "Epoch [5205/20000], Training Loss: 0.0826\n",
            "Epoch [5206/20000], Training Loss: 0.0804\n",
            "Epoch [5207/20000], Training Loss: 0.0818\n",
            "Epoch [5208/20000], Training Loss: 0.0783\n",
            "Epoch [5209/20000], Training Loss: 0.0801\n",
            "Epoch [5210/20000], Training Loss: 0.0762\n",
            "Epoch [5211/20000], Training Loss: 0.0754\n",
            "Epoch [5212/20000], Training Loss: 0.0789\n",
            "Epoch [5213/20000], Training Loss: 0.0840\n",
            "Epoch [5214/20000], Training Loss: 0.0811\n",
            "Epoch [5215/20000], Training Loss: 0.0789\n",
            "Epoch [5216/20000], Training Loss: 0.0856\n",
            "Epoch [5217/20000], Training Loss: 0.0766\n",
            "Epoch [5218/20000], Training Loss: 0.0742\n",
            "Epoch [5219/20000], Training Loss: 0.0754\n",
            "Epoch [5220/20000], Training Loss: 0.0807\n",
            "Epoch [5221/20000], Training Loss: 0.0832\n",
            "Epoch [5222/20000], Training Loss: 0.0779\n",
            "Epoch [5223/20000], Training Loss: 0.0840\n",
            "Epoch [5224/20000], Training Loss: 0.0812\n",
            "Epoch [5225/20000], Training Loss: 0.0803\n",
            "Epoch [5226/20000], Training Loss: 0.0745\n",
            "Epoch [5227/20000], Training Loss: 0.0804\n",
            "Epoch [5228/20000], Training Loss: 0.0765\n",
            "Epoch [5229/20000], Training Loss: 0.0779\n",
            "Epoch [5230/20000], Training Loss: 0.0844\n",
            "Epoch [5231/20000], Training Loss: 0.0737\n",
            "Epoch [5232/20000], Training Loss: 0.0824\n",
            "Epoch [5233/20000], Training Loss: 0.0776\n",
            "Epoch [5234/20000], Training Loss: 0.0735\n",
            "Epoch [5235/20000], Training Loss: 0.0793\n",
            "Epoch [5236/20000], Training Loss: 0.0744\n",
            "Epoch [5237/20000], Training Loss: 0.0806\n",
            "Epoch [5238/20000], Training Loss: 0.0819\n",
            "Epoch [5239/20000], Training Loss: 0.0793\n",
            "Epoch [5240/20000], Training Loss: 0.0803\n",
            "Epoch [5241/20000], Training Loss: 0.0834\n",
            "Epoch [5242/20000], Training Loss: 0.0751\n",
            "Epoch [5243/20000], Training Loss: 0.0744\n",
            "Epoch [5244/20000], Training Loss: 0.0773\n",
            "Epoch [5245/20000], Training Loss: 0.0768\n",
            "Epoch [5246/20000], Training Loss: 0.0783\n",
            "Epoch [5247/20000], Training Loss: 0.0764\n",
            "Epoch [5248/20000], Training Loss: 0.0849\n",
            "Epoch [5249/20000], Training Loss: 0.0857\n",
            "Epoch [5250/20000], Training Loss: 0.0887\n",
            "Epoch [5251/20000], Training Loss: 0.0786\n",
            "Epoch [5252/20000], Training Loss: 0.0814\n",
            "Epoch [5253/20000], Training Loss: 0.0789\n",
            "Epoch [5254/20000], Training Loss: 0.0874\n",
            "Epoch [5255/20000], Training Loss: 0.0858\n",
            "Epoch [5256/20000], Training Loss: 0.0806\n",
            "Epoch [5257/20000], Training Loss: 0.0807\n",
            "Epoch [5258/20000], Training Loss: 0.0855\n",
            "Epoch [5259/20000], Training Loss: 0.0745\n",
            "Epoch [5260/20000], Training Loss: 0.0809\n",
            "Epoch [5261/20000], Training Loss: 0.0861\n",
            "Epoch [5262/20000], Training Loss: 0.0846\n",
            "Epoch [5263/20000], Training Loss: 0.0782\n",
            "Epoch [5264/20000], Training Loss: 0.0822\n",
            "Epoch [5265/20000], Training Loss: 0.0788\n",
            "Epoch [5266/20000], Training Loss: 0.0753\n",
            "Epoch [5267/20000], Training Loss: 0.0792\n",
            "Epoch [5268/20000], Training Loss: 0.0757\n",
            "Epoch [5269/20000], Training Loss: 0.0797\n",
            "Epoch [5270/20000], Training Loss: 0.0817\n",
            "Epoch [5271/20000], Training Loss: 0.0806\n",
            "Epoch [5272/20000], Training Loss: 0.0799\n",
            "Epoch [5273/20000], Training Loss: 0.0805\n",
            "Epoch [5274/20000], Training Loss: 0.0751\n",
            "Epoch [5275/20000], Training Loss: 0.0785\n",
            "Epoch [5276/20000], Training Loss: 0.0771\n",
            "Epoch [5277/20000], Training Loss: 0.0828\n",
            "Epoch [5278/20000], Training Loss: 0.0829\n",
            "Epoch [5279/20000], Training Loss: 0.0809\n",
            "Epoch [5280/20000], Training Loss: 0.0765\n",
            "Epoch [5281/20000], Training Loss: 0.0846\n",
            "Epoch [5282/20000], Training Loss: 0.0781\n",
            "Epoch [5283/20000], Training Loss: 0.0842\n",
            "Epoch [5284/20000], Training Loss: 0.0787\n",
            "Epoch [5285/20000], Training Loss: 0.0834\n",
            "Epoch [5286/20000], Training Loss: 0.0728\n",
            "Epoch [5287/20000], Training Loss: 0.0791\n",
            "Epoch [5288/20000], Training Loss: 0.0824\n",
            "Epoch [5289/20000], Training Loss: 0.0801\n",
            "Epoch [5290/20000], Training Loss: 0.0784\n",
            "Epoch [5291/20000], Training Loss: 0.0807\n",
            "Epoch [5292/20000], Training Loss: 0.0881\n",
            "Epoch [5293/20000], Training Loss: 0.0768\n",
            "Epoch [5294/20000], Training Loss: 0.0780\n",
            "Epoch [5295/20000], Training Loss: 0.0776\n",
            "Epoch [5296/20000], Training Loss: 0.0854\n",
            "Epoch [5297/20000], Training Loss: 0.0800\n",
            "Epoch [5298/20000], Training Loss: 0.0748\n",
            "Epoch [5299/20000], Training Loss: 0.0782\n",
            "Epoch [5300/20000], Training Loss: 0.0797\n",
            "Epoch [5301/20000], Training Loss: 0.0725\n",
            "Epoch [5302/20000], Training Loss: 0.0769\n",
            "Epoch [5303/20000], Training Loss: 0.0821\n",
            "Epoch [5304/20000], Training Loss: 0.0851\n",
            "Epoch [5305/20000], Training Loss: 0.0753\n",
            "Epoch [5306/20000], Training Loss: 0.0774\n",
            "Epoch [5307/20000], Training Loss: 0.0809\n",
            "Epoch [5308/20000], Training Loss: 0.0809\n",
            "Epoch [5309/20000], Training Loss: 0.0754\n",
            "Epoch [5310/20000], Training Loss: 0.0832\n",
            "Epoch [5311/20000], Training Loss: 0.0813\n",
            "Epoch [5312/20000], Training Loss: 0.0777\n",
            "Epoch [5313/20000], Training Loss: 0.0837\n",
            "Epoch [5314/20000], Training Loss: 0.0786\n",
            "Epoch [5315/20000], Training Loss: 0.0720\n",
            "Epoch [5316/20000], Training Loss: 0.0805\n",
            "Epoch [5317/20000], Training Loss: 0.0816\n",
            "Epoch [5318/20000], Training Loss: 0.0814\n",
            "Epoch [5319/20000], Training Loss: 0.0776\n",
            "Epoch [5320/20000], Training Loss: 0.0824\n",
            "Epoch [5321/20000], Training Loss: 0.0824\n",
            "Epoch [5322/20000], Training Loss: 0.0747\n",
            "Epoch [5323/20000], Training Loss: 0.0785\n",
            "Epoch [5324/20000], Training Loss: 0.0753\n",
            "Epoch [5325/20000], Training Loss: 0.0754\n",
            "Epoch [5326/20000], Training Loss: 0.0759\n",
            "Epoch [5327/20000], Training Loss: 0.0741\n",
            "Epoch [5328/20000], Training Loss: 0.0770\n",
            "Epoch [5329/20000], Training Loss: 0.0805\n",
            "Epoch [5330/20000], Training Loss: 0.0758\n",
            "Epoch [5331/20000], Training Loss: 0.0807\n",
            "Epoch [5332/20000], Training Loss: 0.0839\n",
            "Epoch [5333/20000], Training Loss: 0.0838\n",
            "Epoch [5334/20000], Training Loss: 0.0799\n",
            "Epoch [5335/20000], Training Loss: 0.0738\n",
            "Epoch [5336/20000], Training Loss: 0.0841\n",
            "Epoch [5337/20000], Training Loss: 0.0748\n",
            "Epoch [5338/20000], Training Loss: 0.0815\n",
            "Epoch [5339/20000], Training Loss: 0.0801\n",
            "Epoch [5340/20000], Training Loss: 0.0733\n",
            "Epoch [5341/20000], Training Loss: 0.0748\n",
            "Epoch [5342/20000], Training Loss: 0.0814\n",
            "Epoch [5343/20000], Training Loss: 0.0865\n",
            "Epoch [5344/20000], Training Loss: 0.0787\n",
            "Epoch [5345/20000], Training Loss: 0.0804\n",
            "Epoch [5346/20000], Training Loss: 0.0765\n",
            "Epoch [5347/20000], Training Loss: 0.0819\n",
            "Epoch [5348/20000], Training Loss: 0.0793\n",
            "Epoch [5349/20000], Training Loss: 0.0818\n",
            "Epoch [5350/20000], Training Loss: 0.0866\n",
            "Epoch [5351/20000], Training Loss: 0.0778\n",
            "Epoch [5352/20000], Training Loss: 0.0843\n",
            "Epoch [5353/20000], Training Loss: 0.0791\n",
            "Epoch [5354/20000], Training Loss: 0.0761\n",
            "Epoch [5355/20000], Training Loss: 0.0785\n",
            "Epoch [5356/20000], Training Loss: 0.0732\n",
            "Epoch [5357/20000], Training Loss: 0.0843\n",
            "Epoch [5358/20000], Training Loss: 0.0816\n",
            "Epoch [5359/20000], Training Loss: 0.0749\n",
            "Epoch [5360/20000], Training Loss: 0.0793\n",
            "Epoch [5361/20000], Training Loss: 0.0772\n",
            "Epoch [5362/20000], Training Loss: 0.0810\n",
            "Epoch [5363/20000], Training Loss: 0.0800\n",
            "Epoch [5364/20000], Training Loss: 0.0746\n",
            "Epoch [5365/20000], Training Loss: 0.0803\n",
            "Epoch [5366/20000], Training Loss: 0.0742\n",
            "Epoch [5367/20000], Training Loss: 0.0814\n",
            "Epoch [5368/20000], Training Loss: 0.0769\n",
            "Epoch [5369/20000], Training Loss: 0.0762\n",
            "Epoch [5370/20000], Training Loss: 0.0766\n",
            "Epoch [5371/20000], Training Loss: 0.0806\n",
            "Epoch [5372/20000], Training Loss: 0.0798\n",
            "Epoch [5373/20000], Training Loss: 0.0792\n",
            "Epoch [5374/20000], Training Loss: 0.0756\n",
            "Epoch [5375/20000], Training Loss: 0.0827\n",
            "Epoch [5376/20000], Training Loss: 0.0756\n",
            "Epoch [5377/20000], Training Loss: 0.0790\n",
            "Epoch [5378/20000], Training Loss: 0.0770\n",
            "Epoch [5379/20000], Training Loss: 0.0802\n",
            "Epoch [5380/20000], Training Loss: 0.0869\n",
            "Epoch [5381/20000], Training Loss: 0.0799\n",
            "Epoch [5382/20000], Training Loss: 0.0796\n",
            "Epoch [5383/20000], Training Loss: 0.0792\n",
            "Epoch [5384/20000], Training Loss: 0.0743\n",
            "Epoch [5385/20000], Training Loss: 0.0764\n",
            "Epoch [5386/20000], Training Loss: 0.0818\n",
            "Epoch [5387/20000], Training Loss: 0.0786\n",
            "Epoch [5388/20000], Training Loss: 0.0777\n",
            "Epoch [5389/20000], Training Loss: 0.0770\n",
            "Epoch [5390/20000], Training Loss: 0.0842\n",
            "Epoch [5391/20000], Training Loss: 0.0801\n",
            "Epoch [5392/20000], Training Loss: 0.0809\n",
            "Epoch [5393/20000], Training Loss: 0.0856\n",
            "Epoch [5394/20000], Training Loss: 0.0777\n",
            "Epoch [5395/20000], Training Loss: 0.0720\n",
            "Epoch [5396/20000], Training Loss: 0.0742\n",
            "Epoch [5397/20000], Training Loss: 0.0827\n",
            "Epoch [5398/20000], Training Loss: 0.0804\n",
            "Epoch [5399/20000], Training Loss: 0.0727\n",
            "Epoch [5400/20000], Training Loss: 0.0824\n",
            "Epoch [5401/20000], Training Loss: 0.0770\n",
            "Epoch [5402/20000], Training Loss: 0.0799\n",
            "Epoch [5403/20000], Training Loss: 0.0758\n",
            "Epoch [5404/20000], Training Loss: 0.0842\n",
            "Epoch [5405/20000], Training Loss: 0.0823\n",
            "Epoch [5406/20000], Training Loss: 0.0796\n",
            "Epoch [5407/20000], Training Loss: 0.0743\n",
            "Epoch [5408/20000], Training Loss: 0.0764\n",
            "Epoch [5409/20000], Training Loss: 0.0769\n",
            "Epoch [5410/20000], Training Loss: 0.0805\n",
            "Epoch [5411/20000], Training Loss: 0.0754\n",
            "Epoch [5412/20000], Training Loss: 0.0761\n",
            "Epoch [5413/20000], Training Loss: 0.0806\n",
            "Epoch [5414/20000], Training Loss: 0.0752\n",
            "Epoch [5415/20000], Training Loss: 0.0767\n",
            "Epoch [5416/20000], Training Loss: 0.0743\n",
            "Epoch [5417/20000], Training Loss: 0.0759\n",
            "Epoch [5418/20000], Training Loss: 0.0769\n",
            "Epoch [5419/20000], Training Loss: 0.0815\n",
            "Epoch [5420/20000], Training Loss: 0.0798\n",
            "Epoch [5421/20000], Training Loss: 0.0795\n",
            "Epoch [5422/20000], Training Loss: 0.0812\n",
            "Epoch [5423/20000], Training Loss: 0.0740\n",
            "Epoch [5424/20000], Training Loss: 0.0803\n",
            "Epoch [5425/20000], Training Loss: 0.0791\n",
            "Epoch [5426/20000], Training Loss: 0.0812\n",
            "Epoch [5427/20000], Training Loss: 0.0762\n",
            "Epoch [5428/20000], Training Loss: 0.0818\n",
            "Epoch [5429/20000], Training Loss: 0.0865\n",
            "Epoch [5430/20000], Training Loss: 0.0751\n",
            "Epoch [5431/20000], Training Loss: 0.0810\n",
            "Epoch [5432/20000], Training Loss: 0.0774\n",
            "Epoch [5433/20000], Training Loss: 0.0825\n",
            "Epoch [5434/20000], Training Loss: 0.0777\n",
            "Epoch [5435/20000], Training Loss: 0.0785\n",
            "Epoch [5436/20000], Training Loss: 0.0795\n",
            "Epoch [5437/20000], Training Loss: 0.0791\n",
            "Epoch [5438/20000], Training Loss: 0.0853\n",
            "Epoch [5439/20000], Training Loss: 0.0857\n",
            "Epoch [5440/20000], Training Loss: 0.0832\n",
            "Epoch [5441/20000], Training Loss: 0.0791\n",
            "Epoch [5442/20000], Training Loss: 0.0814\n",
            "Epoch [5443/20000], Training Loss: 0.0743\n",
            "Epoch [5444/20000], Training Loss: 0.0851\n",
            "Epoch [5445/20000], Training Loss: 0.0807\n",
            "Epoch [5446/20000], Training Loss: 0.0808\n",
            "Epoch [5447/20000], Training Loss: 0.0761\n",
            "Epoch [5448/20000], Training Loss: 0.0824\n",
            "Epoch [5449/20000], Training Loss: 0.0803\n",
            "Epoch [5450/20000], Training Loss: 0.0758\n",
            "Epoch [5451/20000], Training Loss: 0.0739\n",
            "Epoch [5452/20000], Training Loss: 0.0756\n",
            "Epoch [5453/20000], Training Loss: 0.0819\n",
            "Epoch [5454/20000], Training Loss: 0.0787\n",
            "Epoch [5455/20000], Training Loss: 0.0800\n",
            "Epoch [5456/20000], Training Loss: 0.0780\n",
            "Epoch [5457/20000], Training Loss: 0.0814\n",
            "Epoch [5458/20000], Training Loss: 0.0827\n",
            "Epoch [5459/20000], Training Loss: 0.0808\n",
            "Epoch [5460/20000], Training Loss: 0.0735\n",
            "Epoch [5461/20000], Training Loss: 0.0751\n",
            "Epoch [5462/20000], Training Loss: 0.0765\n",
            "Epoch [5463/20000], Training Loss: 0.0756\n",
            "Epoch [5464/20000], Training Loss: 0.0830\n",
            "Epoch [5465/20000], Training Loss: 0.0801\n",
            "Epoch [5466/20000], Training Loss: 0.0789\n",
            "Epoch [5467/20000], Training Loss: 0.0849\n",
            "Epoch [5468/20000], Training Loss: 0.0807\n",
            "Epoch [5469/20000], Training Loss: 0.0803\n",
            "Epoch [5470/20000], Training Loss: 0.0774\n",
            "Epoch [5471/20000], Training Loss: 0.0759\n",
            "Epoch [5472/20000], Training Loss: 0.0803\n",
            "Epoch [5473/20000], Training Loss: 0.0767\n",
            "Epoch [5474/20000], Training Loss: 0.0763\n",
            "Epoch [5475/20000], Training Loss: 0.0849\n",
            "Epoch [5476/20000], Training Loss: 0.0783\n",
            "Epoch [5477/20000], Training Loss: 0.0785\n",
            "Epoch [5478/20000], Training Loss: 0.0805\n",
            "Epoch [5479/20000], Training Loss: 0.0869\n",
            "Epoch [5480/20000], Training Loss: 0.0805\n",
            "Epoch [5481/20000], Training Loss: 0.0779\n",
            "Epoch [5482/20000], Training Loss: 0.0799\n",
            "Epoch [5483/20000], Training Loss: 0.0807\n",
            "Epoch [5484/20000], Training Loss: 0.0805\n",
            "Epoch [5485/20000], Training Loss: 0.0754\n",
            "Epoch [5486/20000], Training Loss: 0.0776\n",
            "Epoch [5487/20000], Training Loss: 0.0808\n",
            "Epoch [5488/20000], Training Loss: 0.0736\n",
            "Epoch [5489/20000], Training Loss: 0.0758\n",
            "Epoch [5490/20000], Training Loss: 0.0775\n",
            "Epoch [5491/20000], Training Loss: 0.0779\n",
            "Epoch [5492/20000], Training Loss: 0.0807\n",
            "Epoch [5493/20000], Training Loss: 0.0823\n",
            "Epoch [5494/20000], Training Loss: 0.0779\n",
            "Epoch [5495/20000], Training Loss: 0.0831\n",
            "Epoch [5496/20000], Training Loss: 0.0799\n",
            "Epoch [5497/20000], Training Loss: 0.0750\n",
            "Epoch [5498/20000], Training Loss: 0.0756\n",
            "Epoch [5499/20000], Training Loss: 0.0745\n",
            "Epoch [5500/20000], Training Loss: 0.0864\n",
            "Epoch [5501/20000], Training Loss: 0.0802\n",
            "Epoch [5502/20000], Training Loss: 0.0778\n",
            "Epoch [5503/20000], Training Loss: 0.0748\n",
            "Epoch [5504/20000], Training Loss: 0.0788\n",
            "Epoch [5505/20000], Training Loss: 0.0754\n",
            "Epoch [5506/20000], Training Loss: 0.0861\n",
            "Epoch [5507/20000], Training Loss: 0.0844\n",
            "Epoch [5508/20000], Training Loss: 0.0786\n",
            "Epoch [5509/20000], Training Loss: 0.0767\n",
            "Epoch [5510/20000], Training Loss: 0.0803\n",
            "Epoch [5511/20000], Training Loss: 0.0797\n",
            "Epoch [5512/20000], Training Loss: 0.0815\n",
            "Epoch [5513/20000], Training Loss: 0.0835\n",
            "Epoch [5514/20000], Training Loss: 0.0810\n",
            "Epoch [5515/20000], Training Loss: 0.0853\n",
            "Epoch [5516/20000], Training Loss: 0.0824\n",
            "Epoch [5517/20000], Training Loss: 0.0849\n",
            "Epoch [5518/20000], Training Loss: 0.0821\n",
            "Epoch [5519/20000], Training Loss: 0.0774\n",
            "Epoch [5520/20000], Training Loss: 0.0791\n",
            "Epoch [5521/20000], Training Loss: 0.0884\n",
            "Epoch [5522/20000], Training Loss: 0.0812\n",
            "Epoch [5523/20000], Training Loss: 0.0772\n",
            "Epoch [5524/20000], Training Loss: 0.0810\n",
            "Epoch [5525/20000], Training Loss: 0.0794\n",
            "Epoch [5526/20000], Training Loss: 0.0802\n",
            "Epoch [5527/20000], Training Loss: 0.0748\n",
            "Epoch [5528/20000], Training Loss: 0.0796\n",
            "Epoch [5529/20000], Training Loss: 0.0755\n",
            "Epoch [5530/20000], Training Loss: 0.0745\n",
            "Epoch [5531/20000], Training Loss: 0.0776\n",
            "Epoch [5532/20000], Training Loss: 0.0814\n",
            "Epoch [5533/20000], Training Loss: 0.0798\n",
            "Epoch [5534/20000], Training Loss: 0.0762\n",
            "Epoch [5535/20000], Training Loss: 0.0812\n",
            "Epoch [5536/20000], Training Loss: 0.0811\n",
            "Epoch [5537/20000], Training Loss: 0.0778\n",
            "Epoch [5538/20000], Training Loss: 0.0850\n",
            "Epoch [5539/20000], Training Loss: 0.0771\n",
            "Epoch [5540/20000], Training Loss: 0.0766\n",
            "Epoch [5541/20000], Training Loss: 0.0772\n",
            "Epoch [5542/20000], Training Loss: 0.0845\n",
            "Epoch [5543/20000], Training Loss: 0.0790\n",
            "Epoch [5544/20000], Training Loss: 0.0827\n",
            "Epoch [5545/20000], Training Loss: 0.0792\n",
            "Epoch [5546/20000], Training Loss: 0.0778\n",
            "Epoch [5547/20000], Training Loss: 0.0748\n",
            "Epoch [5548/20000], Training Loss: 0.0758\n",
            "Epoch [5549/20000], Training Loss: 0.0859\n",
            "Epoch [5550/20000], Training Loss: 0.0826\n",
            "Epoch [5551/20000], Training Loss: 0.0846\n",
            "Epoch [5552/20000], Training Loss: 0.0857\n",
            "Epoch [5553/20000], Training Loss: 0.0747\n",
            "Epoch [5554/20000], Training Loss: 0.0792\n",
            "Epoch [5555/20000], Training Loss: 0.0810\n",
            "Epoch [5556/20000], Training Loss: 0.0808\n",
            "Epoch [5557/20000], Training Loss: 0.0822\n",
            "Epoch [5558/20000], Training Loss: 0.0742\n",
            "Epoch [5559/20000], Training Loss: 0.0738\n",
            "Epoch [5560/20000], Training Loss: 0.0822\n",
            "Epoch [5561/20000], Training Loss: 0.0784\n",
            "Epoch [5562/20000], Training Loss: 0.0775\n",
            "Epoch [5563/20000], Training Loss: 0.0770\n",
            "Epoch [5564/20000], Training Loss: 0.0846\n",
            "Epoch [5565/20000], Training Loss: 0.0764\n",
            "Epoch [5566/20000], Training Loss: 0.0750\n",
            "Epoch [5567/20000], Training Loss: 0.0725\n",
            "Epoch [5568/20000], Training Loss: 0.0753\n",
            "Epoch [5569/20000], Training Loss: 0.0771\n",
            "Epoch [5570/20000], Training Loss: 0.0802\n",
            "Epoch [5571/20000], Training Loss: 0.0813\n",
            "Epoch [5572/20000], Training Loss: 0.0787\n",
            "Epoch [5573/20000], Training Loss: 0.0768\n",
            "Epoch [5574/20000], Training Loss: 0.0804\n",
            "Epoch [5575/20000], Training Loss: 0.0801\n",
            "Epoch [5576/20000], Training Loss: 0.0867\n",
            "Epoch [5577/20000], Training Loss: 0.0784\n",
            "Epoch [5578/20000], Training Loss: 0.0742\n",
            "Epoch [5579/20000], Training Loss: 0.0771\n",
            "Epoch [5580/20000], Training Loss: 0.0799\n",
            "Epoch [5581/20000], Training Loss: 0.0799\n",
            "Epoch [5582/20000], Training Loss: 0.0804\n",
            "Epoch [5583/20000], Training Loss: 0.0817\n",
            "Epoch [5584/20000], Training Loss: 0.0778\n",
            "Epoch [5585/20000], Training Loss: 0.0790\n",
            "Epoch [5586/20000], Training Loss: 0.0772\n",
            "Epoch [5587/20000], Training Loss: 0.0833\n",
            "Epoch [5588/20000], Training Loss: 0.0819\n",
            "Epoch [5589/20000], Training Loss: 0.0825\n",
            "Epoch [5590/20000], Training Loss: 0.0827\n",
            "Epoch [5591/20000], Training Loss: 0.0793\n",
            "Epoch [5592/20000], Training Loss: 0.0770\n",
            "Epoch [5593/20000], Training Loss: 0.0801\n",
            "Epoch [5594/20000], Training Loss: 0.0831\n",
            "Epoch [5595/20000], Training Loss: 0.0779\n",
            "Epoch [5596/20000], Training Loss: 0.0816\n",
            "Epoch [5597/20000], Training Loss: 0.0848\n",
            "Epoch [5598/20000], Training Loss: 0.0879\n",
            "Epoch [5599/20000], Training Loss: 0.0824\n",
            "Epoch [5600/20000], Training Loss: 0.0853\n",
            "Epoch [5601/20000], Training Loss: 0.0797\n",
            "Epoch [5602/20000], Training Loss: 0.0806\n",
            "Epoch [5603/20000], Training Loss: 0.0840\n",
            "Epoch [5604/20000], Training Loss: 0.0768\n",
            "Epoch [5605/20000], Training Loss: 0.0809\n",
            "Epoch [5606/20000], Training Loss: 0.0800\n",
            "Epoch [5607/20000], Training Loss: 0.0811\n",
            "Epoch [5608/20000], Training Loss: 0.0789\n",
            "Epoch [5609/20000], Training Loss: 0.0748\n",
            "Epoch [5610/20000], Training Loss: 0.0803\n",
            "Epoch [5611/20000], Training Loss: 0.0812\n",
            "Epoch [5612/20000], Training Loss: 0.0785\n",
            "Epoch [5613/20000], Training Loss: 0.0782\n",
            "Epoch [5614/20000], Training Loss: 0.0790\n",
            "Epoch [5615/20000], Training Loss: 0.0830\n",
            "Epoch [5616/20000], Training Loss: 0.0784\n",
            "Epoch [5617/20000], Training Loss: 0.0830\n",
            "Epoch [5618/20000], Training Loss: 0.0803\n",
            "Epoch [5619/20000], Training Loss: 0.0841\n",
            "Epoch [5620/20000], Training Loss: 0.0753\n",
            "Epoch [5621/20000], Training Loss: 0.0818\n",
            "Epoch [5622/20000], Training Loss: 0.0795\n",
            "Epoch [5623/20000], Training Loss: 0.0850\n",
            "Epoch [5624/20000], Training Loss: 0.0815\n",
            "Epoch [5625/20000], Training Loss: 0.0805\n",
            "Epoch [5626/20000], Training Loss: 0.0795\n",
            "Epoch [5627/20000], Training Loss: 0.0840\n",
            "Epoch [5628/20000], Training Loss: 0.0738\n",
            "Epoch [5629/20000], Training Loss: 0.0772\n",
            "Epoch [5630/20000], Training Loss: 0.0842\n",
            "Epoch [5631/20000], Training Loss: 0.0781\n",
            "Epoch [5632/20000], Training Loss: 0.0832\n",
            "Epoch [5633/20000], Training Loss: 0.0769\n",
            "Epoch [5634/20000], Training Loss: 0.0760\n",
            "Epoch [5635/20000], Training Loss: 0.0844\n",
            "Epoch [5636/20000], Training Loss: 0.0804\n",
            "Epoch [5637/20000], Training Loss: 0.0769\n",
            "Epoch [5638/20000], Training Loss: 0.0802\n",
            "Epoch [5639/20000], Training Loss: 0.0810\n",
            "Epoch [5640/20000], Training Loss: 0.0786\n",
            "Epoch [5641/20000], Training Loss: 0.0796\n",
            "Epoch [5642/20000], Training Loss: 0.0754\n",
            "Epoch [5643/20000], Training Loss: 0.0756\n",
            "Epoch [5644/20000], Training Loss: 0.0827\n",
            "Epoch [5645/20000], Training Loss: 0.0805\n",
            "Epoch [5646/20000], Training Loss: 0.0800\n",
            "Epoch [5647/20000], Training Loss: 0.0775\n",
            "Epoch [5648/20000], Training Loss: 0.0813\n",
            "Epoch [5649/20000], Training Loss: 0.0803\n",
            "Epoch [5650/20000], Training Loss: 0.0822\n",
            "Epoch [5651/20000], Training Loss: 0.0822\n",
            "Epoch [5652/20000], Training Loss: 0.0785\n",
            "Epoch [5653/20000], Training Loss: 0.0792\n",
            "Epoch [5654/20000], Training Loss: 0.0763\n",
            "Epoch [5655/20000], Training Loss: 0.0808\n",
            "Epoch [5656/20000], Training Loss: 0.0796\n",
            "Epoch [5657/20000], Training Loss: 0.0800\n",
            "Epoch [5658/20000], Training Loss: 0.0794\n",
            "Epoch [5659/20000], Training Loss: 0.0777\n",
            "Epoch [5660/20000], Training Loss: 0.0741\n",
            "Epoch [5661/20000], Training Loss: 0.0753\n",
            "Epoch [5662/20000], Training Loss: 0.0762\n",
            "Epoch [5663/20000], Training Loss: 0.0749\n",
            "Epoch [5664/20000], Training Loss: 0.0761\n",
            "Epoch [5665/20000], Training Loss: 0.0811\n",
            "Epoch [5666/20000], Training Loss: 0.0796\n",
            "Epoch [5667/20000], Training Loss: 0.0750\n",
            "Epoch [5668/20000], Training Loss: 0.0813\n",
            "Epoch [5669/20000], Training Loss: 0.0779\n",
            "Epoch [5670/20000], Training Loss: 0.0829\n",
            "Epoch [5671/20000], Training Loss: 0.0787\n",
            "Epoch [5672/20000], Training Loss: 0.0735\n",
            "Epoch [5673/20000], Training Loss: 0.0753\n",
            "Epoch [5674/20000], Training Loss: 0.0788\n",
            "Epoch [5675/20000], Training Loss: 0.0795\n",
            "Epoch [5676/20000], Training Loss: 0.0847\n",
            "Epoch [5677/20000], Training Loss: 0.0798\n",
            "Epoch [5678/20000], Training Loss: 0.0795\n",
            "Epoch [5679/20000], Training Loss: 0.0721\n",
            "Epoch [5680/20000], Training Loss: 0.0817\n",
            "Epoch [5681/20000], Training Loss: 0.0770\n",
            "Epoch [5682/20000], Training Loss: 0.0813\n",
            "Epoch [5683/20000], Training Loss: 0.0762\n",
            "Epoch [5684/20000], Training Loss: 0.0806\n",
            "Epoch [5685/20000], Training Loss: 0.0757\n",
            "Epoch [5686/20000], Training Loss: 0.0778\n",
            "Epoch [5687/20000], Training Loss: 0.0797\n",
            "Epoch [5688/20000], Training Loss: 0.0772\n",
            "Epoch [5689/20000], Training Loss: 0.0807\n",
            "Epoch [5690/20000], Training Loss: 0.0775\n",
            "Epoch [5691/20000], Training Loss: 0.0819\n",
            "Epoch [5692/20000], Training Loss: 0.0758\n",
            "Epoch [5693/20000], Training Loss: 0.0788\n",
            "Epoch [5694/20000], Training Loss: 0.0852\n",
            "Epoch [5695/20000], Training Loss: 0.0771\n",
            "Epoch [5696/20000], Training Loss: 0.0840\n",
            "Epoch [5697/20000], Training Loss: 0.0810\n",
            "Epoch [5698/20000], Training Loss: 0.0813\n",
            "Epoch [5699/20000], Training Loss: 0.0809\n",
            "Epoch [5700/20000], Training Loss: 0.0779\n",
            "Epoch [5701/20000], Training Loss: 0.0755\n",
            "Epoch [5702/20000], Training Loss: 0.0791\n",
            "Epoch [5703/20000], Training Loss: 0.0739\n",
            "Epoch [5704/20000], Training Loss: 0.0795\n",
            "Epoch [5705/20000], Training Loss: 0.0839\n",
            "Epoch [5706/20000], Training Loss: 0.0784\n",
            "Epoch [5707/20000], Training Loss: 0.0808\n",
            "Epoch [5708/20000], Training Loss: 0.0797\n",
            "Epoch [5709/20000], Training Loss: 0.0847\n",
            "Epoch [5710/20000], Training Loss: 0.0757\n",
            "Epoch [5711/20000], Training Loss: 0.0759\n",
            "Epoch [5712/20000], Training Loss: 0.0792\n",
            "Epoch [5713/20000], Training Loss: 0.0831\n",
            "Epoch [5714/20000], Training Loss: 0.0779\n",
            "Epoch [5715/20000], Training Loss: 0.0781\n",
            "Epoch [5716/20000], Training Loss: 0.0795\n",
            "Epoch [5717/20000], Training Loss: 0.0771\n",
            "Epoch [5718/20000], Training Loss: 0.0753\n",
            "Epoch [5719/20000], Training Loss: 0.0765\n",
            "Epoch [5720/20000], Training Loss: 0.0760\n",
            "Epoch [5721/20000], Training Loss: 0.0802\n",
            "Epoch [5722/20000], Training Loss: 0.0813\n",
            "Epoch [5723/20000], Training Loss: 0.0778\n",
            "Epoch [5724/20000], Training Loss: 0.0781\n",
            "Epoch [5725/20000], Training Loss: 0.0738\n",
            "Epoch [5726/20000], Training Loss: 0.0840\n",
            "Epoch [5727/20000], Training Loss: 0.0759\n",
            "Epoch [5728/20000], Training Loss: 0.0852\n",
            "Epoch [5729/20000], Training Loss: 0.0769\n",
            "Epoch [5730/20000], Training Loss: 0.0746\n",
            "Epoch [5731/20000], Training Loss: 0.0792\n",
            "Epoch [5732/20000], Training Loss: 0.0750\n",
            "Epoch [5733/20000], Training Loss: 0.0832\n",
            "Epoch [5734/20000], Training Loss: 0.0810\n",
            "Epoch [5735/20000], Training Loss: 0.0798\n",
            "Epoch [5736/20000], Training Loss: 0.0834\n",
            "Epoch [5737/20000], Training Loss: 0.0748\n",
            "Epoch [5738/20000], Training Loss: 0.0801\n",
            "Epoch [5739/20000], Training Loss: 0.0791\n",
            "Epoch [5740/20000], Training Loss: 0.0816\n",
            "Epoch [5741/20000], Training Loss: 0.0834\n",
            "Epoch [5742/20000], Training Loss: 0.0807\n",
            "Epoch [5743/20000], Training Loss: 0.0750\n",
            "Epoch [5744/20000], Training Loss: 0.0794\n",
            "Epoch [5745/20000], Training Loss: 0.0817\n",
            "Epoch [5746/20000], Training Loss: 0.0855\n",
            "Epoch [5747/20000], Training Loss: 0.0807\n",
            "Epoch [5748/20000], Training Loss: 0.0834\n",
            "Epoch [5749/20000], Training Loss: 0.0813\n",
            "Epoch [5750/20000], Training Loss: 0.0812\n",
            "Epoch [5751/20000], Training Loss: 0.0802\n",
            "Epoch [5752/20000], Training Loss: 0.0763\n",
            "Epoch [5753/20000], Training Loss: 0.0779\n",
            "Epoch [5754/20000], Training Loss: 0.0787\n",
            "Epoch [5755/20000], Training Loss: 0.0795\n",
            "Epoch [5756/20000], Training Loss: 0.0812\n",
            "Epoch [5757/20000], Training Loss: 0.0879\n",
            "Epoch [5758/20000], Training Loss: 0.0811\n",
            "Epoch [5759/20000], Training Loss: 0.0784\n",
            "Epoch [5760/20000], Training Loss: 0.0833\n",
            "Epoch [5761/20000], Training Loss: 0.0794\n",
            "Epoch [5762/20000], Training Loss: 0.0775\n",
            "Epoch [5763/20000], Training Loss: 0.0790\n",
            "Epoch [5764/20000], Training Loss: 0.0807\n",
            "Epoch [5765/20000], Training Loss: 0.0803\n",
            "Epoch [5766/20000], Training Loss: 0.0850\n",
            "Epoch [5767/20000], Training Loss: 0.0804\n",
            "Epoch [5768/20000], Training Loss: 0.0789\n",
            "Epoch [5769/20000], Training Loss: 0.0744\n",
            "Epoch [5770/20000], Training Loss: 0.0823\n",
            "Epoch [5771/20000], Training Loss: 0.0763\n",
            "Epoch [5772/20000], Training Loss: 0.0821\n",
            "Epoch [5773/20000], Training Loss: 0.0840\n",
            "Epoch [5774/20000], Training Loss: 0.0752\n",
            "Epoch [5775/20000], Training Loss: 0.0824\n",
            "Epoch [5776/20000], Training Loss: 0.0812\n",
            "Epoch [5777/20000], Training Loss: 0.0835\n",
            "Epoch [5778/20000], Training Loss: 0.0764\n",
            "Epoch [5779/20000], Training Loss: 0.0801\n",
            "Epoch [5780/20000], Training Loss: 0.0778\n",
            "Epoch [5781/20000], Training Loss: 0.0760\n",
            "Epoch [5782/20000], Training Loss: 0.0735\n",
            "Epoch [5783/20000], Training Loss: 0.0755\n",
            "Epoch [5784/20000], Training Loss: 0.0726\n",
            "Epoch [5785/20000], Training Loss: 0.0755\n",
            "Epoch [5786/20000], Training Loss: 0.0793\n",
            "Epoch [5787/20000], Training Loss: 0.0781\n",
            "Epoch [5788/20000], Training Loss: 0.0769\n",
            "Epoch [5789/20000], Training Loss: 0.0782\n",
            "Epoch [5790/20000], Training Loss: 0.0763\n",
            "Epoch [5791/20000], Training Loss: 0.0841\n",
            "Epoch [5792/20000], Training Loss: 0.0752\n",
            "Epoch [5793/20000], Training Loss: 0.0769\n",
            "Epoch [5794/20000], Training Loss: 0.0809\n",
            "Epoch [5795/20000], Training Loss: 0.0836\n",
            "Epoch [5796/20000], Training Loss: 0.0837\n",
            "Epoch [5797/20000], Training Loss: 0.0801\n",
            "Epoch [5798/20000], Training Loss: 0.0799\n",
            "Epoch [5799/20000], Training Loss: 0.0773\n",
            "Epoch [5800/20000], Training Loss: 0.0797\n",
            "Epoch [5801/20000], Training Loss: 0.0781\n",
            "Epoch [5802/20000], Training Loss: 0.0762\n",
            "Epoch [5803/20000], Training Loss: 0.0775\n",
            "Epoch [5804/20000], Training Loss: 0.0834\n",
            "Epoch [5805/20000], Training Loss: 0.0785\n",
            "Epoch [5806/20000], Training Loss: 0.0782\n",
            "Epoch [5807/20000], Training Loss: 0.0814\n",
            "Epoch [5808/20000], Training Loss: 0.0830\n",
            "Epoch [5809/20000], Training Loss: 0.0742\n",
            "Epoch [5810/20000], Training Loss: 0.0771\n",
            "Epoch [5811/20000], Training Loss: 0.0786\n",
            "Epoch [5812/20000], Training Loss: 0.0816\n",
            "Epoch [5813/20000], Training Loss: 0.0757\n",
            "Epoch [5814/20000], Training Loss: 0.0825\n",
            "Epoch [5815/20000], Training Loss: 0.0840\n",
            "Epoch [5816/20000], Training Loss: 0.0749\n",
            "Epoch [5817/20000], Training Loss: 0.0812\n",
            "Epoch [5818/20000], Training Loss: 0.0800\n",
            "Epoch [5819/20000], Training Loss: 0.0769\n",
            "Epoch [5820/20000], Training Loss: 0.0799\n",
            "Epoch [5821/20000], Training Loss: 0.0734\n",
            "Epoch [5822/20000], Training Loss: 0.0851\n",
            "Epoch [5823/20000], Training Loss: 0.0816\n",
            "Epoch [5824/20000], Training Loss: 0.0785\n",
            "Epoch [5825/20000], Training Loss: 0.0780\n",
            "Epoch [5826/20000], Training Loss: 0.0786\n",
            "Epoch [5827/20000], Training Loss: 0.0779\n",
            "Epoch [5828/20000], Training Loss: 0.0760\n",
            "Epoch [5829/20000], Training Loss: 0.0855\n",
            "Epoch [5830/20000], Training Loss: 0.0792\n",
            "Epoch [5831/20000], Training Loss: 0.0855\n",
            "Epoch [5832/20000], Training Loss: 0.0785\n",
            "Epoch [5833/20000], Training Loss: 0.0856\n",
            "Epoch [5834/20000], Training Loss: 0.0817\n",
            "Epoch [5835/20000], Training Loss: 0.0795\n",
            "Epoch [5836/20000], Training Loss: 0.0736\n",
            "Epoch [5837/20000], Training Loss: 0.0780\n",
            "Epoch [5838/20000], Training Loss: 0.0777\n",
            "Epoch [5839/20000], Training Loss: 0.0845\n",
            "Epoch [5840/20000], Training Loss: 0.0800\n",
            "Epoch [5841/20000], Training Loss: 0.0826\n",
            "Epoch [5842/20000], Training Loss: 0.0859\n",
            "Epoch [5843/20000], Training Loss: 0.0846\n",
            "Epoch [5844/20000], Training Loss: 0.0773\n",
            "Epoch [5845/20000], Training Loss: 0.0828\n",
            "Epoch [5846/20000], Training Loss: 0.0798\n",
            "Epoch [5847/20000], Training Loss: 0.0808\n",
            "Epoch [5848/20000], Training Loss: 0.0752\n",
            "Epoch [5849/20000], Training Loss: 0.0840\n",
            "Epoch [5850/20000], Training Loss: 0.0775\n",
            "Epoch [5851/20000], Training Loss: 0.0745\n",
            "Epoch [5852/20000], Training Loss: 0.0808\n",
            "Epoch [5853/20000], Training Loss: 0.0772\n",
            "Epoch [5854/20000], Training Loss: 0.0789\n",
            "Epoch [5855/20000], Training Loss: 0.0845\n",
            "Epoch [5856/20000], Training Loss: 0.0748\n",
            "Epoch [5857/20000], Training Loss: 0.0767\n",
            "Epoch [5858/20000], Training Loss: 0.0784\n",
            "Epoch [5859/20000], Training Loss: 0.0826\n",
            "Epoch [5860/20000], Training Loss: 0.0883\n",
            "Epoch [5861/20000], Training Loss: 0.0778\n",
            "Epoch [5862/20000], Training Loss: 0.0807\n",
            "Epoch [5863/20000], Training Loss: 0.0809\n",
            "Epoch [5864/20000], Training Loss: 0.0798\n",
            "Epoch [5865/20000], Training Loss: 0.0842\n",
            "Epoch [5866/20000], Training Loss: 0.0809\n",
            "Epoch [5867/20000], Training Loss: 0.0802\n",
            "Epoch [5868/20000], Training Loss: 0.0803\n",
            "Epoch [5869/20000], Training Loss: 0.0849\n",
            "Epoch [5870/20000], Training Loss: 0.0762\n",
            "Epoch [5871/20000], Training Loss: 0.0835\n",
            "Epoch [5872/20000], Training Loss: 0.0803\n",
            "Epoch [5873/20000], Training Loss: 0.0821\n",
            "Epoch [5874/20000], Training Loss: 0.0755\n",
            "Epoch [5875/20000], Training Loss: 0.0783\n",
            "Epoch [5876/20000], Training Loss: 0.0802\n",
            "Epoch [5877/20000], Training Loss: 0.0779\n",
            "Epoch [5878/20000], Training Loss: 0.0811\n",
            "Epoch [5879/20000], Training Loss: 0.0815\n",
            "Epoch [5880/20000], Training Loss: 0.0791\n",
            "Epoch [5881/20000], Training Loss: 0.0802\n",
            "Epoch [5882/20000], Training Loss: 0.0744\n",
            "Epoch [5883/20000], Training Loss: 0.0832\n",
            "Epoch [5884/20000], Training Loss: 0.0784\n",
            "Epoch [5885/20000], Training Loss: 0.0760\n",
            "Epoch [5886/20000], Training Loss: 0.0794\n",
            "Epoch [5887/20000], Training Loss: 0.0794\n",
            "Epoch [5888/20000], Training Loss: 0.0827\n",
            "Epoch [5889/20000], Training Loss: 0.0803\n",
            "Epoch [5890/20000], Training Loss: 0.0812\n",
            "Epoch [5891/20000], Training Loss: 0.0820\n",
            "Epoch [5892/20000], Training Loss: 0.0802\n",
            "Epoch [5893/20000], Training Loss: 0.0803\n",
            "Epoch [5894/20000], Training Loss: 0.0785\n",
            "Epoch [5895/20000], Training Loss: 0.0808\n",
            "Epoch [5896/20000], Training Loss: 0.0825\n",
            "Epoch [5897/20000], Training Loss: 0.0811\n",
            "Epoch [5898/20000], Training Loss: 0.0761\n",
            "Epoch [5899/20000], Training Loss: 0.0808\n",
            "Epoch [5900/20000], Training Loss: 0.0788\n",
            "Epoch [5901/20000], Training Loss: 0.0739\n",
            "Epoch [5902/20000], Training Loss: 0.0836\n",
            "Epoch [5903/20000], Training Loss: 0.0853\n",
            "Epoch [5904/20000], Training Loss: 0.0804\n",
            "Epoch [5905/20000], Training Loss: 0.0860\n",
            "Epoch [5906/20000], Training Loss: 0.0768\n",
            "Epoch [5907/20000], Training Loss: 0.0820\n",
            "Epoch [5908/20000], Training Loss: 0.0801\n",
            "Epoch [5909/20000], Training Loss: 0.0796\n",
            "Epoch [5910/20000], Training Loss: 0.0853\n",
            "Epoch [5911/20000], Training Loss: 0.0784\n",
            "Epoch [5912/20000], Training Loss: 0.0777\n",
            "Epoch [5913/20000], Training Loss: 0.0774\n",
            "Epoch [5914/20000], Training Loss: 0.0803\n",
            "Epoch [5915/20000], Training Loss: 0.0821\n",
            "Epoch [5916/20000], Training Loss: 0.0808\n",
            "Epoch [5917/20000], Training Loss: 0.0751\n",
            "Epoch [5918/20000], Training Loss: 0.0768\n",
            "Epoch [5919/20000], Training Loss: 0.0795\n",
            "Epoch [5920/20000], Training Loss: 0.0790\n",
            "Epoch [5921/20000], Training Loss: 0.0772\n",
            "Epoch [5922/20000], Training Loss: 0.0762\n",
            "Epoch [5923/20000], Training Loss: 0.0791\n",
            "Epoch [5924/20000], Training Loss: 0.0853\n",
            "Epoch [5925/20000], Training Loss: 0.0715\n",
            "Epoch [5926/20000], Training Loss: 0.0809\n",
            "Epoch [5927/20000], Training Loss: 0.0785\n",
            "Epoch [5928/20000], Training Loss: 0.0800\n",
            "Epoch [5929/20000], Training Loss: 0.0870\n",
            "Epoch [5930/20000], Training Loss: 0.0781\n",
            "Epoch [5931/20000], Training Loss: 0.0759\n",
            "Epoch [5932/20000], Training Loss: 0.0812\n",
            "Epoch [5933/20000], Training Loss: 0.0815\n",
            "Epoch [5934/20000], Training Loss: 0.0786\n",
            "Epoch [5935/20000], Training Loss: 0.0781\n",
            "Epoch [5936/20000], Training Loss: 0.0736\n",
            "Epoch [5937/20000], Training Loss: 0.0815\n",
            "Epoch [5938/20000], Training Loss: 0.0806\n",
            "Epoch [5939/20000], Training Loss: 0.0806\n",
            "Epoch [5940/20000], Training Loss: 0.0791\n",
            "Epoch [5941/20000], Training Loss: 0.0839\n",
            "Epoch [5942/20000], Training Loss: 0.0805\n",
            "Epoch [5943/20000], Training Loss: 0.0791\n",
            "Epoch [5944/20000], Training Loss: 0.0774\n",
            "Epoch [5945/20000], Training Loss: 0.0769\n",
            "Epoch [5946/20000], Training Loss: 0.0728\n",
            "Epoch [5947/20000], Training Loss: 0.0764\n",
            "Epoch [5948/20000], Training Loss: 0.0790\n",
            "Epoch [5949/20000], Training Loss: 0.0825\n",
            "Epoch [5950/20000], Training Loss: 0.0795\n",
            "Epoch [5951/20000], Training Loss: 0.0757\n",
            "Epoch [5952/20000], Training Loss: 0.0842\n",
            "Epoch [5953/20000], Training Loss: 0.0832\n",
            "Epoch [5954/20000], Training Loss: 0.0763\n",
            "Epoch [5955/20000], Training Loss: 0.0789\n",
            "Epoch [5956/20000], Training Loss: 0.0825\n",
            "Epoch [5957/20000], Training Loss: 0.0825\n",
            "Epoch [5958/20000], Training Loss: 0.0789\n",
            "Epoch [5959/20000], Training Loss: 0.0822\n",
            "Epoch [5960/20000], Training Loss: 0.0797\n",
            "Epoch [5961/20000], Training Loss: 0.0764\n",
            "Epoch [5962/20000], Training Loss: 0.0779\n",
            "Epoch [5963/20000], Training Loss: 0.0784\n",
            "Epoch [5964/20000], Training Loss: 0.0827\n",
            "Epoch [5965/20000], Training Loss: 0.0778\n",
            "Epoch [5966/20000], Training Loss: 0.0785\n",
            "Epoch [5967/20000], Training Loss: 0.0843\n",
            "Epoch [5968/20000], Training Loss: 0.0779\n",
            "Epoch [5969/20000], Training Loss: 0.0801\n",
            "Epoch [5970/20000], Training Loss: 0.0757\n",
            "Epoch [5971/20000], Training Loss: 0.0739\n",
            "Epoch [5972/20000], Training Loss: 0.0753\n",
            "Epoch [5973/20000], Training Loss: 0.0740\n",
            "Epoch [5974/20000], Training Loss: 0.0800\n",
            "Epoch [5975/20000], Training Loss: 0.0845\n",
            "Epoch [5976/20000], Training Loss: 0.0760\n",
            "Epoch [5977/20000], Training Loss: 0.0779\n",
            "Epoch [5978/20000], Training Loss: 0.0769\n",
            "Epoch [5979/20000], Training Loss: 0.0749\n",
            "Epoch [5980/20000], Training Loss: 0.0784\n",
            "Epoch [5981/20000], Training Loss: 0.0826\n",
            "Epoch [5982/20000], Training Loss: 0.0809\n",
            "Epoch [5983/20000], Training Loss: 0.0797\n",
            "Epoch [5984/20000], Training Loss: 0.0795\n",
            "Epoch [5985/20000], Training Loss: 0.0792\n",
            "Epoch [5986/20000], Training Loss: 0.0782\n",
            "Epoch [5987/20000], Training Loss: 0.0791\n",
            "Epoch [5988/20000], Training Loss: 0.0791\n",
            "Epoch [5989/20000], Training Loss: 0.0757\n",
            "Epoch [5990/20000], Training Loss: 0.0729\n",
            "Epoch [5991/20000], Training Loss: 0.0801\n",
            "Epoch [5992/20000], Training Loss: 0.0860\n",
            "Epoch [5993/20000], Training Loss: 0.0794\n",
            "Epoch [5994/20000], Training Loss: 0.0762\n",
            "Epoch [5995/20000], Training Loss: 0.0777\n",
            "Epoch [5996/20000], Training Loss: 0.0737\n",
            "Epoch [5997/20000], Training Loss: 0.0811\n",
            "Epoch [5998/20000], Training Loss: 0.0811\n",
            "Epoch [5999/20000], Training Loss: 0.0834\n",
            "Epoch [6000/20000], Training Loss: 0.0787\n",
            "Epoch [6001/20000], Training Loss: 0.0801\n",
            "Epoch [6002/20000], Training Loss: 0.0803\n",
            "Epoch [6003/20000], Training Loss: 0.0795\n",
            "Epoch [6004/20000], Training Loss: 0.0824\n",
            "Epoch [6005/20000], Training Loss: 0.0768\n",
            "Epoch [6006/20000], Training Loss: 0.0798\n",
            "Epoch [6007/20000], Training Loss: 0.0810\n",
            "Epoch [6008/20000], Training Loss: 0.0770\n",
            "Epoch [6009/20000], Training Loss: 0.0845\n",
            "Epoch [6010/20000], Training Loss: 0.0799\n",
            "Epoch [6011/20000], Training Loss: 0.0778\n",
            "Epoch [6012/20000], Training Loss: 0.0811\n",
            "Epoch [6013/20000], Training Loss: 0.0791\n",
            "Epoch [6014/20000], Training Loss: 0.0773\n",
            "Epoch [6015/20000], Training Loss: 0.0758\n",
            "Epoch [6016/20000], Training Loss: 0.0766\n",
            "Epoch [6017/20000], Training Loss: 0.0792\n",
            "Epoch [6018/20000], Training Loss: 0.0812\n",
            "Epoch [6019/20000], Training Loss: 0.0780\n",
            "Epoch [6020/20000], Training Loss: 0.0801\n",
            "Epoch [6021/20000], Training Loss: 0.0834\n",
            "Epoch [6022/20000], Training Loss: 0.0750\n",
            "Epoch [6023/20000], Training Loss: 0.0814\n",
            "Epoch [6024/20000], Training Loss: 0.0839\n",
            "Epoch [6025/20000], Training Loss: 0.0781\n",
            "Epoch [6026/20000], Training Loss: 0.0808\n",
            "Epoch [6027/20000], Training Loss: 0.0801\n",
            "Epoch [6028/20000], Training Loss: 0.0763\n",
            "Epoch [6029/20000], Training Loss: 0.0753\n",
            "Epoch [6030/20000], Training Loss: 0.0810\n",
            "Epoch [6031/20000], Training Loss: 0.0805\n",
            "Epoch [6032/20000], Training Loss: 0.0845\n",
            "Epoch [6033/20000], Training Loss: 0.0792\n",
            "Epoch [6034/20000], Training Loss: 0.0832\n",
            "Epoch [6035/20000], Training Loss: 0.0791\n",
            "Epoch [6036/20000], Training Loss: 0.0805\n",
            "Epoch [6037/20000], Training Loss: 0.0824\n",
            "Epoch [6038/20000], Training Loss: 0.0815\n",
            "Epoch [6039/20000], Training Loss: 0.0770\n",
            "Epoch [6040/20000], Training Loss: 0.0768\n",
            "Epoch [6041/20000], Training Loss: 0.0812\n",
            "Epoch [6042/20000], Training Loss: 0.0787\n",
            "Epoch [6043/20000], Training Loss: 0.0768\n",
            "Epoch [6044/20000], Training Loss: 0.0838\n",
            "Epoch [6045/20000], Training Loss: 0.0817\n",
            "Epoch [6046/20000], Training Loss: 0.0790\n",
            "Epoch [6047/20000], Training Loss: 0.0831\n",
            "Epoch [6048/20000], Training Loss: 0.0747\n",
            "Epoch [6049/20000], Training Loss: 0.0824\n",
            "Epoch [6050/20000], Training Loss: 0.0761\n",
            "Epoch [6051/20000], Training Loss: 0.0786\n",
            "Epoch [6052/20000], Training Loss: 0.0755\n",
            "Epoch [6053/20000], Training Loss: 0.0737\n",
            "Epoch [6054/20000], Training Loss: 0.0817\n",
            "Epoch [6055/20000], Training Loss: 0.0783\n",
            "Epoch [6056/20000], Training Loss: 0.0745\n",
            "Epoch [6057/20000], Training Loss: 0.0797\n",
            "Epoch [6058/20000], Training Loss: 0.0797\n",
            "Epoch [6059/20000], Training Loss: 0.0760\n",
            "Epoch [6060/20000], Training Loss: 0.0833\n",
            "Epoch [6061/20000], Training Loss: 0.0844\n",
            "Epoch [6062/20000], Training Loss: 0.0794\n",
            "Epoch [6063/20000], Training Loss: 0.0748\n",
            "Epoch [6064/20000], Training Loss: 0.0793\n",
            "Epoch [6065/20000], Training Loss: 0.0746\n",
            "Epoch [6066/20000], Training Loss: 0.0786\n",
            "Epoch [6067/20000], Training Loss: 0.0760\n",
            "Epoch [6068/20000], Training Loss: 0.0861\n",
            "Epoch [6069/20000], Training Loss: 0.0828\n",
            "Epoch [6070/20000], Training Loss: 0.0794\n",
            "Epoch [6071/20000], Training Loss: 0.0796\n",
            "Epoch [6072/20000], Training Loss: 0.0822\n",
            "Epoch [6073/20000], Training Loss: 0.0801\n",
            "Epoch [6074/20000], Training Loss: 0.0796\n",
            "Epoch [6075/20000], Training Loss: 0.0851\n",
            "Epoch [6076/20000], Training Loss: 0.0796\n",
            "Epoch [6077/20000], Training Loss: 0.0764\n",
            "Epoch [6078/20000], Training Loss: 0.0856\n",
            "Epoch [6079/20000], Training Loss: 0.0824\n",
            "Epoch [6080/20000], Training Loss: 0.0747\n",
            "Epoch [6081/20000], Training Loss: 0.0749\n",
            "Epoch [6082/20000], Training Loss: 0.0739\n",
            "Epoch [6083/20000], Training Loss: 0.0802\n",
            "Epoch [6084/20000], Training Loss: 0.0754\n",
            "Epoch [6085/20000], Training Loss: 0.0774\n",
            "Epoch [6086/20000], Training Loss: 0.0804\n",
            "Epoch [6087/20000], Training Loss: 0.0812\n",
            "Epoch [6088/20000], Training Loss: 0.0812\n",
            "Epoch [6089/20000], Training Loss: 0.0787\n",
            "Epoch [6090/20000], Training Loss: 0.0867\n",
            "Epoch [6091/20000], Training Loss: 0.0846\n",
            "Epoch [6092/20000], Training Loss: 0.0792\n",
            "Epoch [6093/20000], Training Loss: 0.0786\n",
            "Epoch [6094/20000], Training Loss: 0.0769\n",
            "Epoch [6095/20000], Training Loss: 0.0858\n",
            "Epoch [6096/20000], Training Loss: 0.0834\n",
            "Epoch [6097/20000], Training Loss: 0.0742\n",
            "Epoch [6098/20000], Training Loss: 0.0746\n",
            "Epoch [6099/20000], Training Loss: 0.0762\n",
            "Epoch [6100/20000], Training Loss: 0.0791\n",
            "Epoch [6101/20000], Training Loss: 0.0801\n",
            "Epoch [6102/20000], Training Loss: 0.0773\n",
            "Epoch [6103/20000], Training Loss: 0.0778\n",
            "Epoch [6104/20000], Training Loss: 0.0788\n",
            "Epoch [6105/20000], Training Loss: 0.0798\n",
            "Epoch [6106/20000], Training Loss: 0.0788\n",
            "Epoch [6107/20000], Training Loss: 0.0732\n",
            "Epoch [6108/20000], Training Loss: 0.0754\n",
            "Epoch [6109/20000], Training Loss: 0.0775\n",
            "Epoch [6110/20000], Training Loss: 0.0784\n",
            "Epoch [6111/20000], Training Loss: 0.0788\n",
            "Epoch [6112/20000], Training Loss: 0.0738\n",
            "Epoch [6113/20000], Training Loss: 0.0828\n",
            "Epoch [6114/20000], Training Loss: 0.0805\n",
            "Epoch [6115/20000], Training Loss: 0.0806\n",
            "Epoch [6116/20000], Training Loss: 0.0780\n",
            "Epoch [6117/20000], Training Loss: 0.0852\n",
            "Epoch [6118/20000], Training Loss: 0.0838\n",
            "Epoch [6119/20000], Training Loss: 0.0786\n",
            "Epoch [6120/20000], Training Loss: 0.0804\n",
            "Epoch [6121/20000], Training Loss: 0.0799\n",
            "Epoch [6122/20000], Training Loss: 0.0799\n",
            "Epoch [6123/20000], Training Loss: 0.0844\n",
            "Epoch [6124/20000], Training Loss: 0.0817\n",
            "Epoch [6125/20000], Training Loss: 0.0805\n",
            "Epoch [6126/20000], Training Loss: 0.0833\n",
            "Epoch [6127/20000], Training Loss: 0.0768\n",
            "Epoch [6128/20000], Training Loss: 0.0776\n",
            "Epoch [6129/20000], Training Loss: 0.0793\n",
            "Epoch [6130/20000], Training Loss: 0.0781\n",
            "Epoch [6131/20000], Training Loss: 0.0846\n",
            "Epoch [6132/20000], Training Loss: 0.0773\n",
            "Epoch [6133/20000], Training Loss: 0.0787\n",
            "Epoch [6134/20000], Training Loss: 0.0788\n",
            "Epoch [6135/20000], Training Loss: 0.0799\n",
            "Epoch [6136/20000], Training Loss: 0.0790\n",
            "Epoch [6137/20000], Training Loss: 0.0844\n",
            "Epoch [6138/20000], Training Loss: 0.0767\n",
            "Epoch [6139/20000], Training Loss: 0.0838\n",
            "Epoch [6140/20000], Training Loss: 0.0841\n",
            "Epoch [6141/20000], Training Loss: 0.0825\n",
            "Epoch [6142/20000], Training Loss: 0.0829\n",
            "Epoch [6143/20000], Training Loss: 0.0778\n",
            "Epoch [6144/20000], Training Loss: 0.0756\n",
            "Epoch [6145/20000], Training Loss: 0.0804\n",
            "Epoch [6146/20000], Training Loss: 0.0817\n",
            "Epoch [6147/20000], Training Loss: 0.0779\n",
            "Epoch [6148/20000], Training Loss: 0.0814\n",
            "Epoch [6149/20000], Training Loss: 0.0744\n",
            "Epoch [6150/20000], Training Loss: 0.0816\n",
            "Epoch [6151/20000], Training Loss: 0.0852\n",
            "Epoch [6152/20000], Training Loss: 0.0805\n",
            "Epoch [6153/20000], Training Loss: 0.0750\n",
            "Epoch [6154/20000], Training Loss: 0.0802\n",
            "Epoch [6155/20000], Training Loss: 0.0754\n",
            "Epoch [6156/20000], Training Loss: 0.0829\n",
            "Epoch [6157/20000], Training Loss: 0.0779\n",
            "Epoch [6158/20000], Training Loss: 0.0738\n",
            "Epoch [6159/20000], Training Loss: 0.0796\n",
            "Epoch [6160/20000], Training Loss: 0.0819\n",
            "Epoch [6161/20000], Training Loss: 0.0822\n",
            "Epoch [6162/20000], Training Loss: 0.0806\n",
            "Epoch [6163/20000], Training Loss: 0.0784\n",
            "Epoch [6164/20000], Training Loss: 0.0762\n",
            "Epoch [6165/20000], Training Loss: 0.0794\n",
            "Epoch [6166/20000], Training Loss: 0.0833\n",
            "Epoch [6167/20000], Training Loss: 0.0818\n",
            "Epoch [6168/20000], Training Loss: 0.0817\n",
            "Epoch [6169/20000], Training Loss: 0.0771\n",
            "Epoch [6170/20000], Training Loss: 0.0837\n",
            "Epoch [6171/20000], Training Loss: 0.0786\n",
            "Epoch [6172/20000], Training Loss: 0.0809\n",
            "Epoch [6173/20000], Training Loss: 0.0861\n",
            "Epoch [6174/20000], Training Loss: 0.0844\n",
            "Epoch [6175/20000], Training Loss: 0.0790\n",
            "Epoch [6176/20000], Training Loss: 0.0820\n",
            "Epoch [6177/20000], Training Loss: 0.0803\n",
            "Epoch [6178/20000], Training Loss: 0.0847\n",
            "Epoch [6179/20000], Training Loss: 0.0826\n",
            "Epoch [6180/20000], Training Loss: 0.0866\n",
            "Epoch [6181/20000], Training Loss: 0.0792\n",
            "Epoch [6182/20000], Training Loss: 0.0821\n",
            "Epoch [6183/20000], Training Loss: 0.0751\n",
            "Epoch [6184/20000], Training Loss: 0.0778\n",
            "Epoch [6185/20000], Training Loss: 0.0776\n",
            "Epoch [6186/20000], Training Loss: 0.0771\n",
            "Epoch [6187/20000], Training Loss: 0.0796\n",
            "Epoch [6188/20000], Training Loss: 0.0863\n",
            "Epoch [6189/20000], Training Loss: 0.0750\n",
            "Epoch [6190/20000], Training Loss: 0.0833\n",
            "Epoch [6191/20000], Training Loss: 0.0783\n",
            "Epoch [6192/20000], Training Loss: 0.0812\n",
            "Epoch [6193/20000], Training Loss: 0.0803\n",
            "Epoch [6194/20000], Training Loss: 0.0772\n",
            "Epoch [6195/20000], Training Loss: 0.0772\n",
            "Epoch [6196/20000], Training Loss: 0.0847\n",
            "Epoch [6197/20000], Training Loss: 0.0816\n",
            "Epoch [6198/20000], Training Loss: 0.0794\n",
            "Epoch [6199/20000], Training Loss: 0.0755\n",
            "Epoch [6200/20000], Training Loss: 0.0770\n",
            "Epoch [6201/20000], Training Loss: 0.0795\n",
            "Epoch [6202/20000], Training Loss: 0.0793\n",
            "Epoch [6203/20000], Training Loss: 0.0825\n",
            "Epoch [6204/20000], Training Loss: 0.0852\n",
            "Epoch [6205/20000], Training Loss: 0.0790\n",
            "Epoch [6206/20000], Training Loss: 0.0751\n",
            "Epoch [6207/20000], Training Loss: 0.0784\n",
            "Epoch [6208/20000], Training Loss: 0.0785\n",
            "Epoch [6209/20000], Training Loss: 0.0792\n",
            "Epoch [6210/20000], Training Loss: 0.0856\n",
            "Epoch [6211/20000], Training Loss: 0.0728\n",
            "Epoch [6212/20000], Training Loss: 0.0808\n",
            "Epoch [6213/20000], Training Loss: 0.0824\n",
            "Epoch [6214/20000], Training Loss: 0.0864\n",
            "Epoch [6215/20000], Training Loss: 0.0786\n",
            "Epoch [6216/20000], Training Loss: 0.0746\n",
            "Epoch [6217/20000], Training Loss: 0.0815\n",
            "Epoch [6218/20000], Training Loss: 0.0793\n",
            "Epoch [6219/20000], Training Loss: 0.0770\n",
            "Epoch [6220/20000], Training Loss: 0.0787\n",
            "Epoch [6221/20000], Training Loss: 0.0834\n",
            "Epoch [6222/20000], Training Loss: 0.0802\n",
            "Epoch [6223/20000], Training Loss: 0.0780\n",
            "Epoch [6224/20000], Training Loss: 0.0738\n",
            "Epoch [6225/20000], Training Loss: 0.0743\n",
            "Epoch [6226/20000], Training Loss: 0.0829\n",
            "Epoch [6227/20000], Training Loss: 0.0753\n",
            "Epoch [6228/20000], Training Loss: 0.0853\n",
            "Epoch [6229/20000], Training Loss: 0.0759\n",
            "Epoch [6230/20000], Training Loss: 0.0825\n",
            "Epoch [6231/20000], Training Loss: 0.0785\n",
            "Epoch [6232/20000], Training Loss: 0.0769\n",
            "Epoch [6233/20000], Training Loss: 0.0852\n",
            "Epoch [6234/20000], Training Loss: 0.0728\n",
            "Epoch [6235/20000], Training Loss: 0.0786\n",
            "Epoch [6236/20000], Training Loss: 0.0790\n",
            "Epoch [6237/20000], Training Loss: 0.0873\n",
            "Epoch [6238/20000], Training Loss: 0.0827\n",
            "Epoch [6239/20000], Training Loss: 0.0763\n",
            "Epoch [6240/20000], Training Loss: 0.0813\n",
            "Epoch [6241/20000], Training Loss: 0.0787\n",
            "Epoch [6242/20000], Training Loss: 0.0784\n",
            "Epoch [6243/20000], Training Loss: 0.0755\n",
            "Epoch [6244/20000], Training Loss: 0.0832\n",
            "Epoch [6245/20000], Training Loss: 0.0837\n",
            "Epoch [6246/20000], Training Loss: 0.0802\n",
            "Epoch [6247/20000], Training Loss: 0.0859\n",
            "Epoch [6248/20000], Training Loss: 0.0754\n",
            "Epoch [6249/20000], Training Loss: 0.0747\n",
            "Epoch [6250/20000], Training Loss: 0.0780\n",
            "Epoch [6251/20000], Training Loss: 0.0741\n",
            "Epoch [6252/20000], Training Loss: 0.0814\n",
            "Epoch [6253/20000], Training Loss: 0.0802\n",
            "Epoch [6254/20000], Training Loss: 0.0743\n",
            "Epoch [6255/20000], Training Loss: 0.0755\n",
            "Epoch [6256/20000], Training Loss: 0.0801\n",
            "Epoch [6257/20000], Training Loss: 0.0836\n",
            "Epoch [6258/20000], Training Loss: 0.0770\n",
            "Epoch [6259/20000], Training Loss: 0.0777\n",
            "Epoch [6260/20000], Training Loss: 0.0816\n",
            "Epoch [6261/20000], Training Loss: 0.0793\n",
            "Epoch [6262/20000], Training Loss: 0.0786\n",
            "Epoch [6263/20000], Training Loss: 0.0747\n",
            "Epoch [6264/20000], Training Loss: 0.0858\n",
            "Epoch [6265/20000], Training Loss: 0.0726\n",
            "Epoch [6266/20000], Training Loss: 0.0833\n",
            "Epoch [6267/20000], Training Loss: 0.0760\n",
            "Epoch [6268/20000], Training Loss: 0.0827\n",
            "Epoch [6269/20000], Training Loss: 0.0781\n",
            "Epoch [6270/20000], Training Loss: 0.0833\n",
            "Epoch [6271/20000], Training Loss: 0.0779\n",
            "Epoch [6272/20000], Training Loss: 0.0785\n",
            "Epoch [6273/20000], Training Loss: 0.0799\n",
            "Epoch [6274/20000], Training Loss: 0.0795\n",
            "Epoch [6275/20000], Training Loss: 0.0789\n",
            "Epoch [6276/20000], Training Loss: 0.0827\n",
            "Epoch [6277/20000], Training Loss: 0.0803\n",
            "Epoch [6278/20000], Training Loss: 0.0758\n",
            "Epoch [6279/20000], Training Loss: 0.0843\n",
            "Epoch [6280/20000], Training Loss: 0.0770\n",
            "Epoch [6281/20000], Training Loss: 0.0733\n",
            "Epoch [6282/20000], Training Loss: 0.0794\n",
            "Epoch [6283/20000], Training Loss: 0.0786\n",
            "Epoch [6284/20000], Training Loss: 0.0791\n",
            "Epoch [6285/20000], Training Loss: 0.0774\n",
            "Epoch [6286/20000], Training Loss: 0.0780\n",
            "Epoch [6287/20000], Training Loss: 0.0778\n",
            "Epoch [6288/20000], Training Loss: 0.0795\n",
            "Epoch [6289/20000], Training Loss: 0.0853\n",
            "Epoch [6290/20000], Training Loss: 0.0807\n",
            "Epoch [6291/20000], Training Loss: 0.0759\n",
            "Epoch [6292/20000], Training Loss: 0.0757\n",
            "Epoch [6293/20000], Training Loss: 0.0761\n",
            "Epoch [6294/20000], Training Loss: 0.0877\n",
            "Epoch [6295/20000], Training Loss: 0.0826\n",
            "Epoch [6296/20000], Training Loss: 0.0739\n",
            "Epoch [6297/20000], Training Loss: 0.0821\n",
            "Epoch [6298/20000], Training Loss: 0.0755\n",
            "Epoch [6299/20000], Training Loss: 0.0819\n",
            "Epoch [6300/20000], Training Loss: 0.0782\n",
            "Epoch [6301/20000], Training Loss: 0.0804\n",
            "Epoch [6302/20000], Training Loss: 0.0792\n",
            "Epoch [6303/20000], Training Loss: 0.0808\n",
            "Epoch [6304/20000], Training Loss: 0.0849\n",
            "Epoch [6305/20000], Training Loss: 0.0822\n",
            "Epoch [6306/20000], Training Loss: 0.0747\n",
            "Epoch [6307/20000], Training Loss: 0.0819\n",
            "Epoch [6308/20000], Training Loss: 0.0833\n",
            "Epoch [6309/20000], Training Loss: 0.0765\n",
            "Epoch [6310/20000], Training Loss: 0.0761\n",
            "Epoch [6311/20000], Training Loss: 0.0772\n",
            "Epoch [6312/20000], Training Loss: 0.0820\n",
            "Epoch [6313/20000], Training Loss: 0.0802\n",
            "Epoch [6314/20000], Training Loss: 0.0745\n",
            "Epoch [6315/20000], Training Loss: 0.0751\n",
            "Epoch [6316/20000], Training Loss: 0.0751\n",
            "Epoch [6317/20000], Training Loss: 0.0741\n",
            "Epoch [6318/20000], Training Loss: 0.0816\n",
            "Epoch [6319/20000], Training Loss: 0.0807\n",
            "Epoch [6320/20000], Training Loss: 0.0799\n",
            "Epoch [6321/20000], Training Loss: 0.0776\n",
            "Epoch [6322/20000], Training Loss: 0.0853\n",
            "Epoch [6323/20000], Training Loss: 0.0839\n",
            "Epoch [6324/20000], Training Loss: 0.0767\n",
            "Epoch [6325/20000], Training Loss: 0.0777\n",
            "Epoch [6326/20000], Training Loss: 0.0806\n",
            "Epoch [6327/20000], Training Loss: 0.0788\n",
            "Epoch [6328/20000], Training Loss: 0.0763\n",
            "Epoch [6329/20000], Training Loss: 0.0812\n",
            "Epoch [6330/20000], Training Loss: 0.0767\n",
            "Epoch [6331/20000], Training Loss: 0.0839\n",
            "Epoch [6332/20000], Training Loss: 0.0792\n",
            "Epoch [6333/20000], Training Loss: 0.0813\n",
            "Epoch [6334/20000], Training Loss: 0.0787\n",
            "Epoch [6335/20000], Training Loss: 0.0755\n",
            "Epoch [6336/20000], Training Loss: 0.0792\n",
            "Epoch [6337/20000], Training Loss: 0.0787\n",
            "Epoch [6338/20000], Training Loss: 0.0786\n",
            "Epoch [6339/20000], Training Loss: 0.0838\n",
            "Epoch [6340/20000], Training Loss: 0.0783\n",
            "Epoch [6341/20000], Training Loss: 0.0778\n",
            "Epoch [6342/20000], Training Loss: 0.0729\n",
            "Epoch [6343/20000], Training Loss: 0.0787\n",
            "Epoch [6344/20000], Training Loss: 0.0815\n",
            "Epoch [6345/20000], Training Loss: 0.0795\n",
            "Epoch [6346/20000], Training Loss: 0.0799\n",
            "Epoch [6347/20000], Training Loss: 0.0745\n",
            "Epoch [6348/20000], Training Loss: 0.0747\n",
            "Epoch [6349/20000], Training Loss: 0.0819\n",
            "Epoch [6350/20000], Training Loss: 0.0753\n",
            "Epoch [6351/20000], Training Loss: 0.0757\n",
            "Epoch [6352/20000], Training Loss: 0.0750\n",
            "Epoch [6353/20000], Training Loss: 0.0739\n",
            "Epoch [6354/20000], Training Loss: 0.0843\n",
            "Epoch [6355/20000], Training Loss: 0.0722\n",
            "Epoch [6356/20000], Training Loss: 0.0844\n",
            "Epoch [6357/20000], Training Loss: 0.0850\n",
            "Epoch [6358/20000], Training Loss: 0.0797\n",
            "Epoch [6359/20000], Training Loss: 0.0804\n",
            "Epoch [6360/20000], Training Loss: 0.0772\n",
            "Epoch [6361/20000], Training Loss: 0.0776\n",
            "Epoch [6362/20000], Training Loss: 0.0810\n",
            "Epoch [6363/20000], Training Loss: 0.0799\n",
            "Epoch [6364/20000], Training Loss: 0.0803\n",
            "Epoch [6365/20000], Training Loss: 0.0844\n",
            "Epoch [6366/20000], Training Loss: 0.0740\n",
            "Epoch [6367/20000], Training Loss: 0.0809\n",
            "Epoch [6368/20000], Training Loss: 0.0763\n",
            "Epoch [6369/20000], Training Loss: 0.0823\n",
            "Epoch [6370/20000], Training Loss: 0.0755\n",
            "Epoch [6371/20000], Training Loss: 0.0828\n",
            "Epoch [6372/20000], Training Loss: 0.0784\n",
            "Epoch [6373/20000], Training Loss: 0.0774\n",
            "Epoch [6374/20000], Training Loss: 0.0739\n",
            "Epoch [6375/20000], Training Loss: 0.0730\n",
            "Epoch [6376/20000], Training Loss: 0.0869\n",
            "Epoch [6377/20000], Training Loss: 0.0800\n",
            "Epoch [6378/20000], Training Loss: 0.0755\n",
            "Epoch [6379/20000], Training Loss: 0.0822\n",
            "Epoch [6380/20000], Training Loss: 0.0832\n",
            "Epoch [6381/20000], Training Loss: 0.0817\n",
            "Epoch [6382/20000], Training Loss: 0.0746\n",
            "Epoch [6383/20000], Training Loss: 0.0822\n",
            "Epoch [6384/20000], Training Loss: 0.0754\n",
            "Epoch [6385/20000], Training Loss: 0.0787\n",
            "Epoch [6386/20000], Training Loss: 0.0798\n",
            "Epoch [6387/20000], Training Loss: 0.0736\n",
            "Epoch [6388/20000], Training Loss: 0.0875\n",
            "Epoch [6389/20000], Training Loss: 0.0825\n",
            "Epoch [6390/20000], Training Loss: 0.0832\n",
            "Epoch [6391/20000], Training Loss: 0.0848\n",
            "Epoch [6392/20000], Training Loss: 0.0817\n",
            "Epoch [6393/20000], Training Loss: 0.0807\n",
            "Epoch [6394/20000], Training Loss: 0.0851\n",
            "Epoch [6395/20000], Training Loss: 0.0832\n",
            "Epoch [6396/20000], Training Loss: 0.0771\n",
            "Epoch [6397/20000], Training Loss: 0.0809\n",
            "Epoch [6398/20000], Training Loss: 0.0767\n",
            "Epoch [6399/20000], Training Loss: 0.0778\n",
            "Epoch [6400/20000], Training Loss: 0.0818\n",
            "Epoch [6401/20000], Training Loss: 0.0758\n",
            "Epoch [6402/20000], Training Loss: 0.0814\n",
            "Epoch [6403/20000], Training Loss: 0.0755\n",
            "Epoch [6404/20000], Training Loss: 0.0786\n",
            "Epoch [6405/20000], Training Loss: 0.0728\n",
            "Epoch [6406/20000], Training Loss: 0.0729\n",
            "Epoch [6407/20000], Training Loss: 0.0838\n",
            "Epoch [6408/20000], Training Loss: 0.0754\n",
            "Epoch [6409/20000], Training Loss: 0.0833\n",
            "Epoch [6410/20000], Training Loss: 0.0797\n",
            "Epoch [6411/20000], Training Loss: 0.0747\n",
            "Epoch [6412/20000], Training Loss: 0.0778\n",
            "Epoch [6413/20000], Training Loss: 0.0745\n",
            "Epoch [6414/20000], Training Loss: 0.0807\n",
            "Epoch [6415/20000], Training Loss: 0.0793\n",
            "Epoch [6416/20000], Training Loss: 0.0747\n",
            "Epoch [6417/20000], Training Loss: 0.0810\n",
            "Epoch [6418/20000], Training Loss: 0.0817\n",
            "Epoch [6419/20000], Training Loss: 0.0809\n",
            "Epoch [6420/20000], Training Loss: 0.0844\n",
            "Epoch [6421/20000], Training Loss: 0.0798\n",
            "Epoch [6422/20000], Training Loss: 0.0738\n",
            "Epoch [6423/20000], Training Loss: 0.0797\n",
            "Epoch [6424/20000], Training Loss: 0.0789\n",
            "Epoch [6425/20000], Training Loss: 0.0758\n",
            "Epoch [6426/20000], Training Loss: 0.0802\n",
            "Epoch [6427/20000], Training Loss: 0.0744\n",
            "Epoch [6428/20000], Training Loss: 0.0852\n",
            "Epoch [6429/20000], Training Loss: 0.0837\n",
            "Epoch [6430/20000], Training Loss: 0.0818\n",
            "Epoch [6431/20000], Training Loss: 0.0844\n",
            "Epoch [6432/20000], Training Loss: 0.0748\n",
            "Epoch [6433/20000], Training Loss: 0.0794\n",
            "Epoch [6434/20000], Training Loss: 0.0774\n",
            "Epoch [6435/20000], Training Loss: 0.0808\n",
            "Epoch [6436/20000], Training Loss: 0.0817\n",
            "Epoch [6437/20000], Training Loss: 0.0795\n",
            "Epoch [6438/20000], Training Loss: 0.0799\n",
            "Epoch [6439/20000], Training Loss: 0.0838\n",
            "Epoch [6440/20000], Training Loss: 0.0740\n",
            "Epoch [6441/20000], Training Loss: 0.0801\n",
            "Epoch [6442/20000], Training Loss: 0.0722\n",
            "Epoch [6443/20000], Training Loss: 0.0817\n",
            "Epoch [6444/20000], Training Loss: 0.0758\n",
            "Epoch [6445/20000], Training Loss: 0.0764\n",
            "Epoch [6446/20000], Training Loss: 0.0810\n",
            "Epoch [6447/20000], Training Loss: 0.0747\n",
            "Epoch [6448/20000], Training Loss: 0.0754\n",
            "Epoch [6449/20000], Training Loss: 0.0783\n",
            "Epoch [6450/20000], Training Loss: 0.0763\n",
            "Epoch [6451/20000], Training Loss: 0.0812\n",
            "Epoch [6452/20000], Training Loss: 0.0763\n",
            "Epoch [6453/20000], Training Loss: 0.0742\n",
            "Epoch [6454/20000], Training Loss: 0.0846\n",
            "Epoch [6455/20000], Training Loss: 0.0818\n",
            "Epoch [6456/20000], Training Loss: 0.0837\n",
            "Epoch [6457/20000], Training Loss: 0.0778\n",
            "Epoch [6458/20000], Training Loss: 0.0836\n",
            "Epoch [6459/20000], Training Loss: 0.0848\n",
            "Epoch [6460/20000], Training Loss: 0.0769\n",
            "Epoch [6461/20000], Training Loss: 0.0850\n",
            "Epoch [6462/20000], Training Loss: 0.0814\n",
            "Epoch [6463/20000], Training Loss: 0.0782\n",
            "Epoch [6464/20000], Training Loss: 0.0795\n",
            "Epoch [6465/20000], Training Loss: 0.0860\n",
            "Epoch [6466/20000], Training Loss: 0.0824\n",
            "Epoch [6467/20000], Training Loss: 0.0807\n",
            "Epoch [6468/20000], Training Loss: 0.0793\n",
            "Epoch [6469/20000], Training Loss: 0.0819\n",
            "Epoch [6470/20000], Training Loss: 0.0792\n",
            "Epoch [6471/20000], Training Loss: 0.0811\n",
            "Epoch [6472/20000], Training Loss: 0.0853\n",
            "Epoch [6473/20000], Training Loss: 0.0820\n",
            "Epoch [6474/20000], Training Loss: 0.0771\n",
            "Epoch [6475/20000], Training Loss: 0.0812\n",
            "Epoch [6476/20000], Training Loss: 0.0765\n",
            "Epoch [6477/20000], Training Loss: 0.0805\n",
            "Epoch [6478/20000], Training Loss: 0.0821\n",
            "Epoch [6479/20000], Training Loss: 0.0768\n",
            "Epoch [6480/20000], Training Loss: 0.0798\n",
            "Epoch [6481/20000], Training Loss: 0.0796\n",
            "Epoch [6482/20000], Training Loss: 0.0824\n",
            "Epoch [6483/20000], Training Loss: 0.0788\n",
            "Epoch [6484/20000], Training Loss: 0.0779\n",
            "Epoch [6485/20000], Training Loss: 0.0754\n",
            "Epoch [6486/20000], Training Loss: 0.0723\n",
            "Epoch [6487/20000], Training Loss: 0.0800\n",
            "Epoch [6488/20000], Training Loss: 0.0769\n",
            "Epoch [6489/20000], Training Loss: 0.0835\n",
            "Epoch [6490/20000], Training Loss: 0.0832\n",
            "Epoch [6491/20000], Training Loss: 0.0802\n",
            "Epoch [6492/20000], Training Loss: 0.0808\n",
            "Epoch [6493/20000], Training Loss: 0.0757\n",
            "Epoch [6494/20000], Training Loss: 0.0776\n",
            "Epoch [6495/20000], Training Loss: 0.0797\n",
            "Epoch [6496/20000], Training Loss: 0.0788\n",
            "Epoch [6497/20000], Training Loss: 0.0879\n",
            "Epoch [6498/20000], Training Loss: 0.0799\n",
            "Epoch [6499/20000], Training Loss: 0.0788\n",
            "Epoch [6500/20000], Training Loss: 0.0794\n",
            "Epoch [6501/20000], Training Loss: 0.0801\n",
            "Epoch [6502/20000], Training Loss: 0.0790\n",
            "Epoch [6503/20000], Training Loss: 0.0775\n",
            "Epoch [6504/20000], Training Loss: 0.0874\n",
            "Epoch [6505/20000], Training Loss: 0.0817\n",
            "Epoch [6506/20000], Training Loss: 0.0794\n",
            "Epoch [6507/20000], Training Loss: 0.0811\n",
            "Epoch [6508/20000], Training Loss: 0.0768\n",
            "Epoch [6509/20000], Training Loss: 0.0794\n",
            "Epoch [6510/20000], Training Loss: 0.0830\n",
            "Epoch [6511/20000], Training Loss: 0.0833\n",
            "Epoch [6512/20000], Training Loss: 0.0761\n",
            "Epoch [6513/20000], Training Loss: 0.0775\n",
            "Epoch [6514/20000], Training Loss: 0.0830\n",
            "Epoch [6515/20000], Training Loss: 0.0749\n",
            "Epoch [6516/20000], Training Loss: 0.0784\n",
            "Epoch [6517/20000], Training Loss: 0.0752\n",
            "Epoch [6518/20000], Training Loss: 0.0832\n",
            "Epoch [6519/20000], Training Loss: 0.0758\n",
            "Epoch [6520/20000], Training Loss: 0.0784\n",
            "Epoch [6521/20000], Training Loss: 0.0777\n",
            "Epoch [6522/20000], Training Loss: 0.0747\n",
            "Epoch [6523/20000], Training Loss: 0.0735\n",
            "Epoch [6524/20000], Training Loss: 0.0847\n",
            "Epoch [6525/20000], Training Loss: 0.0721\n",
            "Epoch [6526/20000], Training Loss: 0.0796\n",
            "Epoch [6527/20000], Training Loss: 0.0806\n",
            "Epoch [6528/20000], Training Loss: 0.0849\n",
            "Epoch [6529/20000], Training Loss: 0.0789\n",
            "Epoch [6530/20000], Training Loss: 0.0869\n",
            "Epoch [6531/20000], Training Loss: 0.0799\n",
            "Epoch [6532/20000], Training Loss: 0.0792\n",
            "Epoch [6533/20000], Training Loss: 0.0784\n",
            "Epoch [6534/20000], Training Loss: 0.0738\n",
            "Epoch [6535/20000], Training Loss: 0.0737\n",
            "Epoch [6536/20000], Training Loss: 0.0785\n",
            "Epoch [6537/20000], Training Loss: 0.0831\n",
            "Epoch [6538/20000], Training Loss: 0.0813\n",
            "Epoch [6539/20000], Training Loss: 0.0821\n",
            "Epoch [6540/20000], Training Loss: 0.0797\n",
            "Epoch [6541/20000], Training Loss: 0.0792\n",
            "Epoch [6542/20000], Training Loss: 0.0787\n",
            "Epoch [6543/20000], Training Loss: 0.0818\n",
            "Epoch [6544/20000], Training Loss: 0.0725\n",
            "Epoch [6545/20000], Training Loss: 0.0772\n",
            "Epoch [6546/20000], Training Loss: 0.0852\n",
            "Epoch [6547/20000], Training Loss: 0.0740\n",
            "Epoch [6548/20000], Training Loss: 0.0811\n",
            "Epoch [6549/20000], Training Loss: 0.0758\n",
            "Epoch [6550/20000], Training Loss: 0.0736\n",
            "Epoch [6551/20000], Training Loss: 0.0837\n",
            "Epoch [6552/20000], Training Loss: 0.0828\n",
            "Epoch [6553/20000], Training Loss: 0.0843\n",
            "Epoch [6554/20000], Training Loss: 0.0785\n",
            "Epoch [6555/20000], Training Loss: 0.0787\n",
            "Epoch [6556/20000], Training Loss: 0.0812\n",
            "Epoch [6557/20000], Training Loss: 0.0808\n",
            "Epoch [6558/20000], Training Loss: 0.0790\n",
            "Epoch [6559/20000], Training Loss: 0.0730\n",
            "Epoch [6560/20000], Training Loss: 0.0741\n",
            "Epoch [6561/20000], Training Loss: 0.0760\n",
            "Epoch [6562/20000], Training Loss: 0.0813\n",
            "Epoch [6563/20000], Training Loss: 0.0760\n",
            "Epoch [6564/20000], Training Loss: 0.0850\n",
            "Epoch [6565/20000], Training Loss: 0.0808\n",
            "Epoch [6566/20000], Training Loss: 0.0759\n",
            "Epoch [6567/20000], Training Loss: 0.0824\n",
            "Epoch [6568/20000], Training Loss: 0.0807\n",
            "Epoch [6569/20000], Training Loss: 0.0764\n",
            "Epoch [6570/20000], Training Loss: 0.0721\n",
            "Epoch [6571/20000], Training Loss: 0.0744\n",
            "Epoch [6572/20000], Training Loss: 0.0799\n",
            "Epoch [6573/20000], Training Loss: 0.0849\n",
            "Epoch [6574/20000], Training Loss: 0.0789\n",
            "Epoch [6575/20000], Training Loss: 0.0768\n",
            "Epoch [6576/20000], Training Loss: 0.0767\n",
            "Epoch [6577/20000], Training Loss: 0.0778\n",
            "Epoch [6578/20000], Training Loss: 0.0807\n",
            "Epoch [6579/20000], Training Loss: 0.0786\n",
            "Epoch [6580/20000], Training Loss: 0.0743\n",
            "Epoch [6581/20000], Training Loss: 0.0765\n",
            "Epoch [6582/20000], Training Loss: 0.0781\n",
            "Epoch [6583/20000], Training Loss: 0.0818\n",
            "Epoch [6584/20000], Training Loss: 0.0755\n",
            "Epoch [6585/20000], Training Loss: 0.0790\n",
            "Epoch [6586/20000], Training Loss: 0.0786\n",
            "Epoch [6587/20000], Training Loss: 0.0849\n",
            "Epoch [6588/20000], Training Loss: 0.0808\n",
            "Epoch [6589/20000], Training Loss: 0.0740\n",
            "Epoch [6590/20000], Training Loss: 0.0816\n",
            "Epoch [6591/20000], Training Loss: 0.0840\n",
            "Epoch [6592/20000], Training Loss: 0.0779\n",
            "Epoch [6593/20000], Training Loss: 0.0762\n",
            "Epoch [6594/20000], Training Loss: 0.0743\n",
            "Epoch [6595/20000], Training Loss: 0.0784\n",
            "Epoch [6596/20000], Training Loss: 0.0757\n",
            "Epoch [6597/20000], Training Loss: 0.0724\n",
            "Epoch [6598/20000], Training Loss: 0.0781\n",
            "Epoch [6599/20000], Training Loss: 0.0789\n",
            "Epoch [6600/20000], Training Loss: 0.0841\n",
            "Epoch [6601/20000], Training Loss: 0.0783\n",
            "Epoch [6602/20000], Training Loss: 0.0855\n",
            "Epoch [6603/20000], Training Loss: 0.0843\n",
            "Epoch [6604/20000], Training Loss: 0.0841\n",
            "Epoch [6605/20000], Training Loss: 0.0748\n",
            "Epoch [6606/20000], Training Loss: 0.0762\n",
            "Epoch [6607/20000], Training Loss: 0.0785\n",
            "Epoch [6608/20000], Training Loss: 0.0866\n",
            "Epoch [6609/20000], Training Loss: 0.0810\n",
            "Epoch [6610/20000], Training Loss: 0.0736\n",
            "Epoch [6611/20000], Training Loss: 0.0833\n",
            "Epoch [6612/20000], Training Loss: 0.0723\n",
            "Epoch [6613/20000], Training Loss: 0.0790\n",
            "Epoch [6614/20000], Training Loss: 0.0751\n",
            "Epoch [6615/20000], Training Loss: 0.0825\n",
            "Epoch [6616/20000], Training Loss: 0.0795\n",
            "Epoch [6617/20000], Training Loss: 0.0777\n",
            "Epoch [6618/20000], Training Loss: 0.0824\n",
            "Epoch [6619/20000], Training Loss: 0.0746\n",
            "Epoch [6620/20000], Training Loss: 0.0801\n",
            "Epoch [6621/20000], Training Loss: 0.0865\n",
            "Epoch [6622/20000], Training Loss: 0.0820\n",
            "Epoch [6623/20000], Training Loss: 0.0816\n",
            "Epoch [6624/20000], Training Loss: 0.0768\n",
            "Epoch [6625/20000], Training Loss: 0.0812\n",
            "Epoch [6626/20000], Training Loss: 0.0803\n",
            "Epoch [6627/20000], Training Loss: 0.0764\n",
            "Epoch [6628/20000], Training Loss: 0.0803\n",
            "Epoch [6629/20000], Training Loss: 0.0809\n",
            "Epoch [6630/20000], Training Loss: 0.0738\n",
            "Epoch [6631/20000], Training Loss: 0.0839\n",
            "Epoch [6632/20000], Training Loss: 0.0779\n",
            "Epoch [6633/20000], Training Loss: 0.0755\n",
            "Epoch [6634/20000], Training Loss: 0.0831\n",
            "Epoch [6635/20000], Training Loss: 0.0810\n",
            "Epoch [6636/20000], Training Loss: 0.0820\n",
            "Epoch [6637/20000], Training Loss: 0.0826\n",
            "Epoch [6638/20000], Training Loss: 0.0822\n",
            "Epoch [6639/20000], Training Loss: 0.0781\n",
            "Epoch [6640/20000], Training Loss: 0.0819\n",
            "Epoch [6641/20000], Training Loss: 0.0812\n",
            "Epoch [6642/20000], Training Loss: 0.0838\n",
            "Epoch [6643/20000], Training Loss: 0.0766\n",
            "Epoch [6644/20000], Training Loss: 0.0801\n",
            "Epoch [6645/20000], Training Loss: 0.0769\n",
            "Epoch [6646/20000], Training Loss: 0.0840\n",
            "Epoch [6647/20000], Training Loss: 0.0777\n",
            "Epoch [6648/20000], Training Loss: 0.0817\n",
            "Epoch [6649/20000], Training Loss: 0.0819\n",
            "Epoch [6650/20000], Training Loss: 0.0827\n",
            "Epoch [6651/20000], Training Loss: 0.0822\n",
            "Epoch [6652/20000], Training Loss: 0.0789\n",
            "Epoch [6653/20000], Training Loss: 0.0843\n",
            "Epoch [6654/20000], Training Loss: 0.0840\n",
            "Epoch [6655/20000], Training Loss: 0.0862\n",
            "Epoch [6656/20000], Training Loss: 0.0754\n",
            "Epoch [6657/20000], Training Loss: 0.0802\n",
            "Epoch [6658/20000], Training Loss: 0.0845\n",
            "Epoch [6659/20000], Training Loss: 0.0785\n",
            "Epoch [6660/20000], Training Loss: 0.0746\n",
            "Epoch [6661/20000], Training Loss: 0.0755\n",
            "Epoch [6662/20000], Training Loss: 0.0825\n",
            "Epoch [6663/20000], Training Loss: 0.0739\n",
            "Epoch [6664/20000], Training Loss: 0.0786\n",
            "Epoch [6665/20000], Training Loss: 0.0765\n",
            "Epoch [6666/20000], Training Loss: 0.0839\n",
            "Epoch [6667/20000], Training Loss: 0.0792\n",
            "Epoch [6668/20000], Training Loss: 0.0802\n",
            "Epoch [6669/20000], Training Loss: 0.0815\n",
            "Epoch [6670/20000], Training Loss: 0.0776\n",
            "Epoch [6671/20000], Training Loss: 0.0853\n",
            "Epoch [6672/20000], Training Loss: 0.0752\n",
            "Epoch [6673/20000], Training Loss: 0.0759\n",
            "Epoch [6674/20000], Training Loss: 0.0792\n",
            "Epoch [6675/20000], Training Loss: 0.0793\n",
            "Epoch [6676/20000], Training Loss: 0.0758\n",
            "Epoch [6677/20000], Training Loss: 0.0774\n",
            "Epoch [6678/20000], Training Loss: 0.0805\n",
            "Epoch [6679/20000], Training Loss: 0.0815\n",
            "Epoch [6680/20000], Training Loss: 0.0849\n",
            "Epoch [6681/20000], Training Loss: 0.0786\n",
            "Epoch [6682/20000], Training Loss: 0.0793\n",
            "Epoch [6683/20000], Training Loss: 0.0809\n",
            "Epoch [6684/20000], Training Loss: 0.0759\n",
            "Epoch [6685/20000], Training Loss: 0.0753\n",
            "Epoch [6686/20000], Training Loss: 0.0844\n",
            "Epoch [6687/20000], Training Loss: 0.0782\n",
            "Epoch [6688/20000], Training Loss: 0.0757\n",
            "Epoch [6689/20000], Training Loss: 0.0814\n",
            "Epoch [6690/20000], Training Loss: 0.0793\n",
            "Epoch [6691/20000], Training Loss: 0.0790\n",
            "Epoch [6692/20000], Training Loss: 0.0827\n",
            "Epoch [6693/20000], Training Loss: 0.0786\n",
            "Epoch [6694/20000], Training Loss: 0.0863\n",
            "Epoch [6695/20000], Training Loss: 0.0810\n",
            "Epoch [6696/20000], Training Loss: 0.0809\n",
            "Epoch [6697/20000], Training Loss: 0.0750\n",
            "Epoch [6698/20000], Training Loss: 0.0793\n",
            "Epoch [6699/20000], Training Loss: 0.0781\n",
            "Epoch [6700/20000], Training Loss: 0.0803\n",
            "Epoch [6701/20000], Training Loss: 0.0818\n",
            "Epoch [6702/20000], Training Loss: 0.0740\n",
            "Epoch [6703/20000], Training Loss: 0.0741\n",
            "Epoch [6704/20000], Training Loss: 0.0804\n",
            "Epoch [6705/20000], Training Loss: 0.0816\n",
            "Epoch [6706/20000], Training Loss: 0.0739\n",
            "Epoch [6707/20000], Training Loss: 0.0774\n",
            "Epoch [6708/20000], Training Loss: 0.0850\n",
            "Epoch [6709/20000], Training Loss: 0.0760\n",
            "Epoch [6710/20000], Training Loss: 0.0746\n",
            "Epoch [6711/20000], Training Loss: 0.0876\n",
            "Epoch [6712/20000], Training Loss: 0.0742\n",
            "Epoch [6713/20000], Training Loss: 0.0806\n",
            "Epoch [6714/20000], Training Loss: 0.0799\n",
            "Epoch [6715/20000], Training Loss: 0.0860\n",
            "Epoch [6716/20000], Training Loss: 0.0768\n",
            "Epoch [6717/20000], Training Loss: 0.0773\n",
            "Epoch [6718/20000], Training Loss: 0.0764\n",
            "Epoch [6719/20000], Training Loss: 0.0785\n",
            "Epoch [6720/20000], Training Loss: 0.0784\n",
            "Epoch [6721/20000], Training Loss: 0.0802\n",
            "Epoch [6722/20000], Training Loss: 0.0810\n",
            "Epoch [6723/20000], Training Loss: 0.0721\n",
            "Epoch [6724/20000], Training Loss: 0.0770\n",
            "Epoch [6725/20000], Training Loss: 0.0789\n",
            "Epoch [6726/20000], Training Loss: 0.0747\n",
            "Epoch [6727/20000], Training Loss: 0.0750\n",
            "Epoch [6728/20000], Training Loss: 0.0837\n",
            "Epoch [6729/20000], Training Loss: 0.0771\n",
            "Epoch [6730/20000], Training Loss: 0.0866\n",
            "Epoch [6731/20000], Training Loss: 0.0800\n",
            "Epoch [6732/20000], Training Loss: 0.0808\n",
            "Epoch [6733/20000], Training Loss: 0.0746\n",
            "Epoch [6734/20000], Training Loss: 0.0745\n",
            "Epoch [6735/20000], Training Loss: 0.0804\n",
            "Epoch [6736/20000], Training Loss: 0.0771\n",
            "Epoch [6737/20000], Training Loss: 0.0870\n",
            "Epoch [6738/20000], Training Loss: 0.0822\n",
            "Epoch [6739/20000], Training Loss: 0.0766\n",
            "Epoch [6740/20000], Training Loss: 0.0807\n",
            "Epoch [6741/20000], Training Loss: 0.0736\n",
            "Epoch [6742/20000], Training Loss: 0.0793\n",
            "Epoch [6743/20000], Training Loss: 0.0796\n",
            "Epoch [6744/20000], Training Loss: 0.0807\n",
            "Epoch [6745/20000], Training Loss: 0.0755\n",
            "Epoch [6746/20000], Training Loss: 0.0741\n",
            "Epoch [6747/20000], Training Loss: 0.0769\n",
            "Epoch [6748/20000], Training Loss: 0.0786\n",
            "Epoch [6749/20000], Training Loss: 0.0743\n",
            "Epoch [6750/20000], Training Loss: 0.0814\n",
            "Epoch [6751/20000], Training Loss: 0.0794\n",
            "Epoch [6752/20000], Training Loss: 0.0763\n",
            "Epoch [6753/20000], Training Loss: 0.0781\n",
            "Epoch [6754/20000], Training Loss: 0.0794\n",
            "Epoch [6755/20000], Training Loss: 0.0820\n",
            "Epoch [6756/20000], Training Loss: 0.0804\n",
            "Epoch [6757/20000], Training Loss: 0.0854\n",
            "Epoch [6758/20000], Training Loss: 0.0759\n",
            "Epoch [6759/20000], Training Loss: 0.0830\n",
            "Epoch [6760/20000], Training Loss: 0.0761\n",
            "Epoch [6761/20000], Training Loss: 0.0833\n",
            "Epoch [6762/20000], Training Loss: 0.0738\n",
            "Epoch [6763/20000], Training Loss: 0.0759\n",
            "Epoch [6764/20000], Training Loss: 0.0793\n",
            "Epoch [6765/20000], Training Loss: 0.0778\n",
            "Epoch [6766/20000], Training Loss: 0.0807\n",
            "Epoch [6767/20000], Training Loss: 0.0779\n",
            "Epoch [6768/20000], Training Loss: 0.0747\n",
            "Epoch [6769/20000], Training Loss: 0.0740\n",
            "Epoch [6770/20000], Training Loss: 0.0790\n",
            "Epoch [6771/20000], Training Loss: 0.0796\n",
            "Epoch [6772/20000], Training Loss: 0.0859\n",
            "Epoch [6773/20000], Training Loss: 0.0815\n",
            "Epoch [6774/20000], Training Loss: 0.0836\n",
            "Epoch [6775/20000], Training Loss: 0.0813\n",
            "Epoch [6776/20000], Training Loss: 0.0798\n",
            "Epoch [6777/20000], Training Loss: 0.0839\n",
            "Epoch [6778/20000], Training Loss: 0.0864\n",
            "Epoch [6779/20000], Training Loss: 0.0790\n",
            "Epoch [6780/20000], Training Loss: 0.0854\n",
            "Epoch [6781/20000], Training Loss: 0.0834\n",
            "Epoch [6782/20000], Training Loss: 0.0752\n",
            "Epoch [6783/20000], Training Loss: 0.0808\n",
            "Epoch [6784/20000], Training Loss: 0.0760\n",
            "Epoch [6785/20000], Training Loss: 0.0814\n",
            "Epoch [6786/20000], Training Loss: 0.0781\n",
            "Epoch [6787/20000], Training Loss: 0.0814\n",
            "Epoch [6788/20000], Training Loss: 0.0855\n",
            "Epoch [6789/20000], Training Loss: 0.0827\n",
            "Epoch [6790/20000], Training Loss: 0.0797\n",
            "Epoch [6791/20000], Training Loss: 0.0810\n",
            "Epoch [6792/20000], Training Loss: 0.0772\n",
            "Epoch [6793/20000], Training Loss: 0.0752\n",
            "Epoch [6794/20000], Training Loss: 0.0813\n",
            "Epoch [6795/20000], Training Loss: 0.0815\n",
            "Epoch [6796/20000], Training Loss: 0.0815\n",
            "Epoch [6797/20000], Training Loss: 0.0828\n",
            "Epoch [6798/20000], Training Loss: 0.0773\n",
            "Epoch [6799/20000], Training Loss: 0.0759\n",
            "Epoch [6800/20000], Training Loss: 0.0753\n",
            "Epoch [6801/20000], Training Loss: 0.0751\n",
            "Epoch [6802/20000], Training Loss: 0.0762\n",
            "Epoch [6803/20000], Training Loss: 0.0777\n",
            "Epoch [6804/20000], Training Loss: 0.0871\n",
            "Epoch [6805/20000], Training Loss: 0.0760\n",
            "Epoch [6806/20000], Training Loss: 0.0785\n",
            "Epoch [6807/20000], Training Loss: 0.0763\n",
            "Epoch [6808/20000], Training Loss: 0.0815\n",
            "Epoch [6809/20000], Training Loss: 0.0793\n",
            "Epoch [6810/20000], Training Loss: 0.0794\n",
            "Epoch [6811/20000], Training Loss: 0.0777\n",
            "Epoch [6812/20000], Training Loss: 0.0825\n",
            "Epoch [6813/20000], Training Loss: 0.0852\n",
            "Epoch [6814/20000], Training Loss: 0.0768\n",
            "Epoch [6815/20000], Training Loss: 0.0803\n",
            "Epoch [6816/20000], Training Loss: 0.0764\n",
            "Epoch [6817/20000], Training Loss: 0.0763\n",
            "Epoch [6818/20000], Training Loss: 0.0818\n",
            "Epoch [6819/20000], Training Loss: 0.0816\n",
            "Epoch [6820/20000], Training Loss: 0.0857\n",
            "Epoch [6821/20000], Training Loss: 0.0766\n",
            "Epoch [6822/20000], Training Loss: 0.0816\n",
            "Epoch [6823/20000], Training Loss: 0.0741\n",
            "Epoch [6824/20000], Training Loss: 0.0839\n",
            "Epoch [6825/20000], Training Loss: 0.0838\n",
            "Epoch [6826/20000], Training Loss: 0.0833\n",
            "Epoch [6827/20000], Training Loss: 0.0748\n",
            "Epoch [6828/20000], Training Loss: 0.0791\n",
            "Epoch [6829/20000], Training Loss: 0.0815\n",
            "Epoch [6830/20000], Training Loss: 0.0819\n",
            "Epoch [6831/20000], Training Loss: 0.0805\n",
            "Epoch [6832/20000], Training Loss: 0.0754\n",
            "Epoch [6833/20000], Training Loss: 0.0798\n",
            "Epoch [6834/20000], Training Loss: 0.0759\n",
            "Epoch [6835/20000], Training Loss: 0.0815\n",
            "Epoch [6836/20000], Training Loss: 0.0827\n",
            "Epoch [6837/20000], Training Loss: 0.0805\n",
            "Epoch [6838/20000], Training Loss: 0.0741\n",
            "Epoch [6839/20000], Training Loss: 0.0794\n",
            "Epoch [6840/20000], Training Loss: 0.0775\n",
            "Epoch [6841/20000], Training Loss: 0.0785\n",
            "Epoch [6842/20000], Training Loss: 0.0763\n",
            "Epoch [6843/20000], Training Loss: 0.0782\n",
            "Epoch [6844/20000], Training Loss: 0.0731\n",
            "Epoch [6845/20000], Training Loss: 0.0814\n",
            "Epoch [6846/20000], Training Loss: 0.0783\n",
            "Epoch [6847/20000], Training Loss: 0.0831\n",
            "Epoch [6848/20000], Training Loss: 0.0787\n",
            "Epoch [6849/20000], Training Loss: 0.0884\n",
            "Epoch [6850/20000], Training Loss: 0.0820\n",
            "Epoch [6851/20000], Training Loss: 0.0844\n",
            "Epoch [6852/20000], Training Loss: 0.0816\n",
            "Epoch [6853/20000], Training Loss: 0.0815\n",
            "Epoch [6854/20000], Training Loss: 0.0721\n",
            "Epoch [6855/20000], Training Loss: 0.0798\n",
            "Epoch [6856/20000], Training Loss: 0.0824\n",
            "Epoch [6857/20000], Training Loss: 0.0810\n",
            "Epoch [6858/20000], Training Loss: 0.0797\n",
            "Epoch [6859/20000], Training Loss: 0.0785\n",
            "Epoch [6860/20000], Training Loss: 0.0735\n",
            "Epoch [6861/20000], Training Loss: 0.0820\n",
            "Epoch [6862/20000], Training Loss: 0.0720\n",
            "Epoch [6863/20000], Training Loss: 0.0842\n",
            "Epoch [6864/20000], Training Loss: 0.0755\n",
            "Epoch [6865/20000], Training Loss: 0.0805\n",
            "Epoch [6866/20000], Training Loss: 0.0827\n",
            "Epoch [6867/20000], Training Loss: 0.0826\n",
            "Epoch [6868/20000], Training Loss: 0.0798\n",
            "Epoch [6869/20000], Training Loss: 0.0819\n",
            "Epoch [6870/20000], Training Loss: 0.0791\n",
            "Epoch [6871/20000], Training Loss: 0.0758\n",
            "Epoch [6872/20000], Training Loss: 0.0750\n",
            "Epoch [6873/20000], Training Loss: 0.0746\n",
            "Epoch [6874/20000], Training Loss: 0.0791\n",
            "Epoch [6875/20000], Training Loss: 0.0816\n",
            "Epoch [6876/20000], Training Loss: 0.0813\n",
            "Epoch [6877/20000], Training Loss: 0.0796\n",
            "Epoch [6878/20000], Training Loss: 0.0798\n",
            "Epoch [6879/20000], Training Loss: 0.0779\n",
            "Epoch [6880/20000], Training Loss: 0.0760\n",
            "Epoch [6881/20000], Training Loss: 0.0817\n",
            "Epoch [6882/20000], Training Loss: 0.0747\n",
            "Epoch [6883/20000], Training Loss: 0.0765\n",
            "Epoch [6884/20000], Training Loss: 0.0776\n",
            "Epoch [6885/20000], Training Loss: 0.0812\n",
            "Epoch [6886/20000], Training Loss: 0.0749\n",
            "Epoch [6887/20000], Training Loss: 0.0823\n",
            "Epoch [6888/20000], Training Loss: 0.0855\n",
            "Epoch [6889/20000], Training Loss: 0.0791\n",
            "Epoch [6890/20000], Training Loss: 0.0785\n",
            "Epoch [6891/20000], Training Loss: 0.0799\n",
            "Epoch [6892/20000], Training Loss: 0.0723\n",
            "Epoch [6893/20000], Training Loss: 0.0736\n",
            "Epoch [6894/20000], Training Loss: 0.0769\n",
            "Epoch [6895/20000], Training Loss: 0.0806\n",
            "Epoch [6896/20000], Training Loss: 0.0817\n",
            "Epoch [6897/20000], Training Loss: 0.0816\n",
            "Epoch [6898/20000], Training Loss: 0.0796\n",
            "Epoch [6899/20000], Training Loss: 0.0835\n",
            "Epoch [6900/20000], Training Loss: 0.0818\n",
            "Epoch [6901/20000], Training Loss: 0.0840\n",
            "Epoch [6902/20000], Training Loss: 0.0838\n",
            "Epoch [6903/20000], Training Loss: 0.0786\n",
            "Epoch [6904/20000], Training Loss: 0.0762\n",
            "Epoch [6905/20000], Training Loss: 0.0759\n",
            "Epoch [6906/20000], Training Loss: 0.0856\n",
            "Epoch [6907/20000], Training Loss: 0.0812\n",
            "Epoch [6908/20000], Training Loss: 0.0802\n",
            "Epoch [6909/20000], Training Loss: 0.0788\n",
            "Epoch [6910/20000], Training Loss: 0.0738\n",
            "Epoch [6911/20000], Training Loss: 0.0758\n",
            "Epoch [6912/20000], Training Loss: 0.0813\n",
            "Epoch [6913/20000], Training Loss: 0.0858\n",
            "Epoch [6914/20000], Training Loss: 0.0843\n",
            "Epoch [6915/20000], Training Loss: 0.0761\n",
            "Epoch [6916/20000], Training Loss: 0.0783\n",
            "Epoch [6917/20000], Training Loss: 0.0829\n",
            "Epoch [6918/20000], Training Loss: 0.0738\n",
            "Epoch [6919/20000], Training Loss: 0.0773\n",
            "Epoch [6920/20000], Training Loss: 0.0736\n",
            "Epoch [6921/20000], Training Loss: 0.0788\n",
            "Epoch [6922/20000], Training Loss: 0.0777\n",
            "Epoch [6923/20000], Training Loss: 0.0825\n",
            "Epoch [6924/20000], Training Loss: 0.0782\n",
            "Epoch [6925/20000], Training Loss: 0.0783\n",
            "Epoch [6926/20000], Training Loss: 0.0881\n",
            "Epoch [6927/20000], Training Loss: 0.0813\n",
            "Epoch [6928/20000], Training Loss: 0.0738\n",
            "Epoch [6929/20000], Training Loss: 0.0846\n",
            "Epoch [6930/20000], Training Loss: 0.0867\n",
            "Epoch [6931/20000], Training Loss: 0.0765\n",
            "Epoch [6932/20000], Training Loss: 0.0787\n",
            "Epoch [6933/20000], Training Loss: 0.0736\n",
            "Epoch [6934/20000], Training Loss: 0.0793\n",
            "Epoch [6935/20000], Training Loss: 0.0816\n",
            "Epoch [6936/20000], Training Loss: 0.0804\n",
            "Epoch [6937/20000], Training Loss: 0.0773\n",
            "Epoch [6938/20000], Training Loss: 0.0810\n",
            "Epoch [6939/20000], Training Loss: 0.0793\n",
            "Epoch [6940/20000], Training Loss: 0.0781\n",
            "Epoch [6941/20000], Training Loss: 0.0726\n",
            "Epoch [6942/20000], Training Loss: 0.0810\n",
            "Epoch [6943/20000], Training Loss: 0.0753\n",
            "Epoch [6944/20000], Training Loss: 0.0832\n",
            "Epoch [6945/20000], Training Loss: 0.0817\n",
            "Epoch [6946/20000], Training Loss: 0.0807\n",
            "Epoch [6947/20000], Training Loss: 0.0772\n",
            "Epoch [6948/20000], Training Loss: 0.0797\n",
            "Epoch [6949/20000], Training Loss: 0.0788\n",
            "Epoch [6950/20000], Training Loss: 0.0819\n",
            "Epoch [6951/20000], Training Loss: 0.0818\n",
            "Epoch [6952/20000], Training Loss: 0.0791\n",
            "Epoch [6953/20000], Training Loss: 0.0800\n",
            "Epoch [6954/20000], Training Loss: 0.0835\n",
            "Epoch [6955/20000], Training Loss: 0.0730\n",
            "Epoch [6956/20000], Training Loss: 0.0802\n",
            "Epoch [6957/20000], Training Loss: 0.0795\n",
            "Epoch [6958/20000], Training Loss: 0.0826\n",
            "Epoch [6959/20000], Training Loss: 0.0805\n",
            "Epoch [6960/20000], Training Loss: 0.0734\n",
            "Epoch [6961/20000], Training Loss: 0.0788\n",
            "Epoch [6962/20000], Training Loss: 0.0823\n",
            "Epoch [6963/20000], Training Loss: 0.0826\n",
            "Epoch [6964/20000], Training Loss: 0.0745\n",
            "Epoch [6965/20000], Training Loss: 0.0799\n",
            "Epoch [6966/20000], Training Loss: 0.0802\n",
            "Epoch [6967/20000], Training Loss: 0.0761\n",
            "Epoch [6968/20000], Training Loss: 0.0813\n",
            "Epoch [6969/20000], Training Loss: 0.0780\n",
            "Epoch [6970/20000], Training Loss: 0.0740\n",
            "Epoch [6971/20000], Training Loss: 0.0766\n",
            "Epoch [6972/20000], Training Loss: 0.0860\n",
            "Epoch [6973/20000], Training Loss: 0.0791\n",
            "Epoch [6974/20000], Training Loss: 0.0754\n",
            "Epoch [6975/20000], Training Loss: 0.0779\n",
            "Epoch [6976/20000], Training Loss: 0.0816\n",
            "Epoch [6977/20000], Training Loss: 0.0748\n",
            "Epoch [6978/20000], Training Loss: 0.0807\n",
            "Epoch [6979/20000], Training Loss: 0.0845\n",
            "Epoch [6980/20000], Training Loss: 0.0825\n",
            "Epoch [6981/20000], Training Loss: 0.0788\n",
            "Epoch [6982/20000], Training Loss: 0.0758\n",
            "Epoch [6983/20000], Training Loss: 0.0816\n",
            "Epoch [6984/20000], Training Loss: 0.0753\n",
            "Epoch [6985/20000], Training Loss: 0.0793\n",
            "Epoch [6986/20000], Training Loss: 0.0811\n",
            "Epoch [6987/20000], Training Loss: 0.0817\n",
            "Epoch [6988/20000], Training Loss: 0.0729\n",
            "Epoch [6989/20000], Training Loss: 0.0809\n",
            "Epoch [6990/20000], Training Loss: 0.0793\n",
            "Epoch [6991/20000], Training Loss: 0.0777\n",
            "Epoch [6992/20000], Training Loss: 0.0744\n",
            "Epoch [6993/20000], Training Loss: 0.0814\n",
            "Epoch [6994/20000], Training Loss: 0.0839\n",
            "Epoch [6995/20000], Training Loss: 0.0827\n",
            "Epoch [6996/20000], Training Loss: 0.0820\n",
            "Epoch [6997/20000], Training Loss: 0.0789\n",
            "Epoch [6998/20000], Training Loss: 0.0826\n",
            "Epoch [6999/20000], Training Loss: 0.0859\n",
            "Epoch [7000/20000], Training Loss: 0.0803\n",
            "Epoch [7001/20000], Training Loss: 0.0805\n",
            "Epoch [7002/20000], Training Loss: 0.0831\n",
            "Epoch [7003/20000], Training Loss: 0.0826\n",
            "Epoch [7004/20000], Training Loss: 0.0783\n",
            "Epoch [7005/20000], Training Loss: 0.0769\n",
            "Epoch [7006/20000], Training Loss: 0.0793\n",
            "Epoch [7007/20000], Training Loss: 0.0801\n",
            "Epoch [7008/20000], Training Loss: 0.0855\n",
            "Epoch [7009/20000], Training Loss: 0.0771\n",
            "Epoch [7010/20000], Training Loss: 0.0815\n",
            "Epoch [7011/20000], Training Loss: 0.0856\n",
            "Epoch [7012/20000], Training Loss: 0.0874\n",
            "Epoch [7013/20000], Training Loss: 0.0818\n",
            "Epoch [7014/20000], Training Loss: 0.0791\n",
            "Epoch [7015/20000], Training Loss: 0.0801\n",
            "Epoch [7016/20000], Training Loss: 0.0875\n",
            "Epoch [7017/20000], Training Loss: 0.0810\n",
            "Epoch [7018/20000], Training Loss: 0.0748\n",
            "Epoch [7019/20000], Training Loss: 0.0788\n",
            "Epoch [7020/20000], Training Loss: 0.0808\n",
            "Epoch [7021/20000], Training Loss: 0.0834\n",
            "Epoch [7022/20000], Training Loss: 0.0793\n",
            "Epoch [7023/20000], Training Loss: 0.0795\n",
            "Epoch [7024/20000], Training Loss: 0.0799\n",
            "Epoch [7025/20000], Training Loss: 0.0778\n",
            "Epoch [7026/20000], Training Loss: 0.0796\n",
            "Epoch [7027/20000], Training Loss: 0.0801\n",
            "Epoch [7028/20000], Training Loss: 0.0856\n",
            "Epoch [7029/20000], Training Loss: 0.0785\n",
            "Epoch [7030/20000], Training Loss: 0.0748\n",
            "Epoch [7031/20000], Training Loss: 0.0789\n",
            "Epoch [7032/20000], Training Loss: 0.0858\n",
            "Epoch [7033/20000], Training Loss: 0.0784\n",
            "Epoch [7034/20000], Training Loss: 0.0855\n",
            "Epoch [7035/20000], Training Loss: 0.0746\n",
            "Epoch [7036/20000], Training Loss: 0.0785\n",
            "Epoch [7037/20000], Training Loss: 0.0759\n",
            "Epoch [7038/20000], Training Loss: 0.0760\n",
            "Epoch [7039/20000], Training Loss: 0.0846\n",
            "Epoch [7040/20000], Training Loss: 0.0810\n",
            "Epoch [7041/20000], Training Loss: 0.0815\n",
            "Epoch [7042/20000], Training Loss: 0.0792\n",
            "Epoch [7043/20000], Training Loss: 0.0792\n",
            "Epoch [7044/20000], Training Loss: 0.0836\n",
            "Epoch [7045/20000], Training Loss: 0.0770\n",
            "Epoch [7046/20000], Training Loss: 0.0811\n",
            "Epoch [7047/20000], Training Loss: 0.0793\n",
            "Epoch [7048/20000], Training Loss: 0.0862\n",
            "Epoch [7049/20000], Training Loss: 0.0858\n",
            "Epoch [7050/20000], Training Loss: 0.0792\n",
            "Epoch [7051/20000], Training Loss: 0.0795\n",
            "Epoch [7052/20000], Training Loss: 0.0759\n",
            "Epoch [7053/20000], Training Loss: 0.0793\n",
            "Epoch [7054/20000], Training Loss: 0.0743\n",
            "Epoch [7055/20000], Training Loss: 0.0735\n",
            "Epoch [7056/20000], Training Loss: 0.0791\n",
            "Epoch [7057/20000], Training Loss: 0.0784\n",
            "Epoch [7058/20000], Training Loss: 0.0785\n",
            "Epoch [7059/20000], Training Loss: 0.0816\n",
            "Epoch [7060/20000], Training Loss: 0.0822\n",
            "Epoch [7061/20000], Training Loss: 0.0794\n",
            "Epoch [7062/20000], Training Loss: 0.0796\n",
            "Epoch [7063/20000], Training Loss: 0.0823\n",
            "Epoch [7064/20000], Training Loss: 0.0764\n",
            "Epoch [7065/20000], Training Loss: 0.0780\n",
            "Epoch [7066/20000], Training Loss: 0.0809\n",
            "Epoch [7067/20000], Training Loss: 0.0762\n",
            "Epoch [7068/20000], Training Loss: 0.0852\n",
            "Epoch [7069/20000], Training Loss: 0.0821\n",
            "Epoch [7070/20000], Training Loss: 0.0789\n",
            "Epoch [7071/20000], Training Loss: 0.0743\n",
            "Epoch [7072/20000], Training Loss: 0.0873\n",
            "Epoch [7073/20000], Training Loss: 0.0855\n",
            "Epoch [7074/20000], Training Loss: 0.0840\n",
            "Epoch [7075/20000], Training Loss: 0.0759\n",
            "Epoch [7076/20000], Training Loss: 0.0763\n",
            "Epoch [7077/20000], Training Loss: 0.0791\n",
            "Epoch [7078/20000], Training Loss: 0.0855\n",
            "Epoch [7079/20000], Training Loss: 0.0812\n",
            "Epoch [7080/20000], Training Loss: 0.0851\n",
            "Epoch [7081/20000], Training Loss: 0.0828\n",
            "Epoch [7082/20000], Training Loss: 0.0777\n",
            "Epoch [7083/20000], Training Loss: 0.0845\n",
            "Epoch [7084/20000], Training Loss: 0.0756\n",
            "Epoch [7085/20000], Training Loss: 0.0761\n",
            "Epoch [7086/20000], Training Loss: 0.0826\n",
            "Epoch [7087/20000], Training Loss: 0.0856\n",
            "Epoch [7088/20000], Training Loss: 0.0828\n",
            "Epoch [7089/20000], Training Loss: 0.0792\n",
            "Epoch [7090/20000], Training Loss: 0.0801\n",
            "Epoch [7091/20000], Training Loss: 0.0856\n",
            "Epoch [7092/20000], Training Loss: 0.0849\n",
            "Epoch [7093/20000], Training Loss: 0.0784\n",
            "Epoch [7094/20000], Training Loss: 0.0781\n",
            "Epoch [7095/20000], Training Loss: 0.0807\n",
            "Epoch [7096/20000], Training Loss: 0.0769\n",
            "Epoch [7097/20000], Training Loss: 0.0773\n",
            "Epoch [7098/20000], Training Loss: 0.0844\n",
            "Epoch [7099/20000], Training Loss: 0.0801\n",
            "Epoch [7100/20000], Training Loss: 0.0825\n",
            "Epoch [7101/20000], Training Loss: 0.0790\n",
            "Epoch [7102/20000], Training Loss: 0.0726\n",
            "Epoch [7103/20000], Training Loss: 0.0789\n",
            "Epoch [7104/20000], Training Loss: 0.0795\n",
            "Epoch [7105/20000], Training Loss: 0.0794\n",
            "Epoch [7106/20000], Training Loss: 0.0795\n",
            "Epoch [7107/20000], Training Loss: 0.0748\n",
            "Epoch [7108/20000], Training Loss: 0.0749\n",
            "Epoch [7109/20000], Training Loss: 0.0745\n",
            "Epoch [7110/20000], Training Loss: 0.0808\n",
            "Epoch [7111/20000], Training Loss: 0.0744\n",
            "Epoch [7112/20000], Training Loss: 0.0748\n",
            "Epoch [7113/20000], Training Loss: 0.0787\n",
            "Epoch [7114/20000], Training Loss: 0.0759\n",
            "Epoch [7115/20000], Training Loss: 0.0804\n",
            "Epoch [7116/20000], Training Loss: 0.0760\n",
            "Epoch [7117/20000], Training Loss: 0.0808\n",
            "Epoch [7118/20000], Training Loss: 0.0794\n",
            "Epoch [7119/20000], Training Loss: 0.0831\n",
            "Epoch [7120/20000], Training Loss: 0.0860\n",
            "Epoch [7121/20000], Training Loss: 0.0801\n",
            "Epoch [7122/20000], Training Loss: 0.0826\n",
            "Epoch [7123/20000], Training Loss: 0.0832\n",
            "Epoch [7124/20000], Training Loss: 0.0784\n",
            "Epoch [7125/20000], Training Loss: 0.0815\n",
            "Epoch [7126/20000], Training Loss: 0.0863\n",
            "Epoch [7127/20000], Training Loss: 0.0838\n",
            "Epoch [7128/20000], Training Loss: 0.0800\n",
            "Epoch [7129/20000], Training Loss: 0.0821\n",
            "Epoch [7130/20000], Training Loss: 0.0839\n",
            "Epoch [7131/20000], Training Loss: 0.0837\n",
            "Epoch [7132/20000], Training Loss: 0.0764\n",
            "Epoch [7133/20000], Training Loss: 0.0848\n",
            "Epoch [7134/20000], Training Loss: 0.0761\n",
            "Epoch [7135/20000], Training Loss: 0.0833\n",
            "Epoch [7136/20000], Training Loss: 0.0809\n",
            "Epoch [7137/20000], Training Loss: 0.0807\n",
            "Epoch [7138/20000], Training Loss: 0.0871\n",
            "Epoch [7139/20000], Training Loss: 0.0787\n",
            "Epoch [7140/20000], Training Loss: 0.0763\n",
            "Epoch [7141/20000], Training Loss: 0.0859\n",
            "Epoch [7142/20000], Training Loss: 0.0817\n",
            "Epoch [7143/20000], Training Loss: 0.0734\n",
            "Epoch [7144/20000], Training Loss: 0.0786\n",
            "Epoch [7145/20000], Training Loss: 0.0781\n",
            "Epoch [7146/20000], Training Loss: 0.0784\n",
            "Epoch [7147/20000], Training Loss: 0.0805\n",
            "Epoch [7148/20000], Training Loss: 0.0838\n",
            "Epoch [7149/20000], Training Loss: 0.0763\n",
            "Epoch [7150/20000], Training Loss: 0.0876\n",
            "Epoch [7151/20000], Training Loss: 0.0798\n",
            "Epoch [7152/20000], Training Loss: 0.0776\n",
            "Epoch [7153/20000], Training Loss: 0.0748\n",
            "Epoch [7154/20000], Training Loss: 0.0811\n",
            "Epoch [7155/20000], Training Loss: 0.0814\n",
            "Epoch [7156/20000], Training Loss: 0.0826\n",
            "Epoch [7157/20000], Training Loss: 0.0857\n",
            "Epoch [7158/20000], Training Loss: 0.0816\n",
            "Epoch [7159/20000], Training Loss: 0.0802\n",
            "Epoch [7160/20000], Training Loss: 0.0833\n",
            "Epoch [7161/20000], Training Loss: 0.0804\n",
            "Epoch [7162/20000], Training Loss: 0.0831\n",
            "Epoch [7163/20000], Training Loss: 0.0798\n",
            "Epoch [7164/20000], Training Loss: 0.0823\n",
            "Epoch [7165/20000], Training Loss: 0.0820\n",
            "Epoch [7166/20000], Training Loss: 0.0805\n",
            "Epoch [7167/20000], Training Loss: 0.0811\n",
            "Epoch [7168/20000], Training Loss: 0.0820\n",
            "Epoch [7169/20000], Training Loss: 0.0833\n",
            "Epoch [7170/20000], Training Loss: 0.0888\n",
            "Epoch [7171/20000], Training Loss: 0.0812\n",
            "Epoch [7172/20000], Training Loss: 0.0798\n",
            "Epoch [7173/20000], Training Loss: 0.0740\n",
            "Epoch [7174/20000], Training Loss: 0.0779\n",
            "Epoch [7175/20000], Training Loss: 0.0751\n",
            "Epoch [7176/20000], Training Loss: 0.0770\n",
            "Epoch [7177/20000], Training Loss: 0.0875\n",
            "Epoch [7178/20000], Training Loss: 0.0850\n",
            "Epoch [7179/20000], Training Loss: 0.0860\n",
            "Epoch [7180/20000], Training Loss: 0.0724\n",
            "Epoch [7181/20000], Training Loss: 0.0820\n",
            "Epoch [7182/20000], Training Loss: 0.0808\n",
            "Epoch [7183/20000], Training Loss: 0.0765\n",
            "Epoch [7184/20000], Training Loss: 0.0751\n",
            "Epoch [7185/20000], Training Loss: 0.0754\n",
            "Epoch [7186/20000], Training Loss: 0.0770\n",
            "Epoch [7187/20000], Training Loss: 0.0877\n",
            "Epoch [7188/20000], Training Loss: 0.0769\n",
            "Epoch [7189/20000], Training Loss: 0.0784\n",
            "Epoch [7190/20000], Training Loss: 0.0743\n",
            "Epoch [7191/20000], Training Loss: 0.0789\n",
            "Epoch [7192/20000], Training Loss: 0.0817\n",
            "Epoch [7193/20000], Training Loss: 0.0724\n",
            "Epoch [7194/20000], Training Loss: 0.0741\n",
            "Epoch [7195/20000], Training Loss: 0.0835\n",
            "Epoch [7196/20000], Training Loss: 0.0775\n",
            "Epoch [7197/20000], Training Loss: 0.0803\n",
            "Epoch [7198/20000], Training Loss: 0.0826\n",
            "Epoch [7199/20000], Training Loss: 0.0757\n",
            "Epoch [7200/20000], Training Loss: 0.0778\n",
            "Epoch [7201/20000], Training Loss: 0.0818\n",
            "Epoch [7202/20000], Training Loss: 0.0802\n",
            "Epoch [7203/20000], Training Loss: 0.0813\n",
            "Epoch [7204/20000], Training Loss: 0.0811\n",
            "Epoch [7205/20000], Training Loss: 0.0754\n",
            "Epoch [7206/20000], Training Loss: 0.0830\n",
            "Epoch [7207/20000], Training Loss: 0.0845\n",
            "Epoch [7208/20000], Training Loss: 0.0766\n",
            "Epoch [7209/20000], Training Loss: 0.0769\n",
            "Epoch [7210/20000], Training Loss: 0.0797\n",
            "Epoch [7211/20000], Training Loss: 0.0842\n",
            "Epoch [7212/20000], Training Loss: 0.0852\n",
            "Epoch [7213/20000], Training Loss: 0.0771\n",
            "Epoch [7214/20000], Training Loss: 0.0803\n",
            "Epoch [7215/20000], Training Loss: 0.0798\n",
            "Epoch [7216/20000], Training Loss: 0.0779\n",
            "Epoch [7217/20000], Training Loss: 0.0799\n",
            "Epoch [7218/20000], Training Loss: 0.0800\n",
            "Epoch [7219/20000], Training Loss: 0.0838\n",
            "Epoch [7220/20000], Training Loss: 0.0771\n",
            "Epoch [7221/20000], Training Loss: 0.0749\n",
            "Epoch [7222/20000], Training Loss: 0.0769\n",
            "Epoch [7223/20000], Training Loss: 0.0819\n",
            "Epoch [7224/20000], Training Loss: 0.0845\n",
            "Epoch [7225/20000], Training Loss: 0.0862\n",
            "Epoch [7226/20000], Training Loss: 0.0807\n",
            "Epoch [7227/20000], Training Loss: 0.0812\n",
            "Epoch [7228/20000], Training Loss: 0.0797\n",
            "Epoch [7229/20000], Training Loss: 0.0789\n",
            "Epoch [7230/20000], Training Loss: 0.0774\n",
            "Epoch [7231/20000], Training Loss: 0.0784\n",
            "Epoch [7232/20000], Training Loss: 0.0868\n",
            "Epoch [7233/20000], Training Loss: 0.0796\n",
            "Epoch [7234/20000], Training Loss: 0.0862\n",
            "Epoch [7235/20000], Training Loss: 0.0786\n",
            "Epoch [7236/20000], Training Loss: 0.0789\n",
            "Epoch [7237/20000], Training Loss: 0.0795\n",
            "Epoch [7238/20000], Training Loss: 0.0730\n",
            "Epoch [7239/20000], Training Loss: 0.0809\n",
            "Epoch [7240/20000], Training Loss: 0.0873\n",
            "Epoch [7241/20000], Training Loss: 0.0844\n",
            "Epoch [7242/20000], Training Loss: 0.0736\n",
            "Epoch [7243/20000], Training Loss: 0.0807\n",
            "Epoch [7244/20000], Training Loss: 0.0750\n",
            "Epoch [7245/20000], Training Loss: 0.0810\n",
            "Epoch [7246/20000], Training Loss: 0.0866\n",
            "Epoch [7247/20000], Training Loss: 0.0819\n",
            "Epoch [7248/20000], Training Loss: 0.0780\n",
            "Epoch [7249/20000], Training Loss: 0.0729\n",
            "Epoch [7250/20000], Training Loss: 0.0834\n",
            "Epoch [7251/20000], Training Loss: 0.0773\n",
            "Epoch [7252/20000], Training Loss: 0.0780\n",
            "Epoch [7253/20000], Training Loss: 0.0864\n",
            "Epoch [7254/20000], Training Loss: 0.0804\n",
            "Epoch [7255/20000], Training Loss: 0.0799\n",
            "Epoch [7256/20000], Training Loss: 0.0755\n",
            "Epoch [7257/20000], Training Loss: 0.0799\n",
            "Epoch [7258/20000], Training Loss: 0.0785\n",
            "Epoch [7259/20000], Training Loss: 0.0795\n",
            "Epoch [7260/20000], Training Loss: 0.0802\n",
            "Epoch [7261/20000], Training Loss: 0.0848\n",
            "Epoch [7262/20000], Training Loss: 0.0786\n",
            "Epoch [7263/20000], Training Loss: 0.0764\n",
            "Epoch [7264/20000], Training Loss: 0.0774\n",
            "Epoch [7265/20000], Training Loss: 0.0853\n",
            "Epoch [7266/20000], Training Loss: 0.0769\n",
            "Epoch [7267/20000], Training Loss: 0.0780\n",
            "Epoch [7268/20000], Training Loss: 0.0738\n",
            "Epoch [7269/20000], Training Loss: 0.0845\n",
            "Epoch [7270/20000], Training Loss: 0.0790\n",
            "Epoch [7271/20000], Training Loss: 0.0800\n",
            "Epoch [7272/20000], Training Loss: 0.0857\n",
            "Epoch [7273/20000], Training Loss: 0.0813\n",
            "Epoch [7274/20000], Training Loss: 0.0825\n",
            "Epoch [7275/20000], Training Loss: 0.0808\n",
            "Epoch [7276/20000], Training Loss: 0.0773\n",
            "Epoch [7277/20000], Training Loss: 0.0812\n",
            "Epoch [7278/20000], Training Loss: 0.0805\n",
            "Epoch [7279/20000], Training Loss: 0.0786\n",
            "Epoch [7280/20000], Training Loss: 0.0788\n",
            "Epoch [7281/20000], Training Loss: 0.0785\n",
            "Epoch [7282/20000], Training Loss: 0.0771\n",
            "Epoch [7283/20000], Training Loss: 0.0854\n",
            "Epoch [7284/20000], Training Loss: 0.0728\n",
            "Epoch [7285/20000], Training Loss: 0.0864\n",
            "Epoch [7286/20000], Training Loss: 0.0734\n",
            "Epoch [7287/20000], Training Loss: 0.0786\n",
            "Epoch [7288/20000], Training Loss: 0.0771\n",
            "Epoch [7289/20000], Training Loss: 0.0786\n",
            "Epoch [7290/20000], Training Loss: 0.0830\n",
            "Epoch [7291/20000], Training Loss: 0.0784\n",
            "Epoch [7292/20000], Training Loss: 0.0867\n",
            "Epoch [7293/20000], Training Loss: 0.0839\n",
            "Epoch [7294/20000], Training Loss: 0.0836\n",
            "Epoch [7295/20000], Training Loss: 0.0788\n",
            "Epoch [7296/20000], Training Loss: 0.0850\n",
            "Epoch [7297/20000], Training Loss: 0.0826\n",
            "Epoch [7298/20000], Training Loss: 0.0760\n",
            "Epoch [7299/20000], Training Loss: 0.0800\n",
            "Epoch [7300/20000], Training Loss: 0.0802\n",
            "Epoch [7301/20000], Training Loss: 0.0756\n",
            "Epoch [7302/20000], Training Loss: 0.0820\n",
            "Epoch [7303/20000], Training Loss: 0.0819\n",
            "Epoch [7304/20000], Training Loss: 0.0802\n",
            "Epoch [7305/20000], Training Loss: 0.0795\n",
            "Epoch [7306/20000], Training Loss: 0.0868\n",
            "Epoch [7307/20000], Training Loss: 0.0817\n",
            "Epoch [7308/20000], Training Loss: 0.0761\n",
            "Epoch [7309/20000], Training Loss: 0.0874\n",
            "Epoch [7310/20000], Training Loss: 0.0792\n",
            "Epoch [7311/20000], Training Loss: 0.0769\n",
            "Epoch [7312/20000], Training Loss: 0.0834\n",
            "Epoch [7313/20000], Training Loss: 0.0808\n",
            "Epoch [7314/20000], Training Loss: 0.0820\n",
            "Epoch [7315/20000], Training Loss: 0.0782\n",
            "Epoch [7316/20000], Training Loss: 0.0797\n",
            "Epoch [7317/20000], Training Loss: 0.0843\n",
            "Epoch [7318/20000], Training Loss: 0.0800\n",
            "Epoch [7319/20000], Training Loss: 0.0777\n",
            "Epoch [7320/20000], Training Loss: 0.0812\n",
            "Epoch [7321/20000], Training Loss: 0.0796\n",
            "Epoch [7322/20000], Training Loss: 0.0759\n",
            "Epoch [7323/20000], Training Loss: 0.0747\n",
            "Epoch [7324/20000], Training Loss: 0.0788\n",
            "Epoch [7325/20000], Training Loss: 0.0814\n",
            "Epoch [7326/20000], Training Loss: 0.0795\n",
            "Epoch [7327/20000], Training Loss: 0.0792\n",
            "Epoch [7328/20000], Training Loss: 0.0829\n",
            "Epoch [7329/20000], Training Loss: 0.0749\n",
            "Epoch [7330/20000], Training Loss: 0.0835\n",
            "Epoch [7331/20000], Training Loss: 0.0777\n",
            "Epoch [7332/20000], Training Loss: 0.0852\n",
            "Epoch [7333/20000], Training Loss: 0.0754\n",
            "Epoch [7334/20000], Training Loss: 0.0756\n",
            "Epoch [7335/20000], Training Loss: 0.0789\n",
            "Epoch [7336/20000], Training Loss: 0.0822\n",
            "Epoch [7337/20000], Training Loss: 0.0791\n",
            "Epoch [7338/20000], Training Loss: 0.0812\n",
            "Epoch [7339/20000], Training Loss: 0.0805\n",
            "Epoch [7340/20000], Training Loss: 0.0730\n",
            "Epoch [7341/20000], Training Loss: 0.0807\n",
            "Epoch [7342/20000], Training Loss: 0.0795\n",
            "Epoch [7343/20000], Training Loss: 0.0777\n",
            "Epoch [7344/20000], Training Loss: 0.0806\n",
            "Epoch [7345/20000], Training Loss: 0.0795\n",
            "Epoch [7346/20000], Training Loss: 0.0855\n",
            "Epoch [7347/20000], Training Loss: 0.0807\n",
            "Epoch [7348/20000], Training Loss: 0.0795\n",
            "Epoch [7349/20000], Training Loss: 0.0782\n",
            "Epoch [7350/20000], Training Loss: 0.0807\n",
            "Epoch [7351/20000], Training Loss: 0.0804\n",
            "Epoch [7352/20000], Training Loss: 0.0876\n",
            "Epoch [7353/20000], Training Loss: 0.0816\n",
            "Epoch [7354/20000], Training Loss: 0.0756\n",
            "Epoch [7355/20000], Training Loss: 0.0831\n",
            "Epoch [7356/20000], Training Loss: 0.0846\n",
            "Epoch [7357/20000], Training Loss: 0.0824\n",
            "Epoch [7358/20000], Training Loss: 0.0813\n",
            "Epoch [7359/20000], Training Loss: 0.0749\n",
            "Epoch [7360/20000], Training Loss: 0.0853\n",
            "Epoch [7361/20000], Training Loss: 0.0793\n",
            "Epoch [7362/20000], Training Loss: 0.0796\n",
            "Epoch [7363/20000], Training Loss: 0.0850\n",
            "Epoch [7364/20000], Training Loss: 0.0758\n",
            "Epoch [7365/20000], Training Loss: 0.0767\n",
            "Epoch [7366/20000], Training Loss: 0.0761\n",
            "Epoch [7367/20000], Training Loss: 0.0820\n",
            "Epoch [7368/20000], Training Loss: 0.0810\n",
            "Epoch [7369/20000], Training Loss: 0.0754\n",
            "Epoch [7370/20000], Training Loss: 0.0818\n",
            "Epoch [7371/20000], Training Loss: 0.0766\n",
            "Epoch [7372/20000], Training Loss: 0.0826\n",
            "Epoch [7373/20000], Training Loss: 0.0813\n",
            "Epoch [7374/20000], Training Loss: 0.0868\n",
            "Epoch [7375/20000], Training Loss: 0.0841\n",
            "Epoch [7376/20000], Training Loss: 0.0816\n",
            "Epoch [7377/20000], Training Loss: 0.0793\n",
            "Epoch [7378/20000], Training Loss: 0.0780\n",
            "Epoch [7379/20000], Training Loss: 0.0849\n",
            "Epoch [7380/20000], Training Loss: 0.0793\n",
            "Epoch [7381/20000], Training Loss: 0.0756\n",
            "Epoch [7382/20000], Training Loss: 0.0816\n",
            "Epoch [7383/20000], Training Loss: 0.0825\n",
            "Epoch [7384/20000], Training Loss: 0.0812\n",
            "Epoch [7385/20000], Training Loss: 0.0730\n",
            "Epoch [7386/20000], Training Loss: 0.0799\n",
            "Epoch [7387/20000], Training Loss: 0.0739\n",
            "Epoch [7388/20000], Training Loss: 0.0761\n",
            "Epoch [7389/20000], Training Loss: 0.0808\n",
            "Epoch [7390/20000], Training Loss: 0.0774\n",
            "Epoch [7391/20000], Training Loss: 0.0730\n",
            "Epoch [7392/20000], Training Loss: 0.0832\n",
            "Epoch [7393/20000], Training Loss: 0.0819\n",
            "Epoch [7394/20000], Training Loss: 0.0783\n",
            "Epoch [7395/20000], Training Loss: 0.0811\n",
            "Epoch [7396/20000], Training Loss: 0.0813\n",
            "Epoch [7397/20000], Training Loss: 0.0794\n",
            "Epoch [7398/20000], Training Loss: 0.0855\n",
            "Epoch [7399/20000], Training Loss: 0.0731\n",
            "Epoch [7400/20000], Training Loss: 0.0827\n",
            "Epoch [7401/20000], Training Loss: 0.0801\n",
            "Epoch [7402/20000], Training Loss: 0.0781\n",
            "Epoch [7403/20000], Training Loss: 0.0858\n",
            "Epoch [7404/20000], Training Loss: 0.0793\n",
            "Epoch [7405/20000], Training Loss: 0.0765\n",
            "Epoch [7406/20000], Training Loss: 0.0787\n",
            "Epoch [7407/20000], Training Loss: 0.0768\n",
            "Epoch [7408/20000], Training Loss: 0.0788\n",
            "Epoch [7409/20000], Training Loss: 0.0752\n",
            "Epoch [7410/20000], Training Loss: 0.0800\n",
            "Epoch [7411/20000], Training Loss: 0.0838\n",
            "Epoch [7412/20000], Training Loss: 0.0837\n",
            "Epoch [7413/20000], Training Loss: 0.0726\n",
            "Epoch [7414/20000], Training Loss: 0.0770\n",
            "Epoch [7415/20000], Training Loss: 0.0783\n",
            "Epoch [7416/20000], Training Loss: 0.0832\n",
            "Epoch [7417/20000], Training Loss: 0.0739\n",
            "Epoch [7418/20000], Training Loss: 0.0761\n",
            "Epoch [7419/20000], Training Loss: 0.0797\n",
            "Epoch [7420/20000], Training Loss: 0.0738\n",
            "Epoch [7421/20000], Training Loss: 0.0841\n",
            "Epoch [7422/20000], Training Loss: 0.0795\n",
            "Epoch [7423/20000], Training Loss: 0.0786\n",
            "Epoch [7424/20000], Training Loss: 0.0854\n",
            "Epoch [7425/20000], Training Loss: 0.0798\n",
            "Epoch [7426/20000], Training Loss: 0.0807\n",
            "Epoch [7427/20000], Training Loss: 0.0819\n",
            "Epoch [7428/20000], Training Loss: 0.0782\n",
            "Epoch [7429/20000], Training Loss: 0.0849\n",
            "Epoch [7430/20000], Training Loss: 0.0790\n",
            "Epoch [7431/20000], Training Loss: 0.0742\n",
            "Epoch [7432/20000], Training Loss: 0.0829\n",
            "Epoch [7433/20000], Training Loss: 0.0835\n",
            "Epoch [7434/20000], Training Loss: 0.0789\n",
            "Epoch [7435/20000], Training Loss: 0.0813\n",
            "Epoch [7436/20000], Training Loss: 0.0760\n",
            "Epoch [7437/20000], Training Loss: 0.0802\n",
            "Epoch [7438/20000], Training Loss: 0.0835\n",
            "Epoch [7439/20000], Training Loss: 0.0774\n",
            "Epoch [7440/20000], Training Loss: 0.0777\n",
            "Epoch [7441/20000], Training Loss: 0.0813\n",
            "Epoch [7442/20000], Training Loss: 0.0830\n",
            "Epoch [7443/20000], Training Loss: 0.0778\n",
            "Epoch [7444/20000], Training Loss: 0.0764\n",
            "Epoch [7445/20000], Training Loss: 0.0803\n",
            "Epoch [7446/20000], Training Loss: 0.0770\n",
            "Epoch [7447/20000], Training Loss: 0.0739\n",
            "Epoch [7448/20000], Training Loss: 0.0844\n",
            "Epoch [7449/20000], Training Loss: 0.0789\n",
            "Epoch [7450/20000], Training Loss: 0.0868\n",
            "Epoch [7451/20000], Training Loss: 0.0780\n",
            "Epoch [7452/20000], Training Loss: 0.0745\n",
            "Epoch [7453/20000], Training Loss: 0.0794\n",
            "Epoch [7454/20000], Training Loss: 0.0830\n",
            "Epoch [7455/20000], Training Loss: 0.0751\n",
            "Epoch [7456/20000], Training Loss: 0.0805\n",
            "Epoch [7457/20000], Training Loss: 0.0742\n",
            "Epoch [7458/20000], Training Loss: 0.0767\n",
            "Epoch [7459/20000], Training Loss: 0.0779\n",
            "Epoch [7460/20000], Training Loss: 0.0766\n",
            "Epoch [7461/20000], Training Loss: 0.0829\n",
            "Epoch [7462/20000], Training Loss: 0.0781\n",
            "Epoch [7463/20000], Training Loss: 0.0743\n",
            "Epoch [7464/20000], Training Loss: 0.0826\n",
            "Epoch [7465/20000], Training Loss: 0.0799\n",
            "Epoch [7466/20000], Training Loss: 0.0741\n",
            "Epoch [7467/20000], Training Loss: 0.0788\n",
            "Epoch [7468/20000], Training Loss: 0.0805\n",
            "Epoch [7469/20000], Training Loss: 0.0765\n",
            "Epoch [7470/20000], Training Loss: 0.0837\n",
            "Epoch [7471/20000], Training Loss: 0.0824\n",
            "Epoch [7472/20000], Training Loss: 0.0829\n",
            "Epoch [7473/20000], Training Loss: 0.0790\n",
            "Epoch [7474/20000], Training Loss: 0.0766\n",
            "Epoch [7475/20000], Training Loss: 0.0774\n",
            "Epoch [7476/20000], Training Loss: 0.0813\n",
            "Epoch [7477/20000], Training Loss: 0.0820\n",
            "Epoch [7478/20000], Training Loss: 0.0756\n",
            "Epoch [7479/20000], Training Loss: 0.0808\n",
            "Epoch [7480/20000], Training Loss: 0.0866\n",
            "Epoch [7481/20000], Training Loss: 0.0850\n",
            "Epoch [7482/20000], Training Loss: 0.0758\n",
            "Epoch [7483/20000], Training Loss: 0.0786\n",
            "Epoch [7484/20000], Training Loss: 0.0843\n",
            "Epoch [7485/20000], Training Loss: 0.0840\n",
            "Epoch [7486/20000], Training Loss: 0.0805\n",
            "Epoch [7487/20000], Training Loss: 0.0833\n",
            "Epoch [7488/20000], Training Loss: 0.0756\n",
            "Epoch [7489/20000], Training Loss: 0.0752\n",
            "Epoch [7490/20000], Training Loss: 0.0730\n",
            "Epoch [7491/20000], Training Loss: 0.0757\n",
            "Epoch [7492/20000], Training Loss: 0.0785\n",
            "Epoch [7493/20000], Training Loss: 0.0843\n",
            "Epoch [7494/20000], Training Loss: 0.0778\n",
            "Epoch [7495/20000], Training Loss: 0.0779\n",
            "Epoch [7496/20000], Training Loss: 0.0811\n",
            "Epoch [7497/20000], Training Loss: 0.0729\n",
            "Epoch [7498/20000], Training Loss: 0.0768\n",
            "Epoch [7499/20000], Training Loss: 0.0799\n",
            "Epoch [7500/20000], Training Loss: 0.0840\n",
            "Epoch [7501/20000], Training Loss: 0.0830\n",
            "Epoch [7502/20000], Training Loss: 0.0802\n",
            "Epoch [7503/20000], Training Loss: 0.0861\n",
            "Epoch [7504/20000], Training Loss: 0.0809\n",
            "Epoch [7505/20000], Training Loss: 0.0796\n",
            "Epoch [7506/20000], Training Loss: 0.0781\n",
            "Epoch [7507/20000], Training Loss: 0.0849\n",
            "Epoch [7508/20000], Training Loss: 0.0794\n",
            "Epoch [7509/20000], Training Loss: 0.0791\n",
            "Epoch [7510/20000], Training Loss: 0.0805\n",
            "Epoch [7511/20000], Training Loss: 0.0759\n",
            "Epoch [7512/20000], Training Loss: 0.0795\n",
            "Epoch [7513/20000], Training Loss: 0.0806\n",
            "Epoch [7514/20000], Training Loss: 0.0809\n",
            "Epoch [7515/20000], Training Loss: 0.0788\n",
            "Epoch [7516/20000], Training Loss: 0.0812\n",
            "Epoch [7517/20000], Training Loss: 0.0821\n",
            "Epoch [7518/20000], Training Loss: 0.0841\n",
            "Epoch [7519/20000], Training Loss: 0.0821\n",
            "Epoch [7520/20000], Training Loss: 0.0796\n",
            "Epoch [7521/20000], Training Loss: 0.0730\n",
            "Epoch [7522/20000], Training Loss: 0.0760\n",
            "Epoch [7523/20000], Training Loss: 0.0762\n",
            "Epoch [7524/20000], Training Loss: 0.0779\n",
            "Epoch [7525/20000], Training Loss: 0.0809\n",
            "Epoch [7526/20000], Training Loss: 0.0772\n",
            "Epoch [7527/20000], Training Loss: 0.0808\n",
            "Epoch [7528/20000], Training Loss: 0.0839\n",
            "Epoch [7529/20000], Training Loss: 0.0795\n",
            "Epoch [7530/20000], Training Loss: 0.0764\n",
            "Epoch [7531/20000], Training Loss: 0.0813\n",
            "Epoch [7532/20000], Training Loss: 0.0790\n",
            "Epoch [7533/20000], Training Loss: 0.0833\n",
            "Epoch [7534/20000], Training Loss: 0.0871\n",
            "Epoch [7535/20000], Training Loss: 0.0765\n",
            "Epoch [7536/20000], Training Loss: 0.0814\n",
            "Epoch [7537/20000], Training Loss: 0.0791\n",
            "Epoch [7538/20000], Training Loss: 0.0847\n",
            "Epoch [7539/20000], Training Loss: 0.0750\n",
            "Epoch [7540/20000], Training Loss: 0.0865\n",
            "Epoch [7541/20000], Training Loss: 0.0795\n",
            "Epoch [7542/20000], Training Loss: 0.0791\n",
            "Epoch [7543/20000], Training Loss: 0.0760\n",
            "Epoch [7544/20000], Training Loss: 0.0788\n",
            "Epoch [7545/20000], Training Loss: 0.0812\n",
            "Epoch [7546/20000], Training Loss: 0.0778\n",
            "Epoch [7547/20000], Training Loss: 0.0814\n",
            "Epoch [7548/20000], Training Loss: 0.0794\n",
            "Epoch [7549/20000], Training Loss: 0.0793\n",
            "Epoch [7550/20000], Training Loss: 0.0821\n",
            "Epoch [7551/20000], Training Loss: 0.0766\n",
            "Epoch [7552/20000], Training Loss: 0.0737\n",
            "Epoch [7553/20000], Training Loss: 0.0814\n",
            "Epoch [7554/20000], Training Loss: 0.0814\n",
            "Epoch [7555/20000], Training Loss: 0.0815\n",
            "Epoch [7556/20000], Training Loss: 0.0762\n",
            "Epoch [7557/20000], Training Loss: 0.0803\n",
            "Epoch [7558/20000], Training Loss: 0.0792\n",
            "Epoch [7559/20000], Training Loss: 0.0767\n",
            "Epoch [7560/20000], Training Loss: 0.0796\n",
            "Epoch [7561/20000], Training Loss: 0.0784\n",
            "Epoch [7562/20000], Training Loss: 0.0751\n",
            "Epoch [7563/20000], Training Loss: 0.0859\n",
            "Epoch [7564/20000], Training Loss: 0.0811\n",
            "Epoch [7565/20000], Training Loss: 0.0803\n",
            "Epoch [7566/20000], Training Loss: 0.0802\n",
            "Epoch [7567/20000], Training Loss: 0.0779\n",
            "Epoch [7568/20000], Training Loss: 0.0744\n",
            "Epoch [7569/20000], Training Loss: 0.0794\n",
            "Epoch [7570/20000], Training Loss: 0.0775\n",
            "Epoch [7571/20000], Training Loss: 0.0795\n",
            "Epoch [7572/20000], Training Loss: 0.0783\n",
            "Epoch [7573/20000], Training Loss: 0.0790\n",
            "Epoch [7574/20000], Training Loss: 0.0739\n",
            "Epoch [7575/20000], Training Loss: 0.0776\n",
            "Epoch [7576/20000], Training Loss: 0.0794\n",
            "Epoch [7577/20000], Training Loss: 0.0819\n",
            "Epoch [7578/20000], Training Loss: 0.0819\n",
            "Epoch [7579/20000], Training Loss: 0.0793\n",
            "Epoch [7580/20000], Training Loss: 0.0776\n",
            "Epoch [7581/20000], Training Loss: 0.0789\n",
            "Epoch [7582/20000], Training Loss: 0.0802\n",
            "Epoch [7583/20000], Training Loss: 0.0820\n",
            "Epoch [7584/20000], Training Loss: 0.0749\n",
            "Epoch [7585/20000], Training Loss: 0.0803\n",
            "Epoch [7586/20000], Training Loss: 0.0775\n",
            "Epoch [7587/20000], Training Loss: 0.0781\n",
            "Epoch [7588/20000], Training Loss: 0.0800\n",
            "Epoch [7589/20000], Training Loss: 0.0737\n",
            "Epoch [7590/20000], Training Loss: 0.0735\n",
            "Epoch [7591/20000], Training Loss: 0.0853\n",
            "Epoch [7592/20000], Training Loss: 0.0769\n",
            "Epoch [7593/20000], Training Loss: 0.0813\n",
            "Epoch [7594/20000], Training Loss: 0.0784\n",
            "Epoch [7595/20000], Training Loss: 0.0760\n",
            "Epoch [7596/20000], Training Loss: 0.0730\n",
            "Epoch [7597/20000], Training Loss: 0.0861\n",
            "Epoch [7598/20000], Training Loss: 0.0815\n",
            "Epoch [7599/20000], Training Loss: 0.0849\n",
            "Epoch [7600/20000], Training Loss: 0.0827\n",
            "Epoch [7601/20000], Training Loss: 0.0748\n",
            "Epoch [7602/20000], Training Loss: 0.0792\n",
            "Epoch [7603/20000], Training Loss: 0.0813\n",
            "Epoch [7604/20000], Training Loss: 0.0820\n",
            "Epoch [7605/20000], Training Loss: 0.0779\n",
            "Epoch [7606/20000], Training Loss: 0.0803\n",
            "Epoch [7607/20000], Training Loss: 0.0755\n",
            "Epoch [7608/20000], Training Loss: 0.0801\n",
            "Epoch [7609/20000], Training Loss: 0.0778\n",
            "Epoch [7610/20000], Training Loss: 0.0808\n",
            "Epoch [7611/20000], Training Loss: 0.0843\n",
            "Epoch [7612/20000], Training Loss: 0.0819\n",
            "Epoch [7613/20000], Training Loss: 0.0810\n",
            "Epoch [7614/20000], Training Loss: 0.0811\n",
            "Epoch [7615/20000], Training Loss: 0.0821\n",
            "Epoch [7616/20000], Training Loss: 0.0841\n",
            "Epoch [7617/20000], Training Loss: 0.0756\n",
            "Epoch [7618/20000], Training Loss: 0.0810\n",
            "Epoch [7619/20000], Training Loss: 0.0751\n",
            "Epoch [7620/20000], Training Loss: 0.0808\n",
            "Epoch [7621/20000], Training Loss: 0.0754\n",
            "Epoch [7622/20000], Training Loss: 0.0784\n",
            "Epoch [7623/20000], Training Loss: 0.0803\n",
            "Epoch [7624/20000], Training Loss: 0.0795\n",
            "Epoch [7625/20000], Training Loss: 0.0744\n",
            "Epoch [7626/20000], Training Loss: 0.0765\n",
            "Epoch [7627/20000], Training Loss: 0.0789\n",
            "Epoch [7628/20000], Training Loss: 0.0804\n",
            "Epoch [7629/20000], Training Loss: 0.0817\n",
            "Epoch [7630/20000], Training Loss: 0.0796\n",
            "Epoch [7631/20000], Training Loss: 0.0837\n",
            "Epoch [7632/20000], Training Loss: 0.0785\n",
            "Epoch [7633/20000], Training Loss: 0.0843\n",
            "Epoch [7634/20000], Training Loss: 0.0770\n",
            "Epoch [7635/20000], Training Loss: 0.0790\n",
            "Epoch [7636/20000], Training Loss: 0.0748\n",
            "Epoch [7637/20000], Training Loss: 0.0830\n",
            "Epoch [7638/20000], Training Loss: 0.0874\n",
            "Epoch [7639/20000], Training Loss: 0.0759\n",
            "Epoch [7640/20000], Training Loss: 0.0752\n",
            "Epoch [7641/20000], Training Loss: 0.0761\n",
            "Epoch [7642/20000], Training Loss: 0.0836\n",
            "Epoch [7643/20000], Training Loss: 0.0859\n",
            "Epoch [7644/20000], Training Loss: 0.0821\n",
            "Epoch [7645/20000], Training Loss: 0.0774\n",
            "Epoch [7646/20000], Training Loss: 0.0809\n",
            "Epoch [7647/20000], Training Loss: 0.0821\n",
            "Epoch [7648/20000], Training Loss: 0.0768\n",
            "Epoch [7649/20000], Training Loss: 0.0788\n",
            "Epoch [7650/20000], Training Loss: 0.0787\n",
            "Epoch [7651/20000], Training Loss: 0.0848\n",
            "Epoch [7652/20000], Training Loss: 0.0848\n",
            "Epoch [7653/20000], Training Loss: 0.0766\n",
            "Epoch [7654/20000], Training Loss: 0.0744\n",
            "Epoch [7655/20000], Training Loss: 0.0833\n",
            "Epoch [7656/20000], Training Loss: 0.0784\n",
            "Epoch [7657/20000], Training Loss: 0.0739\n",
            "Epoch [7658/20000], Training Loss: 0.0786\n",
            "Epoch [7659/20000], Training Loss: 0.0803\n",
            "Epoch [7660/20000], Training Loss: 0.0760\n",
            "Epoch [7661/20000], Training Loss: 0.0750\n",
            "Epoch [7662/20000], Training Loss: 0.0776\n",
            "Epoch [7663/20000], Training Loss: 0.0803\n",
            "Epoch [7664/20000], Training Loss: 0.0807\n",
            "Epoch [7665/20000], Training Loss: 0.0770\n",
            "Epoch [7666/20000], Training Loss: 0.0846\n",
            "Epoch [7667/20000], Training Loss: 0.0775\n",
            "Epoch [7668/20000], Training Loss: 0.0770\n",
            "Epoch [7669/20000], Training Loss: 0.0808\n",
            "Epoch [7670/20000], Training Loss: 0.0847\n",
            "Epoch [7671/20000], Training Loss: 0.0801\n",
            "Epoch [7672/20000], Training Loss: 0.0771\n",
            "Epoch [7673/20000], Training Loss: 0.0737\n",
            "Epoch [7674/20000], Training Loss: 0.0744\n",
            "Epoch [7675/20000], Training Loss: 0.0818\n",
            "Epoch [7676/20000], Training Loss: 0.0792\n",
            "Epoch [7677/20000], Training Loss: 0.0836\n",
            "Epoch [7678/20000], Training Loss: 0.0809\n",
            "Epoch [7679/20000], Training Loss: 0.0844\n",
            "Epoch [7680/20000], Training Loss: 0.0800\n",
            "Epoch [7681/20000], Training Loss: 0.0851\n",
            "Epoch [7682/20000], Training Loss: 0.0767\n",
            "Epoch [7683/20000], Training Loss: 0.0815\n",
            "Epoch [7684/20000], Training Loss: 0.0861\n",
            "Epoch [7685/20000], Training Loss: 0.0807\n",
            "Epoch [7686/20000], Training Loss: 0.0746\n",
            "Epoch [7687/20000], Training Loss: 0.0763\n",
            "Epoch [7688/20000], Training Loss: 0.0765\n",
            "Epoch [7689/20000], Training Loss: 0.0799\n",
            "Epoch [7690/20000], Training Loss: 0.0738\n",
            "Epoch [7691/20000], Training Loss: 0.0842\n",
            "Epoch [7692/20000], Training Loss: 0.0830\n",
            "Epoch [7693/20000], Training Loss: 0.0803\n",
            "Epoch [7694/20000], Training Loss: 0.0823\n",
            "Epoch [7695/20000], Training Loss: 0.0828\n",
            "Epoch [7696/20000], Training Loss: 0.0792\n",
            "Epoch [7697/20000], Training Loss: 0.0841\n",
            "Epoch [7698/20000], Training Loss: 0.0823\n",
            "Epoch [7699/20000], Training Loss: 0.0807\n",
            "Epoch [7700/20000], Training Loss: 0.0856\n",
            "Epoch [7701/20000], Training Loss: 0.0799\n",
            "Epoch [7702/20000], Training Loss: 0.0737\n",
            "Epoch [7703/20000], Training Loss: 0.0758\n",
            "Epoch [7704/20000], Training Loss: 0.0804\n",
            "Epoch [7705/20000], Training Loss: 0.0883\n",
            "Epoch [7706/20000], Training Loss: 0.0788\n",
            "Epoch [7707/20000], Training Loss: 0.0816\n",
            "Epoch [7708/20000], Training Loss: 0.0788\n",
            "Epoch [7709/20000], Training Loss: 0.0743\n",
            "Epoch [7710/20000], Training Loss: 0.0852\n",
            "Epoch [7711/20000], Training Loss: 0.0782\n",
            "Epoch [7712/20000], Training Loss: 0.0836\n",
            "Epoch [7713/20000], Training Loss: 0.0776\n",
            "Epoch [7714/20000], Training Loss: 0.0779\n",
            "Epoch [7715/20000], Training Loss: 0.0740\n",
            "Epoch [7716/20000], Training Loss: 0.0774\n",
            "Epoch [7717/20000], Training Loss: 0.0874\n",
            "Epoch [7718/20000], Training Loss: 0.0764\n",
            "Epoch [7719/20000], Training Loss: 0.0821\n",
            "Epoch [7720/20000], Training Loss: 0.0841\n",
            "Epoch [7721/20000], Training Loss: 0.0834\n",
            "Epoch [7722/20000], Training Loss: 0.0817\n",
            "Epoch [7723/20000], Training Loss: 0.0775\n",
            "Epoch [7724/20000], Training Loss: 0.0752\n",
            "Epoch [7725/20000], Training Loss: 0.0766\n",
            "Epoch [7726/20000], Training Loss: 0.0726\n",
            "Epoch [7727/20000], Training Loss: 0.0807\n",
            "Epoch [7728/20000], Training Loss: 0.0755\n",
            "Epoch [7729/20000], Training Loss: 0.0836\n",
            "Epoch [7730/20000], Training Loss: 0.0734\n",
            "Epoch [7731/20000], Training Loss: 0.0738\n",
            "Epoch [7732/20000], Training Loss: 0.0806\n",
            "Epoch [7733/20000], Training Loss: 0.0782\n",
            "Epoch [7734/20000], Training Loss: 0.0816\n",
            "Epoch [7735/20000], Training Loss: 0.0751\n",
            "Epoch [7736/20000], Training Loss: 0.0746\n",
            "Epoch [7737/20000], Training Loss: 0.0807\n",
            "Epoch [7738/20000], Training Loss: 0.0771\n",
            "Epoch [7739/20000], Training Loss: 0.0826\n",
            "Epoch [7740/20000], Training Loss: 0.0787\n",
            "Epoch [7741/20000], Training Loss: 0.0813\n",
            "Epoch [7742/20000], Training Loss: 0.0811\n",
            "Epoch [7743/20000], Training Loss: 0.0761\n",
            "Epoch [7744/20000], Training Loss: 0.0809\n",
            "Epoch [7745/20000], Training Loss: 0.0798\n",
            "Epoch [7746/20000], Training Loss: 0.0810\n",
            "Epoch [7747/20000], Training Loss: 0.0751\n",
            "Epoch [7748/20000], Training Loss: 0.0756\n",
            "Epoch [7749/20000], Training Loss: 0.0819\n",
            "Epoch [7750/20000], Training Loss: 0.0815\n",
            "Epoch [7751/20000], Training Loss: 0.0803\n",
            "Epoch [7752/20000], Training Loss: 0.0841\n",
            "Epoch [7753/20000], Training Loss: 0.0773\n",
            "Epoch [7754/20000], Training Loss: 0.0747\n",
            "Epoch [7755/20000], Training Loss: 0.0760\n",
            "Epoch [7756/20000], Training Loss: 0.0806\n",
            "Epoch [7757/20000], Training Loss: 0.0800\n",
            "Epoch [7758/20000], Training Loss: 0.0791\n",
            "Epoch [7759/20000], Training Loss: 0.0762\n",
            "Epoch [7760/20000], Training Loss: 0.0851\n",
            "Epoch [7761/20000], Training Loss: 0.0836\n",
            "Epoch [7762/20000], Training Loss: 0.0821\n",
            "Epoch [7763/20000], Training Loss: 0.0764\n",
            "Epoch [7764/20000], Training Loss: 0.0847\n",
            "Epoch [7765/20000], Training Loss: 0.0801\n",
            "Epoch [7766/20000], Training Loss: 0.0818\n",
            "Epoch [7767/20000], Training Loss: 0.0766\n",
            "Epoch [7768/20000], Training Loss: 0.0824\n",
            "Epoch [7769/20000], Training Loss: 0.0825\n",
            "Epoch [7770/20000], Training Loss: 0.0733\n",
            "Epoch [7771/20000], Training Loss: 0.0771\n",
            "Epoch [7772/20000], Training Loss: 0.0898\n",
            "Epoch [7773/20000], Training Loss: 0.0786\n",
            "Epoch [7774/20000], Training Loss: 0.0738\n",
            "Epoch [7775/20000], Training Loss: 0.0779\n",
            "Epoch [7776/20000], Training Loss: 0.0773\n",
            "Epoch [7777/20000], Training Loss: 0.0802\n",
            "Epoch [7778/20000], Training Loss: 0.0795\n",
            "Epoch [7779/20000], Training Loss: 0.0739\n",
            "Epoch [7780/20000], Training Loss: 0.0788\n",
            "Epoch [7781/20000], Training Loss: 0.0789\n",
            "Epoch [7782/20000], Training Loss: 0.0808\n",
            "Epoch [7783/20000], Training Loss: 0.0811\n",
            "Epoch [7784/20000], Training Loss: 0.0756\n",
            "Epoch [7785/20000], Training Loss: 0.0739\n",
            "Epoch [7786/20000], Training Loss: 0.0763\n",
            "Epoch [7787/20000], Training Loss: 0.0862\n",
            "Epoch [7788/20000], Training Loss: 0.0735\n",
            "Epoch [7789/20000], Training Loss: 0.0767\n",
            "Epoch [7790/20000], Training Loss: 0.0811\n",
            "Epoch [7791/20000], Training Loss: 0.0771\n",
            "Epoch [7792/20000], Training Loss: 0.0796\n",
            "Epoch [7793/20000], Training Loss: 0.0781\n",
            "Epoch [7794/20000], Training Loss: 0.0849\n",
            "Epoch [7795/20000], Training Loss: 0.0754\n",
            "Epoch [7796/20000], Training Loss: 0.0743\n",
            "Epoch [7797/20000], Training Loss: 0.0841\n",
            "Epoch [7798/20000], Training Loss: 0.0790\n",
            "Epoch [7799/20000], Training Loss: 0.0786\n",
            "Epoch [7800/20000], Training Loss: 0.0823\n",
            "Epoch [7801/20000], Training Loss: 0.0769\n",
            "Epoch [7802/20000], Training Loss: 0.0812\n",
            "Epoch [7803/20000], Training Loss: 0.0773\n",
            "Epoch [7804/20000], Training Loss: 0.0744\n",
            "Epoch [7805/20000], Training Loss: 0.0732\n",
            "Epoch [7806/20000], Training Loss: 0.0875\n",
            "Epoch [7807/20000], Training Loss: 0.0771\n",
            "Epoch [7808/20000], Training Loss: 0.0808\n",
            "Epoch [7809/20000], Training Loss: 0.0817\n",
            "Epoch [7810/20000], Training Loss: 0.0794\n",
            "Epoch [7811/20000], Training Loss: 0.0753\n",
            "Epoch [7812/20000], Training Loss: 0.0808\n",
            "Epoch [7813/20000], Training Loss: 0.0768\n",
            "Epoch [7814/20000], Training Loss: 0.0758\n",
            "Epoch [7815/20000], Training Loss: 0.0750\n",
            "Epoch [7816/20000], Training Loss: 0.0785\n",
            "Epoch [7817/20000], Training Loss: 0.0750\n",
            "Epoch [7818/20000], Training Loss: 0.0764\n",
            "Epoch [7819/20000], Training Loss: 0.0838\n",
            "Epoch [7820/20000], Training Loss: 0.0744\n",
            "Epoch [7821/20000], Training Loss: 0.0790\n",
            "Epoch [7822/20000], Training Loss: 0.0739\n",
            "Epoch [7823/20000], Training Loss: 0.0764\n",
            "Epoch [7824/20000], Training Loss: 0.0868\n",
            "Epoch [7825/20000], Training Loss: 0.0819\n",
            "Epoch [7826/20000], Training Loss: 0.0772\n",
            "Epoch [7827/20000], Training Loss: 0.0840\n",
            "Epoch [7828/20000], Training Loss: 0.0798\n",
            "Epoch [7829/20000], Training Loss: 0.0827\n",
            "Epoch [7830/20000], Training Loss: 0.0814\n",
            "Epoch [7831/20000], Training Loss: 0.0817\n",
            "Epoch [7832/20000], Training Loss: 0.0820\n",
            "Epoch [7833/20000], Training Loss: 0.0790\n",
            "Epoch [7834/20000], Training Loss: 0.0810\n",
            "Epoch [7835/20000], Training Loss: 0.0796\n",
            "Epoch [7836/20000], Training Loss: 0.0757\n",
            "Epoch [7837/20000], Training Loss: 0.0820\n",
            "Epoch [7838/20000], Training Loss: 0.0834\n",
            "Epoch [7839/20000], Training Loss: 0.0819\n",
            "Epoch [7840/20000], Training Loss: 0.0794\n",
            "Epoch [7841/20000], Training Loss: 0.0750\n",
            "Epoch [7842/20000], Training Loss: 0.0769\n",
            "Epoch [7843/20000], Training Loss: 0.0806\n",
            "Epoch [7844/20000], Training Loss: 0.0868\n",
            "Epoch [7845/20000], Training Loss: 0.0875\n",
            "Epoch [7846/20000], Training Loss: 0.0799\n",
            "Epoch [7847/20000], Training Loss: 0.0748\n",
            "Epoch [7848/20000], Training Loss: 0.0764\n",
            "Epoch [7849/20000], Training Loss: 0.0809\n",
            "Epoch [7850/20000], Training Loss: 0.0829\n",
            "Epoch [7851/20000], Training Loss: 0.0802\n",
            "Epoch [7852/20000], Training Loss: 0.0745\n",
            "Epoch [7853/20000], Training Loss: 0.0812\n",
            "Epoch [7854/20000], Training Loss: 0.0815\n",
            "Epoch [7855/20000], Training Loss: 0.0827\n",
            "Epoch [7856/20000], Training Loss: 0.0744\n",
            "Epoch [7857/20000], Training Loss: 0.0756\n",
            "Epoch [7858/20000], Training Loss: 0.0750\n",
            "Epoch [7859/20000], Training Loss: 0.0772\n",
            "Epoch [7860/20000], Training Loss: 0.0807\n",
            "Epoch [7861/20000], Training Loss: 0.0841\n",
            "Epoch [7862/20000], Training Loss: 0.0781\n",
            "Epoch [7863/20000], Training Loss: 0.0752\n",
            "Epoch [7864/20000], Training Loss: 0.0793\n",
            "Epoch [7865/20000], Training Loss: 0.0794\n",
            "Epoch [7866/20000], Training Loss: 0.0798\n",
            "Epoch [7867/20000], Training Loss: 0.0754\n",
            "Epoch [7868/20000], Training Loss: 0.0768\n",
            "Epoch [7869/20000], Training Loss: 0.0860\n",
            "Epoch [7870/20000], Training Loss: 0.0803\n",
            "Epoch [7871/20000], Training Loss: 0.0809\n",
            "Epoch [7872/20000], Training Loss: 0.0798\n",
            "Epoch [7873/20000], Training Loss: 0.0842\n",
            "Epoch [7874/20000], Training Loss: 0.0810\n",
            "Epoch [7875/20000], Training Loss: 0.0816\n",
            "Epoch [7876/20000], Training Loss: 0.0850\n",
            "Epoch [7877/20000], Training Loss: 0.0740\n",
            "Epoch [7878/20000], Training Loss: 0.0790\n",
            "Epoch [7879/20000], Training Loss: 0.0866\n",
            "Epoch [7880/20000], Training Loss: 0.0774\n",
            "Epoch [7881/20000], Training Loss: 0.0780\n",
            "Epoch [7882/20000], Training Loss: 0.0834\n",
            "Epoch [7883/20000], Training Loss: 0.0810\n",
            "Epoch [7884/20000], Training Loss: 0.0752\n",
            "Epoch [7885/20000], Training Loss: 0.0826\n",
            "Epoch [7886/20000], Training Loss: 0.0826\n",
            "Epoch [7887/20000], Training Loss: 0.0797\n",
            "Epoch [7888/20000], Training Loss: 0.0759\n",
            "Epoch [7889/20000], Training Loss: 0.0842\n",
            "Epoch [7890/20000], Training Loss: 0.0795\n",
            "Epoch [7891/20000], Training Loss: 0.0803\n",
            "Epoch [7892/20000], Training Loss: 0.0850\n",
            "Epoch [7893/20000], Training Loss: 0.0786\n",
            "Epoch [7894/20000], Training Loss: 0.0809\n",
            "Epoch [7895/20000], Training Loss: 0.0745\n",
            "Epoch [7896/20000], Training Loss: 0.0757\n",
            "Epoch [7897/20000], Training Loss: 0.0767\n",
            "Epoch [7898/20000], Training Loss: 0.0783\n",
            "Epoch [7899/20000], Training Loss: 0.0788\n",
            "Epoch [7900/20000], Training Loss: 0.0748\n",
            "Epoch [7901/20000], Training Loss: 0.0813\n",
            "Epoch [7902/20000], Training Loss: 0.0810\n",
            "Epoch [7903/20000], Training Loss: 0.0777\n",
            "Epoch [7904/20000], Training Loss: 0.0784\n",
            "Epoch [7905/20000], Training Loss: 0.0744\n",
            "Epoch [7906/20000], Training Loss: 0.0798\n",
            "Epoch [7907/20000], Training Loss: 0.0769\n",
            "Epoch [7908/20000], Training Loss: 0.0804\n",
            "Epoch [7909/20000], Training Loss: 0.0819\n",
            "Epoch [7910/20000], Training Loss: 0.0759\n",
            "Epoch [7911/20000], Training Loss: 0.0777\n",
            "Epoch [7912/20000], Training Loss: 0.0738\n",
            "Epoch [7913/20000], Training Loss: 0.0788\n",
            "Epoch [7914/20000], Training Loss: 0.0819\n",
            "Epoch [7915/20000], Training Loss: 0.0848\n",
            "Epoch [7916/20000], Training Loss: 0.0796\n",
            "Epoch [7917/20000], Training Loss: 0.0760\n",
            "Epoch [7918/20000], Training Loss: 0.0842\n",
            "Epoch [7919/20000], Training Loss: 0.0763\n",
            "Epoch [7920/20000], Training Loss: 0.0781\n",
            "Epoch [7921/20000], Training Loss: 0.0799\n",
            "Epoch [7922/20000], Training Loss: 0.0801\n",
            "Epoch [7923/20000], Training Loss: 0.0783\n",
            "Epoch [7924/20000], Training Loss: 0.0817\n",
            "Epoch [7925/20000], Training Loss: 0.0796\n",
            "Epoch [7926/20000], Training Loss: 0.0801\n",
            "Epoch [7927/20000], Training Loss: 0.0769\n",
            "Epoch [7928/20000], Training Loss: 0.0838\n",
            "Epoch [7929/20000], Training Loss: 0.0807\n",
            "Epoch [7930/20000], Training Loss: 0.0778\n",
            "Epoch [7931/20000], Training Loss: 0.0760\n",
            "Epoch [7932/20000], Training Loss: 0.0789\n",
            "Epoch [7933/20000], Training Loss: 0.0838\n",
            "Epoch [7934/20000], Training Loss: 0.0808\n",
            "Epoch [7935/20000], Training Loss: 0.0846\n",
            "Epoch [7936/20000], Training Loss: 0.0787\n",
            "Epoch [7937/20000], Training Loss: 0.0732\n",
            "Epoch [7938/20000], Training Loss: 0.0792\n",
            "Epoch [7939/20000], Training Loss: 0.0754\n",
            "Epoch [7940/20000], Training Loss: 0.0785\n",
            "Epoch [7941/20000], Training Loss: 0.0808\n",
            "Epoch [7942/20000], Training Loss: 0.0815\n",
            "Epoch [7943/20000], Training Loss: 0.0746\n",
            "Epoch [7944/20000], Training Loss: 0.0868\n",
            "Epoch [7945/20000], Training Loss: 0.0794\n",
            "Epoch [7946/20000], Training Loss: 0.0752\n",
            "Epoch [7947/20000], Training Loss: 0.0760\n",
            "Epoch [7948/20000], Training Loss: 0.0768\n",
            "Epoch [7949/20000], Training Loss: 0.0778\n",
            "Epoch [7950/20000], Training Loss: 0.0835\n",
            "Epoch [7951/20000], Training Loss: 0.0832\n",
            "Epoch [7952/20000], Training Loss: 0.0759\n",
            "Epoch [7953/20000], Training Loss: 0.0778\n",
            "Epoch [7954/20000], Training Loss: 0.0868\n",
            "Epoch [7955/20000], Training Loss: 0.0850\n",
            "Epoch [7956/20000], Training Loss: 0.0776\n",
            "Epoch [7957/20000], Training Loss: 0.0787\n",
            "Epoch [7958/20000], Training Loss: 0.0746\n",
            "Epoch [7959/20000], Training Loss: 0.0831\n",
            "Epoch [7960/20000], Training Loss: 0.0777\n",
            "Epoch [7961/20000], Training Loss: 0.0748\n",
            "Epoch [7962/20000], Training Loss: 0.0779\n",
            "Epoch [7963/20000], Training Loss: 0.0806\n",
            "Epoch [7964/20000], Training Loss: 0.0783\n",
            "Epoch [7965/20000], Training Loss: 0.0791\n",
            "Epoch [7966/20000], Training Loss: 0.0794\n",
            "Epoch [7967/20000], Training Loss: 0.0848\n",
            "Epoch [7968/20000], Training Loss: 0.0809\n",
            "Epoch [7969/20000], Training Loss: 0.0764\n",
            "Epoch [7970/20000], Training Loss: 0.0750\n",
            "Epoch [7971/20000], Training Loss: 0.0780\n",
            "Epoch [7972/20000], Training Loss: 0.0749\n",
            "Epoch [7973/20000], Training Loss: 0.0821\n",
            "Epoch [7974/20000], Training Loss: 0.0868\n",
            "Epoch [7975/20000], Training Loss: 0.0817\n",
            "Epoch [7976/20000], Training Loss: 0.0747\n",
            "Epoch [7977/20000], Training Loss: 0.0767\n",
            "Epoch [7978/20000], Training Loss: 0.0744\n",
            "Epoch [7979/20000], Training Loss: 0.0793\n",
            "Epoch [7980/20000], Training Loss: 0.0808\n",
            "Epoch [7981/20000], Training Loss: 0.0781\n",
            "Epoch [7982/20000], Training Loss: 0.0798\n",
            "Epoch [7983/20000], Training Loss: 0.0789\n",
            "Epoch [7984/20000], Training Loss: 0.0794\n",
            "Epoch [7985/20000], Training Loss: 0.0776\n",
            "Epoch [7986/20000], Training Loss: 0.0750\n",
            "Epoch [7987/20000], Training Loss: 0.0785\n",
            "Epoch [7988/20000], Training Loss: 0.0803\n",
            "Epoch [7989/20000], Training Loss: 0.0787\n",
            "Epoch [7990/20000], Training Loss: 0.0833\n",
            "Epoch [7991/20000], Training Loss: 0.0800\n",
            "Epoch [7992/20000], Training Loss: 0.0846\n",
            "Epoch [7993/20000], Training Loss: 0.0817\n",
            "Epoch [7994/20000], Training Loss: 0.0747\n",
            "Epoch [7995/20000], Training Loss: 0.0840\n",
            "Epoch [7996/20000], Training Loss: 0.0861\n",
            "Epoch [7997/20000], Training Loss: 0.0782\n",
            "Epoch [7998/20000], Training Loss: 0.0860\n",
            "Epoch [7999/20000], Training Loss: 0.0793\n",
            "Epoch [8000/20000], Training Loss: 0.0783\n",
            "Epoch [8001/20000], Training Loss: 0.0753\n",
            "Epoch [8002/20000], Training Loss: 0.0749\n",
            "Epoch [8003/20000], Training Loss: 0.0832\n",
            "Epoch [8004/20000], Training Loss: 0.0860\n",
            "Epoch [8005/20000], Training Loss: 0.0814\n",
            "Epoch [8006/20000], Training Loss: 0.0883\n",
            "Epoch [8007/20000], Training Loss: 0.0812\n",
            "Epoch [8008/20000], Training Loss: 0.0755\n",
            "Epoch [8009/20000], Training Loss: 0.0757\n",
            "Epoch [8010/20000], Training Loss: 0.0759\n",
            "Epoch [8011/20000], Training Loss: 0.0753\n",
            "Epoch [8012/20000], Training Loss: 0.0801\n",
            "Epoch [8013/20000], Training Loss: 0.0788\n",
            "Epoch [8014/20000], Training Loss: 0.0761\n",
            "Epoch [8015/20000], Training Loss: 0.0875\n",
            "Epoch [8016/20000], Training Loss: 0.0826\n",
            "Epoch [8017/20000], Training Loss: 0.0752\n",
            "Epoch [8018/20000], Training Loss: 0.0801\n",
            "Epoch [8019/20000], Training Loss: 0.0768\n",
            "Epoch [8020/20000], Training Loss: 0.0835\n",
            "Epoch [8021/20000], Training Loss: 0.0754\n",
            "Epoch [8022/20000], Training Loss: 0.0810\n",
            "Epoch [8023/20000], Training Loss: 0.0782\n",
            "Epoch [8024/20000], Training Loss: 0.0759\n",
            "Epoch [8025/20000], Training Loss: 0.0806\n",
            "Epoch [8026/20000], Training Loss: 0.0794\n",
            "Epoch [8027/20000], Training Loss: 0.0786\n",
            "Epoch [8028/20000], Training Loss: 0.0775\n",
            "Epoch [8029/20000], Training Loss: 0.0791\n",
            "Epoch [8030/20000], Training Loss: 0.0811\n",
            "Epoch [8031/20000], Training Loss: 0.0763\n",
            "Epoch [8032/20000], Training Loss: 0.0810\n",
            "Epoch [8033/20000], Training Loss: 0.0829\n",
            "Epoch [8034/20000], Training Loss: 0.0745\n",
            "Epoch [8035/20000], Training Loss: 0.0733\n",
            "Epoch [8036/20000], Training Loss: 0.0793\n",
            "Epoch [8037/20000], Training Loss: 0.0745\n",
            "Epoch [8038/20000], Training Loss: 0.0793\n",
            "Epoch [8039/20000], Training Loss: 0.0766\n",
            "Epoch [8040/20000], Training Loss: 0.0774\n",
            "Epoch [8041/20000], Training Loss: 0.0838\n",
            "Epoch [8042/20000], Training Loss: 0.0741\n",
            "Epoch [8043/20000], Training Loss: 0.0778\n",
            "Epoch [8044/20000], Training Loss: 0.0755\n",
            "Epoch [8045/20000], Training Loss: 0.0824\n",
            "Epoch [8046/20000], Training Loss: 0.0797\n",
            "Epoch [8047/20000], Training Loss: 0.0807\n",
            "Epoch [8048/20000], Training Loss: 0.0809\n",
            "Epoch [8049/20000], Training Loss: 0.0801\n",
            "Epoch [8050/20000], Training Loss: 0.0824\n",
            "Epoch [8051/20000], Training Loss: 0.0766\n",
            "Epoch [8052/20000], Training Loss: 0.0796\n",
            "Epoch [8053/20000], Training Loss: 0.0799\n",
            "Epoch [8054/20000], Training Loss: 0.0776\n",
            "Epoch [8055/20000], Training Loss: 0.0797\n",
            "Epoch [8056/20000], Training Loss: 0.0770\n",
            "Epoch [8057/20000], Training Loss: 0.0818\n",
            "Epoch [8058/20000], Training Loss: 0.0779\n",
            "Epoch [8059/20000], Training Loss: 0.0804\n",
            "Epoch [8060/20000], Training Loss: 0.0803\n",
            "Epoch [8061/20000], Training Loss: 0.0756\n",
            "Epoch [8062/20000], Training Loss: 0.0848\n",
            "Epoch [8063/20000], Training Loss: 0.0840\n",
            "Epoch [8064/20000], Training Loss: 0.0789\n",
            "Epoch [8065/20000], Training Loss: 0.0800\n",
            "Epoch [8066/20000], Training Loss: 0.0866\n",
            "Epoch [8067/20000], Training Loss: 0.0801\n",
            "Epoch [8068/20000], Training Loss: 0.0741\n",
            "Epoch [8069/20000], Training Loss: 0.0766\n",
            "Epoch [8070/20000], Training Loss: 0.0807\n",
            "Epoch [8071/20000], Training Loss: 0.0857\n",
            "Epoch [8072/20000], Training Loss: 0.0812\n",
            "Epoch [8073/20000], Training Loss: 0.0779\n",
            "Epoch [8074/20000], Training Loss: 0.0777\n",
            "Epoch [8075/20000], Training Loss: 0.0791\n",
            "Epoch [8076/20000], Training Loss: 0.0741\n",
            "Epoch [8077/20000], Training Loss: 0.0782\n",
            "Epoch [8078/20000], Training Loss: 0.0827\n",
            "Epoch [8079/20000], Training Loss: 0.0771\n",
            "Epoch [8080/20000], Training Loss: 0.0731\n",
            "Epoch [8081/20000], Training Loss: 0.0772\n",
            "Epoch [8082/20000], Training Loss: 0.0796\n",
            "Epoch [8083/20000], Training Loss: 0.0879\n",
            "Epoch [8084/20000], Training Loss: 0.0824\n",
            "Epoch [8085/20000], Training Loss: 0.0848\n",
            "Epoch [8086/20000], Training Loss: 0.0792\n",
            "Epoch [8087/20000], Training Loss: 0.0786\n",
            "Epoch [8088/20000], Training Loss: 0.0758\n",
            "Epoch [8089/20000], Training Loss: 0.0803\n",
            "Epoch [8090/20000], Training Loss: 0.0765\n",
            "Epoch [8091/20000], Training Loss: 0.0757\n",
            "Epoch [8092/20000], Training Loss: 0.0800\n",
            "Epoch [8093/20000], Training Loss: 0.0796\n",
            "Epoch [8094/20000], Training Loss: 0.0757\n",
            "Epoch [8095/20000], Training Loss: 0.0796\n",
            "Epoch [8096/20000], Training Loss: 0.0859\n",
            "Epoch [8097/20000], Training Loss: 0.0837\n",
            "Epoch [8098/20000], Training Loss: 0.0747\n",
            "Epoch [8099/20000], Training Loss: 0.0821\n",
            "Epoch [8100/20000], Training Loss: 0.0731\n",
            "Epoch [8101/20000], Training Loss: 0.0831\n",
            "Epoch [8102/20000], Training Loss: 0.0791\n",
            "Epoch [8103/20000], Training Loss: 0.0768\n",
            "Epoch [8104/20000], Training Loss: 0.0793\n",
            "Epoch [8105/20000], Training Loss: 0.0759\n",
            "Epoch [8106/20000], Training Loss: 0.0803\n",
            "Epoch [8107/20000], Training Loss: 0.0795\n",
            "Epoch [8108/20000], Training Loss: 0.0764\n",
            "Epoch [8109/20000], Training Loss: 0.0857\n",
            "Epoch [8110/20000], Training Loss: 0.0821\n",
            "Epoch [8111/20000], Training Loss: 0.0754\n",
            "Epoch [8112/20000], Training Loss: 0.0855\n",
            "Epoch [8113/20000], Training Loss: 0.0836\n",
            "Epoch [8114/20000], Training Loss: 0.0764\n",
            "Epoch [8115/20000], Training Loss: 0.0850\n",
            "Epoch [8116/20000], Training Loss: 0.0782\n",
            "Epoch [8117/20000], Training Loss: 0.0819\n",
            "Epoch [8118/20000], Training Loss: 0.0740\n",
            "Epoch [8119/20000], Training Loss: 0.0743\n",
            "Epoch [8120/20000], Training Loss: 0.0737\n",
            "Epoch [8121/20000], Training Loss: 0.0787\n",
            "Epoch [8122/20000], Training Loss: 0.0793\n",
            "Epoch [8123/20000], Training Loss: 0.0833\n",
            "Epoch [8124/20000], Training Loss: 0.0790\n",
            "Epoch [8125/20000], Training Loss: 0.0811\n",
            "Epoch [8126/20000], Training Loss: 0.0808\n",
            "Epoch [8127/20000], Training Loss: 0.0835\n",
            "Epoch [8128/20000], Training Loss: 0.0786\n",
            "Epoch [8129/20000], Training Loss: 0.0791\n",
            "Epoch [8130/20000], Training Loss: 0.0817\n",
            "Epoch [8131/20000], Training Loss: 0.0777\n",
            "Epoch [8132/20000], Training Loss: 0.0768\n",
            "Epoch [8133/20000], Training Loss: 0.0849\n",
            "Epoch [8134/20000], Training Loss: 0.0801\n",
            "Epoch [8135/20000], Training Loss: 0.0790\n",
            "Epoch [8136/20000], Training Loss: 0.0809\n",
            "Epoch [8137/20000], Training Loss: 0.0794\n",
            "Epoch [8138/20000], Training Loss: 0.0742\n",
            "Epoch [8139/20000], Training Loss: 0.0806\n",
            "Epoch [8140/20000], Training Loss: 0.0838\n",
            "Epoch [8141/20000], Training Loss: 0.0820\n",
            "Epoch [8142/20000], Training Loss: 0.0800\n",
            "Epoch [8143/20000], Training Loss: 0.0804\n",
            "Epoch [8144/20000], Training Loss: 0.0847\n",
            "Epoch [8145/20000], Training Loss: 0.0808\n",
            "Epoch [8146/20000], Training Loss: 0.0841\n",
            "Epoch [8147/20000], Training Loss: 0.0777\n",
            "Epoch [8148/20000], Training Loss: 0.0810\n",
            "Epoch [8149/20000], Training Loss: 0.0808\n",
            "Epoch [8150/20000], Training Loss: 0.0773\n",
            "Epoch [8151/20000], Training Loss: 0.0784\n",
            "Epoch [8152/20000], Training Loss: 0.0817\n",
            "Epoch [8153/20000], Training Loss: 0.0791\n",
            "Epoch [8154/20000], Training Loss: 0.0834\n",
            "Epoch [8155/20000], Training Loss: 0.0848\n",
            "Epoch [8156/20000], Training Loss: 0.0759\n",
            "Epoch [8157/20000], Training Loss: 0.0754\n",
            "Epoch [8158/20000], Training Loss: 0.0823\n",
            "Epoch [8159/20000], Training Loss: 0.0772\n",
            "Epoch [8160/20000], Training Loss: 0.0867\n",
            "Epoch [8161/20000], Training Loss: 0.0756\n",
            "Epoch [8162/20000], Training Loss: 0.0811\n",
            "Epoch [8163/20000], Training Loss: 0.0833\n",
            "Epoch [8164/20000], Training Loss: 0.0791\n",
            "Epoch [8165/20000], Training Loss: 0.0750\n",
            "Epoch [8166/20000], Training Loss: 0.0762\n",
            "Epoch [8167/20000], Training Loss: 0.0747\n",
            "Epoch [8168/20000], Training Loss: 0.0737\n",
            "Epoch [8169/20000], Training Loss: 0.0807\n",
            "Epoch [8170/20000], Training Loss: 0.0764\n",
            "Epoch [8171/20000], Training Loss: 0.0833\n",
            "Epoch [8172/20000], Training Loss: 0.0806\n",
            "Epoch [8173/20000], Training Loss: 0.0804\n",
            "Epoch [8174/20000], Training Loss: 0.0781\n",
            "Epoch [8175/20000], Training Loss: 0.0812\n",
            "Epoch [8176/20000], Training Loss: 0.0878\n",
            "Epoch [8177/20000], Training Loss: 0.0730\n",
            "Epoch [8178/20000], Training Loss: 0.0801\n",
            "Epoch [8179/20000], Training Loss: 0.0770\n",
            "Epoch [8180/20000], Training Loss: 0.0777\n",
            "Epoch [8181/20000], Training Loss: 0.0778\n",
            "Epoch [8182/20000], Training Loss: 0.0840\n",
            "Epoch [8183/20000], Training Loss: 0.0773\n",
            "Epoch [8184/20000], Training Loss: 0.0745\n",
            "Epoch [8185/20000], Training Loss: 0.0754\n",
            "Epoch [8186/20000], Training Loss: 0.0799\n",
            "Epoch [8187/20000], Training Loss: 0.0846\n",
            "Epoch [8188/20000], Training Loss: 0.0810\n",
            "Epoch [8189/20000], Training Loss: 0.0808\n",
            "Epoch [8190/20000], Training Loss: 0.0852\n",
            "Epoch [8191/20000], Training Loss: 0.0836\n",
            "Epoch [8192/20000], Training Loss: 0.0825\n",
            "Epoch [8193/20000], Training Loss: 0.0812\n",
            "Epoch [8194/20000], Training Loss: 0.0767\n",
            "Epoch [8195/20000], Training Loss: 0.0820\n",
            "Epoch [8196/20000], Training Loss: 0.0752\n",
            "Epoch [8197/20000], Training Loss: 0.0806\n",
            "Epoch [8198/20000], Training Loss: 0.0857\n",
            "Epoch [8199/20000], Training Loss: 0.0787\n",
            "Epoch [8200/20000], Training Loss: 0.0814\n",
            "Epoch [8201/20000], Training Loss: 0.0752\n",
            "Epoch [8202/20000], Training Loss: 0.0850\n",
            "Epoch [8203/20000], Training Loss: 0.0835\n",
            "Epoch [8204/20000], Training Loss: 0.0770\n",
            "Epoch [8205/20000], Training Loss: 0.0781\n",
            "Epoch [8206/20000], Training Loss: 0.0766\n",
            "Epoch [8207/20000], Training Loss: 0.0757\n",
            "Epoch [8208/20000], Training Loss: 0.0831\n",
            "Epoch [8209/20000], Training Loss: 0.0804\n",
            "Epoch [8210/20000], Training Loss: 0.0767\n",
            "Epoch [8211/20000], Training Loss: 0.0823\n",
            "Epoch [8212/20000], Training Loss: 0.0867\n",
            "Epoch [8213/20000], Training Loss: 0.0839\n",
            "Epoch [8214/20000], Training Loss: 0.0801\n",
            "Epoch [8215/20000], Training Loss: 0.0801\n",
            "Epoch [8216/20000], Training Loss: 0.0767\n",
            "Epoch [8217/20000], Training Loss: 0.0854\n",
            "Epoch [8218/20000], Training Loss: 0.0777\n",
            "Epoch [8219/20000], Training Loss: 0.0797\n",
            "Epoch [8220/20000], Training Loss: 0.0759\n",
            "Epoch [8221/20000], Training Loss: 0.0792\n",
            "Epoch [8222/20000], Training Loss: 0.0781\n",
            "Epoch [8223/20000], Training Loss: 0.0767\n",
            "Epoch [8224/20000], Training Loss: 0.0789\n",
            "Epoch [8225/20000], Training Loss: 0.0807\n",
            "Epoch [8226/20000], Training Loss: 0.0778\n",
            "Epoch [8227/20000], Training Loss: 0.0815\n",
            "Epoch [8228/20000], Training Loss: 0.0800\n",
            "Epoch [8229/20000], Training Loss: 0.0753\n",
            "Epoch [8230/20000], Training Loss: 0.0838\n",
            "Epoch [8231/20000], Training Loss: 0.0817\n",
            "Epoch [8232/20000], Training Loss: 0.0789\n",
            "Epoch [8233/20000], Training Loss: 0.0789\n",
            "Epoch [8234/20000], Training Loss: 0.0746\n",
            "Epoch [8235/20000], Training Loss: 0.0815\n",
            "Epoch [8236/20000], Training Loss: 0.0795\n",
            "Epoch [8237/20000], Training Loss: 0.0795\n",
            "Epoch [8238/20000], Training Loss: 0.0794\n",
            "Epoch [8239/20000], Training Loss: 0.0794\n",
            "Epoch [8240/20000], Training Loss: 0.0753\n",
            "Epoch [8241/20000], Training Loss: 0.0788\n",
            "Epoch [8242/20000], Training Loss: 0.0771\n",
            "Epoch [8243/20000], Training Loss: 0.0779\n",
            "Epoch [8244/20000], Training Loss: 0.0768\n",
            "Epoch [8245/20000], Training Loss: 0.0819\n",
            "Epoch [8246/20000], Training Loss: 0.0798\n",
            "Epoch [8247/20000], Training Loss: 0.0781\n",
            "Epoch [8248/20000], Training Loss: 0.0805\n",
            "Epoch [8249/20000], Training Loss: 0.0798\n",
            "Epoch [8250/20000], Training Loss: 0.0782\n",
            "Epoch [8251/20000], Training Loss: 0.0860\n",
            "Epoch [8252/20000], Training Loss: 0.0807\n",
            "Epoch [8253/20000], Training Loss: 0.0873\n",
            "Epoch [8254/20000], Training Loss: 0.0786\n",
            "Epoch [8255/20000], Training Loss: 0.0802\n",
            "Epoch [8256/20000], Training Loss: 0.0809\n",
            "Epoch [8257/20000], Training Loss: 0.0819\n",
            "Epoch [8258/20000], Training Loss: 0.0793\n",
            "Epoch [8259/20000], Training Loss: 0.0755\n",
            "Epoch [8260/20000], Training Loss: 0.0799\n",
            "Epoch [8261/20000], Training Loss: 0.0745\n",
            "Epoch [8262/20000], Training Loss: 0.0777\n",
            "Epoch [8263/20000], Training Loss: 0.0790\n",
            "Epoch [8264/20000], Training Loss: 0.0800\n",
            "Epoch [8265/20000], Training Loss: 0.0798\n",
            "Epoch [8266/20000], Training Loss: 0.0750\n",
            "Epoch [8267/20000], Training Loss: 0.0808\n",
            "Epoch [8268/20000], Training Loss: 0.0813\n",
            "Epoch [8269/20000], Training Loss: 0.0776\n",
            "Epoch [8270/20000], Training Loss: 0.0807\n",
            "Epoch [8271/20000], Training Loss: 0.0844\n",
            "Epoch [8272/20000], Training Loss: 0.0780\n",
            "Epoch [8273/20000], Training Loss: 0.0793\n",
            "Epoch [8274/20000], Training Loss: 0.0793\n",
            "Epoch [8275/20000], Training Loss: 0.0798\n",
            "Epoch [8276/20000], Training Loss: 0.0730\n",
            "Epoch [8277/20000], Training Loss: 0.0842\n",
            "Epoch [8278/20000], Training Loss: 0.0796\n",
            "Epoch [8279/20000], Training Loss: 0.0821\n",
            "Epoch [8280/20000], Training Loss: 0.0846\n",
            "Epoch [8281/20000], Training Loss: 0.0794\n",
            "Epoch [8282/20000], Training Loss: 0.0766\n",
            "Epoch [8283/20000], Training Loss: 0.0814\n",
            "Epoch [8284/20000], Training Loss: 0.0775\n",
            "Epoch [8285/20000], Training Loss: 0.0739\n",
            "Epoch [8286/20000], Training Loss: 0.0774\n",
            "Epoch [8287/20000], Training Loss: 0.0776\n",
            "Epoch [8288/20000], Training Loss: 0.0835\n",
            "Epoch [8289/20000], Training Loss: 0.0818\n",
            "Epoch [8290/20000], Training Loss: 0.0741\n",
            "Epoch [8291/20000], Training Loss: 0.0821\n",
            "Epoch [8292/20000], Training Loss: 0.0790\n",
            "Epoch [8293/20000], Training Loss: 0.0740\n",
            "Epoch [8294/20000], Training Loss: 0.0807\n",
            "Epoch [8295/20000], Training Loss: 0.0762\n",
            "Epoch [8296/20000], Training Loss: 0.0833\n",
            "Epoch [8297/20000], Training Loss: 0.0740\n",
            "Epoch [8298/20000], Training Loss: 0.0810\n",
            "Epoch [8299/20000], Training Loss: 0.0854\n",
            "Epoch [8300/20000], Training Loss: 0.0818\n",
            "Epoch [8301/20000], Training Loss: 0.0816\n",
            "Epoch [8302/20000], Training Loss: 0.0732\n",
            "Epoch [8303/20000], Training Loss: 0.0829\n",
            "Epoch [8304/20000], Training Loss: 0.0854\n",
            "Epoch [8305/20000], Training Loss: 0.0847\n",
            "Epoch [8306/20000], Training Loss: 0.0778\n",
            "Epoch [8307/20000], Training Loss: 0.0778\n",
            "Epoch [8308/20000], Training Loss: 0.0750\n",
            "Epoch [8309/20000], Training Loss: 0.0741\n",
            "Epoch [8310/20000], Training Loss: 0.0779\n",
            "Epoch [8311/20000], Training Loss: 0.0770\n",
            "Epoch [8312/20000], Training Loss: 0.0761\n",
            "Epoch [8313/20000], Training Loss: 0.0786\n",
            "Epoch [8314/20000], Training Loss: 0.0819\n",
            "Epoch [8315/20000], Training Loss: 0.0835\n",
            "Epoch [8316/20000], Training Loss: 0.0762\n",
            "Epoch [8317/20000], Training Loss: 0.0732\n",
            "Epoch [8318/20000], Training Loss: 0.0762\n",
            "Epoch [8319/20000], Training Loss: 0.0786\n",
            "Epoch [8320/20000], Training Loss: 0.0833\n",
            "Epoch [8321/20000], Training Loss: 0.0848\n",
            "Epoch [8322/20000], Training Loss: 0.0870\n",
            "Epoch [8323/20000], Training Loss: 0.0759\n",
            "Epoch [8324/20000], Training Loss: 0.0796\n",
            "Epoch [8325/20000], Training Loss: 0.0805\n",
            "Epoch [8326/20000], Training Loss: 0.0803\n",
            "Epoch [8327/20000], Training Loss: 0.0810\n",
            "Epoch [8328/20000], Training Loss: 0.0821\n",
            "Epoch [8329/20000], Training Loss: 0.0818\n",
            "Epoch [8330/20000], Training Loss: 0.0832\n",
            "Epoch [8331/20000], Training Loss: 0.0818\n",
            "Epoch [8332/20000], Training Loss: 0.0767\n",
            "Epoch [8333/20000], Training Loss: 0.0832\n",
            "Epoch [8334/20000], Training Loss: 0.0835\n",
            "Epoch [8335/20000], Training Loss: 0.0798\n",
            "Epoch [8336/20000], Training Loss: 0.0804\n",
            "Epoch [8337/20000], Training Loss: 0.0810\n",
            "Epoch [8338/20000], Training Loss: 0.0816\n",
            "Epoch [8339/20000], Training Loss: 0.0791\n",
            "Epoch [8340/20000], Training Loss: 0.0841\n",
            "Epoch [8341/20000], Training Loss: 0.0779\n",
            "Epoch [8342/20000], Training Loss: 0.0815\n",
            "Epoch [8343/20000], Training Loss: 0.0810\n",
            "Epoch [8344/20000], Training Loss: 0.0791\n",
            "Epoch [8345/20000], Training Loss: 0.0751\n",
            "Epoch [8346/20000], Training Loss: 0.0802\n",
            "Epoch [8347/20000], Training Loss: 0.0794\n",
            "Epoch [8348/20000], Training Loss: 0.0787\n",
            "Epoch [8349/20000], Training Loss: 0.0834\n",
            "Epoch [8350/20000], Training Loss: 0.0812\n",
            "Epoch [8351/20000], Training Loss: 0.0822\n",
            "Epoch [8352/20000], Training Loss: 0.0795\n",
            "Epoch [8353/20000], Training Loss: 0.0782\n",
            "Epoch [8354/20000], Training Loss: 0.0780\n",
            "Epoch [8355/20000], Training Loss: 0.0805\n",
            "Epoch [8356/20000], Training Loss: 0.0807\n",
            "Epoch [8357/20000], Training Loss: 0.0813\n",
            "Epoch [8358/20000], Training Loss: 0.0876\n",
            "Epoch [8359/20000], Training Loss: 0.0722\n",
            "Epoch [8360/20000], Training Loss: 0.0782\n",
            "Epoch [8361/20000], Training Loss: 0.0750\n",
            "Epoch [8362/20000], Training Loss: 0.0833\n",
            "Epoch [8363/20000], Training Loss: 0.0819\n",
            "Epoch [8364/20000], Training Loss: 0.0804\n",
            "Epoch [8365/20000], Training Loss: 0.0778\n",
            "Epoch [8366/20000], Training Loss: 0.0777\n",
            "Epoch [8367/20000], Training Loss: 0.0832\n",
            "Epoch [8368/20000], Training Loss: 0.0771\n",
            "Epoch [8369/20000], Training Loss: 0.0780\n",
            "Epoch [8370/20000], Training Loss: 0.0796\n",
            "Epoch [8371/20000], Training Loss: 0.0809\n",
            "Epoch [8372/20000], Training Loss: 0.0738\n",
            "Epoch [8373/20000], Training Loss: 0.0768\n",
            "Epoch [8374/20000], Training Loss: 0.0774\n",
            "Epoch [8375/20000], Training Loss: 0.0794\n",
            "Epoch [8376/20000], Training Loss: 0.0757\n",
            "Epoch [8377/20000], Training Loss: 0.0729\n",
            "Epoch [8378/20000], Training Loss: 0.0808\n",
            "Epoch [8379/20000], Training Loss: 0.0806\n",
            "Epoch [8380/20000], Training Loss: 0.0749\n",
            "Epoch [8381/20000], Training Loss: 0.0789\n",
            "Epoch [8382/20000], Training Loss: 0.0764\n",
            "Epoch [8383/20000], Training Loss: 0.0760\n",
            "Epoch [8384/20000], Training Loss: 0.0801\n",
            "Epoch [8385/20000], Training Loss: 0.0764\n",
            "Epoch [8386/20000], Training Loss: 0.0805\n",
            "Epoch [8387/20000], Training Loss: 0.0778\n",
            "Epoch [8388/20000], Training Loss: 0.0780\n",
            "Epoch [8389/20000], Training Loss: 0.0836\n",
            "Epoch [8390/20000], Training Loss: 0.0862\n",
            "Epoch [8391/20000], Training Loss: 0.0801\n",
            "Epoch [8392/20000], Training Loss: 0.0828\n",
            "Epoch [8393/20000], Training Loss: 0.0789\n",
            "Epoch [8394/20000], Training Loss: 0.0803\n",
            "Epoch [8395/20000], Training Loss: 0.0856\n",
            "Epoch [8396/20000], Training Loss: 0.0761\n",
            "Epoch [8397/20000], Training Loss: 0.0753\n",
            "Epoch [8398/20000], Training Loss: 0.0856\n",
            "Epoch [8399/20000], Training Loss: 0.0848\n",
            "Epoch [8400/20000], Training Loss: 0.0795\n",
            "Epoch [8401/20000], Training Loss: 0.0771\n",
            "Epoch [8402/20000], Training Loss: 0.0838\n",
            "Epoch [8403/20000], Training Loss: 0.0832\n",
            "Epoch [8404/20000], Training Loss: 0.0789\n",
            "Epoch [8405/20000], Training Loss: 0.0762\n",
            "Epoch [8406/20000], Training Loss: 0.0796\n",
            "Epoch [8407/20000], Training Loss: 0.0819\n",
            "Epoch [8408/20000], Training Loss: 0.0815\n",
            "Epoch [8409/20000], Training Loss: 0.0825\n",
            "Epoch [8410/20000], Training Loss: 0.0762\n",
            "Epoch [8411/20000], Training Loss: 0.0754\n",
            "Epoch [8412/20000], Training Loss: 0.0807\n",
            "Epoch [8413/20000], Training Loss: 0.0753\n",
            "Epoch [8414/20000], Training Loss: 0.0757\n",
            "Epoch [8415/20000], Training Loss: 0.0788\n",
            "Epoch [8416/20000], Training Loss: 0.0791\n",
            "Epoch [8417/20000], Training Loss: 0.0786\n",
            "Epoch [8418/20000], Training Loss: 0.0804\n",
            "Epoch [8419/20000], Training Loss: 0.0832\n",
            "Epoch [8420/20000], Training Loss: 0.0837\n",
            "Epoch [8421/20000], Training Loss: 0.0800\n",
            "Epoch [8422/20000], Training Loss: 0.0843\n",
            "Epoch [8423/20000], Training Loss: 0.0830\n",
            "Epoch [8424/20000], Training Loss: 0.0841\n",
            "Epoch [8425/20000], Training Loss: 0.0782\n",
            "Epoch [8426/20000], Training Loss: 0.0743\n",
            "Epoch [8427/20000], Training Loss: 0.0876\n",
            "Epoch [8428/20000], Training Loss: 0.0796\n",
            "Epoch [8429/20000], Training Loss: 0.0736\n",
            "Epoch [8430/20000], Training Loss: 0.0734\n",
            "Epoch [8431/20000], Training Loss: 0.0868\n",
            "Epoch [8432/20000], Training Loss: 0.0876\n",
            "Epoch [8433/20000], Training Loss: 0.0761\n",
            "Epoch [8434/20000], Training Loss: 0.0789\n",
            "Epoch [8435/20000], Training Loss: 0.0837\n",
            "Epoch [8436/20000], Training Loss: 0.0844\n",
            "Epoch [8437/20000], Training Loss: 0.0774\n",
            "Epoch [8438/20000], Training Loss: 0.0840\n",
            "Epoch [8439/20000], Training Loss: 0.0835\n",
            "Epoch [8440/20000], Training Loss: 0.0811\n",
            "Epoch [8441/20000], Training Loss: 0.0818\n",
            "Epoch [8442/20000], Training Loss: 0.0799\n",
            "Epoch [8443/20000], Training Loss: 0.0749\n",
            "Epoch [8444/20000], Training Loss: 0.0796\n",
            "Epoch [8445/20000], Training Loss: 0.0802\n",
            "Epoch [8446/20000], Training Loss: 0.0789\n",
            "Epoch [8447/20000], Training Loss: 0.0784\n",
            "Epoch [8448/20000], Training Loss: 0.0796\n",
            "Epoch [8449/20000], Training Loss: 0.0837\n",
            "Epoch [8450/20000], Training Loss: 0.0744\n",
            "Epoch [8451/20000], Training Loss: 0.0763\n",
            "Epoch [8452/20000], Training Loss: 0.0776\n",
            "Epoch [8453/20000], Training Loss: 0.0821\n",
            "Epoch [8454/20000], Training Loss: 0.0758\n",
            "Epoch [8455/20000], Training Loss: 0.0835\n",
            "Epoch [8456/20000], Training Loss: 0.0836\n",
            "Epoch [8457/20000], Training Loss: 0.0812\n",
            "Epoch [8458/20000], Training Loss: 0.0810\n",
            "Epoch [8459/20000], Training Loss: 0.0782\n",
            "Epoch [8460/20000], Training Loss: 0.0803\n",
            "Epoch [8461/20000], Training Loss: 0.0824\n",
            "Epoch [8462/20000], Training Loss: 0.0781\n",
            "Epoch [8463/20000], Training Loss: 0.0820\n",
            "Epoch [8464/20000], Training Loss: 0.0809\n",
            "Epoch [8465/20000], Training Loss: 0.0798\n",
            "Epoch [8466/20000], Training Loss: 0.0824\n",
            "Epoch [8467/20000], Training Loss: 0.0734\n",
            "Epoch [8468/20000], Training Loss: 0.0817\n",
            "Epoch [8469/20000], Training Loss: 0.0824\n",
            "Epoch [8470/20000], Training Loss: 0.0779\n",
            "Epoch [8471/20000], Training Loss: 0.0751\n",
            "Epoch [8472/20000], Training Loss: 0.0814\n",
            "Epoch [8473/20000], Training Loss: 0.0820\n",
            "Epoch [8474/20000], Training Loss: 0.0803\n",
            "Epoch [8475/20000], Training Loss: 0.0804\n",
            "Epoch [8476/20000], Training Loss: 0.0747\n",
            "Epoch [8477/20000], Training Loss: 0.0844\n",
            "Epoch [8478/20000], Training Loss: 0.0778\n",
            "Epoch [8479/20000], Training Loss: 0.0754\n",
            "Epoch [8480/20000], Training Loss: 0.0783\n",
            "Epoch [8481/20000], Training Loss: 0.0852\n",
            "Epoch [8482/20000], Training Loss: 0.0810\n",
            "Epoch [8483/20000], Training Loss: 0.0816\n",
            "Epoch [8484/20000], Training Loss: 0.0795\n",
            "Epoch [8485/20000], Training Loss: 0.0789\n",
            "Epoch [8486/20000], Training Loss: 0.0815\n",
            "Epoch [8487/20000], Training Loss: 0.0729\n",
            "Epoch [8488/20000], Training Loss: 0.0799\n",
            "Epoch [8489/20000], Training Loss: 0.0800\n",
            "Epoch [8490/20000], Training Loss: 0.0803\n",
            "Epoch [8491/20000], Training Loss: 0.0793\n",
            "Epoch [8492/20000], Training Loss: 0.0824\n",
            "Epoch [8493/20000], Training Loss: 0.0739\n",
            "Epoch [8494/20000], Training Loss: 0.0812\n",
            "Epoch [8495/20000], Training Loss: 0.0802\n",
            "Epoch [8496/20000], Training Loss: 0.0745\n",
            "Epoch [8497/20000], Training Loss: 0.0790\n",
            "Epoch [8498/20000], Training Loss: 0.0843\n",
            "Epoch [8499/20000], Training Loss: 0.0833\n",
            "Epoch [8500/20000], Training Loss: 0.0735\n",
            "Epoch [8501/20000], Training Loss: 0.0759\n",
            "Epoch [8502/20000], Training Loss: 0.0810\n",
            "Epoch [8503/20000], Training Loss: 0.0874\n",
            "Epoch [8504/20000], Training Loss: 0.0760\n",
            "Epoch [8505/20000], Training Loss: 0.0807\n",
            "Epoch [8506/20000], Training Loss: 0.0741\n",
            "Epoch [8507/20000], Training Loss: 0.0797\n",
            "Epoch [8508/20000], Training Loss: 0.0844\n",
            "Epoch [8509/20000], Training Loss: 0.0793\n",
            "Epoch [8510/20000], Training Loss: 0.0805\n",
            "Epoch [8511/20000], Training Loss: 0.0806\n",
            "Epoch [8512/20000], Training Loss: 0.0803\n",
            "Epoch [8513/20000], Training Loss: 0.0847\n",
            "Epoch [8514/20000], Training Loss: 0.0785\n",
            "Epoch [8515/20000], Training Loss: 0.0800\n",
            "Epoch [8516/20000], Training Loss: 0.0825\n",
            "Epoch [8517/20000], Training Loss: 0.0805\n",
            "Epoch [8518/20000], Training Loss: 0.0815\n",
            "Epoch [8519/20000], Training Loss: 0.0847\n",
            "Epoch [8520/20000], Training Loss: 0.0807\n",
            "Epoch [8521/20000], Training Loss: 0.0793\n",
            "Epoch [8522/20000], Training Loss: 0.0786\n",
            "Epoch [8523/20000], Training Loss: 0.0786\n",
            "Epoch [8524/20000], Training Loss: 0.0810\n",
            "Epoch [8525/20000], Training Loss: 0.0741\n",
            "Epoch [8526/20000], Training Loss: 0.0799\n",
            "Epoch [8527/20000], Training Loss: 0.0815\n",
            "Epoch [8528/20000], Training Loss: 0.0747\n",
            "Epoch [8529/20000], Training Loss: 0.0799\n",
            "Epoch [8530/20000], Training Loss: 0.0852\n",
            "Epoch [8531/20000], Training Loss: 0.0795\n",
            "Epoch [8532/20000], Training Loss: 0.0803\n",
            "Epoch [8533/20000], Training Loss: 0.0815\n",
            "Epoch [8534/20000], Training Loss: 0.0780\n",
            "Epoch [8535/20000], Training Loss: 0.0834\n",
            "Epoch [8536/20000], Training Loss: 0.0803\n",
            "Epoch [8537/20000], Training Loss: 0.0775\n",
            "Epoch [8538/20000], Training Loss: 0.0815\n",
            "Epoch [8539/20000], Training Loss: 0.0796\n",
            "Epoch [8540/20000], Training Loss: 0.0754\n",
            "Epoch [8541/20000], Training Loss: 0.0830\n",
            "Epoch [8542/20000], Training Loss: 0.0822\n",
            "Epoch [8543/20000], Training Loss: 0.0769\n",
            "Epoch [8544/20000], Training Loss: 0.0775\n",
            "Epoch [8545/20000], Training Loss: 0.0750\n",
            "Epoch [8546/20000], Training Loss: 0.0785\n",
            "Epoch [8547/20000], Training Loss: 0.0836\n",
            "Epoch [8548/20000], Training Loss: 0.0830\n",
            "Epoch [8549/20000], Training Loss: 0.0810\n",
            "Epoch [8550/20000], Training Loss: 0.0856\n",
            "Epoch [8551/20000], Training Loss: 0.0805\n",
            "Epoch [8552/20000], Training Loss: 0.0821\n",
            "Epoch [8553/20000], Training Loss: 0.0851\n",
            "Epoch [8554/20000], Training Loss: 0.0735\n",
            "Epoch [8555/20000], Training Loss: 0.0852\n",
            "Epoch [8556/20000], Training Loss: 0.0767\n",
            "Epoch [8557/20000], Training Loss: 0.0755\n",
            "Epoch [8558/20000], Training Loss: 0.0810\n",
            "Epoch [8559/20000], Training Loss: 0.0739\n",
            "Epoch [8560/20000], Training Loss: 0.0765\n",
            "Epoch [8561/20000], Training Loss: 0.0778\n",
            "Epoch [8562/20000], Training Loss: 0.0780\n",
            "Epoch [8563/20000], Training Loss: 0.0776\n",
            "Epoch [8564/20000], Training Loss: 0.0790\n",
            "Epoch [8565/20000], Training Loss: 0.0803\n",
            "Epoch [8566/20000], Training Loss: 0.0791\n",
            "Epoch [8567/20000], Training Loss: 0.0842\n",
            "Epoch [8568/20000], Training Loss: 0.0873\n",
            "Epoch [8569/20000], Training Loss: 0.0821\n",
            "Epoch [8570/20000], Training Loss: 0.0808\n",
            "Epoch [8571/20000], Training Loss: 0.0740\n",
            "Epoch [8572/20000], Training Loss: 0.0824\n",
            "Epoch [8573/20000], Training Loss: 0.0870\n",
            "Epoch [8574/20000], Training Loss: 0.0855\n",
            "Epoch [8575/20000], Training Loss: 0.0780\n",
            "Epoch [8576/20000], Training Loss: 0.0742\n",
            "Epoch [8577/20000], Training Loss: 0.0783\n",
            "Epoch [8578/20000], Training Loss: 0.0767\n",
            "Epoch [8579/20000], Training Loss: 0.0775\n",
            "Epoch [8580/20000], Training Loss: 0.0788\n",
            "Epoch [8581/20000], Training Loss: 0.0743\n",
            "Epoch [8582/20000], Training Loss: 0.0752\n",
            "Epoch [8583/20000], Training Loss: 0.0769\n",
            "Epoch [8584/20000], Training Loss: 0.0819\n",
            "Epoch [8585/20000], Training Loss: 0.0803\n",
            "Epoch [8586/20000], Training Loss: 0.0861\n",
            "Epoch [8587/20000], Training Loss: 0.0861\n",
            "Epoch [8588/20000], Training Loss: 0.0776\n",
            "Epoch [8589/20000], Training Loss: 0.0767\n",
            "Epoch [8590/20000], Training Loss: 0.0784\n",
            "Epoch [8591/20000], Training Loss: 0.0754\n",
            "Epoch [8592/20000], Training Loss: 0.0788\n",
            "Epoch [8593/20000], Training Loss: 0.0790\n",
            "Epoch [8594/20000], Training Loss: 0.0794\n",
            "Epoch [8595/20000], Training Loss: 0.0762\n",
            "Epoch [8596/20000], Training Loss: 0.0783\n",
            "Epoch [8597/20000], Training Loss: 0.0823\n",
            "Epoch [8598/20000], Training Loss: 0.0833\n",
            "Epoch [8599/20000], Training Loss: 0.0824\n",
            "Epoch [8600/20000], Training Loss: 0.0798\n",
            "Epoch [8601/20000], Training Loss: 0.0763\n",
            "Epoch [8602/20000], Training Loss: 0.0757\n",
            "Epoch [8603/20000], Training Loss: 0.0866\n",
            "Epoch [8604/20000], Training Loss: 0.0773\n",
            "Epoch [8605/20000], Training Loss: 0.0747\n",
            "Epoch [8606/20000], Training Loss: 0.0817\n",
            "Epoch [8607/20000], Training Loss: 0.0803\n",
            "Epoch [8608/20000], Training Loss: 0.0795\n",
            "Epoch [8609/20000], Training Loss: 0.0869\n",
            "Epoch [8610/20000], Training Loss: 0.0753\n",
            "Epoch [8611/20000], Training Loss: 0.0811\n",
            "Epoch [8612/20000], Training Loss: 0.0759\n",
            "Epoch [8613/20000], Training Loss: 0.0826\n",
            "Epoch [8614/20000], Training Loss: 0.0797\n",
            "Epoch [8615/20000], Training Loss: 0.0795\n",
            "Epoch [8616/20000], Training Loss: 0.0842\n",
            "Epoch [8617/20000], Training Loss: 0.0820\n",
            "Epoch [8618/20000], Training Loss: 0.0737\n",
            "Epoch [8619/20000], Training Loss: 0.0762\n",
            "Epoch [8620/20000], Training Loss: 0.0808\n",
            "Epoch [8621/20000], Training Loss: 0.0725\n",
            "Epoch [8622/20000], Training Loss: 0.0810\n",
            "Epoch [8623/20000], Training Loss: 0.0804\n",
            "Epoch [8624/20000], Training Loss: 0.0742\n",
            "Epoch [8625/20000], Training Loss: 0.0776\n",
            "Epoch [8626/20000], Training Loss: 0.0748\n",
            "Epoch [8627/20000], Training Loss: 0.0798\n",
            "Epoch [8628/20000], Training Loss: 0.0818\n",
            "Epoch [8629/20000], Training Loss: 0.0753\n",
            "Epoch [8630/20000], Training Loss: 0.0826\n",
            "Epoch [8631/20000], Training Loss: 0.0785\n",
            "Epoch [8632/20000], Training Loss: 0.0727\n",
            "Epoch [8633/20000], Training Loss: 0.0828\n",
            "Epoch [8634/20000], Training Loss: 0.0792\n",
            "Epoch [8635/20000], Training Loss: 0.0792\n",
            "Epoch [8636/20000], Training Loss: 0.0783\n",
            "Epoch [8637/20000], Training Loss: 0.0799\n",
            "Epoch [8638/20000], Training Loss: 0.0784\n",
            "Epoch [8639/20000], Training Loss: 0.0826\n",
            "Epoch [8640/20000], Training Loss: 0.0785\n",
            "Epoch [8641/20000], Training Loss: 0.0835\n",
            "Epoch [8642/20000], Training Loss: 0.0779\n",
            "Epoch [8643/20000], Training Loss: 0.0773\n",
            "Epoch [8644/20000], Training Loss: 0.0820\n",
            "Epoch [8645/20000], Training Loss: 0.0770\n",
            "Epoch [8646/20000], Training Loss: 0.0846\n",
            "Epoch [8647/20000], Training Loss: 0.0793\n",
            "Epoch [8648/20000], Training Loss: 0.0748\n",
            "Epoch [8649/20000], Training Loss: 0.0794\n",
            "Epoch [8650/20000], Training Loss: 0.0791\n",
            "Epoch [8651/20000], Training Loss: 0.0807\n",
            "Epoch [8652/20000], Training Loss: 0.0803\n",
            "Epoch [8653/20000], Training Loss: 0.0752\n",
            "Epoch [8654/20000], Training Loss: 0.0749\n",
            "Epoch [8655/20000], Training Loss: 0.0815\n",
            "Epoch [8656/20000], Training Loss: 0.0804\n",
            "Epoch [8657/20000], Training Loss: 0.0770\n",
            "Epoch [8658/20000], Training Loss: 0.0850\n",
            "Epoch [8659/20000], Training Loss: 0.0780\n",
            "Epoch [8660/20000], Training Loss: 0.0767\n",
            "Epoch [8661/20000], Training Loss: 0.0800\n",
            "Epoch [8662/20000], Training Loss: 0.0871\n",
            "Epoch [8663/20000], Training Loss: 0.0728\n",
            "Epoch [8664/20000], Training Loss: 0.0772\n",
            "Epoch [8665/20000], Training Loss: 0.0824\n",
            "Epoch [8666/20000], Training Loss: 0.0840\n",
            "Epoch [8667/20000], Training Loss: 0.0809\n",
            "Epoch [8668/20000], Training Loss: 0.0784\n",
            "Epoch [8669/20000], Training Loss: 0.0795\n",
            "Epoch [8670/20000], Training Loss: 0.0770\n",
            "Epoch [8671/20000], Training Loss: 0.0840\n",
            "Epoch [8672/20000], Training Loss: 0.0752\n",
            "Epoch [8673/20000], Training Loss: 0.0820\n",
            "Epoch [8674/20000], Training Loss: 0.0839\n",
            "Epoch [8675/20000], Training Loss: 0.0751\n",
            "Epoch [8676/20000], Training Loss: 0.0868\n",
            "Epoch [8677/20000], Training Loss: 0.0783\n",
            "Epoch [8678/20000], Training Loss: 0.0775\n",
            "Epoch [8679/20000], Training Loss: 0.0784\n",
            "Epoch [8680/20000], Training Loss: 0.0787\n",
            "Epoch [8681/20000], Training Loss: 0.0773\n",
            "Epoch [8682/20000], Training Loss: 0.0813\n",
            "Epoch [8683/20000], Training Loss: 0.0875\n",
            "Epoch [8684/20000], Training Loss: 0.0813\n",
            "Epoch [8685/20000], Training Loss: 0.0790\n",
            "Epoch [8686/20000], Training Loss: 0.0772\n",
            "Epoch [8687/20000], Training Loss: 0.0808\n",
            "Epoch [8688/20000], Training Loss: 0.0746\n",
            "Epoch [8689/20000], Training Loss: 0.0848\n",
            "Epoch [8690/20000], Training Loss: 0.0750\n",
            "Epoch [8691/20000], Training Loss: 0.0779\n",
            "Epoch [8692/20000], Training Loss: 0.0755\n",
            "Epoch [8693/20000], Training Loss: 0.0799\n",
            "Epoch [8694/20000], Training Loss: 0.0814\n",
            "Epoch [8695/20000], Training Loss: 0.0795\n",
            "Epoch [8696/20000], Training Loss: 0.0855\n",
            "Epoch [8697/20000], Training Loss: 0.0820\n",
            "Epoch [8698/20000], Training Loss: 0.0843\n",
            "Epoch [8699/20000], Training Loss: 0.0858\n",
            "Epoch [8700/20000], Training Loss: 0.0773\n",
            "Epoch [8701/20000], Training Loss: 0.0775\n",
            "Epoch [8702/20000], Training Loss: 0.0785\n",
            "Epoch [8703/20000], Training Loss: 0.0784\n",
            "Epoch [8704/20000], Training Loss: 0.0791\n",
            "Epoch [8705/20000], Training Loss: 0.0759\n",
            "Epoch [8706/20000], Training Loss: 0.0829\n",
            "Epoch [8707/20000], Training Loss: 0.0843\n",
            "Epoch [8708/20000], Training Loss: 0.0765\n",
            "Epoch [8709/20000], Training Loss: 0.0802\n",
            "Epoch [8710/20000], Training Loss: 0.0736\n",
            "Epoch [8711/20000], Training Loss: 0.0779\n",
            "Epoch [8712/20000], Training Loss: 0.0767\n",
            "Epoch [8713/20000], Training Loss: 0.0802\n",
            "Epoch [8714/20000], Training Loss: 0.0785\n",
            "Epoch [8715/20000], Training Loss: 0.0794\n",
            "Epoch [8716/20000], Training Loss: 0.0749\n",
            "Epoch [8717/20000], Training Loss: 0.0857\n",
            "Epoch [8718/20000], Training Loss: 0.0742\n",
            "Epoch [8719/20000], Training Loss: 0.0851\n",
            "Epoch [8720/20000], Training Loss: 0.0753\n",
            "Epoch [8721/20000], Training Loss: 0.0855\n",
            "Epoch [8722/20000], Training Loss: 0.0823\n",
            "Epoch [8723/20000], Training Loss: 0.0777\n",
            "Epoch [8724/20000], Training Loss: 0.0786\n",
            "Epoch [8725/20000], Training Loss: 0.0759\n",
            "Epoch [8726/20000], Training Loss: 0.0798\n",
            "Epoch [8727/20000], Training Loss: 0.0801\n",
            "Epoch [8728/20000], Training Loss: 0.0790\n",
            "Epoch [8729/20000], Training Loss: 0.0795\n",
            "Epoch [8730/20000], Training Loss: 0.0761\n",
            "Epoch [8731/20000], Training Loss: 0.0811\n",
            "Epoch [8732/20000], Training Loss: 0.0781\n",
            "Epoch [8733/20000], Training Loss: 0.0801\n",
            "Epoch [8734/20000], Training Loss: 0.0787\n",
            "Epoch [8735/20000], Training Loss: 0.0789\n",
            "Epoch [8736/20000], Training Loss: 0.0731\n",
            "Epoch [8737/20000], Training Loss: 0.0804\n",
            "Epoch [8738/20000], Training Loss: 0.0783\n",
            "Epoch [8739/20000], Training Loss: 0.0767\n",
            "Epoch [8740/20000], Training Loss: 0.0822\n",
            "Epoch [8741/20000], Training Loss: 0.0803\n",
            "Epoch [8742/20000], Training Loss: 0.0787\n",
            "Epoch [8743/20000], Training Loss: 0.0796\n",
            "Epoch [8744/20000], Training Loss: 0.0814\n",
            "Epoch [8745/20000], Training Loss: 0.0758\n",
            "Epoch [8746/20000], Training Loss: 0.0847\n",
            "Epoch [8747/20000], Training Loss: 0.0739\n",
            "Epoch [8748/20000], Training Loss: 0.0792\n",
            "Epoch [8749/20000], Training Loss: 0.0824\n",
            "Epoch [8750/20000], Training Loss: 0.0788\n",
            "Epoch [8751/20000], Training Loss: 0.0794\n",
            "Epoch [8752/20000], Training Loss: 0.0806\n",
            "Epoch [8753/20000], Training Loss: 0.0737\n",
            "Epoch [8754/20000], Training Loss: 0.0794\n",
            "Epoch [8755/20000], Training Loss: 0.0836\n",
            "Epoch [8756/20000], Training Loss: 0.0801\n",
            "Epoch [8757/20000], Training Loss: 0.0798\n",
            "Epoch [8758/20000], Training Loss: 0.0837\n",
            "Epoch [8759/20000], Training Loss: 0.0773\n",
            "Epoch [8760/20000], Training Loss: 0.0788\n",
            "Epoch [8761/20000], Training Loss: 0.0778\n",
            "Epoch [8762/20000], Training Loss: 0.0786\n",
            "Epoch [8763/20000], Training Loss: 0.0798\n",
            "Epoch [8764/20000], Training Loss: 0.0747\n",
            "Epoch [8765/20000], Training Loss: 0.0756\n",
            "Epoch [8766/20000], Training Loss: 0.0821\n",
            "Epoch [8767/20000], Training Loss: 0.0797\n",
            "Epoch [8768/20000], Training Loss: 0.0790\n",
            "Epoch [8769/20000], Training Loss: 0.0817\n",
            "Epoch [8770/20000], Training Loss: 0.0771\n",
            "Epoch [8771/20000], Training Loss: 0.0833\n",
            "Epoch [8772/20000], Training Loss: 0.0799\n",
            "Epoch [8773/20000], Training Loss: 0.0742\n",
            "Epoch [8774/20000], Training Loss: 0.0809\n",
            "Epoch [8775/20000], Training Loss: 0.0791\n",
            "Epoch [8776/20000], Training Loss: 0.0830\n",
            "Epoch [8777/20000], Training Loss: 0.0841\n",
            "Epoch [8778/20000], Training Loss: 0.0755\n",
            "Epoch [8779/20000], Training Loss: 0.0789\n",
            "Epoch [8780/20000], Training Loss: 0.0801\n",
            "Epoch [8781/20000], Training Loss: 0.0754\n",
            "Epoch [8782/20000], Training Loss: 0.0740\n",
            "Epoch [8783/20000], Training Loss: 0.0835\n",
            "Epoch [8784/20000], Training Loss: 0.0809\n",
            "Epoch [8785/20000], Training Loss: 0.0878\n",
            "Epoch [8786/20000], Training Loss: 0.0856\n",
            "Epoch [8787/20000], Training Loss: 0.0853\n",
            "Epoch [8788/20000], Training Loss: 0.0766\n",
            "Epoch [8789/20000], Training Loss: 0.0784\n",
            "Epoch [8790/20000], Training Loss: 0.0738\n",
            "Epoch [8791/20000], Training Loss: 0.0848\n",
            "Epoch [8792/20000], Training Loss: 0.0811\n",
            "Epoch [8793/20000], Training Loss: 0.0848\n",
            "Epoch [8794/20000], Training Loss: 0.0806\n",
            "Epoch [8795/20000], Training Loss: 0.0793\n",
            "Epoch [8796/20000], Training Loss: 0.0751\n",
            "Epoch [8797/20000], Training Loss: 0.0802\n",
            "Epoch [8798/20000], Training Loss: 0.0763\n",
            "Epoch [8799/20000], Training Loss: 0.0800\n",
            "Epoch [8800/20000], Training Loss: 0.0805\n",
            "Epoch [8801/20000], Training Loss: 0.0826\n",
            "Epoch [8802/20000], Training Loss: 0.0752\n",
            "Epoch [8803/20000], Training Loss: 0.0815\n",
            "Epoch [8804/20000], Training Loss: 0.0751\n",
            "Epoch [8805/20000], Training Loss: 0.0809\n",
            "Epoch [8806/20000], Training Loss: 0.0802\n",
            "Epoch [8807/20000], Training Loss: 0.0829\n",
            "Epoch [8808/20000], Training Loss: 0.0795\n",
            "Epoch [8809/20000], Training Loss: 0.0791\n",
            "Epoch [8810/20000], Training Loss: 0.0774\n",
            "Epoch [8811/20000], Training Loss: 0.0845\n",
            "Epoch [8812/20000], Training Loss: 0.0784\n",
            "Epoch [8813/20000], Training Loss: 0.0753\n",
            "Epoch [8814/20000], Training Loss: 0.0839\n",
            "Epoch [8815/20000], Training Loss: 0.0825\n",
            "Epoch [8816/20000], Training Loss: 0.0804\n",
            "Epoch [8817/20000], Training Loss: 0.0820\n",
            "Epoch [8818/20000], Training Loss: 0.0782\n",
            "Epoch [8819/20000], Training Loss: 0.0798\n",
            "Epoch [8820/20000], Training Loss: 0.0858\n",
            "Epoch [8821/20000], Training Loss: 0.0886\n",
            "Epoch [8822/20000], Training Loss: 0.0787\n",
            "Epoch [8823/20000], Training Loss: 0.0729\n",
            "Epoch [8824/20000], Training Loss: 0.0805\n",
            "Epoch [8825/20000], Training Loss: 0.0775\n",
            "Epoch [8826/20000], Training Loss: 0.0810\n",
            "Epoch [8827/20000], Training Loss: 0.0801\n",
            "Epoch [8828/20000], Training Loss: 0.0854\n",
            "Epoch [8829/20000], Training Loss: 0.0767\n",
            "Epoch [8830/20000], Training Loss: 0.0781\n",
            "Epoch [8831/20000], Training Loss: 0.0845\n",
            "Epoch [8832/20000], Training Loss: 0.0763\n",
            "Epoch [8833/20000], Training Loss: 0.0825\n",
            "Epoch [8834/20000], Training Loss: 0.0794\n",
            "Epoch [8835/20000], Training Loss: 0.0854\n",
            "Epoch [8836/20000], Training Loss: 0.0810\n",
            "Epoch [8837/20000], Training Loss: 0.0856\n",
            "Epoch [8838/20000], Training Loss: 0.0757\n",
            "Epoch [8839/20000], Training Loss: 0.0802\n",
            "Epoch [8840/20000], Training Loss: 0.0773\n",
            "Epoch [8841/20000], Training Loss: 0.0854\n",
            "Epoch [8842/20000], Training Loss: 0.0755\n",
            "Epoch [8843/20000], Training Loss: 0.0780\n",
            "Epoch [8844/20000], Training Loss: 0.0801\n",
            "Epoch [8845/20000], Training Loss: 0.0793\n",
            "Epoch [8846/20000], Training Loss: 0.0817\n",
            "Epoch [8847/20000], Training Loss: 0.0746\n",
            "Epoch [8848/20000], Training Loss: 0.0810\n",
            "Epoch [8849/20000], Training Loss: 0.0796\n",
            "Epoch [8850/20000], Training Loss: 0.0735\n",
            "Epoch [8851/20000], Training Loss: 0.0751\n",
            "Epoch [8852/20000], Training Loss: 0.0877\n",
            "Epoch [8853/20000], Training Loss: 0.0749\n",
            "Epoch [8854/20000], Training Loss: 0.0744\n",
            "Epoch [8855/20000], Training Loss: 0.0801\n",
            "Epoch [8856/20000], Training Loss: 0.0816\n",
            "Epoch [8857/20000], Training Loss: 0.0750\n",
            "Epoch [8858/20000], Training Loss: 0.0750\n",
            "Epoch [8859/20000], Training Loss: 0.0781\n",
            "Epoch [8860/20000], Training Loss: 0.0816\n",
            "Epoch [8861/20000], Training Loss: 0.0780\n",
            "Epoch [8862/20000], Training Loss: 0.0836\n",
            "Epoch [8863/20000], Training Loss: 0.0815\n",
            "Epoch [8864/20000], Training Loss: 0.0814\n",
            "Epoch [8865/20000], Training Loss: 0.0849\n",
            "Epoch [8866/20000], Training Loss: 0.0870\n",
            "Epoch [8867/20000], Training Loss: 0.0838\n",
            "Epoch [8868/20000], Training Loss: 0.0766\n",
            "Epoch [8869/20000], Training Loss: 0.0822\n",
            "Epoch [8870/20000], Training Loss: 0.0775\n",
            "Epoch [8871/20000], Training Loss: 0.0803\n",
            "Epoch [8872/20000], Training Loss: 0.0825\n",
            "Epoch [8873/20000], Training Loss: 0.0753\n",
            "Epoch [8874/20000], Training Loss: 0.0837\n",
            "Epoch [8875/20000], Training Loss: 0.0812\n",
            "Epoch [8876/20000], Training Loss: 0.0837\n",
            "Epoch [8877/20000], Training Loss: 0.0810\n",
            "Epoch [8878/20000], Training Loss: 0.0750\n",
            "Epoch [8879/20000], Training Loss: 0.0789\n",
            "Epoch [8880/20000], Training Loss: 0.0758\n",
            "Epoch [8881/20000], Training Loss: 0.0746\n",
            "Epoch [8882/20000], Training Loss: 0.0737\n",
            "Epoch [8883/20000], Training Loss: 0.0825\n",
            "Epoch [8884/20000], Training Loss: 0.0808\n",
            "Epoch [8885/20000], Training Loss: 0.0829\n",
            "Epoch [8886/20000], Training Loss: 0.0745\n",
            "Epoch [8887/20000], Training Loss: 0.0722\n",
            "Epoch [8888/20000], Training Loss: 0.0791\n",
            "Epoch [8889/20000], Training Loss: 0.0816\n",
            "Epoch [8890/20000], Training Loss: 0.0750\n",
            "Epoch [8891/20000], Training Loss: 0.0798\n",
            "Epoch [8892/20000], Training Loss: 0.0816\n",
            "Epoch [8893/20000], Training Loss: 0.0761\n",
            "Epoch [8894/20000], Training Loss: 0.0769\n",
            "Epoch [8895/20000], Training Loss: 0.0815\n",
            "Epoch [8896/20000], Training Loss: 0.0787\n",
            "Epoch [8897/20000], Training Loss: 0.0758\n",
            "Epoch [8898/20000], Training Loss: 0.0766\n",
            "Epoch [8899/20000], Training Loss: 0.0771\n",
            "Epoch [8900/20000], Training Loss: 0.0835\n",
            "Epoch [8901/20000], Training Loss: 0.0773\n",
            "Epoch [8902/20000], Training Loss: 0.0735\n",
            "Epoch [8903/20000], Training Loss: 0.0809\n",
            "Epoch [8904/20000], Training Loss: 0.0858\n",
            "Epoch [8905/20000], Training Loss: 0.0728\n",
            "Epoch [8906/20000], Training Loss: 0.0799\n",
            "Epoch [8907/20000], Training Loss: 0.0743\n",
            "Epoch [8908/20000], Training Loss: 0.0848\n",
            "Epoch [8909/20000], Training Loss: 0.0795\n",
            "Epoch [8910/20000], Training Loss: 0.0763\n",
            "Epoch [8911/20000], Training Loss: 0.0769\n",
            "Epoch [8912/20000], Training Loss: 0.0769\n",
            "Epoch [8913/20000], Training Loss: 0.0806\n",
            "Epoch [8914/20000], Training Loss: 0.0793\n",
            "Epoch [8915/20000], Training Loss: 0.0768\n",
            "Epoch [8916/20000], Training Loss: 0.0809\n",
            "Epoch [8917/20000], Training Loss: 0.0823\n",
            "Epoch [8918/20000], Training Loss: 0.0837\n",
            "Epoch [8919/20000], Training Loss: 0.0738\n",
            "Epoch [8920/20000], Training Loss: 0.0762\n",
            "Epoch [8921/20000], Training Loss: 0.0745\n",
            "Epoch [8922/20000], Training Loss: 0.0752\n",
            "Epoch [8923/20000], Training Loss: 0.0864\n",
            "Epoch [8924/20000], Training Loss: 0.0826\n",
            "Epoch [8925/20000], Training Loss: 0.0796\n",
            "Epoch [8926/20000], Training Loss: 0.0792\n",
            "Epoch [8927/20000], Training Loss: 0.0787\n",
            "Epoch [8928/20000], Training Loss: 0.0861\n",
            "Epoch [8929/20000], Training Loss: 0.0809\n",
            "Epoch [8930/20000], Training Loss: 0.0833\n",
            "Epoch [8931/20000], Training Loss: 0.0805\n",
            "Epoch [8932/20000], Training Loss: 0.0741\n",
            "Epoch [8933/20000], Training Loss: 0.0803\n",
            "Epoch [8934/20000], Training Loss: 0.0856\n",
            "Epoch [8935/20000], Training Loss: 0.0825\n",
            "Epoch [8936/20000], Training Loss: 0.0831\n",
            "Epoch [8937/20000], Training Loss: 0.0750\n",
            "Epoch [8938/20000], Training Loss: 0.0817\n",
            "Epoch [8939/20000], Training Loss: 0.0823\n",
            "Epoch [8940/20000], Training Loss: 0.0804\n",
            "Epoch [8941/20000], Training Loss: 0.0795\n",
            "Epoch [8942/20000], Training Loss: 0.0858\n",
            "Epoch [8943/20000], Training Loss: 0.0746\n",
            "Epoch [8944/20000], Training Loss: 0.0789\n",
            "Epoch [8945/20000], Training Loss: 0.0867\n",
            "Epoch [8946/20000], Training Loss: 0.0789\n",
            "Epoch [8947/20000], Training Loss: 0.0811\n",
            "Epoch [8948/20000], Training Loss: 0.0838\n",
            "Epoch [8949/20000], Training Loss: 0.0814\n",
            "Epoch [8950/20000], Training Loss: 0.0778\n",
            "Epoch [8951/20000], Training Loss: 0.0831\n",
            "Epoch [8952/20000], Training Loss: 0.0785\n",
            "Epoch [8953/20000], Training Loss: 0.0768\n",
            "Epoch [8954/20000], Training Loss: 0.0863\n",
            "Epoch [8955/20000], Training Loss: 0.0858\n",
            "Epoch [8956/20000], Training Loss: 0.0787\n",
            "Epoch [8957/20000], Training Loss: 0.0777\n",
            "Epoch [8958/20000], Training Loss: 0.0757\n",
            "Epoch [8959/20000], Training Loss: 0.0822\n",
            "Epoch [8960/20000], Training Loss: 0.0802\n",
            "Epoch [8961/20000], Training Loss: 0.0778\n",
            "Epoch [8962/20000], Training Loss: 0.0748\n",
            "Epoch [8963/20000], Training Loss: 0.0752\n",
            "Epoch [8964/20000], Training Loss: 0.0813\n",
            "Epoch [8965/20000], Training Loss: 0.0825\n",
            "Epoch [8966/20000], Training Loss: 0.0852\n",
            "Epoch [8967/20000], Training Loss: 0.0761\n",
            "Epoch [8968/20000], Training Loss: 0.0815\n",
            "Epoch [8969/20000], Training Loss: 0.0802\n",
            "Epoch [8970/20000], Training Loss: 0.0816\n",
            "Epoch [8971/20000], Training Loss: 0.0813\n",
            "Epoch [8972/20000], Training Loss: 0.0808\n",
            "Epoch [8973/20000], Training Loss: 0.0826\n",
            "Epoch [8974/20000], Training Loss: 0.0834\n",
            "Epoch [8975/20000], Training Loss: 0.0739\n",
            "Epoch [8976/20000], Training Loss: 0.0727\n",
            "Epoch [8977/20000], Training Loss: 0.0804\n",
            "Epoch [8978/20000], Training Loss: 0.0855\n",
            "Epoch [8979/20000], Training Loss: 0.0761\n",
            "Epoch [8980/20000], Training Loss: 0.0782\n",
            "Epoch [8981/20000], Training Loss: 0.0778\n",
            "Epoch [8982/20000], Training Loss: 0.0740\n",
            "Epoch [8983/20000], Training Loss: 0.0859\n",
            "Epoch [8984/20000], Training Loss: 0.0777\n",
            "Epoch [8985/20000], Training Loss: 0.0764\n",
            "Epoch [8986/20000], Training Loss: 0.0802\n",
            "Epoch [8987/20000], Training Loss: 0.0737\n",
            "Epoch [8988/20000], Training Loss: 0.0788\n",
            "Epoch [8989/20000], Training Loss: 0.0817\n",
            "Epoch [8990/20000], Training Loss: 0.0757\n",
            "Epoch [8991/20000], Training Loss: 0.0795\n",
            "Epoch [8992/20000], Training Loss: 0.0833\n",
            "Epoch [8993/20000], Training Loss: 0.0751\n",
            "Epoch [8994/20000], Training Loss: 0.0784\n",
            "Epoch [8995/20000], Training Loss: 0.0796\n",
            "Epoch [8996/20000], Training Loss: 0.0819\n",
            "Epoch [8997/20000], Training Loss: 0.0823\n",
            "Epoch [8998/20000], Training Loss: 0.0800\n",
            "Epoch [8999/20000], Training Loss: 0.0746\n",
            "Epoch [9000/20000], Training Loss: 0.0796\n",
            "Epoch [9001/20000], Training Loss: 0.0780\n",
            "Epoch [9002/20000], Training Loss: 0.0819\n",
            "Epoch [9003/20000], Training Loss: 0.0755\n",
            "Epoch [9004/20000], Training Loss: 0.0821\n",
            "Epoch [9005/20000], Training Loss: 0.0794\n",
            "Epoch [9006/20000], Training Loss: 0.0766\n",
            "Epoch [9007/20000], Training Loss: 0.0747\n",
            "Epoch [9008/20000], Training Loss: 0.0796\n",
            "Epoch [9009/20000], Training Loss: 0.0743\n",
            "Epoch [9010/20000], Training Loss: 0.0772\n",
            "Epoch [9011/20000], Training Loss: 0.0779\n",
            "Epoch [9012/20000], Training Loss: 0.0858\n",
            "Epoch [9013/20000], Training Loss: 0.0783\n",
            "Epoch [9014/20000], Training Loss: 0.0840\n",
            "Epoch [9015/20000], Training Loss: 0.0855\n",
            "Epoch [9016/20000], Training Loss: 0.0789\n",
            "Epoch [9017/20000], Training Loss: 0.0779\n",
            "Epoch [9018/20000], Training Loss: 0.0785\n",
            "Epoch [9019/20000], Training Loss: 0.0826\n",
            "Epoch [9020/20000], Training Loss: 0.0801\n",
            "Epoch [9021/20000], Training Loss: 0.0765\n",
            "Epoch [9022/20000], Training Loss: 0.0744\n",
            "Epoch [9023/20000], Training Loss: 0.0792\n",
            "Epoch [9024/20000], Training Loss: 0.0836\n",
            "Epoch [9025/20000], Training Loss: 0.0795\n",
            "Epoch [9026/20000], Training Loss: 0.0815\n",
            "Epoch [9027/20000], Training Loss: 0.0798\n",
            "Epoch [9028/20000], Training Loss: 0.0804\n",
            "Epoch [9029/20000], Training Loss: 0.0841\n",
            "Epoch [9030/20000], Training Loss: 0.0752\n",
            "Epoch [9031/20000], Training Loss: 0.0807\n",
            "Epoch [9032/20000], Training Loss: 0.0799\n",
            "Epoch [9033/20000], Training Loss: 0.0835\n",
            "Epoch [9034/20000], Training Loss: 0.0838\n",
            "Epoch [9035/20000], Training Loss: 0.0739\n",
            "Epoch [9036/20000], Training Loss: 0.0826\n",
            "Epoch [9037/20000], Training Loss: 0.0736\n",
            "Epoch [9038/20000], Training Loss: 0.0816\n",
            "Epoch [9039/20000], Training Loss: 0.0744\n",
            "Epoch [9040/20000], Training Loss: 0.0829\n",
            "Epoch [9041/20000], Training Loss: 0.0748\n",
            "Epoch [9042/20000], Training Loss: 0.0750\n",
            "Epoch [9043/20000], Training Loss: 0.0750\n",
            "Epoch [9044/20000], Training Loss: 0.0770\n",
            "Epoch [9045/20000], Training Loss: 0.0749\n",
            "Epoch [9046/20000], Training Loss: 0.0817\n",
            "Epoch [9047/20000], Training Loss: 0.0809\n",
            "Epoch [9048/20000], Training Loss: 0.0759\n",
            "Epoch [9049/20000], Training Loss: 0.0842\n",
            "Epoch [9050/20000], Training Loss: 0.0763\n",
            "Epoch [9051/20000], Training Loss: 0.0822\n",
            "Epoch [9052/20000], Training Loss: 0.0781\n",
            "Epoch [9053/20000], Training Loss: 0.0734\n",
            "Epoch [9054/20000], Training Loss: 0.0735\n",
            "Epoch [9055/20000], Training Loss: 0.0786\n",
            "Epoch [9056/20000], Training Loss: 0.0804\n",
            "Epoch [9057/20000], Training Loss: 0.0805\n",
            "Epoch [9058/20000], Training Loss: 0.0772\n",
            "Epoch [9059/20000], Training Loss: 0.0835\n",
            "Epoch [9060/20000], Training Loss: 0.0765\n",
            "Epoch [9061/20000], Training Loss: 0.0775\n",
            "Epoch [9062/20000], Training Loss: 0.0793\n",
            "Epoch [9063/20000], Training Loss: 0.0793\n",
            "Epoch [9064/20000], Training Loss: 0.0785\n",
            "Epoch [9065/20000], Training Loss: 0.0853\n",
            "Epoch [9066/20000], Training Loss: 0.0842\n",
            "Epoch [9067/20000], Training Loss: 0.0724\n",
            "Epoch [9068/20000], Training Loss: 0.0837\n",
            "Epoch [9069/20000], Training Loss: 0.0832\n",
            "Epoch [9070/20000], Training Loss: 0.0814\n",
            "Epoch [9071/20000], Training Loss: 0.0863\n",
            "Epoch [9072/20000], Training Loss: 0.0829\n",
            "Epoch [9073/20000], Training Loss: 0.0835\n",
            "Epoch [9074/20000], Training Loss: 0.0783\n",
            "Epoch [9075/20000], Training Loss: 0.0813\n",
            "Epoch [9076/20000], Training Loss: 0.0846\n",
            "Epoch [9077/20000], Training Loss: 0.0772\n",
            "Epoch [9078/20000], Training Loss: 0.0794\n",
            "Epoch [9079/20000], Training Loss: 0.0808\n",
            "Epoch [9080/20000], Training Loss: 0.0790\n",
            "Epoch [9081/20000], Training Loss: 0.0795\n",
            "Epoch [9082/20000], Training Loss: 0.0786\n",
            "Epoch [9083/20000], Training Loss: 0.0836\n",
            "Epoch [9084/20000], Training Loss: 0.0834\n",
            "Epoch [9085/20000], Training Loss: 0.0841\n",
            "Epoch [9086/20000], Training Loss: 0.0795\n",
            "Epoch [9087/20000], Training Loss: 0.0803\n",
            "Epoch [9088/20000], Training Loss: 0.0835\n",
            "Epoch [9089/20000], Training Loss: 0.0733\n",
            "Epoch [9090/20000], Training Loss: 0.0807\n",
            "Epoch [9091/20000], Training Loss: 0.0785\n",
            "Epoch [9092/20000], Training Loss: 0.0839\n",
            "Epoch [9093/20000], Training Loss: 0.0760\n",
            "Epoch [9094/20000], Training Loss: 0.0835\n",
            "Epoch [9095/20000], Training Loss: 0.0786\n",
            "Epoch [9096/20000], Training Loss: 0.0843\n",
            "Epoch [9097/20000], Training Loss: 0.0811\n",
            "Epoch [9098/20000], Training Loss: 0.0829\n",
            "Epoch [9099/20000], Training Loss: 0.0757\n",
            "Epoch [9100/20000], Training Loss: 0.0776\n",
            "Epoch [9101/20000], Training Loss: 0.0816\n",
            "Epoch [9102/20000], Training Loss: 0.0766\n",
            "Epoch [9103/20000], Training Loss: 0.0789\n",
            "Epoch [9104/20000], Training Loss: 0.0825\n",
            "Epoch [9105/20000], Training Loss: 0.0803\n",
            "Epoch [9106/20000], Training Loss: 0.0781\n",
            "Epoch [9107/20000], Training Loss: 0.0839\n",
            "Epoch [9108/20000], Training Loss: 0.0800\n",
            "Epoch [9109/20000], Training Loss: 0.0742\n",
            "Epoch [9110/20000], Training Loss: 0.0767\n",
            "Epoch [9111/20000], Training Loss: 0.0802\n",
            "Epoch [9112/20000], Training Loss: 0.0870\n",
            "Epoch [9113/20000], Training Loss: 0.0768\n",
            "Epoch [9114/20000], Training Loss: 0.0779\n",
            "Epoch [9115/20000], Training Loss: 0.0793\n",
            "Epoch [9116/20000], Training Loss: 0.0815\n",
            "Epoch [9117/20000], Training Loss: 0.0776\n",
            "Epoch [9118/20000], Training Loss: 0.0817\n",
            "Epoch [9119/20000], Training Loss: 0.0804\n",
            "Epoch [9120/20000], Training Loss: 0.0791\n",
            "Epoch [9121/20000], Training Loss: 0.0840\n",
            "Epoch [9122/20000], Training Loss: 0.0854\n",
            "Epoch [9123/20000], Training Loss: 0.0787\n",
            "Epoch [9124/20000], Training Loss: 0.0866\n",
            "Epoch [9125/20000], Training Loss: 0.0805\n",
            "Epoch [9126/20000], Training Loss: 0.0757\n",
            "Epoch [9127/20000], Training Loss: 0.0835\n",
            "Epoch [9128/20000], Training Loss: 0.0789\n",
            "Epoch [9129/20000], Training Loss: 0.0728\n",
            "Epoch [9130/20000], Training Loss: 0.0807\n",
            "Epoch [9131/20000], Training Loss: 0.0809\n",
            "Epoch [9132/20000], Training Loss: 0.0806\n",
            "Epoch [9133/20000], Training Loss: 0.0844\n",
            "Epoch [9134/20000], Training Loss: 0.0817\n",
            "Epoch [9135/20000], Training Loss: 0.0858\n",
            "Epoch [9136/20000], Training Loss: 0.0833\n",
            "Epoch [9137/20000], Training Loss: 0.0828\n",
            "Epoch [9138/20000], Training Loss: 0.0879\n",
            "Epoch [9139/20000], Training Loss: 0.0781\n",
            "Epoch [9140/20000], Training Loss: 0.0828\n",
            "Epoch [9141/20000], Training Loss: 0.0814\n",
            "Epoch [9142/20000], Training Loss: 0.0770\n",
            "Epoch [9143/20000], Training Loss: 0.0804\n",
            "Epoch [9144/20000], Training Loss: 0.0760\n",
            "Epoch [9145/20000], Training Loss: 0.0773\n",
            "Epoch [9146/20000], Training Loss: 0.0806\n",
            "Epoch [9147/20000], Training Loss: 0.0731\n",
            "Epoch [9148/20000], Training Loss: 0.0857\n",
            "Epoch [9149/20000], Training Loss: 0.0826\n",
            "Epoch [9150/20000], Training Loss: 0.0819\n",
            "Epoch [9151/20000], Training Loss: 0.0781\n",
            "Epoch [9152/20000], Training Loss: 0.0772\n",
            "Epoch [9153/20000], Training Loss: 0.0842\n",
            "Epoch [9154/20000], Training Loss: 0.0797\n",
            "Epoch [9155/20000], Training Loss: 0.0743\n",
            "Epoch [9156/20000], Training Loss: 0.0828\n",
            "Epoch [9157/20000], Training Loss: 0.0810\n",
            "Epoch [9158/20000], Training Loss: 0.0806\n",
            "Epoch [9159/20000], Training Loss: 0.0819\n",
            "Epoch [9160/20000], Training Loss: 0.0820\n",
            "Epoch [9161/20000], Training Loss: 0.0807\n",
            "Epoch [9162/20000], Training Loss: 0.0836\n",
            "Epoch [9163/20000], Training Loss: 0.0858\n",
            "Epoch [9164/20000], Training Loss: 0.0740\n",
            "Epoch [9165/20000], Training Loss: 0.0793\n",
            "Epoch [9166/20000], Training Loss: 0.0831\n",
            "Epoch [9167/20000], Training Loss: 0.0831\n",
            "Epoch [9168/20000], Training Loss: 0.0755\n",
            "Epoch [9169/20000], Training Loss: 0.0813\n",
            "Epoch [9170/20000], Training Loss: 0.0747\n",
            "Epoch [9171/20000], Training Loss: 0.0829\n",
            "Epoch [9172/20000], Training Loss: 0.0825\n",
            "Epoch [9173/20000], Training Loss: 0.0806\n",
            "Epoch [9174/20000], Training Loss: 0.0829\n",
            "Epoch [9175/20000], Training Loss: 0.0796\n",
            "Epoch [9176/20000], Training Loss: 0.0814\n",
            "Epoch [9177/20000], Training Loss: 0.0795\n",
            "Epoch [9178/20000], Training Loss: 0.0756\n",
            "Epoch [9179/20000], Training Loss: 0.0799\n",
            "Epoch [9180/20000], Training Loss: 0.0794\n",
            "Epoch [9181/20000], Training Loss: 0.0786\n",
            "Epoch [9182/20000], Training Loss: 0.0831\n",
            "Epoch [9183/20000], Training Loss: 0.0732\n",
            "Epoch [9184/20000], Training Loss: 0.0737\n",
            "Epoch [9185/20000], Training Loss: 0.0811\n",
            "Epoch [9186/20000], Training Loss: 0.0807\n",
            "Epoch [9187/20000], Training Loss: 0.0776\n",
            "Epoch [9188/20000], Training Loss: 0.0788\n",
            "Epoch [9189/20000], Training Loss: 0.0826\n",
            "Epoch [9190/20000], Training Loss: 0.0847\n",
            "Epoch [9191/20000], Training Loss: 0.0729\n",
            "Epoch [9192/20000], Training Loss: 0.0811\n",
            "Epoch [9193/20000], Training Loss: 0.0831\n",
            "Epoch [9194/20000], Training Loss: 0.0778\n",
            "Epoch [9195/20000], Training Loss: 0.0756\n",
            "Epoch [9196/20000], Training Loss: 0.0814\n",
            "Epoch [9197/20000], Training Loss: 0.0778\n",
            "Epoch [9198/20000], Training Loss: 0.0797\n",
            "Epoch [9199/20000], Training Loss: 0.0810\n",
            "Epoch [9200/20000], Training Loss: 0.0753\n",
            "Epoch [9201/20000], Training Loss: 0.0859\n",
            "Epoch [9202/20000], Training Loss: 0.0803\n",
            "Epoch [9203/20000], Training Loss: 0.0785\n",
            "Epoch [9204/20000], Training Loss: 0.0784\n",
            "Epoch [9205/20000], Training Loss: 0.0806\n",
            "Epoch [9206/20000], Training Loss: 0.0754\n",
            "Epoch [9207/20000], Training Loss: 0.0770\n",
            "Epoch [9208/20000], Training Loss: 0.0776\n",
            "Epoch [9209/20000], Training Loss: 0.0848\n",
            "Epoch [9210/20000], Training Loss: 0.0778\n",
            "Epoch [9211/20000], Training Loss: 0.0813\n",
            "Epoch [9212/20000], Training Loss: 0.0792\n",
            "Epoch [9213/20000], Training Loss: 0.0806\n",
            "Epoch [9214/20000], Training Loss: 0.0721\n",
            "Epoch [9215/20000], Training Loss: 0.0773\n",
            "Epoch [9216/20000], Training Loss: 0.0786\n",
            "Epoch [9217/20000], Training Loss: 0.0785\n",
            "Epoch [9218/20000], Training Loss: 0.0759\n",
            "Epoch [9219/20000], Training Loss: 0.0746\n",
            "Epoch [9220/20000], Training Loss: 0.0795\n",
            "Epoch [9221/20000], Training Loss: 0.0802\n",
            "Epoch [9222/20000], Training Loss: 0.0796\n",
            "Epoch [9223/20000], Training Loss: 0.0791\n",
            "Epoch [9224/20000], Training Loss: 0.0771\n",
            "Epoch [9225/20000], Training Loss: 0.0794\n",
            "Epoch [9226/20000], Training Loss: 0.0873\n",
            "Epoch [9227/20000], Training Loss: 0.0781\n",
            "Epoch [9228/20000], Training Loss: 0.0769\n",
            "Epoch [9229/20000], Training Loss: 0.0785\n",
            "Epoch [9230/20000], Training Loss: 0.0792\n",
            "Epoch [9231/20000], Training Loss: 0.0769\n",
            "Epoch [9232/20000], Training Loss: 0.0848\n",
            "Epoch [9233/20000], Training Loss: 0.0848\n",
            "Epoch [9234/20000], Training Loss: 0.0769\n",
            "Epoch [9235/20000], Training Loss: 0.0822\n",
            "Epoch [9236/20000], Training Loss: 0.0780\n",
            "Epoch [9237/20000], Training Loss: 0.0862\n",
            "Epoch [9238/20000], Training Loss: 0.0840\n",
            "Epoch [9239/20000], Training Loss: 0.0758\n",
            "Epoch [9240/20000], Training Loss: 0.0867\n",
            "Epoch [9241/20000], Training Loss: 0.0806\n",
            "Epoch [9242/20000], Training Loss: 0.0858\n",
            "Epoch [9243/20000], Training Loss: 0.0762\n",
            "Epoch [9244/20000], Training Loss: 0.0799\n",
            "Epoch [9245/20000], Training Loss: 0.0820\n",
            "Epoch [9246/20000], Training Loss: 0.0771\n",
            "Epoch [9247/20000], Training Loss: 0.0823\n",
            "Epoch [9248/20000], Training Loss: 0.0798\n",
            "Epoch [9249/20000], Training Loss: 0.0754\n",
            "Epoch [9250/20000], Training Loss: 0.0738\n",
            "Epoch [9251/20000], Training Loss: 0.0803\n",
            "Epoch [9252/20000], Training Loss: 0.0842\n",
            "Epoch [9253/20000], Training Loss: 0.0825\n",
            "Epoch [9254/20000], Training Loss: 0.0846\n",
            "Epoch [9255/20000], Training Loss: 0.0776\n",
            "Epoch [9256/20000], Training Loss: 0.0830\n",
            "Epoch [9257/20000], Training Loss: 0.0777\n",
            "Epoch [9258/20000], Training Loss: 0.0789\n",
            "Epoch [9259/20000], Training Loss: 0.0778\n",
            "Epoch [9260/20000], Training Loss: 0.0822\n",
            "Epoch [9261/20000], Training Loss: 0.0765\n",
            "Epoch [9262/20000], Training Loss: 0.0864\n",
            "Epoch [9263/20000], Training Loss: 0.0746\n",
            "Epoch [9264/20000], Training Loss: 0.0794\n",
            "Epoch [9265/20000], Training Loss: 0.0789\n",
            "Epoch [9266/20000], Training Loss: 0.0819\n",
            "Epoch [9267/20000], Training Loss: 0.0803\n",
            "Epoch [9268/20000], Training Loss: 0.0847\n",
            "Epoch [9269/20000], Training Loss: 0.0820\n",
            "Epoch [9270/20000], Training Loss: 0.0768\n",
            "Epoch [9271/20000], Training Loss: 0.0763\n",
            "Epoch [9272/20000], Training Loss: 0.0783\n",
            "Epoch [9273/20000], Training Loss: 0.0777\n",
            "Epoch [9274/20000], Training Loss: 0.0833\n",
            "Epoch [9275/20000], Training Loss: 0.0813\n",
            "Epoch [9276/20000], Training Loss: 0.0789\n",
            "Epoch [9277/20000], Training Loss: 0.0800\n",
            "Epoch [9278/20000], Training Loss: 0.0738\n",
            "Epoch [9279/20000], Training Loss: 0.0801\n",
            "Epoch [9280/20000], Training Loss: 0.0810\n",
            "Epoch [9281/20000], Training Loss: 0.0796\n",
            "Epoch [9282/20000], Training Loss: 0.0781\n",
            "Epoch [9283/20000], Training Loss: 0.0801\n",
            "Epoch [9284/20000], Training Loss: 0.0856\n",
            "Epoch [9285/20000], Training Loss: 0.0763\n",
            "Epoch [9286/20000], Training Loss: 0.0770\n",
            "Epoch [9287/20000], Training Loss: 0.0791\n",
            "Epoch [9288/20000], Training Loss: 0.0843\n",
            "Epoch [9289/20000], Training Loss: 0.0760\n",
            "Epoch [9290/20000], Training Loss: 0.0770\n",
            "Epoch [9291/20000], Training Loss: 0.0804\n",
            "Epoch [9292/20000], Training Loss: 0.0800\n",
            "Epoch [9293/20000], Training Loss: 0.0800\n",
            "Epoch [9294/20000], Training Loss: 0.0786\n",
            "Epoch [9295/20000], Training Loss: 0.0794\n",
            "Epoch [9296/20000], Training Loss: 0.0750\n",
            "Epoch [9297/20000], Training Loss: 0.0785\n",
            "Epoch [9298/20000], Training Loss: 0.0747\n",
            "Epoch [9299/20000], Training Loss: 0.0803\n",
            "Epoch [9300/20000], Training Loss: 0.0790\n",
            "Epoch [9301/20000], Training Loss: 0.0811\n",
            "Epoch [9302/20000], Training Loss: 0.0805\n",
            "Epoch [9303/20000], Training Loss: 0.0811\n",
            "Epoch [9304/20000], Training Loss: 0.0806\n",
            "Epoch [9305/20000], Training Loss: 0.0808\n",
            "Epoch [9306/20000], Training Loss: 0.0786\n",
            "Epoch [9307/20000], Training Loss: 0.0852\n",
            "Epoch [9308/20000], Training Loss: 0.0755\n",
            "Epoch [9309/20000], Training Loss: 0.0776\n",
            "Epoch [9310/20000], Training Loss: 0.0787\n",
            "Epoch [9311/20000], Training Loss: 0.0809\n",
            "Epoch [9312/20000], Training Loss: 0.0819\n",
            "Epoch [9313/20000], Training Loss: 0.0790\n",
            "Epoch [9314/20000], Training Loss: 0.0776\n",
            "Epoch [9315/20000], Training Loss: 0.0874\n",
            "Epoch [9316/20000], Training Loss: 0.0735\n",
            "Epoch [9317/20000], Training Loss: 0.0750\n",
            "Epoch [9318/20000], Training Loss: 0.0778\n",
            "Epoch [9319/20000], Training Loss: 0.0829\n",
            "Epoch [9320/20000], Training Loss: 0.0788\n",
            "Epoch [9321/20000], Training Loss: 0.0786\n",
            "Epoch [9322/20000], Training Loss: 0.0769\n",
            "Epoch [9323/20000], Training Loss: 0.0812\n",
            "Epoch [9324/20000], Training Loss: 0.0792\n",
            "Epoch [9325/20000], Training Loss: 0.0805\n",
            "Epoch [9326/20000], Training Loss: 0.0769\n",
            "Epoch [9327/20000], Training Loss: 0.0844\n",
            "Epoch [9328/20000], Training Loss: 0.0784\n",
            "Epoch [9329/20000], Training Loss: 0.0788\n",
            "Epoch [9330/20000], Training Loss: 0.0807\n",
            "Epoch [9331/20000], Training Loss: 0.0785\n",
            "Epoch [9332/20000], Training Loss: 0.0786\n",
            "Epoch [9333/20000], Training Loss: 0.0805\n",
            "Epoch [9334/20000], Training Loss: 0.0778\n",
            "Epoch [9335/20000], Training Loss: 0.0765\n",
            "Epoch [9336/20000], Training Loss: 0.0800\n",
            "Epoch [9337/20000], Training Loss: 0.0819\n",
            "Epoch [9338/20000], Training Loss: 0.0753\n",
            "Epoch [9339/20000], Training Loss: 0.0789\n",
            "Epoch [9340/20000], Training Loss: 0.0754\n",
            "Epoch [9341/20000], Training Loss: 0.0838\n",
            "Epoch [9342/20000], Training Loss: 0.0787\n",
            "Epoch [9343/20000], Training Loss: 0.0767\n",
            "Epoch [9344/20000], Training Loss: 0.0865\n",
            "Epoch [9345/20000], Training Loss: 0.0754\n",
            "Epoch [9346/20000], Training Loss: 0.0811\n",
            "Epoch [9347/20000], Training Loss: 0.0818\n",
            "Epoch [9348/20000], Training Loss: 0.0825\n",
            "Epoch [9349/20000], Training Loss: 0.0786\n",
            "Epoch [9350/20000], Training Loss: 0.0804\n",
            "Epoch [9351/20000], Training Loss: 0.0775\n",
            "Epoch [9352/20000], Training Loss: 0.0830\n",
            "Epoch [9353/20000], Training Loss: 0.0781\n",
            "Epoch [9354/20000], Training Loss: 0.0753\n",
            "Epoch [9355/20000], Training Loss: 0.0744\n",
            "Epoch [9356/20000], Training Loss: 0.0784\n",
            "Epoch [9357/20000], Training Loss: 0.0831\n",
            "Epoch [9358/20000], Training Loss: 0.0734\n",
            "Epoch [9359/20000], Training Loss: 0.0855\n",
            "Epoch [9360/20000], Training Loss: 0.0820\n",
            "Epoch [9361/20000], Training Loss: 0.0769\n",
            "Epoch [9362/20000], Training Loss: 0.0785\n",
            "Epoch [9363/20000], Training Loss: 0.0843\n",
            "Epoch [9364/20000], Training Loss: 0.0884\n",
            "Epoch [9365/20000], Training Loss: 0.0779\n",
            "Epoch [9366/20000], Training Loss: 0.0812\n",
            "Epoch [9367/20000], Training Loss: 0.0837\n",
            "Epoch [9368/20000], Training Loss: 0.0763\n",
            "Epoch [9369/20000], Training Loss: 0.0846\n",
            "Epoch [9370/20000], Training Loss: 0.0840\n",
            "Epoch [9371/20000], Training Loss: 0.0838\n",
            "Epoch [9372/20000], Training Loss: 0.0775\n",
            "Epoch [9373/20000], Training Loss: 0.0802\n",
            "Epoch [9374/20000], Training Loss: 0.0810\n",
            "Epoch [9375/20000], Training Loss: 0.0810\n",
            "Epoch [9376/20000], Training Loss: 0.0819\n",
            "Epoch [9377/20000], Training Loss: 0.0808\n",
            "Epoch [9378/20000], Training Loss: 0.0822\n",
            "Epoch [9379/20000], Training Loss: 0.0752\n",
            "Epoch [9380/20000], Training Loss: 0.0797\n",
            "Epoch [9381/20000], Training Loss: 0.0762\n",
            "Epoch [9382/20000], Training Loss: 0.0832\n",
            "Epoch [9383/20000], Training Loss: 0.0801\n",
            "Epoch [9384/20000], Training Loss: 0.0858\n",
            "Epoch [9385/20000], Training Loss: 0.0741\n",
            "Epoch [9386/20000], Training Loss: 0.0790\n",
            "Epoch [9387/20000], Training Loss: 0.0810\n",
            "Epoch [9388/20000], Training Loss: 0.0789\n",
            "Epoch [9389/20000], Training Loss: 0.0839\n",
            "Epoch [9390/20000], Training Loss: 0.0809\n",
            "Epoch [9391/20000], Training Loss: 0.0778\n",
            "Epoch [9392/20000], Training Loss: 0.0826\n",
            "Epoch [9393/20000], Training Loss: 0.0738\n",
            "Epoch [9394/20000], Training Loss: 0.0750\n",
            "Epoch [9395/20000], Training Loss: 0.0817\n",
            "Epoch [9396/20000], Training Loss: 0.0738\n",
            "Epoch [9397/20000], Training Loss: 0.0714\n",
            "Epoch [9398/20000], Training Loss: 0.0801\n",
            "Epoch [9399/20000], Training Loss: 0.0844\n",
            "Epoch [9400/20000], Training Loss: 0.0854\n",
            "Epoch [9401/20000], Training Loss: 0.0751\n",
            "Epoch [9402/20000], Training Loss: 0.0811\n",
            "Epoch [9403/20000], Training Loss: 0.0733\n",
            "Epoch [9404/20000], Training Loss: 0.0852\n",
            "Epoch [9405/20000], Training Loss: 0.0781\n",
            "Epoch [9406/20000], Training Loss: 0.0836\n",
            "Epoch [9407/20000], Training Loss: 0.0801\n",
            "Epoch [9408/20000], Training Loss: 0.0845\n",
            "Epoch [9409/20000], Training Loss: 0.0874\n",
            "Epoch [9410/20000], Training Loss: 0.0827\n",
            "Epoch [9411/20000], Training Loss: 0.0780\n",
            "Epoch [9412/20000], Training Loss: 0.0783\n",
            "Epoch [9413/20000], Training Loss: 0.0819\n",
            "Epoch [9414/20000], Training Loss: 0.0762\n",
            "Epoch [9415/20000], Training Loss: 0.0819\n",
            "Epoch [9416/20000], Training Loss: 0.0871\n",
            "Epoch [9417/20000], Training Loss: 0.0782\n",
            "Epoch [9418/20000], Training Loss: 0.0783\n",
            "Epoch [9419/20000], Training Loss: 0.0746\n",
            "Epoch [9420/20000], Training Loss: 0.0751\n",
            "Epoch [9421/20000], Training Loss: 0.0759\n",
            "Epoch [9422/20000], Training Loss: 0.0865\n",
            "Epoch [9423/20000], Training Loss: 0.0793\n",
            "Epoch [9424/20000], Training Loss: 0.0758\n",
            "Epoch [9425/20000], Training Loss: 0.0789\n",
            "Epoch [9426/20000], Training Loss: 0.0813\n",
            "Epoch [9427/20000], Training Loss: 0.0819\n",
            "Epoch [9428/20000], Training Loss: 0.0795\n",
            "Epoch [9429/20000], Training Loss: 0.0803\n",
            "Epoch [9430/20000], Training Loss: 0.0773\n",
            "Epoch [9431/20000], Training Loss: 0.0788\n",
            "Epoch [9432/20000], Training Loss: 0.0816\n",
            "Epoch [9433/20000], Training Loss: 0.0819\n",
            "Epoch [9434/20000], Training Loss: 0.0785\n",
            "Epoch [9435/20000], Training Loss: 0.0797\n",
            "Epoch [9436/20000], Training Loss: 0.0795\n",
            "Epoch [9437/20000], Training Loss: 0.0799\n",
            "Epoch [9438/20000], Training Loss: 0.0781\n",
            "Epoch [9439/20000], Training Loss: 0.0792\n",
            "Epoch [9440/20000], Training Loss: 0.0767\n",
            "Epoch [9441/20000], Training Loss: 0.0834\n",
            "Epoch [9442/20000], Training Loss: 0.0808\n",
            "Epoch [9443/20000], Training Loss: 0.0726\n",
            "Epoch [9444/20000], Training Loss: 0.0769\n",
            "Epoch [9445/20000], Training Loss: 0.0859\n",
            "Epoch [9446/20000], Training Loss: 0.0849\n",
            "Epoch [9447/20000], Training Loss: 0.0746\n",
            "Epoch [9448/20000], Training Loss: 0.0765\n",
            "Epoch [9449/20000], Training Loss: 0.0831\n",
            "Epoch [9450/20000], Training Loss: 0.0811\n",
            "Epoch [9451/20000], Training Loss: 0.0744\n",
            "Epoch [9452/20000], Training Loss: 0.0801\n",
            "Epoch [9453/20000], Training Loss: 0.0816\n",
            "Epoch [9454/20000], Training Loss: 0.0807\n",
            "Epoch [9455/20000], Training Loss: 0.0789\n",
            "Epoch [9456/20000], Training Loss: 0.0771\n",
            "Epoch [9457/20000], Training Loss: 0.0786\n",
            "Epoch [9458/20000], Training Loss: 0.0790\n",
            "Epoch [9459/20000], Training Loss: 0.0778\n",
            "Epoch [9460/20000], Training Loss: 0.0848\n",
            "Epoch [9461/20000], Training Loss: 0.0762\n",
            "Epoch [9462/20000], Training Loss: 0.0745\n",
            "Epoch [9463/20000], Training Loss: 0.0738\n",
            "Epoch [9464/20000], Training Loss: 0.0760\n",
            "Epoch [9465/20000], Training Loss: 0.0797\n",
            "Epoch [9466/20000], Training Loss: 0.0862\n",
            "Epoch [9467/20000], Training Loss: 0.0848\n",
            "Epoch [9468/20000], Training Loss: 0.0792\n",
            "Epoch [9469/20000], Training Loss: 0.0810\n",
            "Epoch [9470/20000], Training Loss: 0.0786\n",
            "Epoch [9471/20000], Training Loss: 0.0826\n",
            "Epoch [9472/20000], Training Loss: 0.0802\n",
            "Epoch [9473/20000], Training Loss: 0.0808\n",
            "Epoch [9474/20000], Training Loss: 0.0799\n",
            "Epoch [9475/20000], Training Loss: 0.0804\n",
            "Epoch [9476/20000], Training Loss: 0.0736\n",
            "Epoch [9477/20000], Training Loss: 0.0763\n",
            "Epoch [9478/20000], Training Loss: 0.0769\n",
            "Epoch [9479/20000], Training Loss: 0.0729\n",
            "Epoch [9480/20000], Training Loss: 0.0835\n",
            "Epoch [9481/20000], Training Loss: 0.0776\n",
            "Epoch [9482/20000], Training Loss: 0.0787\n",
            "Epoch [9483/20000], Training Loss: 0.0782\n",
            "Epoch [9484/20000], Training Loss: 0.0774\n",
            "Epoch [9485/20000], Training Loss: 0.0833\n",
            "Epoch [9486/20000], Training Loss: 0.0787\n",
            "Epoch [9487/20000], Training Loss: 0.0821\n",
            "Epoch [9488/20000], Training Loss: 0.0761\n",
            "Epoch [9489/20000], Training Loss: 0.0808\n",
            "Epoch [9490/20000], Training Loss: 0.0803\n",
            "Epoch [9491/20000], Training Loss: 0.0836\n",
            "Epoch [9492/20000], Training Loss: 0.0753\n",
            "Epoch [9493/20000], Training Loss: 0.0767\n",
            "Epoch [9494/20000], Training Loss: 0.0752\n",
            "Epoch [9495/20000], Training Loss: 0.0798\n",
            "Epoch [9496/20000], Training Loss: 0.0768\n",
            "Epoch [9497/20000], Training Loss: 0.0801\n",
            "Epoch [9498/20000], Training Loss: 0.0818\n",
            "Epoch [9499/20000], Training Loss: 0.0786\n",
            "Epoch [9500/20000], Training Loss: 0.0785\n",
            "Epoch [9501/20000], Training Loss: 0.0849\n",
            "Epoch [9502/20000], Training Loss: 0.0743\n",
            "Epoch [9503/20000], Training Loss: 0.0821\n",
            "Epoch [9504/20000], Training Loss: 0.0804\n",
            "Epoch [9505/20000], Training Loss: 0.0844\n",
            "Epoch [9506/20000], Training Loss: 0.0862\n",
            "Epoch [9507/20000], Training Loss: 0.0849\n",
            "Epoch [9508/20000], Training Loss: 0.0825\n",
            "Epoch [9509/20000], Training Loss: 0.0781\n",
            "Epoch [9510/20000], Training Loss: 0.0788\n",
            "Epoch [9511/20000], Training Loss: 0.0750\n",
            "Epoch [9512/20000], Training Loss: 0.0808\n",
            "Epoch [9513/20000], Training Loss: 0.0761\n",
            "Epoch [9514/20000], Training Loss: 0.0799\n",
            "Epoch [9515/20000], Training Loss: 0.0720\n",
            "Epoch [9516/20000], Training Loss: 0.0781\n",
            "Epoch [9517/20000], Training Loss: 0.0836\n",
            "Epoch [9518/20000], Training Loss: 0.0811\n",
            "Epoch [9519/20000], Training Loss: 0.0856\n",
            "Epoch [9520/20000], Training Loss: 0.0814\n",
            "Epoch [9521/20000], Training Loss: 0.0846\n",
            "Epoch [9522/20000], Training Loss: 0.0794\n",
            "Epoch [9523/20000], Training Loss: 0.0842\n",
            "Epoch [9524/20000], Training Loss: 0.0765\n",
            "Epoch [9525/20000], Training Loss: 0.0846\n",
            "Epoch [9526/20000], Training Loss: 0.0756\n",
            "Epoch [9527/20000], Training Loss: 0.0801\n",
            "Epoch [9528/20000], Training Loss: 0.0793\n",
            "Epoch [9529/20000], Training Loss: 0.0857\n",
            "Epoch [9530/20000], Training Loss: 0.0862\n",
            "Epoch [9531/20000], Training Loss: 0.0760\n",
            "Epoch [9532/20000], Training Loss: 0.0756\n",
            "Epoch [9533/20000], Training Loss: 0.0797\n",
            "Epoch [9534/20000], Training Loss: 0.0765\n",
            "Epoch [9535/20000], Training Loss: 0.0800\n",
            "Epoch [9536/20000], Training Loss: 0.0842\n",
            "Epoch [9537/20000], Training Loss: 0.0754\n",
            "Epoch [9538/20000], Training Loss: 0.0797\n",
            "Epoch [9539/20000], Training Loss: 0.0792\n",
            "Epoch [9540/20000], Training Loss: 0.0798\n",
            "Epoch [9541/20000], Training Loss: 0.0770\n",
            "Epoch [9542/20000], Training Loss: 0.0799\n",
            "Epoch [9543/20000], Training Loss: 0.0807\n",
            "Epoch [9544/20000], Training Loss: 0.0772\n",
            "Epoch [9545/20000], Training Loss: 0.0782\n",
            "Epoch [9546/20000], Training Loss: 0.0828\n",
            "Epoch [9547/20000], Training Loss: 0.0819\n",
            "Epoch [9548/20000], Training Loss: 0.0788\n",
            "Epoch [9549/20000], Training Loss: 0.0844\n",
            "Epoch [9550/20000], Training Loss: 0.0765\n",
            "Epoch [9551/20000], Training Loss: 0.0871\n",
            "Epoch [9552/20000], Training Loss: 0.0803\n",
            "Epoch [9553/20000], Training Loss: 0.0793\n",
            "Epoch [9554/20000], Training Loss: 0.0750\n",
            "Epoch [9555/20000], Training Loss: 0.0782\n",
            "Epoch [9556/20000], Training Loss: 0.0754\n",
            "Epoch [9557/20000], Training Loss: 0.0804\n",
            "Epoch [9558/20000], Training Loss: 0.0757\n",
            "Epoch [9559/20000], Training Loss: 0.0762\n",
            "Epoch [9560/20000], Training Loss: 0.0802\n",
            "Epoch [9561/20000], Training Loss: 0.0826\n",
            "Epoch [9562/20000], Training Loss: 0.0785\n",
            "Epoch [9563/20000], Training Loss: 0.0807\n",
            "Epoch [9564/20000], Training Loss: 0.0758\n",
            "Epoch [9565/20000], Training Loss: 0.0762\n",
            "Epoch [9566/20000], Training Loss: 0.0813\n",
            "Epoch [9567/20000], Training Loss: 0.0839\n",
            "Epoch [9568/20000], Training Loss: 0.0857\n",
            "Epoch [9569/20000], Training Loss: 0.0761\n",
            "Epoch [9570/20000], Training Loss: 0.0762\n",
            "Epoch [9571/20000], Training Loss: 0.0820\n",
            "Epoch [9572/20000], Training Loss: 0.0794\n",
            "Epoch [9573/20000], Training Loss: 0.0850\n",
            "Epoch [9574/20000], Training Loss: 0.0744\n",
            "Epoch [9575/20000], Training Loss: 0.0772\n",
            "Epoch [9576/20000], Training Loss: 0.0788\n",
            "Epoch [9577/20000], Training Loss: 0.0844\n",
            "Epoch [9578/20000], Training Loss: 0.0831\n",
            "Epoch [9579/20000], Training Loss: 0.0851\n",
            "Epoch [9580/20000], Training Loss: 0.0777\n",
            "Epoch [9581/20000], Training Loss: 0.0792\n",
            "Epoch [9582/20000], Training Loss: 0.0814\n",
            "Epoch [9583/20000], Training Loss: 0.0786\n",
            "Epoch [9584/20000], Training Loss: 0.0774\n",
            "Epoch [9585/20000], Training Loss: 0.0849\n",
            "Epoch [9586/20000], Training Loss: 0.0730\n",
            "Epoch [9587/20000], Training Loss: 0.0754\n",
            "Epoch [9588/20000], Training Loss: 0.0793\n",
            "Epoch [9589/20000], Training Loss: 0.0738\n",
            "Epoch [9590/20000], Training Loss: 0.0832\n",
            "Epoch [9591/20000], Training Loss: 0.0798\n",
            "Epoch [9592/20000], Training Loss: 0.0730\n",
            "Epoch [9593/20000], Training Loss: 0.0812\n",
            "Epoch [9594/20000], Training Loss: 0.0851\n",
            "Epoch [9595/20000], Training Loss: 0.0808\n",
            "Epoch [9596/20000], Training Loss: 0.0797\n",
            "Epoch [9597/20000], Training Loss: 0.0757\n",
            "Epoch [9598/20000], Training Loss: 0.0811\n",
            "Epoch [9599/20000], Training Loss: 0.0827\n",
            "Epoch [9600/20000], Training Loss: 0.0776\n",
            "Epoch [9601/20000], Training Loss: 0.0799\n",
            "Epoch [9602/20000], Training Loss: 0.0804\n",
            "Epoch [9603/20000], Training Loss: 0.0808\n",
            "Epoch [9604/20000], Training Loss: 0.0849\n",
            "Epoch [9605/20000], Training Loss: 0.0815\n",
            "Epoch [9606/20000], Training Loss: 0.0812\n",
            "Epoch [9607/20000], Training Loss: 0.0801\n",
            "Epoch [9608/20000], Training Loss: 0.0786\n",
            "Epoch [9609/20000], Training Loss: 0.0850\n",
            "Epoch [9610/20000], Training Loss: 0.0799\n",
            "Epoch [9611/20000], Training Loss: 0.0796\n",
            "Epoch [9612/20000], Training Loss: 0.0784\n",
            "Epoch [9613/20000], Training Loss: 0.0790\n",
            "Epoch [9614/20000], Training Loss: 0.0816\n",
            "Epoch [9615/20000], Training Loss: 0.0733\n",
            "Epoch [9616/20000], Training Loss: 0.0853\n",
            "Epoch [9617/20000], Training Loss: 0.0799\n",
            "Epoch [9618/20000], Training Loss: 0.0767\n",
            "Epoch [9619/20000], Training Loss: 0.0752\n",
            "Epoch [9620/20000], Training Loss: 0.0850\n",
            "Epoch [9621/20000], Training Loss: 0.0834\n",
            "Epoch [9622/20000], Training Loss: 0.0808\n",
            "Epoch [9623/20000], Training Loss: 0.0861\n",
            "Epoch [9624/20000], Training Loss: 0.0773\n",
            "Epoch [9625/20000], Training Loss: 0.0768\n",
            "Epoch [9626/20000], Training Loss: 0.0828\n",
            "Epoch [9627/20000], Training Loss: 0.0758\n",
            "Epoch [9628/20000], Training Loss: 0.0740\n",
            "Epoch [9629/20000], Training Loss: 0.0827\n",
            "Epoch [9630/20000], Training Loss: 0.0737\n",
            "Epoch [9631/20000], Training Loss: 0.0788\n",
            "Epoch [9632/20000], Training Loss: 0.0752\n",
            "Epoch [9633/20000], Training Loss: 0.0786\n",
            "Epoch [9634/20000], Training Loss: 0.0832\n",
            "Epoch [9635/20000], Training Loss: 0.0797\n",
            "Epoch [9636/20000], Training Loss: 0.0833\n",
            "Epoch [9637/20000], Training Loss: 0.0741\n",
            "Epoch [9638/20000], Training Loss: 0.0832\n",
            "Epoch [9639/20000], Training Loss: 0.0760\n",
            "Epoch [9640/20000], Training Loss: 0.0772\n",
            "Epoch [9641/20000], Training Loss: 0.0805\n",
            "Epoch [9642/20000], Training Loss: 0.0844\n",
            "Epoch [9643/20000], Training Loss: 0.0826\n",
            "Epoch [9644/20000], Training Loss: 0.0789\n",
            "Epoch [9645/20000], Training Loss: 0.0803\n",
            "Epoch [9646/20000], Training Loss: 0.0803\n",
            "Epoch [9647/20000], Training Loss: 0.0857\n",
            "Epoch [9648/20000], Training Loss: 0.0798\n",
            "Epoch [9649/20000], Training Loss: 0.0836\n",
            "Epoch [9650/20000], Training Loss: 0.0833\n",
            "Epoch [9651/20000], Training Loss: 0.0803\n",
            "Epoch [9652/20000], Training Loss: 0.0860\n",
            "Epoch [9653/20000], Training Loss: 0.0799\n",
            "Epoch [9654/20000], Training Loss: 0.0769\n",
            "Epoch [9655/20000], Training Loss: 0.0774\n",
            "Epoch [9656/20000], Training Loss: 0.0782\n",
            "Epoch [9657/20000], Training Loss: 0.0746\n",
            "Epoch [9658/20000], Training Loss: 0.0748\n",
            "Epoch [9659/20000], Training Loss: 0.0810\n",
            "Epoch [9660/20000], Training Loss: 0.0810\n",
            "Epoch [9661/20000], Training Loss: 0.0830\n",
            "Epoch [9662/20000], Training Loss: 0.0819\n",
            "Epoch [9663/20000], Training Loss: 0.0773\n",
            "Epoch [9664/20000], Training Loss: 0.0739\n",
            "Epoch [9665/20000], Training Loss: 0.0846\n",
            "Epoch [9666/20000], Training Loss: 0.0785\n",
            "Epoch [9667/20000], Training Loss: 0.0752\n",
            "Epoch [9668/20000], Training Loss: 0.0806\n",
            "Epoch [9669/20000], Training Loss: 0.0800\n",
            "Epoch [9670/20000], Training Loss: 0.0827\n",
            "Epoch [9671/20000], Training Loss: 0.0797\n",
            "Epoch [9672/20000], Training Loss: 0.0768\n",
            "Epoch [9673/20000], Training Loss: 0.0807\n",
            "Epoch [9674/20000], Training Loss: 0.0753\n",
            "Epoch [9675/20000], Training Loss: 0.0852\n",
            "Epoch [9676/20000], Training Loss: 0.0750\n",
            "Epoch [9677/20000], Training Loss: 0.0796\n",
            "Epoch [9678/20000], Training Loss: 0.0815\n",
            "Epoch [9679/20000], Training Loss: 0.0776\n",
            "Epoch [9680/20000], Training Loss: 0.0812\n",
            "Epoch [9681/20000], Training Loss: 0.0773\n",
            "Epoch [9682/20000], Training Loss: 0.0802\n",
            "Epoch [9683/20000], Training Loss: 0.0843\n",
            "Epoch [9684/20000], Training Loss: 0.0842\n",
            "Epoch [9685/20000], Training Loss: 0.0828\n",
            "Epoch [9686/20000], Training Loss: 0.0798\n",
            "Epoch [9687/20000], Training Loss: 0.0797\n",
            "Epoch [9688/20000], Training Loss: 0.0837\n",
            "Epoch [9689/20000], Training Loss: 0.0811\n",
            "Epoch [9690/20000], Training Loss: 0.0752\n",
            "Epoch [9691/20000], Training Loss: 0.0794\n",
            "Epoch [9692/20000], Training Loss: 0.0797\n",
            "Epoch [9693/20000], Training Loss: 0.0795\n",
            "Epoch [9694/20000], Training Loss: 0.0827\n",
            "Epoch [9695/20000], Training Loss: 0.0806\n",
            "Epoch [9696/20000], Training Loss: 0.0805\n",
            "Epoch [9697/20000], Training Loss: 0.0873\n",
            "Epoch [9698/20000], Training Loss: 0.0769\n",
            "Epoch [9699/20000], Training Loss: 0.0808\n",
            "Epoch [9700/20000], Training Loss: 0.0805\n",
            "Epoch [9701/20000], Training Loss: 0.0844\n",
            "Epoch [9702/20000], Training Loss: 0.0816\n",
            "Epoch [9703/20000], Training Loss: 0.0755\n",
            "Epoch [9704/20000], Training Loss: 0.0813\n",
            "Epoch [9705/20000], Training Loss: 0.0810\n",
            "Epoch [9706/20000], Training Loss: 0.0808\n",
            "Epoch [9707/20000], Training Loss: 0.0771\n",
            "Epoch [9708/20000], Training Loss: 0.0772\n",
            "Epoch [9709/20000], Training Loss: 0.0839\n",
            "Epoch [9710/20000], Training Loss: 0.0801\n",
            "Epoch [9711/20000], Training Loss: 0.0808\n",
            "Epoch [9712/20000], Training Loss: 0.0802\n",
            "Epoch [9713/20000], Training Loss: 0.0776\n",
            "Epoch [9714/20000], Training Loss: 0.0787\n",
            "Epoch [9715/20000], Training Loss: 0.0820\n",
            "Epoch [9716/20000], Training Loss: 0.0811\n",
            "Epoch [9717/20000], Training Loss: 0.0754\n",
            "Epoch [9718/20000], Training Loss: 0.0746\n",
            "Epoch [9719/20000], Training Loss: 0.0783\n",
            "Epoch [9720/20000], Training Loss: 0.0806\n",
            "Epoch [9721/20000], Training Loss: 0.0824\n",
            "Epoch [9722/20000], Training Loss: 0.0819\n",
            "Epoch [9723/20000], Training Loss: 0.0790\n",
            "Epoch [9724/20000], Training Loss: 0.0744\n",
            "Epoch [9725/20000], Training Loss: 0.0790\n",
            "Epoch [9726/20000], Training Loss: 0.0782\n",
            "Epoch [9727/20000], Training Loss: 0.0750\n",
            "Epoch [9728/20000], Training Loss: 0.0793\n",
            "Epoch [9729/20000], Training Loss: 0.0786\n",
            "Epoch [9730/20000], Training Loss: 0.0744\n",
            "Epoch [9731/20000], Training Loss: 0.0767\n",
            "Epoch [9732/20000], Training Loss: 0.0786\n",
            "Epoch [9733/20000], Training Loss: 0.0775\n",
            "Epoch [9734/20000], Training Loss: 0.0795\n",
            "Epoch [9735/20000], Training Loss: 0.0760\n",
            "Epoch [9736/20000], Training Loss: 0.0773\n",
            "Epoch [9737/20000], Training Loss: 0.0747\n",
            "Epoch [9738/20000], Training Loss: 0.0846\n",
            "Epoch [9739/20000], Training Loss: 0.0811\n",
            "Epoch [9740/20000], Training Loss: 0.0821\n",
            "Epoch [9741/20000], Training Loss: 0.0792\n",
            "Epoch [9742/20000], Training Loss: 0.0819\n",
            "Epoch [9743/20000], Training Loss: 0.0780\n",
            "Epoch [9744/20000], Training Loss: 0.0754\n",
            "Epoch [9745/20000], Training Loss: 0.0810\n",
            "Epoch [9746/20000], Training Loss: 0.0813\n",
            "Epoch [9747/20000], Training Loss: 0.0759\n",
            "Epoch [9748/20000], Training Loss: 0.0755\n",
            "Epoch [9749/20000], Training Loss: 0.0803\n",
            "Epoch [9750/20000], Training Loss: 0.0835\n",
            "Epoch [9751/20000], Training Loss: 0.0865\n",
            "Epoch [9752/20000], Training Loss: 0.0771\n",
            "Epoch [9753/20000], Training Loss: 0.0859\n",
            "Epoch [9754/20000], Training Loss: 0.0813\n",
            "Epoch [9755/20000], Training Loss: 0.0820\n",
            "Epoch [9756/20000], Training Loss: 0.0790\n",
            "Epoch [9757/20000], Training Loss: 0.0864\n",
            "Epoch [9758/20000], Training Loss: 0.0774\n",
            "Epoch [9759/20000], Training Loss: 0.0799\n",
            "Epoch [9760/20000], Training Loss: 0.0857\n",
            "Epoch [9761/20000], Training Loss: 0.0807\n",
            "Epoch [9762/20000], Training Loss: 0.0725\n",
            "Epoch [9763/20000], Training Loss: 0.0814\n",
            "Epoch [9764/20000], Training Loss: 0.0844\n",
            "Epoch [9765/20000], Training Loss: 0.0794\n",
            "Epoch [9766/20000], Training Loss: 0.0832\n",
            "Epoch [9767/20000], Training Loss: 0.0811\n",
            "Epoch [9768/20000], Training Loss: 0.0872\n",
            "Epoch [9769/20000], Training Loss: 0.0862\n",
            "Epoch [9770/20000], Training Loss: 0.0810\n",
            "Epoch [9771/20000], Training Loss: 0.0860\n",
            "Epoch [9772/20000], Training Loss: 0.0827\n",
            "Epoch [9773/20000], Training Loss: 0.0745\n",
            "Epoch [9774/20000], Training Loss: 0.0755\n",
            "Epoch [9775/20000], Training Loss: 0.0788\n",
            "Epoch [9776/20000], Training Loss: 0.0849\n",
            "Epoch [9777/20000], Training Loss: 0.0772\n",
            "Epoch [9778/20000], Training Loss: 0.0827\n",
            "Epoch [9779/20000], Training Loss: 0.0840\n",
            "Epoch [9780/20000], Training Loss: 0.0787\n",
            "Epoch [9781/20000], Training Loss: 0.0826\n",
            "Epoch [9782/20000], Training Loss: 0.0839\n",
            "Epoch [9783/20000], Training Loss: 0.0766\n",
            "Epoch [9784/20000], Training Loss: 0.0772\n",
            "Epoch [9785/20000], Training Loss: 0.0794\n",
            "Epoch [9786/20000], Training Loss: 0.0766\n",
            "Epoch [9787/20000], Training Loss: 0.0761\n",
            "Epoch [9788/20000], Training Loss: 0.0785\n",
            "Epoch [9789/20000], Training Loss: 0.0785\n",
            "Epoch [9790/20000], Training Loss: 0.0784\n",
            "Epoch [9791/20000], Training Loss: 0.0798\n",
            "Epoch [9792/20000], Training Loss: 0.0800\n",
            "Epoch [9793/20000], Training Loss: 0.0837\n",
            "Epoch [9794/20000], Training Loss: 0.0802\n",
            "Epoch [9795/20000], Training Loss: 0.0784\n",
            "Epoch [9796/20000], Training Loss: 0.0824\n",
            "Epoch [9797/20000], Training Loss: 0.0783\n",
            "Epoch [9798/20000], Training Loss: 0.0765\n",
            "Epoch [9799/20000], Training Loss: 0.0837\n",
            "Epoch [9800/20000], Training Loss: 0.0839\n",
            "Epoch [9801/20000], Training Loss: 0.0796\n",
            "Epoch [9802/20000], Training Loss: 0.0772\n",
            "Epoch [9803/20000], Training Loss: 0.0777\n",
            "Epoch [9804/20000], Training Loss: 0.0793\n",
            "Epoch [9805/20000], Training Loss: 0.0778\n",
            "Epoch [9806/20000], Training Loss: 0.0805\n",
            "Epoch [9807/20000], Training Loss: 0.0836\n",
            "Epoch [9808/20000], Training Loss: 0.0743\n",
            "Epoch [9809/20000], Training Loss: 0.0828\n",
            "Epoch [9810/20000], Training Loss: 0.0788\n",
            "Epoch [9811/20000], Training Loss: 0.0775\n",
            "Epoch [9812/20000], Training Loss: 0.0827\n",
            "Epoch [9813/20000], Training Loss: 0.0837\n",
            "Epoch [9814/20000], Training Loss: 0.0772\n",
            "Epoch [9815/20000], Training Loss: 0.0794\n",
            "Epoch [9816/20000], Training Loss: 0.0736\n",
            "Epoch [9817/20000], Training Loss: 0.0868\n",
            "Epoch [9818/20000], Training Loss: 0.0789\n",
            "Epoch [9819/20000], Training Loss: 0.0814\n",
            "Epoch [9820/20000], Training Loss: 0.0821\n",
            "Epoch [9821/20000], Training Loss: 0.0742\n",
            "Epoch [9822/20000], Training Loss: 0.0745\n",
            "Epoch [9823/20000], Training Loss: 0.0769\n",
            "Epoch [9824/20000], Training Loss: 0.0730\n",
            "Epoch [9825/20000], Training Loss: 0.0821\n",
            "Epoch [9826/20000], Training Loss: 0.0765\n",
            "Epoch [9827/20000], Training Loss: 0.0813\n",
            "Epoch [9828/20000], Training Loss: 0.0771\n",
            "Epoch [9829/20000], Training Loss: 0.0761\n",
            "Epoch [9830/20000], Training Loss: 0.0780\n",
            "Epoch [9831/20000], Training Loss: 0.0764\n",
            "Epoch [9832/20000], Training Loss: 0.0838\n",
            "Epoch [9833/20000], Training Loss: 0.0796\n",
            "Epoch [9834/20000], Training Loss: 0.0833\n",
            "Epoch [9835/20000], Training Loss: 0.0801\n",
            "Epoch [9836/20000], Training Loss: 0.0806\n",
            "Epoch [9837/20000], Training Loss: 0.0772\n",
            "Epoch [9838/20000], Training Loss: 0.0783\n",
            "Epoch [9839/20000], Training Loss: 0.0845\n",
            "Epoch [9840/20000], Training Loss: 0.0825\n",
            "Epoch [9841/20000], Training Loss: 0.0796\n",
            "Epoch [9842/20000], Training Loss: 0.0753\n",
            "Epoch [9843/20000], Training Loss: 0.0778\n",
            "Epoch [9844/20000], Training Loss: 0.0851\n",
            "Epoch [9845/20000], Training Loss: 0.0788\n",
            "Epoch [9846/20000], Training Loss: 0.0748\n",
            "Epoch [9847/20000], Training Loss: 0.0779\n",
            "Epoch [9848/20000], Training Loss: 0.0731\n",
            "Epoch [9849/20000], Training Loss: 0.0772\n",
            "Epoch [9850/20000], Training Loss: 0.0803\n",
            "Epoch [9851/20000], Training Loss: 0.0780\n",
            "Epoch [9852/20000], Training Loss: 0.0796\n",
            "Epoch [9853/20000], Training Loss: 0.0808\n",
            "Epoch [9854/20000], Training Loss: 0.0833\n",
            "Epoch [9855/20000], Training Loss: 0.0754\n",
            "Epoch [9856/20000], Training Loss: 0.0841\n",
            "Epoch [9857/20000], Training Loss: 0.0790\n",
            "Epoch [9858/20000], Training Loss: 0.0724\n",
            "Epoch [9859/20000], Training Loss: 0.0758\n",
            "Epoch [9860/20000], Training Loss: 0.0798\n",
            "Epoch [9861/20000], Training Loss: 0.0747\n",
            "Epoch [9862/20000], Training Loss: 0.0732\n",
            "Epoch [9863/20000], Training Loss: 0.0740\n",
            "Epoch [9864/20000], Training Loss: 0.0794\n",
            "Epoch [9865/20000], Training Loss: 0.0741\n",
            "Epoch [9866/20000], Training Loss: 0.0788\n",
            "Epoch [9867/20000], Training Loss: 0.0808\n",
            "Epoch [9868/20000], Training Loss: 0.0815\n",
            "Epoch [9869/20000], Training Loss: 0.0757\n",
            "Epoch [9870/20000], Training Loss: 0.0813\n",
            "Epoch [9871/20000], Training Loss: 0.0751\n",
            "Epoch [9872/20000], Training Loss: 0.0766\n",
            "Epoch [9873/20000], Training Loss: 0.0765\n",
            "Epoch [9874/20000], Training Loss: 0.0791\n",
            "Epoch [9875/20000], Training Loss: 0.0745\n",
            "Epoch [9876/20000], Training Loss: 0.0874\n",
            "Epoch [9877/20000], Training Loss: 0.0795\n",
            "Epoch [9878/20000], Training Loss: 0.0749\n",
            "Epoch [9879/20000], Training Loss: 0.0765\n",
            "Epoch [9880/20000], Training Loss: 0.0811\n",
            "Epoch [9881/20000], Training Loss: 0.0783\n",
            "Epoch [9882/20000], Training Loss: 0.0812\n",
            "Epoch [9883/20000], Training Loss: 0.0788\n",
            "Epoch [9884/20000], Training Loss: 0.0777\n",
            "Epoch [9885/20000], Training Loss: 0.0765\n",
            "Epoch [9886/20000], Training Loss: 0.0744\n",
            "Epoch [9887/20000], Training Loss: 0.0852\n",
            "Epoch [9888/20000], Training Loss: 0.0781\n",
            "Epoch [9889/20000], Training Loss: 0.0842\n",
            "Epoch [9890/20000], Training Loss: 0.0816\n",
            "Epoch [9891/20000], Training Loss: 0.0799\n",
            "Epoch [9892/20000], Training Loss: 0.0835\n",
            "Epoch [9893/20000], Training Loss: 0.0805\n",
            "Epoch [9894/20000], Training Loss: 0.0806\n",
            "Epoch [9895/20000], Training Loss: 0.0782\n",
            "Epoch [9896/20000], Training Loss: 0.0798\n",
            "Epoch [9897/20000], Training Loss: 0.0841\n",
            "Epoch [9898/20000], Training Loss: 0.0817\n",
            "Epoch [9899/20000], Training Loss: 0.0795\n",
            "Epoch [9900/20000], Training Loss: 0.0804\n",
            "Epoch [9901/20000], Training Loss: 0.0763\n",
            "Epoch [9902/20000], Training Loss: 0.0714\n",
            "Epoch [9903/20000], Training Loss: 0.0840\n",
            "Epoch [9904/20000], Training Loss: 0.0865\n",
            "Epoch [9905/20000], Training Loss: 0.0807\n",
            "Epoch [9906/20000], Training Loss: 0.0787\n",
            "Epoch [9907/20000], Training Loss: 0.0762\n",
            "Epoch [9908/20000], Training Loss: 0.0806\n",
            "Epoch [9909/20000], Training Loss: 0.0786\n",
            "Epoch [9910/20000], Training Loss: 0.0843\n",
            "Epoch [9911/20000], Training Loss: 0.0869\n",
            "Epoch [9912/20000], Training Loss: 0.0801\n",
            "Epoch [9913/20000], Training Loss: 0.0825\n",
            "Epoch [9914/20000], Training Loss: 0.0746\n",
            "Epoch [9915/20000], Training Loss: 0.0819\n",
            "Epoch [9916/20000], Training Loss: 0.0803\n",
            "Epoch [9917/20000], Training Loss: 0.0782\n",
            "Epoch [9918/20000], Training Loss: 0.0785\n",
            "Epoch [9919/20000], Training Loss: 0.0745\n",
            "Epoch [9920/20000], Training Loss: 0.0723\n",
            "Epoch [9921/20000], Training Loss: 0.0794\n",
            "Epoch [9922/20000], Training Loss: 0.0822\n",
            "Epoch [9923/20000], Training Loss: 0.0753\n",
            "Epoch [9924/20000], Training Loss: 0.0779\n",
            "Epoch [9925/20000], Training Loss: 0.0764\n",
            "Epoch [9926/20000], Training Loss: 0.0763\n",
            "Epoch [9927/20000], Training Loss: 0.0828\n",
            "Epoch [9928/20000], Training Loss: 0.0805\n",
            "Epoch [9929/20000], Training Loss: 0.0807\n",
            "Epoch [9930/20000], Training Loss: 0.0849\n",
            "Epoch [9931/20000], Training Loss: 0.0794\n",
            "Epoch [9932/20000], Training Loss: 0.0782\n",
            "Epoch [9933/20000], Training Loss: 0.0800\n",
            "Epoch [9934/20000], Training Loss: 0.0851\n",
            "Epoch [9935/20000], Training Loss: 0.0787\n",
            "Epoch [9936/20000], Training Loss: 0.0806\n",
            "Epoch [9937/20000], Training Loss: 0.0744\n",
            "Epoch [9938/20000], Training Loss: 0.0828\n",
            "Epoch [9939/20000], Training Loss: 0.0792\n",
            "Epoch [9940/20000], Training Loss: 0.0796\n",
            "Epoch [9941/20000], Training Loss: 0.0808\n",
            "Epoch [9942/20000], Training Loss: 0.0797\n",
            "Epoch [9943/20000], Training Loss: 0.0822\n",
            "Epoch [9944/20000], Training Loss: 0.0825\n",
            "Epoch [9945/20000], Training Loss: 0.0747\n",
            "Epoch [9946/20000], Training Loss: 0.0850\n",
            "Epoch [9947/20000], Training Loss: 0.0833\n",
            "Epoch [9948/20000], Training Loss: 0.0805\n",
            "Epoch [9949/20000], Training Loss: 0.0815\n",
            "Epoch [9950/20000], Training Loss: 0.0776\n",
            "Epoch [9951/20000], Training Loss: 0.0857\n",
            "Epoch [9952/20000], Training Loss: 0.0766\n",
            "Epoch [9953/20000], Training Loss: 0.0783\n",
            "Epoch [9954/20000], Training Loss: 0.0789\n",
            "Epoch [9955/20000], Training Loss: 0.0830\n",
            "Epoch [9956/20000], Training Loss: 0.0761\n",
            "Epoch [9957/20000], Training Loss: 0.0800\n",
            "Epoch [9958/20000], Training Loss: 0.0800\n",
            "Epoch [9959/20000], Training Loss: 0.0869\n",
            "Epoch [9960/20000], Training Loss: 0.0816\n",
            "Epoch [9961/20000], Training Loss: 0.0854\n",
            "Epoch [9962/20000], Training Loss: 0.0819\n",
            "Epoch [9963/20000], Training Loss: 0.0758\n",
            "Epoch [9964/20000], Training Loss: 0.0782\n",
            "Epoch [9965/20000], Training Loss: 0.0839\n",
            "Epoch [9966/20000], Training Loss: 0.0749\n",
            "Epoch [9967/20000], Training Loss: 0.0795\n",
            "Epoch [9968/20000], Training Loss: 0.0825\n",
            "Epoch [9969/20000], Training Loss: 0.0828\n",
            "Epoch [9970/20000], Training Loss: 0.0775\n",
            "Epoch [9971/20000], Training Loss: 0.0850\n",
            "Epoch [9972/20000], Training Loss: 0.0805\n",
            "Epoch [9973/20000], Training Loss: 0.0762\n",
            "Epoch [9974/20000], Training Loss: 0.0834\n",
            "Epoch [9975/20000], Training Loss: 0.0777\n",
            "Epoch [9976/20000], Training Loss: 0.0764\n",
            "Epoch [9977/20000], Training Loss: 0.0728\n",
            "Epoch [9978/20000], Training Loss: 0.0859\n",
            "Epoch [9979/20000], Training Loss: 0.0800\n",
            "Epoch [9980/20000], Training Loss: 0.0780\n",
            "Epoch [9981/20000], Training Loss: 0.0843\n",
            "Epoch [9982/20000], Training Loss: 0.0782\n",
            "Epoch [9983/20000], Training Loss: 0.0761\n",
            "Epoch [9984/20000], Training Loss: 0.0829\n",
            "Epoch [9985/20000], Training Loss: 0.0843\n",
            "Epoch [9986/20000], Training Loss: 0.0760\n",
            "Epoch [9987/20000], Training Loss: 0.0850\n",
            "Epoch [9988/20000], Training Loss: 0.0805\n",
            "Epoch [9989/20000], Training Loss: 0.0833\n",
            "Epoch [9990/20000], Training Loss: 0.0779\n",
            "Epoch [9991/20000], Training Loss: 0.0754\n",
            "Epoch [9992/20000], Training Loss: 0.0808\n",
            "Epoch [9993/20000], Training Loss: 0.0840\n",
            "Epoch [9994/20000], Training Loss: 0.0808\n",
            "Epoch [9995/20000], Training Loss: 0.0783\n",
            "Epoch [9996/20000], Training Loss: 0.0775\n",
            "Epoch [9997/20000], Training Loss: 0.0868\n",
            "Epoch [9998/20000], Training Loss: 0.0792\n",
            "Epoch [9999/20000], Training Loss: 0.0761\n",
            "Epoch [10000/20000], Training Loss: 0.0829\n",
            "Epoch [10001/20000], Training Loss: 0.0781\n",
            "Epoch [10002/20000], Training Loss: 0.0750\n",
            "Epoch [10003/20000], Training Loss: 0.0779\n",
            "Epoch [10004/20000], Training Loss: 0.0797\n",
            "Epoch [10005/20000], Training Loss: 0.0780\n",
            "Epoch [10006/20000], Training Loss: 0.0800\n",
            "Epoch [10007/20000], Training Loss: 0.0754\n",
            "Epoch [10008/20000], Training Loss: 0.0740\n",
            "Epoch [10009/20000], Training Loss: 0.0799\n",
            "Epoch [10010/20000], Training Loss: 0.0827\n",
            "Epoch [10011/20000], Training Loss: 0.0747\n",
            "Epoch [10012/20000], Training Loss: 0.0780\n",
            "Epoch [10013/20000], Training Loss: 0.0793\n",
            "Epoch [10014/20000], Training Loss: 0.0857\n",
            "Epoch [10015/20000], Training Loss: 0.0821\n",
            "Epoch [10016/20000], Training Loss: 0.0755\n",
            "Epoch [10017/20000], Training Loss: 0.0808\n",
            "Epoch [10018/20000], Training Loss: 0.0774\n",
            "Epoch [10019/20000], Training Loss: 0.0734\n",
            "Epoch [10020/20000], Training Loss: 0.0841\n",
            "Epoch [10021/20000], Training Loss: 0.0850\n",
            "Epoch [10022/20000], Training Loss: 0.0772\n",
            "Epoch [10023/20000], Training Loss: 0.0766\n",
            "Epoch [10024/20000], Training Loss: 0.0801\n",
            "Epoch [10025/20000], Training Loss: 0.0770\n",
            "Epoch [10026/20000], Training Loss: 0.0826\n",
            "Epoch [10027/20000], Training Loss: 0.0814\n",
            "Epoch [10028/20000], Training Loss: 0.0763\n",
            "Epoch [10029/20000], Training Loss: 0.0772\n",
            "Epoch [10030/20000], Training Loss: 0.0762\n",
            "Epoch [10031/20000], Training Loss: 0.0789\n",
            "Epoch [10032/20000], Training Loss: 0.0819\n",
            "Epoch [10033/20000], Training Loss: 0.0825\n",
            "Epoch [10034/20000], Training Loss: 0.0872\n",
            "Epoch [10035/20000], Training Loss: 0.0751\n",
            "Epoch [10036/20000], Training Loss: 0.0745\n",
            "Epoch [10037/20000], Training Loss: 0.0741\n",
            "Epoch [10038/20000], Training Loss: 0.0783\n",
            "Epoch [10039/20000], Training Loss: 0.0834\n",
            "Epoch [10040/20000], Training Loss: 0.0784\n",
            "Epoch [10041/20000], Training Loss: 0.0789\n",
            "Epoch [10042/20000], Training Loss: 0.0739\n",
            "Epoch [10043/20000], Training Loss: 0.0820\n",
            "Epoch [10044/20000], Training Loss: 0.0781\n",
            "Epoch [10045/20000], Training Loss: 0.0855\n",
            "Epoch [10046/20000], Training Loss: 0.0800\n",
            "Epoch [10047/20000], Training Loss: 0.0786\n",
            "Epoch [10048/20000], Training Loss: 0.0760\n",
            "Epoch [10049/20000], Training Loss: 0.0864\n",
            "Epoch [10050/20000], Training Loss: 0.0866\n",
            "Epoch [10051/20000], Training Loss: 0.0738\n",
            "Epoch [10052/20000], Training Loss: 0.0738\n",
            "Epoch [10053/20000], Training Loss: 0.0801\n",
            "Epoch [10054/20000], Training Loss: 0.0817\n",
            "Epoch [10055/20000], Training Loss: 0.0833\n",
            "Epoch [10056/20000], Training Loss: 0.0748\n",
            "Epoch [10057/20000], Training Loss: 0.0825\n",
            "Epoch [10058/20000], Training Loss: 0.0731\n",
            "Epoch [10059/20000], Training Loss: 0.0805\n",
            "Epoch [10060/20000], Training Loss: 0.0841\n",
            "Epoch [10061/20000], Training Loss: 0.0802\n",
            "Epoch [10062/20000], Training Loss: 0.0825\n",
            "Epoch [10063/20000], Training Loss: 0.0746\n",
            "Epoch [10064/20000], Training Loss: 0.0786\n",
            "Epoch [10065/20000], Training Loss: 0.0813\n",
            "Epoch [10066/20000], Training Loss: 0.0754\n",
            "Epoch [10067/20000], Training Loss: 0.0748\n",
            "Epoch [10068/20000], Training Loss: 0.0758\n",
            "Epoch [10069/20000], Training Loss: 0.0780\n",
            "Epoch [10070/20000], Training Loss: 0.0758\n",
            "Epoch [10071/20000], Training Loss: 0.0758\n",
            "Epoch [10072/20000], Training Loss: 0.0798\n",
            "Epoch [10073/20000], Training Loss: 0.0775\n",
            "Epoch [10074/20000], Training Loss: 0.0781\n",
            "Epoch [10075/20000], Training Loss: 0.0734\n",
            "Epoch [10076/20000], Training Loss: 0.0776\n",
            "Epoch [10077/20000], Training Loss: 0.0787\n",
            "Epoch [10078/20000], Training Loss: 0.0764\n",
            "Epoch [10079/20000], Training Loss: 0.0739\n",
            "Epoch [10080/20000], Training Loss: 0.0815\n",
            "Epoch [10081/20000], Training Loss: 0.0790\n",
            "Epoch [10082/20000], Training Loss: 0.0816\n",
            "Epoch [10083/20000], Training Loss: 0.0795\n",
            "Epoch [10084/20000], Training Loss: 0.0771\n",
            "Epoch [10085/20000], Training Loss: 0.0783\n",
            "Epoch [10086/20000], Training Loss: 0.0820\n",
            "Epoch [10087/20000], Training Loss: 0.0810\n",
            "Epoch [10088/20000], Training Loss: 0.0760\n",
            "Epoch [10089/20000], Training Loss: 0.0760\n",
            "Epoch [10090/20000], Training Loss: 0.0797\n",
            "Epoch [10091/20000], Training Loss: 0.0820\n",
            "Epoch [10092/20000], Training Loss: 0.0746\n",
            "Epoch [10093/20000], Training Loss: 0.0777\n",
            "Epoch [10094/20000], Training Loss: 0.0797\n",
            "Epoch [10095/20000], Training Loss: 0.0821\n",
            "Epoch [10096/20000], Training Loss: 0.0780\n",
            "Epoch [10097/20000], Training Loss: 0.0750\n",
            "Epoch [10098/20000], Training Loss: 0.0778\n",
            "Epoch [10099/20000], Training Loss: 0.0757\n",
            "Epoch [10100/20000], Training Loss: 0.0817\n",
            "Epoch [10101/20000], Training Loss: 0.0847\n",
            "Epoch [10102/20000], Training Loss: 0.0739\n",
            "Epoch [10103/20000], Training Loss: 0.0796\n",
            "Epoch [10104/20000], Training Loss: 0.0822\n",
            "Epoch [10105/20000], Training Loss: 0.0795\n",
            "Epoch [10106/20000], Training Loss: 0.0868\n",
            "Epoch [10107/20000], Training Loss: 0.0816\n",
            "Epoch [10108/20000], Training Loss: 0.0755\n",
            "Epoch [10109/20000], Training Loss: 0.0818\n",
            "Epoch [10110/20000], Training Loss: 0.0779\n",
            "Epoch [10111/20000], Training Loss: 0.0818\n",
            "Epoch [10112/20000], Training Loss: 0.0815\n",
            "Epoch [10113/20000], Training Loss: 0.0766\n",
            "Epoch [10114/20000], Training Loss: 0.0835\n",
            "Epoch [10115/20000], Training Loss: 0.0761\n",
            "Epoch [10116/20000], Training Loss: 0.0794\n",
            "Epoch [10117/20000], Training Loss: 0.0736\n",
            "Epoch [10118/20000], Training Loss: 0.0739\n",
            "Epoch [10119/20000], Training Loss: 0.0803\n",
            "Epoch [10120/20000], Training Loss: 0.0764\n",
            "Epoch [10121/20000], Training Loss: 0.0797\n",
            "Epoch [10122/20000], Training Loss: 0.0825\n",
            "Epoch [10123/20000], Training Loss: 0.0799\n",
            "Epoch [10124/20000], Training Loss: 0.0749\n",
            "Epoch [10125/20000], Training Loss: 0.0754\n",
            "Epoch [10126/20000], Training Loss: 0.0792\n",
            "Epoch [10127/20000], Training Loss: 0.0781\n",
            "Epoch [10128/20000], Training Loss: 0.0762\n",
            "Epoch [10129/20000], Training Loss: 0.0819\n",
            "Epoch [10130/20000], Training Loss: 0.0880\n",
            "Epoch [10131/20000], Training Loss: 0.0765\n",
            "Epoch [10132/20000], Training Loss: 0.0744\n",
            "Epoch [10133/20000], Training Loss: 0.0818\n",
            "Epoch [10134/20000], Training Loss: 0.0779\n",
            "Epoch [10135/20000], Training Loss: 0.0848\n",
            "Epoch [10136/20000], Training Loss: 0.0798\n",
            "Epoch [10137/20000], Training Loss: 0.0752\n",
            "Epoch [10138/20000], Training Loss: 0.0780\n",
            "Epoch [10139/20000], Training Loss: 0.0729\n",
            "Epoch [10140/20000], Training Loss: 0.0809\n",
            "Epoch [10141/20000], Training Loss: 0.0766\n",
            "Epoch [10142/20000], Training Loss: 0.0826\n",
            "Epoch [10143/20000], Training Loss: 0.0844\n",
            "Epoch [10144/20000], Training Loss: 0.0799\n",
            "Epoch [10145/20000], Training Loss: 0.0855\n",
            "Epoch [10146/20000], Training Loss: 0.0792\n",
            "Epoch [10147/20000], Training Loss: 0.0755\n",
            "Epoch [10148/20000], Training Loss: 0.0765\n",
            "Epoch [10149/20000], Training Loss: 0.0815\n",
            "Epoch [10150/20000], Training Loss: 0.0853\n",
            "Epoch [10151/20000], Training Loss: 0.0793\n",
            "Epoch [10152/20000], Training Loss: 0.0819\n",
            "Epoch [10153/20000], Training Loss: 0.0742\n",
            "Epoch [10154/20000], Training Loss: 0.0790\n",
            "Epoch [10155/20000], Training Loss: 0.0787\n",
            "Epoch [10156/20000], Training Loss: 0.0752\n",
            "Epoch [10157/20000], Training Loss: 0.0773\n",
            "Epoch [10158/20000], Training Loss: 0.0798\n",
            "Epoch [10159/20000], Training Loss: 0.0853\n",
            "Epoch [10160/20000], Training Loss: 0.0858\n",
            "Epoch [10161/20000], Training Loss: 0.0792\n",
            "Epoch [10162/20000], Training Loss: 0.0778\n",
            "Epoch [10163/20000], Training Loss: 0.0756\n",
            "Epoch [10164/20000], Training Loss: 0.0805\n",
            "Epoch [10165/20000], Training Loss: 0.0801\n",
            "Epoch [10166/20000], Training Loss: 0.0822\n",
            "Epoch [10167/20000], Training Loss: 0.0731\n",
            "Epoch [10168/20000], Training Loss: 0.0775\n",
            "Epoch [10169/20000], Training Loss: 0.0817\n",
            "Epoch [10170/20000], Training Loss: 0.0796\n",
            "Epoch [10171/20000], Training Loss: 0.0775\n",
            "Epoch [10172/20000], Training Loss: 0.0789\n",
            "Epoch [10173/20000], Training Loss: 0.0798\n",
            "Epoch [10174/20000], Training Loss: 0.0720\n",
            "Epoch [10175/20000], Training Loss: 0.0825\n",
            "Epoch [10176/20000], Training Loss: 0.0779\n",
            "Epoch [10177/20000], Training Loss: 0.0818\n",
            "Epoch [10178/20000], Training Loss: 0.0737\n",
            "Epoch [10179/20000], Training Loss: 0.0813\n",
            "Epoch [10180/20000], Training Loss: 0.0763\n",
            "Epoch [10181/20000], Training Loss: 0.0747\n",
            "Epoch [10182/20000], Training Loss: 0.0788\n",
            "Epoch [10183/20000], Training Loss: 0.0784\n",
            "Epoch [10184/20000], Training Loss: 0.0793\n",
            "Epoch [10185/20000], Training Loss: 0.0803\n",
            "Epoch [10186/20000], Training Loss: 0.0796\n",
            "Epoch [10187/20000], Training Loss: 0.0781\n",
            "Epoch [10188/20000], Training Loss: 0.0776\n",
            "Epoch [10189/20000], Training Loss: 0.0840\n",
            "Epoch [10190/20000], Training Loss: 0.0818\n",
            "Epoch [10191/20000], Training Loss: 0.0825\n",
            "Epoch [10192/20000], Training Loss: 0.0828\n",
            "Epoch [10193/20000], Training Loss: 0.0794\n",
            "Epoch [10194/20000], Training Loss: 0.0751\n",
            "Epoch [10195/20000], Training Loss: 0.0803\n",
            "Epoch [10196/20000], Training Loss: 0.0810\n",
            "Epoch [10197/20000], Training Loss: 0.0809\n",
            "Epoch [10198/20000], Training Loss: 0.0787\n",
            "Epoch [10199/20000], Training Loss: 0.0790\n",
            "Epoch [10200/20000], Training Loss: 0.0809\n",
            "Epoch [10201/20000], Training Loss: 0.0740\n",
            "Epoch [10202/20000], Training Loss: 0.0757\n",
            "Epoch [10203/20000], Training Loss: 0.0750\n",
            "Epoch [10204/20000], Training Loss: 0.0825\n",
            "Epoch [10205/20000], Training Loss: 0.0758\n",
            "Epoch [10206/20000], Training Loss: 0.0755\n",
            "Epoch [10207/20000], Training Loss: 0.0747\n",
            "Epoch [10208/20000], Training Loss: 0.0794\n",
            "Epoch [10209/20000], Training Loss: 0.0843\n",
            "Epoch [10210/20000], Training Loss: 0.0811\n",
            "Epoch [10211/20000], Training Loss: 0.0845\n",
            "Epoch [10212/20000], Training Loss: 0.0805\n",
            "Epoch [10213/20000], Training Loss: 0.0760\n",
            "Epoch [10214/20000], Training Loss: 0.0818\n",
            "Epoch [10215/20000], Training Loss: 0.0806\n",
            "Epoch [10216/20000], Training Loss: 0.0779\n",
            "Epoch [10217/20000], Training Loss: 0.0807\n",
            "Epoch [10218/20000], Training Loss: 0.0825\n",
            "Epoch [10219/20000], Training Loss: 0.0794\n",
            "Epoch [10220/20000], Training Loss: 0.0818\n",
            "Epoch [10221/20000], Training Loss: 0.0816\n",
            "Epoch [10222/20000], Training Loss: 0.0758\n",
            "Epoch [10223/20000], Training Loss: 0.0786\n",
            "Epoch [10224/20000], Training Loss: 0.0785\n",
            "Epoch [10225/20000], Training Loss: 0.0820\n",
            "Epoch [10226/20000], Training Loss: 0.0749\n",
            "Epoch [10227/20000], Training Loss: 0.0891\n",
            "Epoch [10228/20000], Training Loss: 0.0781\n",
            "Epoch [10229/20000], Training Loss: 0.0793\n",
            "Epoch [10230/20000], Training Loss: 0.0826\n",
            "Epoch [10231/20000], Training Loss: 0.0819\n",
            "Epoch [10232/20000], Training Loss: 0.0779\n",
            "Epoch [10233/20000], Training Loss: 0.0832\n",
            "Epoch [10234/20000], Training Loss: 0.0805\n",
            "Epoch [10235/20000], Training Loss: 0.0861\n",
            "Epoch [10236/20000], Training Loss: 0.0785\n",
            "Epoch [10237/20000], Training Loss: 0.0791\n",
            "Epoch [10238/20000], Training Loss: 0.0813\n",
            "Epoch [10239/20000], Training Loss: 0.0825\n",
            "Epoch [10240/20000], Training Loss: 0.0738\n",
            "Epoch [10241/20000], Training Loss: 0.0758\n",
            "Epoch [10242/20000], Training Loss: 0.0807\n",
            "Epoch [10243/20000], Training Loss: 0.0861\n",
            "Epoch [10244/20000], Training Loss: 0.0838\n",
            "Epoch [10245/20000], Training Loss: 0.0778\n",
            "Epoch [10246/20000], Training Loss: 0.0776\n",
            "Epoch [10247/20000], Training Loss: 0.0750\n",
            "Epoch [10248/20000], Training Loss: 0.0812\n",
            "Epoch [10249/20000], Training Loss: 0.0769\n",
            "Epoch [10250/20000], Training Loss: 0.0863\n",
            "Epoch [10251/20000], Training Loss: 0.0767\n",
            "Epoch [10252/20000], Training Loss: 0.0804\n",
            "Epoch [10253/20000], Training Loss: 0.0824\n",
            "Epoch [10254/20000], Training Loss: 0.0762\n",
            "Epoch [10255/20000], Training Loss: 0.0860\n",
            "Epoch [10256/20000], Training Loss: 0.0812\n",
            "Epoch [10257/20000], Training Loss: 0.0792\n",
            "Epoch [10258/20000], Training Loss: 0.0805\n",
            "Epoch [10259/20000], Training Loss: 0.0755\n",
            "Epoch [10260/20000], Training Loss: 0.0826\n",
            "Epoch [10261/20000], Training Loss: 0.0795\n",
            "Epoch [10262/20000], Training Loss: 0.0805\n",
            "Epoch [10263/20000], Training Loss: 0.0765\n",
            "Epoch [10264/20000], Training Loss: 0.0757\n",
            "Epoch [10265/20000], Training Loss: 0.0845\n",
            "Epoch [10266/20000], Training Loss: 0.0782\n",
            "Epoch [10267/20000], Training Loss: 0.0819\n",
            "Epoch [10268/20000], Training Loss: 0.0815\n",
            "Epoch [10269/20000], Training Loss: 0.0804\n",
            "Epoch [10270/20000], Training Loss: 0.0748\n",
            "Epoch [10271/20000], Training Loss: 0.0837\n",
            "Epoch [10272/20000], Training Loss: 0.0782\n",
            "Epoch [10273/20000], Training Loss: 0.0744\n",
            "Epoch [10274/20000], Training Loss: 0.0813\n",
            "Epoch [10275/20000], Training Loss: 0.0787\n",
            "Epoch [10276/20000], Training Loss: 0.0759\n",
            "Epoch [10277/20000], Training Loss: 0.0806\n",
            "Epoch [10278/20000], Training Loss: 0.0745\n",
            "Epoch [10279/20000], Training Loss: 0.0810\n",
            "Epoch [10280/20000], Training Loss: 0.0737\n",
            "Epoch [10281/20000], Training Loss: 0.0814\n",
            "Epoch [10282/20000], Training Loss: 0.0772\n",
            "Epoch [10283/20000], Training Loss: 0.0750\n",
            "Epoch [10284/20000], Training Loss: 0.0777\n",
            "Epoch [10285/20000], Training Loss: 0.0801\n",
            "Epoch [10286/20000], Training Loss: 0.0765\n",
            "Epoch [10287/20000], Training Loss: 0.0801\n",
            "Epoch [10288/20000], Training Loss: 0.0781\n",
            "Epoch [10289/20000], Training Loss: 0.0855\n",
            "Epoch [10290/20000], Training Loss: 0.0818\n",
            "Epoch [10291/20000], Training Loss: 0.0752\n",
            "Epoch [10292/20000], Training Loss: 0.0830\n",
            "Epoch [10293/20000], Training Loss: 0.0779\n",
            "Epoch [10294/20000], Training Loss: 0.0809\n",
            "Epoch [10295/20000], Training Loss: 0.0746\n",
            "Epoch [10296/20000], Training Loss: 0.0805\n",
            "Epoch [10297/20000], Training Loss: 0.0808\n",
            "Epoch [10298/20000], Training Loss: 0.0825\n",
            "Epoch [10299/20000], Training Loss: 0.0785\n",
            "Epoch [10300/20000], Training Loss: 0.0769\n",
            "Epoch [10301/20000], Training Loss: 0.0792\n",
            "Epoch [10302/20000], Training Loss: 0.0742\n",
            "Epoch [10303/20000], Training Loss: 0.0849\n",
            "Epoch [10304/20000], Training Loss: 0.0789\n",
            "Epoch [10305/20000], Training Loss: 0.0790\n",
            "Epoch [10306/20000], Training Loss: 0.0806\n",
            "Epoch [10307/20000], Training Loss: 0.0750\n",
            "Epoch [10308/20000], Training Loss: 0.0818\n",
            "Epoch [10309/20000], Training Loss: 0.0812\n",
            "Epoch [10310/20000], Training Loss: 0.0817\n",
            "Epoch [10311/20000], Training Loss: 0.0787\n",
            "Epoch [10312/20000], Training Loss: 0.0745\n",
            "Epoch [10313/20000], Training Loss: 0.0760\n",
            "Epoch [10314/20000], Training Loss: 0.0767\n",
            "Epoch [10315/20000], Training Loss: 0.0803\n",
            "Epoch [10316/20000], Training Loss: 0.0754\n",
            "Epoch [10317/20000], Training Loss: 0.0804\n",
            "Epoch [10318/20000], Training Loss: 0.0805\n",
            "Epoch [10319/20000], Training Loss: 0.0787\n",
            "Epoch [10320/20000], Training Loss: 0.0763\n",
            "Epoch [10321/20000], Training Loss: 0.0817\n",
            "Epoch [10322/20000], Training Loss: 0.0778\n",
            "Epoch [10323/20000], Training Loss: 0.0838\n",
            "Epoch [10324/20000], Training Loss: 0.0781\n",
            "Epoch [10325/20000], Training Loss: 0.0860\n",
            "Epoch [10326/20000], Training Loss: 0.0759\n",
            "Epoch [10327/20000], Training Loss: 0.0802\n",
            "Epoch [10328/20000], Training Loss: 0.0747\n",
            "Epoch [10329/20000], Training Loss: 0.0782\n",
            "Epoch [10330/20000], Training Loss: 0.0823\n",
            "Epoch [10331/20000], Training Loss: 0.0824\n",
            "Epoch [10332/20000], Training Loss: 0.0743\n",
            "Epoch [10333/20000], Training Loss: 0.0795\n",
            "Epoch [10334/20000], Training Loss: 0.0793\n",
            "Epoch [10335/20000], Training Loss: 0.0889\n",
            "Epoch [10336/20000], Training Loss: 0.0841\n",
            "Epoch [10337/20000], Training Loss: 0.0810\n",
            "Epoch [10338/20000], Training Loss: 0.0813\n",
            "Epoch [10339/20000], Training Loss: 0.0814\n",
            "Epoch [10340/20000], Training Loss: 0.0757\n",
            "Epoch [10341/20000], Training Loss: 0.0741\n",
            "Epoch [10342/20000], Training Loss: 0.0845\n",
            "Epoch [10343/20000], Training Loss: 0.0874\n",
            "Epoch [10344/20000], Training Loss: 0.0855\n",
            "Epoch [10345/20000], Training Loss: 0.0842\n",
            "Epoch [10346/20000], Training Loss: 0.0853\n",
            "Epoch [10347/20000], Training Loss: 0.0768\n",
            "Epoch [10348/20000], Training Loss: 0.0794\n",
            "Epoch [10349/20000], Training Loss: 0.0810\n",
            "Epoch [10350/20000], Training Loss: 0.0728\n",
            "Epoch [10351/20000], Training Loss: 0.0829\n",
            "Epoch [10352/20000], Training Loss: 0.0749\n",
            "Epoch [10353/20000], Training Loss: 0.0872\n",
            "Epoch [10354/20000], Training Loss: 0.0831\n",
            "Epoch [10355/20000], Training Loss: 0.0746\n",
            "Epoch [10356/20000], Training Loss: 0.0809\n",
            "Epoch [10357/20000], Training Loss: 0.0817\n",
            "Epoch [10358/20000], Training Loss: 0.0814\n",
            "Epoch [10359/20000], Training Loss: 0.0756\n",
            "Epoch [10360/20000], Training Loss: 0.0867\n",
            "Epoch [10361/20000], Training Loss: 0.0755\n",
            "Epoch [10362/20000], Training Loss: 0.0862\n",
            "Epoch [10363/20000], Training Loss: 0.0753\n",
            "Epoch [10364/20000], Training Loss: 0.0833\n",
            "Epoch [10365/20000], Training Loss: 0.0857\n",
            "Epoch [10366/20000], Training Loss: 0.0797\n",
            "Epoch [10367/20000], Training Loss: 0.0757\n",
            "Epoch [10368/20000], Training Loss: 0.0786\n",
            "Epoch [10369/20000], Training Loss: 0.0864\n",
            "Epoch [10370/20000], Training Loss: 0.0813\n",
            "Epoch [10371/20000], Training Loss: 0.0804\n",
            "Epoch [10372/20000], Training Loss: 0.0761\n",
            "Epoch [10373/20000], Training Loss: 0.0773\n",
            "Epoch [10374/20000], Training Loss: 0.0810\n",
            "Epoch [10375/20000], Training Loss: 0.0806\n",
            "Epoch [10376/20000], Training Loss: 0.0819\n",
            "Epoch [10377/20000], Training Loss: 0.0825\n",
            "Epoch [10378/20000], Training Loss: 0.0835\n",
            "Epoch [10379/20000], Training Loss: 0.0761\n",
            "Epoch [10380/20000], Training Loss: 0.0803\n",
            "Epoch [10381/20000], Training Loss: 0.0760\n",
            "Epoch [10382/20000], Training Loss: 0.0838\n",
            "Epoch [10383/20000], Training Loss: 0.0762\n",
            "Epoch [10384/20000], Training Loss: 0.0781\n",
            "Epoch [10385/20000], Training Loss: 0.0825\n",
            "Epoch [10386/20000], Training Loss: 0.0811\n",
            "Epoch [10387/20000], Training Loss: 0.0788\n",
            "Epoch [10388/20000], Training Loss: 0.0751\n",
            "Epoch [10389/20000], Training Loss: 0.0828\n",
            "Epoch [10390/20000], Training Loss: 0.0801\n",
            "Epoch [10391/20000], Training Loss: 0.0767\n",
            "Epoch [10392/20000], Training Loss: 0.0868\n",
            "Epoch [10393/20000], Training Loss: 0.0808\n",
            "Epoch [10394/20000], Training Loss: 0.0774\n",
            "Epoch [10395/20000], Training Loss: 0.0813\n",
            "Epoch [10396/20000], Training Loss: 0.0803\n",
            "Epoch [10397/20000], Training Loss: 0.0783\n",
            "Epoch [10398/20000], Training Loss: 0.0778\n",
            "Epoch [10399/20000], Training Loss: 0.0832\n",
            "Epoch [10400/20000], Training Loss: 0.0768\n",
            "Epoch [10401/20000], Training Loss: 0.0856\n",
            "Epoch [10402/20000], Training Loss: 0.0778\n",
            "Epoch [10403/20000], Training Loss: 0.0831\n",
            "Epoch [10404/20000], Training Loss: 0.0859\n",
            "Epoch [10405/20000], Training Loss: 0.0768\n",
            "Epoch [10406/20000], Training Loss: 0.0746\n",
            "Epoch [10407/20000], Training Loss: 0.0833\n",
            "Epoch [10408/20000], Training Loss: 0.0802\n",
            "Epoch [10409/20000], Training Loss: 0.0796\n",
            "Epoch [10410/20000], Training Loss: 0.0757\n",
            "Epoch [10411/20000], Training Loss: 0.0756\n",
            "Epoch [10412/20000], Training Loss: 0.0797\n",
            "Epoch [10413/20000], Training Loss: 0.0747\n",
            "Epoch [10414/20000], Training Loss: 0.0752\n",
            "Epoch [10415/20000], Training Loss: 0.0810\n",
            "Epoch [10416/20000], Training Loss: 0.0797\n",
            "Epoch [10417/20000], Training Loss: 0.0816\n",
            "Epoch [10418/20000], Training Loss: 0.0795\n",
            "Epoch [10419/20000], Training Loss: 0.0749\n",
            "Epoch [10420/20000], Training Loss: 0.0793\n",
            "Epoch [10421/20000], Training Loss: 0.0813\n",
            "Epoch [10422/20000], Training Loss: 0.0768\n",
            "Epoch [10423/20000], Training Loss: 0.0817\n",
            "Epoch [10424/20000], Training Loss: 0.0797\n",
            "Epoch [10425/20000], Training Loss: 0.0794\n",
            "Epoch [10426/20000], Training Loss: 0.0739\n",
            "Epoch [10427/20000], Training Loss: 0.0747\n",
            "Epoch [10428/20000], Training Loss: 0.0830\n",
            "Epoch [10429/20000], Training Loss: 0.0807\n",
            "Epoch [10430/20000], Training Loss: 0.0787\n",
            "Epoch [10431/20000], Training Loss: 0.0766\n",
            "Epoch [10432/20000], Training Loss: 0.0795\n",
            "Epoch [10433/20000], Training Loss: 0.0786\n",
            "Epoch [10434/20000], Training Loss: 0.0759\n",
            "Epoch [10435/20000], Training Loss: 0.0807\n",
            "Epoch [10436/20000], Training Loss: 0.0779\n",
            "Epoch [10437/20000], Training Loss: 0.0738\n",
            "Epoch [10438/20000], Training Loss: 0.0768\n",
            "Epoch [10439/20000], Training Loss: 0.0802\n",
            "Epoch [10440/20000], Training Loss: 0.0792\n",
            "Epoch [10441/20000], Training Loss: 0.0786\n",
            "Epoch [10442/20000], Training Loss: 0.0848\n",
            "Epoch [10443/20000], Training Loss: 0.0793\n",
            "Epoch [10444/20000], Training Loss: 0.0768\n",
            "Epoch [10445/20000], Training Loss: 0.0761\n",
            "Epoch [10446/20000], Training Loss: 0.0806\n",
            "Epoch [10447/20000], Training Loss: 0.0809\n",
            "Epoch [10448/20000], Training Loss: 0.0798\n",
            "Epoch [10449/20000], Training Loss: 0.0849\n",
            "Epoch [10450/20000], Training Loss: 0.0802\n",
            "Epoch [10451/20000], Training Loss: 0.0786\n",
            "Epoch [10452/20000], Training Loss: 0.0810\n",
            "Epoch [10453/20000], Training Loss: 0.0750\n",
            "Epoch [10454/20000], Training Loss: 0.0802\n",
            "Epoch [10455/20000], Training Loss: 0.0757\n",
            "Epoch [10456/20000], Training Loss: 0.0831\n",
            "Epoch [10457/20000], Training Loss: 0.0780\n",
            "Epoch [10458/20000], Training Loss: 0.0827\n",
            "Epoch [10459/20000], Training Loss: 0.0790\n",
            "Epoch [10460/20000], Training Loss: 0.0828\n",
            "Epoch [10461/20000], Training Loss: 0.0728\n",
            "Epoch [10462/20000], Training Loss: 0.0861\n",
            "Epoch [10463/20000], Training Loss: 0.0804\n",
            "Epoch [10464/20000], Training Loss: 0.0764\n",
            "Epoch [10465/20000], Training Loss: 0.0791\n",
            "Epoch [10466/20000], Training Loss: 0.0793\n",
            "Epoch [10467/20000], Training Loss: 0.0745\n",
            "Epoch [10468/20000], Training Loss: 0.0857\n",
            "Epoch [10469/20000], Training Loss: 0.0863\n",
            "Epoch [10470/20000], Training Loss: 0.0779\n",
            "Epoch [10471/20000], Training Loss: 0.0817\n",
            "Epoch [10472/20000], Training Loss: 0.0793\n",
            "Epoch [10473/20000], Training Loss: 0.0811\n",
            "Epoch [10474/20000], Training Loss: 0.0857\n",
            "Epoch [10475/20000], Training Loss: 0.0756\n",
            "Epoch [10476/20000], Training Loss: 0.0778\n",
            "Epoch [10477/20000], Training Loss: 0.0722\n",
            "Epoch [10478/20000], Training Loss: 0.0798\n",
            "Epoch [10479/20000], Training Loss: 0.0798\n",
            "Epoch [10480/20000], Training Loss: 0.0793\n",
            "Epoch [10481/20000], Training Loss: 0.0767\n",
            "Epoch [10482/20000], Training Loss: 0.0777\n",
            "Epoch [10483/20000], Training Loss: 0.0782\n",
            "Epoch [10484/20000], Training Loss: 0.0831\n",
            "Epoch [10485/20000], Training Loss: 0.0870\n",
            "Epoch [10486/20000], Training Loss: 0.0804\n",
            "Epoch [10487/20000], Training Loss: 0.0845\n",
            "Epoch [10488/20000], Training Loss: 0.0751\n",
            "Epoch [10489/20000], Training Loss: 0.0790\n",
            "Epoch [10490/20000], Training Loss: 0.0771\n",
            "Epoch [10491/20000], Training Loss: 0.0810\n",
            "Epoch [10492/20000], Training Loss: 0.0866\n",
            "Epoch [10493/20000], Training Loss: 0.0840\n",
            "Epoch [10494/20000], Training Loss: 0.0799\n",
            "Epoch [10495/20000], Training Loss: 0.0791\n",
            "Epoch [10496/20000], Training Loss: 0.0757\n",
            "Epoch [10497/20000], Training Loss: 0.0852\n",
            "Epoch [10498/20000], Training Loss: 0.0809\n",
            "Epoch [10499/20000], Training Loss: 0.0794\n",
            "Epoch [10500/20000], Training Loss: 0.0764\n",
            "Epoch [10501/20000], Training Loss: 0.0818\n",
            "Epoch [10502/20000], Training Loss: 0.0805\n",
            "Epoch [10503/20000], Training Loss: 0.0811\n",
            "Epoch [10504/20000], Training Loss: 0.0799\n",
            "Epoch [10505/20000], Training Loss: 0.0729\n",
            "Epoch [10506/20000], Training Loss: 0.0808\n",
            "Epoch [10507/20000], Training Loss: 0.0761\n",
            "Epoch [10508/20000], Training Loss: 0.0777\n",
            "Epoch [10509/20000], Training Loss: 0.0778\n",
            "Epoch [10510/20000], Training Loss: 0.0803\n",
            "Epoch [10511/20000], Training Loss: 0.0854\n",
            "Epoch [10512/20000], Training Loss: 0.0874\n",
            "Epoch [10513/20000], Training Loss: 0.0822\n",
            "Epoch [10514/20000], Training Loss: 0.0829\n",
            "Epoch [10515/20000], Training Loss: 0.0823\n",
            "Epoch [10516/20000], Training Loss: 0.0798\n",
            "Epoch [10517/20000], Training Loss: 0.0812\n",
            "Epoch [10518/20000], Training Loss: 0.0799\n",
            "Epoch [10519/20000], Training Loss: 0.0761\n",
            "Epoch [10520/20000], Training Loss: 0.0804\n",
            "Epoch [10521/20000], Training Loss: 0.0760\n",
            "Epoch [10522/20000], Training Loss: 0.0811\n",
            "Epoch [10523/20000], Training Loss: 0.0808\n",
            "Epoch [10524/20000], Training Loss: 0.0793\n",
            "Epoch [10525/20000], Training Loss: 0.0802\n",
            "Epoch [10526/20000], Training Loss: 0.0841\n",
            "Epoch [10527/20000], Training Loss: 0.0803\n",
            "Epoch [10528/20000], Training Loss: 0.0851\n",
            "Epoch [10529/20000], Training Loss: 0.0753\n",
            "Epoch [10530/20000], Training Loss: 0.0760\n",
            "Epoch [10531/20000], Training Loss: 0.0772\n",
            "Epoch [10532/20000], Training Loss: 0.0798\n",
            "Epoch [10533/20000], Training Loss: 0.0748\n",
            "Epoch [10534/20000], Training Loss: 0.0793\n",
            "Epoch [10535/20000], Training Loss: 0.0836\n",
            "Epoch [10536/20000], Training Loss: 0.0825\n",
            "Epoch [10537/20000], Training Loss: 0.0770\n",
            "Epoch [10538/20000], Training Loss: 0.0800\n",
            "Epoch [10539/20000], Training Loss: 0.0769\n",
            "Epoch [10540/20000], Training Loss: 0.0786\n",
            "Epoch [10541/20000], Training Loss: 0.0858\n",
            "Epoch [10542/20000], Training Loss: 0.0820\n",
            "Epoch [10543/20000], Training Loss: 0.0763\n",
            "Epoch [10544/20000], Training Loss: 0.0785\n",
            "Epoch [10545/20000], Training Loss: 0.0760\n",
            "Epoch [10546/20000], Training Loss: 0.0810\n",
            "Epoch [10547/20000], Training Loss: 0.0806\n",
            "Epoch [10548/20000], Training Loss: 0.0748\n",
            "Epoch [10549/20000], Training Loss: 0.0856\n",
            "Epoch [10550/20000], Training Loss: 0.0815\n",
            "Epoch [10551/20000], Training Loss: 0.0786\n",
            "Epoch [10552/20000], Training Loss: 0.0780\n",
            "Epoch [10553/20000], Training Loss: 0.0886\n",
            "Epoch [10554/20000], Training Loss: 0.0747\n",
            "Epoch [10555/20000], Training Loss: 0.0785\n",
            "Epoch [10556/20000], Training Loss: 0.0786\n",
            "Epoch [10557/20000], Training Loss: 0.0826\n",
            "Epoch [10558/20000], Training Loss: 0.0755\n",
            "Epoch [10559/20000], Training Loss: 0.0778\n",
            "Epoch [10560/20000], Training Loss: 0.0815\n",
            "Epoch [10561/20000], Training Loss: 0.0774\n",
            "Epoch [10562/20000], Training Loss: 0.0803\n",
            "Epoch [10563/20000], Training Loss: 0.0789\n",
            "Epoch [10564/20000], Training Loss: 0.0757\n",
            "Epoch [10565/20000], Training Loss: 0.0776\n",
            "Epoch [10566/20000], Training Loss: 0.0825\n",
            "Epoch [10567/20000], Training Loss: 0.0769\n",
            "Epoch [10568/20000], Training Loss: 0.0814\n",
            "Epoch [10569/20000], Training Loss: 0.0811\n",
            "Epoch [10570/20000], Training Loss: 0.0809\n",
            "Epoch [10571/20000], Training Loss: 0.0755\n",
            "Epoch [10572/20000], Training Loss: 0.0779\n",
            "Epoch [10573/20000], Training Loss: 0.0793\n",
            "Epoch [10574/20000], Training Loss: 0.0773\n",
            "Epoch [10575/20000], Training Loss: 0.0802\n",
            "Epoch [10576/20000], Training Loss: 0.0803\n",
            "Epoch [10577/20000], Training Loss: 0.0806\n",
            "Epoch [10578/20000], Training Loss: 0.0785\n",
            "Epoch [10579/20000], Training Loss: 0.0805\n",
            "Epoch [10580/20000], Training Loss: 0.0832\n",
            "Epoch [10581/20000], Training Loss: 0.0747\n",
            "Epoch [10582/20000], Training Loss: 0.0802\n",
            "Epoch [10583/20000], Training Loss: 0.0758\n",
            "Epoch [10584/20000], Training Loss: 0.0768\n",
            "Epoch [10585/20000], Training Loss: 0.0798\n",
            "Epoch [10586/20000], Training Loss: 0.0835\n",
            "Epoch [10587/20000], Training Loss: 0.0812\n",
            "Epoch [10588/20000], Training Loss: 0.0837\n",
            "Epoch [10589/20000], Training Loss: 0.0823\n",
            "Epoch [10590/20000], Training Loss: 0.0772\n",
            "Epoch [10591/20000], Training Loss: 0.0774\n",
            "Epoch [10592/20000], Training Loss: 0.0841\n",
            "Epoch [10593/20000], Training Loss: 0.0851\n",
            "Epoch [10594/20000], Training Loss: 0.0796\n",
            "Epoch [10595/20000], Training Loss: 0.0769\n",
            "Epoch [10596/20000], Training Loss: 0.0751\n",
            "Epoch [10597/20000], Training Loss: 0.0746\n",
            "Epoch [10598/20000], Training Loss: 0.0768\n",
            "Epoch [10599/20000], Training Loss: 0.0795\n",
            "Epoch [10600/20000], Training Loss: 0.0812\n",
            "Epoch [10601/20000], Training Loss: 0.0749\n",
            "Epoch [10602/20000], Training Loss: 0.0813\n",
            "Epoch [10603/20000], Training Loss: 0.0770\n",
            "Epoch [10604/20000], Training Loss: 0.0823\n",
            "Epoch [10605/20000], Training Loss: 0.0736\n",
            "Epoch [10606/20000], Training Loss: 0.0852\n",
            "Epoch [10607/20000], Training Loss: 0.0778\n",
            "Epoch [10608/20000], Training Loss: 0.0840\n",
            "Epoch [10609/20000], Training Loss: 0.0781\n",
            "Epoch [10610/20000], Training Loss: 0.0858\n",
            "Epoch [10611/20000], Training Loss: 0.0781\n",
            "Epoch [10612/20000], Training Loss: 0.0831\n",
            "Epoch [10613/20000], Training Loss: 0.0793\n",
            "Epoch [10614/20000], Training Loss: 0.0789\n",
            "Epoch [10615/20000], Training Loss: 0.0783\n",
            "Epoch [10616/20000], Training Loss: 0.0751\n",
            "Epoch [10617/20000], Training Loss: 0.0765\n",
            "Epoch [10618/20000], Training Loss: 0.0864\n",
            "Epoch [10619/20000], Training Loss: 0.0787\n",
            "Epoch [10620/20000], Training Loss: 0.0780\n",
            "Epoch [10621/20000], Training Loss: 0.0825\n",
            "Epoch [10622/20000], Training Loss: 0.0772\n",
            "Epoch [10623/20000], Training Loss: 0.0852\n",
            "Epoch [10624/20000], Training Loss: 0.0844\n",
            "Epoch [10625/20000], Training Loss: 0.0745\n",
            "Epoch [10626/20000], Training Loss: 0.0766\n",
            "Epoch [10627/20000], Training Loss: 0.0808\n",
            "Epoch [10628/20000], Training Loss: 0.0781\n",
            "Epoch [10629/20000], Training Loss: 0.0867\n",
            "Epoch [10630/20000], Training Loss: 0.0776\n",
            "Epoch [10631/20000], Training Loss: 0.0767\n",
            "Epoch [10632/20000], Training Loss: 0.0794\n",
            "Epoch [10633/20000], Training Loss: 0.0828\n",
            "Epoch [10634/20000], Training Loss: 0.0750\n",
            "Epoch [10635/20000], Training Loss: 0.0854\n",
            "Epoch [10636/20000], Training Loss: 0.0745\n",
            "Epoch [10637/20000], Training Loss: 0.0834\n",
            "Epoch [10638/20000], Training Loss: 0.0814\n",
            "Epoch [10639/20000], Training Loss: 0.0792\n",
            "Epoch [10640/20000], Training Loss: 0.0833\n",
            "Epoch [10641/20000], Training Loss: 0.0779\n",
            "Epoch [10642/20000], Training Loss: 0.0786\n",
            "Epoch [10643/20000], Training Loss: 0.0807\n",
            "Epoch [10644/20000], Training Loss: 0.0792\n",
            "Epoch [10645/20000], Training Loss: 0.0775\n",
            "Epoch [10646/20000], Training Loss: 0.0794\n",
            "Epoch [10647/20000], Training Loss: 0.0792\n",
            "Epoch [10648/20000], Training Loss: 0.0744\n",
            "Epoch [10649/20000], Training Loss: 0.0745\n",
            "Epoch [10650/20000], Training Loss: 0.0809\n",
            "Epoch [10651/20000], Training Loss: 0.0797\n",
            "Epoch [10652/20000], Training Loss: 0.0825\n",
            "Epoch [10653/20000], Training Loss: 0.0735\n",
            "Epoch [10654/20000], Training Loss: 0.0735\n",
            "Epoch [10655/20000], Training Loss: 0.0789\n",
            "Epoch [10656/20000], Training Loss: 0.0736\n",
            "Epoch [10657/20000], Training Loss: 0.0807\n",
            "Epoch [10658/20000], Training Loss: 0.0780\n",
            "Epoch [10659/20000], Training Loss: 0.0803\n",
            "Epoch [10660/20000], Training Loss: 0.0795\n",
            "Epoch [10661/20000], Training Loss: 0.0741\n",
            "Epoch [10662/20000], Training Loss: 0.0765\n",
            "Epoch [10663/20000], Training Loss: 0.0859\n",
            "Epoch [10664/20000], Training Loss: 0.0782\n",
            "Epoch [10665/20000], Training Loss: 0.0762\n",
            "Epoch [10666/20000], Training Loss: 0.0830\n",
            "Epoch [10667/20000], Training Loss: 0.0788\n",
            "Epoch [10668/20000], Training Loss: 0.0786\n",
            "Epoch [10669/20000], Training Loss: 0.0853\n",
            "Epoch [10670/20000], Training Loss: 0.0809\n",
            "Epoch [10671/20000], Training Loss: 0.0858\n",
            "Epoch [10672/20000], Training Loss: 0.0801\n",
            "Epoch [10673/20000], Training Loss: 0.0823\n",
            "Epoch [10674/20000], Training Loss: 0.0782\n",
            "Epoch [10675/20000], Training Loss: 0.0844\n",
            "Epoch [10676/20000], Training Loss: 0.0808\n",
            "Epoch [10677/20000], Training Loss: 0.0750\n",
            "Epoch [10678/20000], Training Loss: 0.0788\n",
            "Epoch [10679/20000], Training Loss: 0.0761\n",
            "Epoch [10680/20000], Training Loss: 0.0806\n",
            "Epoch [10681/20000], Training Loss: 0.0814\n",
            "Epoch [10682/20000], Training Loss: 0.0739\n",
            "Epoch [10683/20000], Training Loss: 0.0848\n",
            "Epoch [10684/20000], Training Loss: 0.0851\n",
            "Epoch [10685/20000], Training Loss: 0.0790\n",
            "Epoch [10686/20000], Training Loss: 0.0762\n",
            "Epoch [10687/20000], Training Loss: 0.0846\n",
            "Epoch [10688/20000], Training Loss: 0.0826\n",
            "Epoch [10689/20000], Training Loss: 0.0737\n",
            "Epoch [10690/20000], Training Loss: 0.0800\n",
            "Epoch [10691/20000], Training Loss: 0.0814\n",
            "Epoch [10692/20000], Training Loss: 0.0800\n",
            "Epoch [10693/20000], Training Loss: 0.0831\n",
            "Epoch [10694/20000], Training Loss: 0.0833\n",
            "Epoch [10695/20000], Training Loss: 0.0814\n",
            "Epoch [10696/20000], Training Loss: 0.0795\n",
            "Epoch [10697/20000], Training Loss: 0.0761\n",
            "Epoch [10698/20000], Training Loss: 0.0861\n",
            "Epoch [10699/20000], Training Loss: 0.0788\n",
            "Epoch [10700/20000], Training Loss: 0.0807\n",
            "Epoch [10701/20000], Training Loss: 0.0780\n",
            "Epoch [10702/20000], Training Loss: 0.0768\n",
            "Epoch [10703/20000], Training Loss: 0.0848\n",
            "Epoch [10704/20000], Training Loss: 0.0810\n",
            "Epoch [10705/20000], Training Loss: 0.0805\n",
            "Epoch [10706/20000], Training Loss: 0.0753\n",
            "Epoch [10707/20000], Training Loss: 0.0799\n",
            "Epoch [10708/20000], Training Loss: 0.0780\n",
            "Epoch [10709/20000], Training Loss: 0.0807\n",
            "Epoch [10710/20000], Training Loss: 0.0822\n",
            "Epoch [10711/20000], Training Loss: 0.0769\n",
            "Epoch [10712/20000], Training Loss: 0.0752\n",
            "Epoch [10713/20000], Training Loss: 0.0745\n",
            "Epoch [10714/20000], Training Loss: 0.0781\n",
            "Epoch [10715/20000], Training Loss: 0.0791\n",
            "Epoch [10716/20000], Training Loss: 0.0796\n",
            "Epoch [10717/20000], Training Loss: 0.0825\n",
            "Epoch [10718/20000], Training Loss: 0.0834\n",
            "Epoch [10719/20000], Training Loss: 0.0873\n",
            "Epoch [10720/20000], Training Loss: 0.0763\n",
            "Epoch [10721/20000], Training Loss: 0.0774\n",
            "Epoch [10722/20000], Training Loss: 0.0799\n",
            "Epoch [10723/20000], Training Loss: 0.0844\n",
            "Epoch [10724/20000], Training Loss: 0.0777\n",
            "Epoch [10725/20000], Training Loss: 0.0772\n",
            "Epoch [10726/20000], Training Loss: 0.0828\n",
            "Epoch [10727/20000], Training Loss: 0.0796\n",
            "Epoch [10728/20000], Training Loss: 0.0776\n",
            "Epoch [10729/20000], Training Loss: 0.0754\n",
            "Epoch [10730/20000], Training Loss: 0.0830\n",
            "Epoch [10731/20000], Training Loss: 0.0768\n",
            "Epoch [10732/20000], Training Loss: 0.0816\n",
            "Epoch [10733/20000], Training Loss: 0.0759\n",
            "Epoch [10734/20000], Training Loss: 0.0739\n",
            "Epoch [10735/20000], Training Loss: 0.0802\n",
            "Epoch [10736/20000], Training Loss: 0.0798\n",
            "Epoch [10737/20000], Training Loss: 0.0810\n",
            "Epoch [10738/20000], Training Loss: 0.0788\n",
            "Epoch [10739/20000], Training Loss: 0.0803\n",
            "Epoch [10740/20000], Training Loss: 0.0832\n",
            "Epoch [10741/20000], Training Loss: 0.0793\n",
            "Epoch [10742/20000], Training Loss: 0.0825\n",
            "Epoch [10743/20000], Training Loss: 0.0779\n",
            "Epoch [10744/20000], Training Loss: 0.0837\n",
            "Epoch [10745/20000], Training Loss: 0.0850\n",
            "Epoch [10746/20000], Training Loss: 0.0743\n",
            "Epoch [10747/20000], Training Loss: 0.0763\n",
            "Epoch [10748/20000], Training Loss: 0.0747\n",
            "Epoch [10749/20000], Training Loss: 0.0801\n",
            "Epoch [10750/20000], Training Loss: 0.0760\n",
            "Epoch [10751/20000], Training Loss: 0.0788\n",
            "Epoch [10752/20000], Training Loss: 0.0813\n",
            "Epoch [10753/20000], Training Loss: 0.0849\n",
            "Epoch [10754/20000], Training Loss: 0.0751\n",
            "Epoch [10755/20000], Training Loss: 0.0785\n",
            "Epoch [10756/20000], Training Loss: 0.0802\n",
            "Epoch [10757/20000], Training Loss: 0.0749\n",
            "Epoch [10758/20000], Training Loss: 0.0857\n",
            "Epoch [10759/20000], Training Loss: 0.0871\n",
            "Epoch [10760/20000], Training Loss: 0.0794\n",
            "Epoch [10761/20000], Training Loss: 0.0761\n",
            "Epoch [10762/20000], Training Loss: 0.0866\n",
            "Epoch [10763/20000], Training Loss: 0.0823\n",
            "Epoch [10764/20000], Training Loss: 0.0770\n",
            "Epoch [10765/20000], Training Loss: 0.0787\n",
            "Epoch [10766/20000], Training Loss: 0.0775\n",
            "Epoch [10767/20000], Training Loss: 0.0796\n",
            "Epoch [10768/20000], Training Loss: 0.0744\n",
            "Epoch [10769/20000], Training Loss: 0.0833\n",
            "Epoch [10770/20000], Training Loss: 0.0740\n",
            "Epoch [10771/20000], Training Loss: 0.0759\n",
            "Epoch [10772/20000], Training Loss: 0.0762\n",
            "Epoch [10773/20000], Training Loss: 0.0783\n",
            "Epoch [10774/20000], Training Loss: 0.0758\n",
            "Epoch [10775/20000], Training Loss: 0.0806\n",
            "Epoch [10776/20000], Training Loss: 0.0822\n",
            "Epoch [10777/20000], Training Loss: 0.0790\n",
            "Epoch [10778/20000], Training Loss: 0.0759\n",
            "Epoch [10779/20000], Training Loss: 0.0767\n",
            "Epoch [10780/20000], Training Loss: 0.0766\n",
            "Epoch [10781/20000], Training Loss: 0.0811\n",
            "Epoch [10782/20000], Training Loss: 0.0751\n",
            "Epoch [10783/20000], Training Loss: 0.0774\n",
            "Epoch [10784/20000], Training Loss: 0.0747\n",
            "Epoch [10785/20000], Training Loss: 0.0780\n",
            "Epoch [10786/20000], Training Loss: 0.0827\n",
            "Epoch [10787/20000], Training Loss: 0.0841\n",
            "Epoch [10788/20000], Training Loss: 0.0780\n",
            "Epoch [10789/20000], Training Loss: 0.0800\n",
            "Epoch [10790/20000], Training Loss: 0.0753\n",
            "Epoch [10791/20000], Training Loss: 0.0842\n",
            "Epoch [10792/20000], Training Loss: 0.0840\n",
            "Epoch [10793/20000], Training Loss: 0.0799\n",
            "Epoch [10794/20000], Training Loss: 0.0758\n",
            "Epoch [10795/20000], Training Loss: 0.0817\n",
            "Epoch [10796/20000], Training Loss: 0.0757\n",
            "Epoch [10797/20000], Training Loss: 0.0804\n",
            "Epoch [10798/20000], Training Loss: 0.0799\n",
            "Epoch [10799/20000], Training Loss: 0.0805\n",
            "Epoch [10800/20000], Training Loss: 0.0843\n",
            "Epoch [10801/20000], Training Loss: 0.0808\n",
            "Epoch [10802/20000], Training Loss: 0.0769\n",
            "Epoch [10803/20000], Training Loss: 0.0882\n",
            "Epoch [10804/20000], Training Loss: 0.0818\n",
            "Epoch [10805/20000], Training Loss: 0.0777\n",
            "Epoch [10806/20000], Training Loss: 0.0783\n",
            "Epoch [10807/20000], Training Loss: 0.0758\n",
            "Epoch [10808/20000], Training Loss: 0.0762\n",
            "Epoch [10809/20000], Training Loss: 0.0748\n",
            "Epoch [10810/20000], Training Loss: 0.0767\n",
            "Epoch [10811/20000], Training Loss: 0.0820\n",
            "Epoch [10812/20000], Training Loss: 0.0808\n",
            "Epoch [10813/20000], Training Loss: 0.0836\n",
            "Epoch [10814/20000], Training Loss: 0.0792\n",
            "Epoch [10815/20000], Training Loss: 0.0794\n",
            "Epoch [10816/20000], Training Loss: 0.0842\n",
            "Epoch [10817/20000], Training Loss: 0.0747\n",
            "Epoch [10818/20000], Training Loss: 0.0795\n",
            "Epoch [10819/20000], Training Loss: 0.0787\n",
            "Epoch [10820/20000], Training Loss: 0.0787\n",
            "Epoch [10821/20000], Training Loss: 0.0811\n",
            "Epoch [10822/20000], Training Loss: 0.0751\n",
            "Epoch [10823/20000], Training Loss: 0.0857\n",
            "Epoch [10824/20000], Training Loss: 0.0772\n",
            "Epoch [10825/20000], Training Loss: 0.0784\n",
            "Epoch [10826/20000], Training Loss: 0.0777\n",
            "Epoch [10827/20000], Training Loss: 0.0767\n",
            "Epoch [10828/20000], Training Loss: 0.0763\n",
            "Epoch [10829/20000], Training Loss: 0.0798\n",
            "Epoch [10830/20000], Training Loss: 0.0750\n",
            "Epoch [10831/20000], Training Loss: 0.0841\n",
            "Epoch [10832/20000], Training Loss: 0.0776\n",
            "Epoch [10833/20000], Training Loss: 0.0800\n",
            "Epoch [10834/20000], Training Loss: 0.0786\n",
            "Epoch [10835/20000], Training Loss: 0.0845\n",
            "Epoch [10836/20000], Training Loss: 0.0807\n",
            "Epoch [10837/20000], Training Loss: 0.0818\n",
            "Epoch [10838/20000], Training Loss: 0.0757\n",
            "Epoch [10839/20000], Training Loss: 0.0800\n",
            "Epoch [10840/20000], Training Loss: 0.0859\n",
            "Epoch [10841/20000], Training Loss: 0.0828\n",
            "Epoch [10842/20000], Training Loss: 0.0792\n",
            "Epoch [10843/20000], Training Loss: 0.0755\n",
            "Epoch [10844/20000], Training Loss: 0.0806\n",
            "Epoch [10845/20000], Training Loss: 0.0742\n",
            "Epoch [10846/20000], Training Loss: 0.0834\n",
            "Epoch [10847/20000], Training Loss: 0.0816\n",
            "Epoch [10848/20000], Training Loss: 0.0806\n",
            "Epoch [10849/20000], Training Loss: 0.0838\n",
            "Epoch [10850/20000], Training Loss: 0.0799\n",
            "Epoch [10851/20000], Training Loss: 0.0801\n",
            "Epoch [10852/20000], Training Loss: 0.0820\n",
            "Epoch [10853/20000], Training Loss: 0.0772\n",
            "Epoch [10854/20000], Training Loss: 0.0716\n",
            "Epoch [10855/20000], Training Loss: 0.0760\n",
            "Epoch [10856/20000], Training Loss: 0.0836\n",
            "Epoch [10857/20000], Training Loss: 0.0739\n",
            "Epoch [10858/20000], Training Loss: 0.0839\n",
            "Epoch [10859/20000], Training Loss: 0.0817\n",
            "Epoch [10860/20000], Training Loss: 0.0780\n",
            "Epoch [10861/20000], Training Loss: 0.0748\n",
            "Epoch [10862/20000], Training Loss: 0.0850\n",
            "Epoch [10863/20000], Training Loss: 0.0779\n",
            "Epoch [10864/20000], Training Loss: 0.0787\n",
            "Epoch [10865/20000], Training Loss: 0.0785\n",
            "Epoch [10866/20000], Training Loss: 0.0778\n",
            "Epoch [10867/20000], Training Loss: 0.0829\n",
            "Epoch [10868/20000], Training Loss: 0.0792\n",
            "Epoch [10869/20000], Training Loss: 0.0788\n",
            "Epoch [10870/20000], Training Loss: 0.0873\n",
            "Epoch [10871/20000], Training Loss: 0.0825\n",
            "Epoch [10872/20000], Training Loss: 0.0869\n",
            "Epoch [10873/20000], Training Loss: 0.0785\n",
            "Epoch [10874/20000], Training Loss: 0.0784\n",
            "Epoch [10875/20000], Training Loss: 0.0739\n",
            "Epoch [10876/20000], Training Loss: 0.0791\n",
            "Epoch [10877/20000], Training Loss: 0.0740\n",
            "Epoch [10878/20000], Training Loss: 0.0828\n",
            "Epoch [10879/20000], Training Loss: 0.0798\n",
            "Epoch [10880/20000], Training Loss: 0.0749\n",
            "Epoch [10881/20000], Training Loss: 0.0729\n",
            "Epoch [10882/20000], Training Loss: 0.0825\n",
            "Epoch [10883/20000], Training Loss: 0.0867\n",
            "Epoch [10884/20000], Training Loss: 0.0836\n",
            "Epoch [10885/20000], Training Loss: 0.0819\n",
            "Epoch [10886/20000], Training Loss: 0.0806\n",
            "Epoch [10887/20000], Training Loss: 0.0818\n",
            "Epoch [10888/20000], Training Loss: 0.0853\n",
            "Epoch [10889/20000], Training Loss: 0.0793\n",
            "Epoch [10890/20000], Training Loss: 0.0755\n",
            "Epoch [10891/20000], Training Loss: 0.0851\n",
            "Epoch [10892/20000], Training Loss: 0.0743\n",
            "Epoch [10893/20000], Training Loss: 0.0837\n",
            "Epoch [10894/20000], Training Loss: 0.0832\n",
            "Epoch [10895/20000], Training Loss: 0.0883\n",
            "Epoch [10896/20000], Training Loss: 0.0797\n",
            "Epoch [10897/20000], Training Loss: 0.0816\n",
            "Epoch [10898/20000], Training Loss: 0.0855\n",
            "Epoch [10899/20000], Training Loss: 0.0813\n",
            "Epoch [10900/20000], Training Loss: 0.0826\n",
            "Epoch [10901/20000], Training Loss: 0.0818\n",
            "Epoch [10902/20000], Training Loss: 0.0729\n",
            "Epoch [10903/20000], Training Loss: 0.0737\n",
            "Epoch [10904/20000], Training Loss: 0.0771\n",
            "Epoch [10905/20000], Training Loss: 0.0828\n",
            "Epoch [10906/20000], Training Loss: 0.0768\n",
            "Epoch [10907/20000], Training Loss: 0.0825\n",
            "Epoch [10908/20000], Training Loss: 0.0827\n",
            "Epoch [10909/20000], Training Loss: 0.0826\n",
            "Epoch [10910/20000], Training Loss: 0.0801\n",
            "Epoch [10911/20000], Training Loss: 0.0788\n",
            "Epoch [10912/20000], Training Loss: 0.0821\n",
            "Epoch [10913/20000], Training Loss: 0.0772\n",
            "Epoch [10914/20000], Training Loss: 0.0866\n",
            "Epoch [10915/20000], Training Loss: 0.0768\n",
            "Epoch [10916/20000], Training Loss: 0.0836\n",
            "Epoch [10917/20000], Training Loss: 0.0754\n",
            "Epoch [10918/20000], Training Loss: 0.0832\n",
            "Epoch [10919/20000], Training Loss: 0.0746\n",
            "Epoch [10920/20000], Training Loss: 0.0816\n",
            "Epoch [10921/20000], Training Loss: 0.0806\n",
            "Epoch [10922/20000], Training Loss: 0.0816\n",
            "Epoch [10923/20000], Training Loss: 0.0744\n",
            "Epoch [10924/20000], Training Loss: 0.0794\n",
            "Epoch [10925/20000], Training Loss: 0.0834\n",
            "Epoch [10926/20000], Training Loss: 0.0804\n",
            "Epoch [10927/20000], Training Loss: 0.0796\n",
            "Epoch [10928/20000], Training Loss: 0.0793\n",
            "Epoch [10929/20000], Training Loss: 0.0853\n",
            "Epoch [10930/20000], Training Loss: 0.0757\n",
            "Epoch [10931/20000], Training Loss: 0.0797\n",
            "Epoch [10932/20000], Training Loss: 0.0773\n",
            "Epoch [10933/20000], Training Loss: 0.0732\n",
            "Epoch [10934/20000], Training Loss: 0.0754\n",
            "Epoch [10935/20000], Training Loss: 0.0793\n",
            "Epoch [10936/20000], Training Loss: 0.0838\n",
            "Epoch [10937/20000], Training Loss: 0.0744\n",
            "Epoch [10938/20000], Training Loss: 0.0757\n",
            "Epoch [10939/20000], Training Loss: 0.0803\n",
            "Epoch [10940/20000], Training Loss: 0.0741\n",
            "Epoch [10941/20000], Training Loss: 0.0757\n",
            "Epoch [10942/20000], Training Loss: 0.0862\n",
            "Epoch [10943/20000], Training Loss: 0.0748\n",
            "Epoch [10944/20000], Training Loss: 0.0849\n",
            "Epoch [10945/20000], Training Loss: 0.0800\n",
            "Epoch [10946/20000], Training Loss: 0.0806\n",
            "Epoch [10947/20000], Training Loss: 0.0797\n",
            "Epoch [10948/20000], Training Loss: 0.0778\n",
            "Epoch [10949/20000], Training Loss: 0.0853\n",
            "Epoch [10950/20000], Training Loss: 0.0774\n",
            "Epoch [10951/20000], Training Loss: 0.0760\n",
            "Epoch [10952/20000], Training Loss: 0.0794\n",
            "Epoch [10953/20000], Training Loss: 0.0830\n",
            "Epoch [10954/20000], Training Loss: 0.0812\n",
            "Epoch [10955/20000], Training Loss: 0.0809\n",
            "Epoch [10956/20000], Training Loss: 0.0793\n",
            "Epoch [10957/20000], Training Loss: 0.0799\n",
            "Epoch [10958/20000], Training Loss: 0.0826\n",
            "Epoch [10959/20000], Training Loss: 0.0747\n",
            "Epoch [10960/20000], Training Loss: 0.0745\n",
            "Epoch [10961/20000], Training Loss: 0.0787\n",
            "Epoch [10962/20000], Training Loss: 0.0785\n",
            "Epoch [10963/20000], Training Loss: 0.0803\n",
            "Epoch [10964/20000], Training Loss: 0.0813\n",
            "Epoch [10965/20000], Training Loss: 0.0758\n",
            "Epoch [10966/20000], Training Loss: 0.0813\n",
            "Epoch [10967/20000], Training Loss: 0.0762\n",
            "Epoch [10968/20000], Training Loss: 0.0741\n",
            "Epoch [10969/20000], Training Loss: 0.0809\n",
            "Epoch [10970/20000], Training Loss: 0.0783\n",
            "Epoch [10971/20000], Training Loss: 0.0861\n",
            "Epoch [10972/20000], Training Loss: 0.0819\n",
            "Epoch [10973/20000], Training Loss: 0.0771\n",
            "Epoch [10974/20000], Training Loss: 0.0793\n",
            "Epoch [10975/20000], Training Loss: 0.0801\n",
            "Epoch [10976/20000], Training Loss: 0.0761\n",
            "Epoch [10977/20000], Training Loss: 0.0823\n",
            "Epoch [10978/20000], Training Loss: 0.0785\n",
            "Epoch [10979/20000], Training Loss: 0.0758\n",
            "Epoch [10980/20000], Training Loss: 0.0756\n",
            "Epoch [10981/20000], Training Loss: 0.0826\n",
            "Epoch [10982/20000], Training Loss: 0.0811\n",
            "Epoch [10983/20000], Training Loss: 0.0777\n",
            "Epoch [10984/20000], Training Loss: 0.0829\n",
            "Epoch [10985/20000], Training Loss: 0.0783\n",
            "Epoch [10986/20000], Training Loss: 0.0796\n",
            "Epoch [10987/20000], Training Loss: 0.0732\n",
            "Epoch [10988/20000], Training Loss: 0.0767\n",
            "Epoch [10989/20000], Training Loss: 0.0802\n",
            "Epoch [10990/20000], Training Loss: 0.0834\n",
            "Epoch [10991/20000], Training Loss: 0.0743\n",
            "Epoch [10992/20000], Training Loss: 0.0786\n",
            "Epoch [10993/20000], Training Loss: 0.0825\n",
            "Epoch [10994/20000], Training Loss: 0.0804\n",
            "Epoch [10995/20000], Training Loss: 0.0802\n",
            "Epoch [10996/20000], Training Loss: 0.0774\n",
            "Epoch [10997/20000], Training Loss: 0.0832\n",
            "Epoch [10998/20000], Training Loss: 0.0817\n",
            "Epoch [10999/20000], Training Loss: 0.0727\n",
            "Epoch [11000/20000], Training Loss: 0.0835\n",
            "Epoch [11001/20000], Training Loss: 0.0855\n",
            "Epoch [11002/20000], Training Loss: 0.0769\n",
            "Epoch [11003/20000], Training Loss: 0.0783\n",
            "Epoch [11004/20000], Training Loss: 0.0769\n",
            "Epoch [11005/20000], Training Loss: 0.0829\n",
            "Epoch [11006/20000], Training Loss: 0.0751\n",
            "Epoch [11007/20000], Training Loss: 0.0774\n",
            "Epoch [11008/20000], Training Loss: 0.0794\n",
            "Epoch [11009/20000], Training Loss: 0.0731\n",
            "Epoch [11010/20000], Training Loss: 0.0771\n",
            "Epoch [11011/20000], Training Loss: 0.0784\n",
            "Epoch [11012/20000], Training Loss: 0.0783\n",
            "Epoch [11013/20000], Training Loss: 0.0774\n",
            "Epoch [11014/20000], Training Loss: 0.0809\n",
            "Epoch [11015/20000], Training Loss: 0.0762\n",
            "Epoch [11016/20000], Training Loss: 0.0776\n",
            "Epoch [11017/20000], Training Loss: 0.0809\n",
            "Epoch [11018/20000], Training Loss: 0.0825\n",
            "Epoch [11019/20000], Training Loss: 0.0834\n",
            "Epoch [11020/20000], Training Loss: 0.0826\n",
            "Epoch [11021/20000], Training Loss: 0.0755\n",
            "Epoch [11022/20000], Training Loss: 0.0737\n",
            "Epoch [11023/20000], Training Loss: 0.0769\n",
            "Epoch [11024/20000], Training Loss: 0.0835\n",
            "Epoch [11025/20000], Training Loss: 0.0791\n",
            "Epoch [11026/20000], Training Loss: 0.0819\n",
            "Epoch [11027/20000], Training Loss: 0.0838\n",
            "Epoch [11028/20000], Training Loss: 0.0808\n",
            "Epoch [11029/20000], Training Loss: 0.0789\n",
            "Epoch [11030/20000], Training Loss: 0.0789\n",
            "Epoch [11031/20000], Training Loss: 0.0817\n",
            "Epoch [11032/20000], Training Loss: 0.0752\n",
            "Epoch [11033/20000], Training Loss: 0.0811\n",
            "Epoch [11034/20000], Training Loss: 0.0733\n",
            "Epoch [11035/20000], Training Loss: 0.0796\n",
            "Epoch [11036/20000], Training Loss: 0.0792\n",
            "Epoch [11037/20000], Training Loss: 0.0779\n",
            "Epoch [11038/20000], Training Loss: 0.0828\n",
            "Epoch [11039/20000], Training Loss: 0.0744\n",
            "Epoch [11040/20000], Training Loss: 0.0832\n",
            "Epoch [11041/20000], Training Loss: 0.0737\n",
            "Epoch [11042/20000], Training Loss: 0.0795\n",
            "Epoch [11043/20000], Training Loss: 0.0813\n",
            "Epoch [11044/20000], Training Loss: 0.0802\n",
            "Epoch [11045/20000], Training Loss: 0.0801\n",
            "Epoch [11046/20000], Training Loss: 0.0785\n",
            "Epoch [11047/20000], Training Loss: 0.0822\n",
            "Epoch [11048/20000], Training Loss: 0.0768\n",
            "Epoch [11049/20000], Training Loss: 0.0804\n",
            "Epoch [11050/20000], Training Loss: 0.0783\n",
            "Epoch [11051/20000], Training Loss: 0.0769\n",
            "Epoch [11052/20000], Training Loss: 0.0810\n",
            "Epoch [11053/20000], Training Loss: 0.0828\n",
            "Epoch [11054/20000], Training Loss: 0.0790\n",
            "Epoch [11055/20000], Training Loss: 0.0842\n",
            "Epoch [11056/20000], Training Loss: 0.0822\n",
            "Epoch [11057/20000], Training Loss: 0.0793\n",
            "Epoch [11058/20000], Training Loss: 0.0792\n",
            "Epoch [11059/20000], Training Loss: 0.0790\n",
            "Epoch [11060/20000], Training Loss: 0.0749\n",
            "Epoch [11061/20000], Training Loss: 0.0845\n",
            "Epoch [11062/20000], Training Loss: 0.0773\n",
            "Epoch [11063/20000], Training Loss: 0.0791\n",
            "Epoch [11064/20000], Training Loss: 0.0769\n",
            "Epoch [11065/20000], Training Loss: 0.0778\n",
            "Epoch [11066/20000], Training Loss: 0.0810\n",
            "Epoch [11067/20000], Training Loss: 0.0843\n",
            "Epoch [11068/20000], Training Loss: 0.0754\n",
            "Epoch [11069/20000], Training Loss: 0.0825\n",
            "Epoch [11070/20000], Training Loss: 0.0799\n",
            "Epoch [11071/20000], Training Loss: 0.0804\n",
            "Epoch [11072/20000], Training Loss: 0.0792\n",
            "Epoch [11073/20000], Training Loss: 0.0766\n",
            "Epoch [11074/20000], Training Loss: 0.0820\n",
            "Epoch [11075/20000], Training Loss: 0.0860\n",
            "Epoch [11076/20000], Training Loss: 0.0828\n",
            "Epoch [11077/20000], Training Loss: 0.0849\n",
            "Epoch [11078/20000], Training Loss: 0.0749\n",
            "Epoch [11079/20000], Training Loss: 0.0736\n",
            "Epoch [11080/20000], Training Loss: 0.0786\n",
            "Epoch [11081/20000], Training Loss: 0.0848\n",
            "Epoch [11082/20000], Training Loss: 0.0835\n",
            "Epoch [11083/20000], Training Loss: 0.0763\n",
            "Epoch [11084/20000], Training Loss: 0.0817\n",
            "Epoch [11085/20000], Training Loss: 0.0745\n",
            "Epoch [11086/20000], Training Loss: 0.0775\n",
            "Epoch [11087/20000], Training Loss: 0.0755\n",
            "Epoch [11088/20000], Training Loss: 0.0890\n",
            "Epoch [11089/20000], Training Loss: 0.0758\n",
            "Epoch [11090/20000], Training Loss: 0.0740\n",
            "Epoch [11091/20000], Training Loss: 0.0826\n",
            "Epoch [11092/20000], Training Loss: 0.0794\n",
            "Epoch [11093/20000], Training Loss: 0.0751\n",
            "Epoch [11094/20000], Training Loss: 0.0817\n",
            "Epoch [11095/20000], Training Loss: 0.0765\n",
            "Epoch [11096/20000], Training Loss: 0.0787\n",
            "Epoch [11097/20000], Training Loss: 0.0810\n",
            "Epoch [11098/20000], Training Loss: 0.0794\n",
            "Epoch [11099/20000], Training Loss: 0.0803\n",
            "Epoch [11100/20000], Training Loss: 0.0763\n",
            "Epoch [11101/20000], Training Loss: 0.0789\n",
            "Epoch [11102/20000], Training Loss: 0.0799\n",
            "Epoch [11103/20000], Training Loss: 0.0770\n",
            "Epoch [11104/20000], Training Loss: 0.0842\n",
            "Epoch [11105/20000], Training Loss: 0.0762\n",
            "Epoch [11106/20000], Training Loss: 0.0775\n",
            "Epoch [11107/20000], Training Loss: 0.0795\n",
            "Epoch [11108/20000], Training Loss: 0.0783\n",
            "Epoch [11109/20000], Training Loss: 0.0795\n",
            "Epoch [11110/20000], Training Loss: 0.0732\n",
            "Epoch [11111/20000], Training Loss: 0.0787\n",
            "Epoch [11112/20000], Training Loss: 0.0791\n",
            "Epoch [11113/20000], Training Loss: 0.0820\n",
            "Epoch [11114/20000], Training Loss: 0.0732\n",
            "Epoch [11115/20000], Training Loss: 0.0775\n",
            "Epoch [11116/20000], Training Loss: 0.0743\n",
            "Epoch [11117/20000], Training Loss: 0.0764\n",
            "Epoch [11118/20000], Training Loss: 0.0758\n",
            "Epoch [11119/20000], Training Loss: 0.0816\n",
            "Epoch [11120/20000], Training Loss: 0.0817\n",
            "Epoch [11121/20000], Training Loss: 0.0829\n",
            "Epoch [11122/20000], Training Loss: 0.0771\n",
            "Epoch [11123/20000], Training Loss: 0.0819\n",
            "Epoch [11124/20000], Training Loss: 0.0786\n",
            "Epoch [11125/20000], Training Loss: 0.0802\n",
            "Epoch [11126/20000], Training Loss: 0.0779\n",
            "Epoch [11127/20000], Training Loss: 0.0772\n",
            "Epoch [11128/20000], Training Loss: 0.0847\n",
            "Epoch [11129/20000], Training Loss: 0.0765\n",
            "Epoch [11130/20000], Training Loss: 0.0829\n",
            "Epoch [11131/20000], Training Loss: 0.0792\n",
            "Epoch [11132/20000], Training Loss: 0.0780\n",
            "Epoch [11133/20000], Training Loss: 0.0790\n",
            "Epoch [11134/20000], Training Loss: 0.0842\n",
            "Epoch [11135/20000], Training Loss: 0.0802\n",
            "Epoch [11136/20000], Training Loss: 0.0799\n",
            "Epoch [11137/20000], Training Loss: 0.0787\n",
            "Epoch [11138/20000], Training Loss: 0.0850\n",
            "Epoch [11139/20000], Training Loss: 0.0810\n",
            "Epoch [11140/20000], Training Loss: 0.0818\n",
            "Epoch [11141/20000], Training Loss: 0.0739\n",
            "Epoch [11142/20000], Training Loss: 0.0835\n",
            "Epoch [11143/20000], Training Loss: 0.0774\n",
            "Epoch [11144/20000], Training Loss: 0.0752\n",
            "Epoch [11145/20000], Training Loss: 0.0798\n",
            "Epoch [11146/20000], Training Loss: 0.0812\n",
            "Epoch [11147/20000], Training Loss: 0.0801\n",
            "Epoch [11148/20000], Training Loss: 0.0784\n",
            "Epoch [11149/20000], Training Loss: 0.0771\n",
            "Epoch [11150/20000], Training Loss: 0.0787\n",
            "Epoch [11151/20000], Training Loss: 0.0791\n",
            "Epoch [11152/20000], Training Loss: 0.0842\n",
            "Epoch [11153/20000], Training Loss: 0.0751\n",
            "Epoch [11154/20000], Training Loss: 0.0818\n",
            "Epoch [11155/20000], Training Loss: 0.0764\n",
            "Epoch [11156/20000], Training Loss: 0.0758\n",
            "Epoch [11157/20000], Training Loss: 0.0780\n",
            "Epoch [11158/20000], Training Loss: 0.0822\n",
            "Epoch [11159/20000], Training Loss: 0.0795\n",
            "Epoch [11160/20000], Training Loss: 0.0835\n",
            "Epoch [11161/20000], Training Loss: 0.0853\n",
            "Epoch [11162/20000], Training Loss: 0.0793\n",
            "Epoch [11163/20000], Training Loss: 0.0811\n",
            "Epoch [11164/20000], Training Loss: 0.0776\n",
            "Epoch [11165/20000], Training Loss: 0.0755\n",
            "Epoch [11166/20000], Training Loss: 0.0808\n",
            "Epoch [11167/20000], Training Loss: 0.0771\n",
            "Epoch [11168/20000], Training Loss: 0.0797\n",
            "Epoch [11169/20000], Training Loss: 0.0848\n",
            "Epoch [11170/20000], Training Loss: 0.0797\n",
            "Epoch [11171/20000], Training Loss: 0.0792\n",
            "Epoch [11172/20000], Training Loss: 0.0825\n",
            "Epoch [11173/20000], Training Loss: 0.0739\n",
            "Epoch [11174/20000], Training Loss: 0.0775\n",
            "Epoch [11175/20000], Training Loss: 0.0751\n",
            "Epoch [11176/20000], Training Loss: 0.0800\n",
            "Epoch [11177/20000], Training Loss: 0.0747\n",
            "Epoch [11178/20000], Training Loss: 0.0830\n",
            "Epoch [11179/20000], Training Loss: 0.0816\n",
            "Epoch [11180/20000], Training Loss: 0.0795\n",
            "Epoch [11181/20000], Training Loss: 0.0793\n",
            "Epoch [11182/20000], Training Loss: 0.0757\n",
            "Epoch [11183/20000], Training Loss: 0.0830\n",
            "Epoch [11184/20000], Training Loss: 0.0799\n",
            "Epoch [11185/20000], Training Loss: 0.0769\n",
            "Epoch [11186/20000], Training Loss: 0.0784\n",
            "Epoch [11187/20000], Training Loss: 0.0783\n",
            "Epoch [11188/20000], Training Loss: 0.0845\n",
            "Epoch [11189/20000], Training Loss: 0.0751\n",
            "Epoch [11190/20000], Training Loss: 0.0775\n",
            "Epoch [11191/20000], Training Loss: 0.0825\n",
            "Epoch [11192/20000], Training Loss: 0.0779\n",
            "Epoch [11193/20000], Training Loss: 0.0833\n",
            "Epoch [11194/20000], Training Loss: 0.0857\n",
            "Epoch [11195/20000], Training Loss: 0.0764\n",
            "Epoch [11196/20000], Training Loss: 0.0795\n",
            "Epoch [11197/20000], Training Loss: 0.0823\n",
            "Epoch [11198/20000], Training Loss: 0.0855\n",
            "Epoch [11199/20000], Training Loss: 0.0794\n",
            "Epoch [11200/20000], Training Loss: 0.0721\n",
            "Epoch [11201/20000], Training Loss: 0.0818\n",
            "Epoch [11202/20000], Training Loss: 0.0808\n",
            "Epoch [11203/20000], Training Loss: 0.0813\n",
            "Epoch [11204/20000], Training Loss: 0.0791\n",
            "Epoch [11205/20000], Training Loss: 0.0806\n",
            "Epoch [11206/20000], Training Loss: 0.0800\n",
            "Epoch [11207/20000], Training Loss: 0.0795\n",
            "Epoch [11208/20000], Training Loss: 0.0827\n",
            "Epoch [11209/20000], Training Loss: 0.0813\n",
            "Epoch [11210/20000], Training Loss: 0.0780\n",
            "Epoch [11211/20000], Training Loss: 0.0760\n",
            "Epoch [11212/20000], Training Loss: 0.0786\n",
            "Epoch [11213/20000], Training Loss: 0.0767\n",
            "Epoch [11214/20000], Training Loss: 0.0831\n",
            "Epoch [11215/20000], Training Loss: 0.0747\n",
            "Epoch [11216/20000], Training Loss: 0.0854\n",
            "Epoch [11217/20000], Training Loss: 0.0857\n",
            "Epoch [11218/20000], Training Loss: 0.0807\n",
            "Epoch [11219/20000], Training Loss: 0.0801\n",
            "Epoch [11220/20000], Training Loss: 0.0860\n",
            "Epoch [11221/20000], Training Loss: 0.0746\n",
            "Epoch [11222/20000], Training Loss: 0.0851\n",
            "Epoch [11223/20000], Training Loss: 0.0802\n",
            "Epoch [11224/20000], Training Loss: 0.0793\n",
            "Epoch [11225/20000], Training Loss: 0.0805\n",
            "Epoch [11226/20000], Training Loss: 0.0845\n",
            "Epoch [11227/20000], Training Loss: 0.0831\n",
            "Epoch [11228/20000], Training Loss: 0.0799\n",
            "Epoch [11229/20000], Training Loss: 0.0801\n",
            "Epoch [11230/20000], Training Loss: 0.0739\n",
            "Epoch [11231/20000], Training Loss: 0.0803\n",
            "Epoch [11232/20000], Training Loss: 0.0852\n",
            "Epoch [11233/20000], Training Loss: 0.0840\n",
            "Epoch [11234/20000], Training Loss: 0.0883\n",
            "Epoch [11235/20000], Training Loss: 0.0789\n",
            "Epoch [11236/20000], Training Loss: 0.0805\n",
            "Epoch [11237/20000], Training Loss: 0.0769\n",
            "Epoch [11238/20000], Training Loss: 0.0835\n",
            "Epoch [11239/20000], Training Loss: 0.0820\n",
            "Epoch [11240/20000], Training Loss: 0.0744\n",
            "Epoch [11241/20000], Training Loss: 0.0818\n",
            "Epoch [11242/20000], Training Loss: 0.0821\n",
            "Epoch [11243/20000], Training Loss: 0.0783\n",
            "Epoch [11244/20000], Training Loss: 0.0780\n",
            "Epoch [11245/20000], Training Loss: 0.0821\n",
            "Epoch [11246/20000], Training Loss: 0.0819\n",
            "Epoch [11247/20000], Training Loss: 0.0802\n",
            "Epoch [11248/20000], Training Loss: 0.0871\n",
            "Epoch [11249/20000], Training Loss: 0.0768\n",
            "Epoch [11250/20000], Training Loss: 0.0764\n",
            "Epoch [11251/20000], Training Loss: 0.0811\n",
            "Epoch [11252/20000], Training Loss: 0.0807\n",
            "Epoch [11253/20000], Training Loss: 0.0751\n",
            "Epoch [11254/20000], Training Loss: 0.0755\n",
            "Epoch [11255/20000], Training Loss: 0.0833\n",
            "Epoch [11256/20000], Training Loss: 0.0775\n",
            "Epoch [11257/20000], Training Loss: 0.0839\n",
            "Epoch [11258/20000], Training Loss: 0.0837\n",
            "Epoch [11259/20000], Training Loss: 0.0737\n",
            "Epoch [11260/20000], Training Loss: 0.0862\n",
            "Epoch [11261/20000], Training Loss: 0.0760\n",
            "Epoch [11262/20000], Training Loss: 0.0750\n",
            "Epoch [11263/20000], Training Loss: 0.0802\n",
            "Epoch [11264/20000], Training Loss: 0.0854\n",
            "Epoch [11265/20000], Training Loss: 0.0794\n",
            "Epoch [11266/20000], Training Loss: 0.0823\n",
            "Epoch [11267/20000], Training Loss: 0.0841\n",
            "Epoch [11268/20000], Training Loss: 0.0805\n",
            "Epoch [11269/20000], Training Loss: 0.0792\n",
            "Epoch [11270/20000], Training Loss: 0.0853\n",
            "Epoch [11271/20000], Training Loss: 0.0863\n",
            "Epoch [11272/20000], Training Loss: 0.0807\n",
            "Epoch [11273/20000], Training Loss: 0.0749\n",
            "Epoch [11274/20000], Training Loss: 0.0766\n",
            "Epoch [11275/20000], Training Loss: 0.0755\n",
            "Epoch [11276/20000], Training Loss: 0.0792\n",
            "Epoch [11277/20000], Training Loss: 0.0844\n",
            "Epoch [11278/20000], Training Loss: 0.0781\n",
            "Epoch [11279/20000], Training Loss: 0.0761\n",
            "Epoch [11280/20000], Training Loss: 0.0753\n",
            "Epoch [11281/20000], Training Loss: 0.0813\n",
            "Epoch [11282/20000], Training Loss: 0.0779\n",
            "Epoch [11283/20000], Training Loss: 0.0841\n",
            "Epoch [11284/20000], Training Loss: 0.0756\n",
            "Epoch [11285/20000], Training Loss: 0.0772\n",
            "Epoch [11286/20000], Training Loss: 0.0755\n",
            "Epoch [11287/20000], Training Loss: 0.0806\n",
            "Epoch [11288/20000], Training Loss: 0.0781\n",
            "Epoch [11289/20000], Training Loss: 0.0797\n",
            "Epoch [11290/20000], Training Loss: 0.0802\n",
            "Epoch [11291/20000], Training Loss: 0.0799\n",
            "Epoch [11292/20000], Training Loss: 0.0802\n",
            "Epoch [11293/20000], Training Loss: 0.0797\n",
            "Epoch [11294/20000], Training Loss: 0.0774\n",
            "Epoch [11295/20000], Training Loss: 0.0772\n",
            "Epoch [11296/20000], Training Loss: 0.0771\n",
            "Epoch [11297/20000], Training Loss: 0.0840\n",
            "Epoch [11298/20000], Training Loss: 0.0746\n",
            "Epoch [11299/20000], Training Loss: 0.0841\n",
            "Epoch [11300/20000], Training Loss: 0.0815\n",
            "Epoch [11301/20000], Training Loss: 0.0812\n",
            "Epoch [11302/20000], Training Loss: 0.0850\n",
            "Epoch [11303/20000], Training Loss: 0.0797\n",
            "Epoch [11304/20000], Training Loss: 0.0777\n",
            "Epoch [11305/20000], Training Loss: 0.0781\n",
            "Epoch [11306/20000], Training Loss: 0.0787\n",
            "Epoch [11307/20000], Training Loss: 0.0819\n",
            "Epoch [11308/20000], Training Loss: 0.0738\n",
            "Epoch [11309/20000], Training Loss: 0.0795\n",
            "Epoch [11310/20000], Training Loss: 0.0762\n",
            "Epoch [11311/20000], Training Loss: 0.0813\n",
            "Epoch [11312/20000], Training Loss: 0.0823\n",
            "Epoch [11313/20000], Training Loss: 0.0809\n",
            "Epoch [11314/20000], Training Loss: 0.0769\n",
            "Epoch [11315/20000], Training Loss: 0.0813\n",
            "Epoch [11316/20000], Training Loss: 0.0770\n",
            "Epoch [11317/20000], Training Loss: 0.0784\n",
            "Epoch [11318/20000], Training Loss: 0.0767\n",
            "Epoch [11319/20000], Training Loss: 0.0747\n",
            "Epoch [11320/20000], Training Loss: 0.0828\n",
            "Epoch [11321/20000], Training Loss: 0.0786\n",
            "Epoch [11322/20000], Training Loss: 0.0752\n",
            "Epoch [11323/20000], Training Loss: 0.0739\n",
            "Epoch [11324/20000], Training Loss: 0.0736\n",
            "Epoch [11325/20000], Training Loss: 0.0754\n",
            "Epoch [11326/20000], Training Loss: 0.0729\n",
            "Epoch [11327/20000], Training Loss: 0.0776\n",
            "Epoch [11328/20000], Training Loss: 0.0815\n",
            "Epoch [11329/20000], Training Loss: 0.0861\n",
            "Epoch [11330/20000], Training Loss: 0.0788\n",
            "Epoch [11331/20000], Training Loss: 0.0804\n",
            "Epoch [11332/20000], Training Loss: 0.0806\n",
            "Epoch [11333/20000], Training Loss: 0.0749\n",
            "Epoch [11334/20000], Training Loss: 0.0751\n",
            "Epoch [11335/20000], Training Loss: 0.0799\n",
            "Epoch [11336/20000], Training Loss: 0.0802\n",
            "Epoch [11337/20000], Training Loss: 0.0771\n",
            "Epoch [11338/20000], Training Loss: 0.0842\n",
            "Epoch [11339/20000], Training Loss: 0.0788\n",
            "Epoch [11340/20000], Training Loss: 0.0745\n",
            "Epoch [11341/20000], Training Loss: 0.0780\n",
            "Epoch [11342/20000], Training Loss: 0.0828\n",
            "Epoch [11343/20000], Training Loss: 0.0753\n",
            "Epoch [11344/20000], Training Loss: 0.0815\n",
            "Epoch [11345/20000], Training Loss: 0.0787\n",
            "Epoch [11346/20000], Training Loss: 0.0760\n",
            "Epoch [11347/20000], Training Loss: 0.0786\n",
            "Epoch [11348/20000], Training Loss: 0.0823\n",
            "Epoch [11349/20000], Training Loss: 0.0774\n",
            "Epoch [11350/20000], Training Loss: 0.0741\n",
            "Epoch [11351/20000], Training Loss: 0.0722\n",
            "Epoch [11352/20000], Training Loss: 0.0847\n",
            "Epoch [11353/20000], Training Loss: 0.0744\n",
            "Epoch [11354/20000], Training Loss: 0.0764\n",
            "Epoch [11355/20000], Training Loss: 0.0824\n",
            "Epoch [11356/20000], Training Loss: 0.0821\n",
            "Epoch [11357/20000], Training Loss: 0.0735\n",
            "Epoch [11358/20000], Training Loss: 0.0751\n",
            "Epoch [11359/20000], Training Loss: 0.0770\n",
            "Epoch [11360/20000], Training Loss: 0.0783\n",
            "Epoch [11361/20000], Training Loss: 0.0831\n",
            "Epoch [11362/20000], Training Loss: 0.0747\n",
            "Epoch [11363/20000], Training Loss: 0.0745\n",
            "Epoch [11364/20000], Training Loss: 0.0821\n",
            "Epoch [11365/20000], Training Loss: 0.0823\n",
            "Epoch [11366/20000], Training Loss: 0.0801\n",
            "Epoch [11367/20000], Training Loss: 0.0784\n",
            "Epoch [11368/20000], Training Loss: 0.0774\n",
            "Epoch [11369/20000], Training Loss: 0.0881\n",
            "Epoch [11370/20000], Training Loss: 0.0824\n",
            "Epoch [11371/20000], Training Loss: 0.0744\n",
            "Epoch [11372/20000], Training Loss: 0.0853\n",
            "Epoch [11373/20000], Training Loss: 0.0740\n",
            "Epoch [11374/20000], Training Loss: 0.0794\n",
            "Epoch [11375/20000], Training Loss: 0.0806\n",
            "Epoch [11376/20000], Training Loss: 0.0782\n",
            "Epoch [11377/20000], Training Loss: 0.0813\n",
            "Epoch [11378/20000], Training Loss: 0.0772\n",
            "Epoch [11379/20000], Training Loss: 0.0811\n",
            "Epoch [11380/20000], Training Loss: 0.0792\n",
            "Epoch [11381/20000], Training Loss: 0.0850\n",
            "Epoch [11382/20000], Training Loss: 0.0761\n",
            "Epoch [11383/20000], Training Loss: 0.0857\n",
            "Epoch [11384/20000], Training Loss: 0.0773\n",
            "Epoch [11385/20000], Training Loss: 0.0807\n",
            "Epoch [11386/20000], Training Loss: 0.0793\n",
            "Epoch [11387/20000], Training Loss: 0.0865\n",
            "Epoch [11388/20000], Training Loss: 0.0813\n",
            "Epoch [11389/20000], Training Loss: 0.0808\n",
            "Epoch [11390/20000], Training Loss: 0.0782\n",
            "Epoch [11391/20000], Training Loss: 0.0787\n",
            "Epoch [11392/20000], Training Loss: 0.0779\n",
            "Epoch [11393/20000], Training Loss: 0.0850\n",
            "Epoch [11394/20000], Training Loss: 0.0820\n",
            "Epoch [11395/20000], Training Loss: 0.0794\n",
            "Epoch [11396/20000], Training Loss: 0.0790\n",
            "Epoch [11397/20000], Training Loss: 0.0855\n",
            "Epoch [11398/20000], Training Loss: 0.0799\n",
            "Epoch [11399/20000], Training Loss: 0.0820\n",
            "Epoch [11400/20000], Training Loss: 0.0749\n",
            "Epoch [11401/20000], Training Loss: 0.0743\n",
            "Epoch [11402/20000], Training Loss: 0.0732\n",
            "Epoch [11403/20000], Training Loss: 0.0780\n",
            "Epoch [11404/20000], Training Loss: 0.0815\n",
            "Epoch [11405/20000], Training Loss: 0.0831\n",
            "Epoch [11406/20000], Training Loss: 0.0797\n",
            "Epoch [11407/20000], Training Loss: 0.0767\n",
            "Epoch [11408/20000], Training Loss: 0.0794\n",
            "Epoch [11409/20000], Training Loss: 0.0819\n",
            "Epoch [11410/20000], Training Loss: 0.0779\n",
            "Epoch [11411/20000], Training Loss: 0.0731\n",
            "Epoch [11412/20000], Training Loss: 0.0821\n",
            "Epoch [11413/20000], Training Loss: 0.0790\n",
            "Epoch [11414/20000], Training Loss: 0.0836\n",
            "Epoch [11415/20000], Training Loss: 0.0801\n",
            "Epoch [11416/20000], Training Loss: 0.0806\n",
            "Epoch [11417/20000], Training Loss: 0.0752\n",
            "Epoch [11418/20000], Training Loss: 0.0832\n",
            "Epoch [11419/20000], Training Loss: 0.0753\n",
            "Epoch [11420/20000], Training Loss: 0.0795\n",
            "Epoch [11421/20000], Training Loss: 0.0784\n",
            "Epoch [11422/20000], Training Loss: 0.0755\n",
            "Epoch [11423/20000], Training Loss: 0.0760\n",
            "Epoch [11424/20000], Training Loss: 0.0736\n",
            "Epoch [11425/20000], Training Loss: 0.0871\n",
            "Epoch [11426/20000], Training Loss: 0.0784\n",
            "Epoch [11427/20000], Training Loss: 0.0784\n",
            "Epoch [11428/20000], Training Loss: 0.0775\n",
            "Epoch [11429/20000], Training Loss: 0.0784\n",
            "Epoch [11430/20000], Training Loss: 0.0783\n",
            "Epoch [11431/20000], Training Loss: 0.0896\n",
            "Epoch [11432/20000], Training Loss: 0.0818\n",
            "Epoch [11433/20000], Training Loss: 0.0794\n",
            "Epoch [11434/20000], Training Loss: 0.0802\n",
            "Epoch [11435/20000], Training Loss: 0.0733\n",
            "Epoch [11436/20000], Training Loss: 0.0805\n",
            "Epoch [11437/20000], Training Loss: 0.0781\n",
            "Epoch [11438/20000], Training Loss: 0.0738\n",
            "Epoch [11439/20000], Training Loss: 0.0780\n",
            "Epoch [11440/20000], Training Loss: 0.0842\n",
            "Epoch [11441/20000], Training Loss: 0.0782\n",
            "Epoch [11442/20000], Training Loss: 0.0784\n",
            "Epoch [11443/20000], Training Loss: 0.0800\n",
            "Epoch [11444/20000], Training Loss: 0.0796\n",
            "Epoch [11445/20000], Training Loss: 0.0803\n",
            "Epoch [11446/20000], Training Loss: 0.0741\n",
            "Epoch [11447/20000], Training Loss: 0.0752\n",
            "Epoch [11448/20000], Training Loss: 0.0815\n",
            "Epoch [11449/20000], Training Loss: 0.0782\n",
            "Epoch [11450/20000], Training Loss: 0.0860\n",
            "Epoch [11451/20000], Training Loss: 0.0855\n",
            "Epoch [11452/20000], Training Loss: 0.0763\n",
            "Epoch [11453/20000], Training Loss: 0.0850\n",
            "Epoch [11454/20000], Training Loss: 0.0824\n",
            "Epoch [11455/20000], Training Loss: 0.0818\n",
            "Epoch [11456/20000], Training Loss: 0.0756\n",
            "Epoch [11457/20000], Training Loss: 0.0855\n",
            "Epoch [11458/20000], Training Loss: 0.0775\n",
            "Epoch [11459/20000], Training Loss: 0.0776\n",
            "Epoch [11460/20000], Training Loss: 0.0820\n",
            "Epoch [11461/20000], Training Loss: 0.0794\n",
            "Epoch [11462/20000], Training Loss: 0.0843\n",
            "Epoch [11463/20000], Training Loss: 0.0769\n",
            "Epoch [11464/20000], Training Loss: 0.0807\n",
            "Epoch [11465/20000], Training Loss: 0.0788\n",
            "Epoch [11466/20000], Training Loss: 0.0793\n",
            "Epoch [11467/20000], Training Loss: 0.0842\n",
            "Epoch [11468/20000], Training Loss: 0.0815\n",
            "Epoch [11469/20000], Training Loss: 0.0778\n",
            "Epoch [11470/20000], Training Loss: 0.0866\n",
            "Epoch [11471/20000], Training Loss: 0.0852\n",
            "Epoch [11472/20000], Training Loss: 0.0805\n",
            "Epoch [11473/20000], Training Loss: 0.0860\n",
            "Epoch [11474/20000], Training Loss: 0.0869\n",
            "Epoch [11475/20000], Training Loss: 0.0747\n",
            "Epoch [11476/20000], Training Loss: 0.0744\n",
            "Epoch [11477/20000], Training Loss: 0.0784\n",
            "Epoch [11478/20000], Training Loss: 0.0827\n",
            "Epoch [11479/20000], Training Loss: 0.0768\n",
            "Epoch [11480/20000], Training Loss: 0.0846\n",
            "Epoch [11481/20000], Training Loss: 0.0856\n",
            "Epoch [11482/20000], Training Loss: 0.0822\n",
            "Epoch [11483/20000], Training Loss: 0.0731\n",
            "Epoch [11484/20000], Training Loss: 0.0857\n",
            "Epoch [11485/20000], Training Loss: 0.0795\n",
            "Epoch [11486/20000], Training Loss: 0.0835\n",
            "Epoch [11487/20000], Training Loss: 0.0746\n",
            "Epoch [11488/20000], Training Loss: 0.0796\n",
            "Epoch [11489/20000], Training Loss: 0.0736\n",
            "Epoch [11490/20000], Training Loss: 0.0811\n",
            "Epoch [11491/20000], Training Loss: 0.0733\n",
            "Epoch [11492/20000], Training Loss: 0.0791\n",
            "Epoch [11493/20000], Training Loss: 0.0849\n",
            "Epoch [11494/20000], Training Loss: 0.0778\n",
            "Epoch [11495/20000], Training Loss: 0.0778\n",
            "Epoch [11496/20000], Training Loss: 0.0795\n",
            "Epoch [11497/20000], Training Loss: 0.0835\n",
            "Epoch [11498/20000], Training Loss: 0.0751\n",
            "Epoch [11499/20000], Training Loss: 0.0781\n",
            "Epoch [11500/20000], Training Loss: 0.0748\n",
            "Epoch [11501/20000], Training Loss: 0.0757\n",
            "Epoch [11502/20000], Training Loss: 0.0737\n",
            "Epoch [11503/20000], Training Loss: 0.0800\n",
            "Epoch [11504/20000], Training Loss: 0.0855\n",
            "Epoch [11505/20000], Training Loss: 0.0828\n",
            "Epoch [11506/20000], Training Loss: 0.0816\n",
            "Epoch [11507/20000], Training Loss: 0.0809\n",
            "Epoch [11508/20000], Training Loss: 0.0813\n",
            "Epoch [11509/20000], Training Loss: 0.0780\n",
            "Epoch [11510/20000], Training Loss: 0.0766\n",
            "Epoch [11511/20000], Training Loss: 0.0764\n",
            "Epoch [11512/20000], Training Loss: 0.0759\n",
            "Epoch [11513/20000], Training Loss: 0.0836\n",
            "Epoch [11514/20000], Training Loss: 0.0774\n",
            "Epoch [11515/20000], Training Loss: 0.0768\n",
            "Epoch [11516/20000], Training Loss: 0.0735\n",
            "Epoch [11517/20000], Training Loss: 0.0778\n",
            "Epoch [11518/20000], Training Loss: 0.0778\n",
            "Epoch [11519/20000], Training Loss: 0.0852\n",
            "Epoch [11520/20000], Training Loss: 0.0856\n",
            "Epoch [11521/20000], Training Loss: 0.0769\n",
            "Epoch [11522/20000], Training Loss: 0.0786\n",
            "Epoch [11523/20000], Training Loss: 0.0821\n",
            "Epoch [11524/20000], Training Loss: 0.0759\n",
            "Epoch [11525/20000], Training Loss: 0.0811\n",
            "Epoch [11526/20000], Training Loss: 0.0796\n",
            "Epoch [11527/20000], Training Loss: 0.0817\n",
            "Epoch [11528/20000], Training Loss: 0.0727\n",
            "Epoch [11529/20000], Training Loss: 0.0800\n",
            "Epoch [11530/20000], Training Loss: 0.0766\n",
            "Epoch [11531/20000], Training Loss: 0.0828\n",
            "Epoch [11532/20000], Training Loss: 0.0766\n",
            "Epoch [11533/20000], Training Loss: 0.0845\n",
            "Epoch [11534/20000], Training Loss: 0.0850\n",
            "Epoch [11535/20000], Training Loss: 0.0743\n",
            "Epoch [11536/20000], Training Loss: 0.0762\n",
            "Epoch [11537/20000], Training Loss: 0.0769\n",
            "Epoch [11538/20000], Training Loss: 0.0791\n",
            "Epoch [11539/20000], Training Loss: 0.0801\n",
            "Epoch [11540/20000], Training Loss: 0.0796\n",
            "Epoch [11541/20000], Training Loss: 0.0805\n",
            "Epoch [11542/20000], Training Loss: 0.0837\n",
            "Epoch [11543/20000], Training Loss: 0.0788\n",
            "Epoch [11544/20000], Training Loss: 0.0873\n",
            "Epoch [11545/20000], Training Loss: 0.0795\n",
            "Epoch [11546/20000], Training Loss: 0.0782\n",
            "Epoch [11547/20000], Training Loss: 0.0787\n",
            "Epoch [11548/20000], Training Loss: 0.0741\n",
            "Epoch [11549/20000], Training Loss: 0.0748\n",
            "Epoch [11550/20000], Training Loss: 0.0843\n",
            "Epoch [11551/20000], Training Loss: 0.0742\n",
            "Epoch [11552/20000], Training Loss: 0.0776\n",
            "Epoch [11553/20000], Training Loss: 0.0762\n",
            "Epoch [11554/20000], Training Loss: 0.0781\n",
            "Epoch [11555/20000], Training Loss: 0.0813\n",
            "Epoch [11556/20000], Training Loss: 0.0766\n",
            "Epoch [11557/20000], Training Loss: 0.0832\n",
            "Epoch [11558/20000], Training Loss: 0.0762\n",
            "Epoch [11559/20000], Training Loss: 0.0739\n",
            "Epoch [11560/20000], Training Loss: 0.0734\n",
            "Epoch [11561/20000], Training Loss: 0.0829\n",
            "Epoch [11562/20000], Training Loss: 0.0833\n",
            "Epoch [11563/20000], Training Loss: 0.0819\n",
            "Epoch [11564/20000], Training Loss: 0.0764\n",
            "Epoch [11565/20000], Training Loss: 0.0747\n",
            "Epoch [11566/20000], Training Loss: 0.0839\n",
            "Epoch [11567/20000], Training Loss: 0.0755\n",
            "Epoch [11568/20000], Training Loss: 0.0791\n",
            "Epoch [11569/20000], Training Loss: 0.0821\n",
            "Epoch [11570/20000], Training Loss: 0.0803\n",
            "Epoch [11571/20000], Training Loss: 0.0749\n",
            "Epoch [11572/20000], Training Loss: 0.0762\n",
            "Epoch [11573/20000], Training Loss: 0.0758\n",
            "Epoch [11574/20000], Training Loss: 0.0729\n",
            "Epoch [11575/20000], Training Loss: 0.0783\n",
            "Epoch [11576/20000], Training Loss: 0.0751\n",
            "Epoch [11577/20000], Training Loss: 0.0782\n",
            "Epoch [11578/20000], Training Loss: 0.0825\n",
            "Epoch [11579/20000], Training Loss: 0.0773\n",
            "Epoch [11580/20000], Training Loss: 0.0844\n",
            "Epoch [11581/20000], Training Loss: 0.0791\n",
            "Epoch [11582/20000], Training Loss: 0.0727\n",
            "Epoch [11583/20000], Training Loss: 0.0854\n",
            "Epoch [11584/20000], Training Loss: 0.0825\n",
            "Epoch [11585/20000], Training Loss: 0.0806\n",
            "Epoch [11586/20000], Training Loss: 0.0758\n",
            "Epoch [11587/20000], Training Loss: 0.0820\n",
            "Epoch [11588/20000], Training Loss: 0.0812\n",
            "Epoch [11589/20000], Training Loss: 0.0781\n",
            "Epoch [11590/20000], Training Loss: 0.0841\n",
            "Epoch [11591/20000], Training Loss: 0.0783\n",
            "Epoch [11592/20000], Training Loss: 0.0752\n",
            "Epoch [11593/20000], Training Loss: 0.0799\n",
            "Epoch [11594/20000], Training Loss: 0.0807\n",
            "Epoch [11595/20000], Training Loss: 0.0747\n",
            "Epoch [11596/20000], Training Loss: 0.0821\n",
            "Epoch [11597/20000], Training Loss: 0.0782\n",
            "Epoch [11598/20000], Training Loss: 0.0771\n",
            "Epoch [11599/20000], Training Loss: 0.0809\n",
            "Epoch [11600/20000], Training Loss: 0.0759\n",
            "Epoch [11601/20000], Training Loss: 0.0791\n",
            "Epoch [11602/20000], Training Loss: 0.0800\n",
            "Epoch [11603/20000], Training Loss: 0.0832\n",
            "Epoch [11604/20000], Training Loss: 0.0799\n",
            "Epoch [11605/20000], Training Loss: 0.0856\n",
            "Epoch [11606/20000], Training Loss: 0.0797\n",
            "Epoch [11607/20000], Training Loss: 0.0778\n",
            "Epoch [11608/20000], Training Loss: 0.0735\n",
            "Epoch [11609/20000], Training Loss: 0.0767\n",
            "Epoch [11610/20000], Training Loss: 0.0865\n",
            "Epoch [11611/20000], Training Loss: 0.0820\n",
            "Epoch [11612/20000], Training Loss: 0.0833\n",
            "Epoch [11613/20000], Training Loss: 0.0793\n",
            "Epoch [11614/20000], Training Loss: 0.0743\n",
            "Epoch [11615/20000], Training Loss: 0.0728\n",
            "Epoch [11616/20000], Training Loss: 0.0764\n",
            "Epoch [11617/20000], Training Loss: 0.0791\n",
            "Epoch [11618/20000], Training Loss: 0.0767\n",
            "Epoch [11619/20000], Training Loss: 0.0805\n",
            "Epoch [11620/20000], Training Loss: 0.0791\n",
            "Epoch [11621/20000], Training Loss: 0.0786\n",
            "Epoch [11622/20000], Training Loss: 0.0721\n",
            "Epoch [11623/20000], Training Loss: 0.0750\n",
            "Epoch [11624/20000], Training Loss: 0.0792\n",
            "Epoch [11625/20000], Training Loss: 0.0774\n",
            "Epoch [11626/20000], Training Loss: 0.0737\n",
            "Epoch [11627/20000], Training Loss: 0.0843\n",
            "Epoch [11628/20000], Training Loss: 0.0811\n",
            "Epoch [11629/20000], Training Loss: 0.0770\n",
            "Epoch [11630/20000], Training Loss: 0.0753\n",
            "Epoch [11631/20000], Training Loss: 0.0878\n",
            "Epoch [11632/20000], Training Loss: 0.0812\n",
            "Epoch [11633/20000], Training Loss: 0.0855\n",
            "Epoch [11634/20000], Training Loss: 0.0827\n",
            "Epoch [11635/20000], Training Loss: 0.0816\n",
            "Epoch [11636/20000], Training Loss: 0.0755\n",
            "Epoch [11637/20000], Training Loss: 0.0765\n",
            "Epoch [11638/20000], Training Loss: 0.0819\n",
            "Epoch [11639/20000], Training Loss: 0.0863\n",
            "Epoch [11640/20000], Training Loss: 0.0859\n",
            "Epoch [11641/20000], Training Loss: 0.0756\n",
            "Epoch [11642/20000], Training Loss: 0.0827\n",
            "Epoch [11643/20000], Training Loss: 0.0801\n",
            "Epoch [11644/20000], Training Loss: 0.0792\n",
            "Epoch [11645/20000], Training Loss: 0.0813\n",
            "Epoch [11646/20000], Training Loss: 0.0820\n",
            "Epoch [11647/20000], Training Loss: 0.0772\n",
            "Epoch [11648/20000], Training Loss: 0.0763\n",
            "Epoch [11649/20000], Training Loss: 0.0784\n",
            "Epoch [11650/20000], Training Loss: 0.0787\n",
            "Epoch [11651/20000], Training Loss: 0.0762\n",
            "Epoch [11652/20000], Training Loss: 0.0773\n",
            "Epoch [11653/20000], Training Loss: 0.0743\n",
            "Epoch [11654/20000], Training Loss: 0.0753\n",
            "Epoch [11655/20000], Training Loss: 0.0829\n",
            "Epoch [11656/20000], Training Loss: 0.0832\n",
            "Epoch [11657/20000], Training Loss: 0.0829\n",
            "Epoch [11658/20000], Training Loss: 0.0760\n",
            "Epoch [11659/20000], Training Loss: 0.0743\n",
            "Epoch [11660/20000], Training Loss: 0.0806\n",
            "Epoch [11661/20000], Training Loss: 0.0781\n",
            "Epoch [11662/20000], Training Loss: 0.0763\n",
            "Epoch [11663/20000], Training Loss: 0.0793\n",
            "Epoch [11664/20000], Training Loss: 0.0747\n",
            "Epoch [11665/20000], Training Loss: 0.0795\n",
            "Epoch [11666/20000], Training Loss: 0.0815\n",
            "Epoch [11667/20000], Training Loss: 0.0728\n",
            "Epoch [11668/20000], Training Loss: 0.0816\n",
            "Epoch [11669/20000], Training Loss: 0.0841\n",
            "Epoch [11670/20000], Training Loss: 0.0850\n",
            "Epoch [11671/20000], Training Loss: 0.0769\n",
            "Epoch [11672/20000], Training Loss: 0.0845\n",
            "Epoch [11673/20000], Training Loss: 0.0856\n",
            "Epoch [11674/20000], Training Loss: 0.0833\n",
            "Epoch [11675/20000], Training Loss: 0.0756\n",
            "Epoch [11676/20000], Training Loss: 0.0801\n",
            "Epoch [11677/20000], Training Loss: 0.0780\n",
            "Epoch [11678/20000], Training Loss: 0.0844\n",
            "Epoch [11679/20000], Training Loss: 0.0793\n",
            "Epoch [11680/20000], Training Loss: 0.0788\n",
            "Epoch [11681/20000], Training Loss: 0.0799\n",
            "Epoch [11682/20000], Training Loss: 0.0788\n",
            "Epoch [11683/20000], Training Loss: 0.0767\n",
            "Epoch [11684/20000], Training Loss: 0.0813\n",
            "Epoch [11685/20000], Training Loss: 0.0792\n",
            "Epoch [11686/20000], Training Loss: 0.0795\n",
            "Epoch [11687/20000], Training Loss: 0.0799\n",
            "Epoch [11688/20000], Training Loss: 0.0765\n",
            "Epoch [11689/20000], Training Loss: 0.0818\n",
            "Epoch [11690/20000], Training Loss: 0.0795\n",
            "Epoch [11691/20000], Training Loss: 0.0754\n",
            "Epoch [11692/20000], Training Loss: 0.0795\n",
            "Epoch [11693/20000], Training Loss: 0.0773\n",
            "Epoch [11694/20000], Training Loss: 0.0835\n",
            "Epoch [11695/20000], Training Loss: 0.0784\n",
            "Epoch [11696/20000], Training Loss: 0.0772\n",
            "Epoch [11697/20000], Training Loss: 0.0753\n",
            "Epoch [11698/20000], Training Loss: 0.0733\n",
            "Epoch [11699/20000], Training Loss: 0.0808\n",
            "Epoch [11700/20000], Training Loss: 0.0860\n",
            "Epoch [11701/20000], Training Loss: 0.0783\n",
            "Epoch [11702/20000], Training Loss: 0.0812\n",
            "Epoch [11703/20000], Training Loss: 0.0843\n",
            "Epoch [11704/20000], Training Loss: 0.0791\n",
            "Epoch [11705/20000], Training Loss: 0.0804\n",
            "Epoch [11706/20000], Training Loss: 0.0836\n",
            "Epoch [11707/20000], Training Loss: 0.0821\n",
            "Epoch [11708/20000], Training Loss: 0.0809\n",
            "Epoch [11709/20000], Training Loss: 0.0762\n",
            "Epoch [11710/20000], Training Loss: 0.0738\n",
            "Epoch [11711/20000], Training Loss: 0.0819\n",
            "Epoch [11712/20000], Training Loss: 0.0801\n",
            "Epoch [11713/20000], Training Loss: 0.0821\n",
            "Epoch [11714/20000], Training Loss: 0.0849\n",
            "Epoch [11715/20000], Training Loss: 0.0769\n",
            "Epoch [11716/20000], Training Loss: 0.0735\n",
            "Epoch [11717/20000], Training Loss: 0.0814\n",
            "Epoch [11718/20000], Training Loss: 0.0839\n",
            "Epoch [11719/20000], Training Loss: 0.0877\n",
            "Epoch [11720/20000], Training Loss: 0.0816\n",
            "Epoch [11721/20000], Training Loss: 0.0746\n",
            "Epoch [11722/20000], Training Loss: 0.0801\n",
            "Epoch [11723/20000], Training Loss: 0.0767\n",
            "Epoch [11724/20000], Training Loss: 0.0761\n",
            "Epoch [11725/20000], Training Loss: 0.0802\n",
            "Epoch [11726/20000], Training Loss: 0.0848\n",
            "Epoch [11727/20000], Training Loss: 0.0795\n",
            "Epoch [11728/20000], Training Loss: 0.0757\n",
            "Epoch [11729/20000], Training Loss: 0.0734\n",
            "Epoch [11730/20000], Training Loss: 0.0787\n",
            "Epoch [11731/20000], Training Loss: 0.0766\n",
            "Epoch [11732/20000], Training Loss: 0.0765\n",
            "Epoch [11733/20000], Training Loss: 0.0796\n",
            "Epoch [11734/20000], Training Loss: 0.0778\n",
            "Epoch [11735/20000], Training Loss: 0.0852\n",
            "Epoch [11736/20000], Training Loss: 0.0795\n",
            "Epoch [11737/20000], Training Loss: 0.0766\n",
            "Epoch [11738/20000], Training Loss: 0.0806\n",
            "Epoch [11739/20000], Training Loss: 0.0791\n",
            "Epoch [11740/20000], Training Loss: 0.0825\n",
            "Epoch [11741/20000], Training Loss: 0.0769\n",
            "Epoch [11742/20000], Training Loss: 0.0777\n",
            "Epoch [11743/20000], Training Loss: 0.0778\n",
            "Epoch [11744/20000], Training Loss: 0.0737\n",
            "Epoch [11745/20000], Training Loss: 0.0775\n",
            "Epoch [11746/20000], Training Loss: 0.0768\n",
            "Epoch [11747/20000], Training Loss: 0.0756\n",
            "Epoch [11748/20000], Training Loss: 0.0814\n",
            "Epoch [11749/20000], Training Loss: 0.0803\n",
            "Epoch [11750/20000], Training Loss: 0.0738\n",
            "Epoch [11751/20000], Training Loss: 0.0800\n",
            "Epoch [11752/20000], Training Loss: 0.0804\n",
            "Epoch [11753/20000], Training Loss: 0.0764\n",
            "Epoch [11754/20000], Training Loss: 0.0855\n",
            "Epoch [11755/20000], Training Loss: 0.0795\n",
            "Epoch [11756/20000], Training Loss: 0.0856\n",
            "Epoch [11757/20000], Training Loss: 0.0787\n",
            "Epoch [11758/20000], Training Loss: 0.0812\n",
            "Epoch [11759/20000], Training Loss: 0.0810\n",
            "Epoch [11760/20000], Training Loss: 0.0749\n",
            "Epoch [11761/20000], Training Loss: 0.0750\n",
            "Epoch [11762/20000], Training Loss: 0.0834\n",
            "Epoch [11763/20000], Training Loss: 0.0821\n",
            "Epoch [11764/20000], Training Loss: 0.0783\n",
            "Epoch [11765/20000], Training Loss: 0.0762\n",
            "Epoch [11766/20000], Training Loss: 0.0800\n",
            "Epoch [11767/20000], Training Loss: 0.0758\n",
            "Epoch [11768/20000], Training Loss: 0.0787\n",
            "Epoch [11769/20000], Training Loss: 0.0798\n",
            "Epoch [11770/20000], Training Loss: 0.0775\n",
            "Epoch [11771/20000], Training Loss: 0.0755\n",
            "Epoch [11772/20000], Training Loss: 0.0762\n",
            "Epoch [11773/20000], Training Loss: 0.0805\n",
            "Epoch [11774/20000], Training Loss: 0.0799\n",
            "Epoch [11775/20000], Training Loss: 0.0793\n",
            "Epoch [11776/20000], Training Loss: 0.0809\n",
            "Epoch [11777/20000], Training Loss: 0.0729\n",
            "Epoch [11778/20000], Training Loss: 0.0811\n",
            "Epoch [11779/20000], Training Loss: 0.0750\n",
            "Epoch [11780/20000], Training Loss: 0.0729\n",
            "Epoch [11781/20000], Training Loss: 0.0808\n",
            "Epoch [11782/20000], Training Loss: 0.0755\n",
            "Epoch [11783/20000], Training Loss: 0.0860\n",
            "Epoch [11784/20000], Training Loss: 0.0747\n",
            "Epoch [11785/20000], Training Loss: 0.0774\n",
            "Epoch [11786/20000], Training Loss: 0.0760\n",
            "Epoch [11787/20000], Training Loss: 0.0848\n",
            "Epoch [11788/20000], Training Loss: 0.0877\n",
            "Epoch [11789/20000], Training Loss: 0.0772\n",
            "Epoch [11790/20000], Training Loss: 0.0800\n",
            "Epoch [11791/20000], Training Loss: 0.0783\n",
            "Epoch [11792/20000], Training Loss: 0.0818\n",
            "Epoch [11793/20000], Training Loss: 0.0797\n",
            "Epoch [11794/20000], Training Loss: 0.0773\n",
            "Epoch [11795/20000], Training Loss: 0.0792\n",
            "Epoch [11796/20000], Training Loss: 0.0783\n",
            "Epoch [11797/20000], Training Loss: 0.0774\n",
            "Epoch [11798/20000], Training Loss: 0.0742\n",
            "Epoch [11799/20000], Training Loss: 0.0761\n",
            "Epoch [11800/20000], Training Loss: 0.0796\n",
            "Epoch [11801/20000], Training Loss: 0.0753\n",
            "Epoch [11802/20000], Training Loss: 0.0739\n",
            "Epoch [11803/20000], Training Loss: 0.0779\n",
            "Epoch [11804/20000], Training Loss: 0.0813\n",
            "Epoch [11805/20000], Training Loss: 0.0786\n",
            "Epoch [11806/20000], Training Loss: 0.0786\n",
            "Epoch [11807/20000], Training Loss: 0.0811\n",
            "Epoch [11808/20000], Training Loss: 0.0731\n",
            "Epoch [11809/20000], Training Loss: 0.0805\n",
            "Epoch [11810/20000], Training Loss: 0.0802\n",
            "Epoch [11811/20000], Training Loss: 0.0762\n",
            "Epoch [11812/20000], Training Loss: 0.0814\n",
            "Epoch [11813/20000], Training Loss: 0.0760\n",
            "Epoch [11814/20000], Training Loss: 0.0722\n",
            "Epoch [11815/20000], Training Loss: 0.0768\n",
            "Epoch [11816/20000], Training Loss: 0.0762\n",
            "Epoch [11817/20000], Training Loss: 0.0760\n",
            "Epoch [11818/20000], Training Loss: 0.0792\n",
            "Epoch [11819/20000], Training Loss: 0.0760\n",
            "Epoch [11820/20000], Training Loss: 0.0799\n",
            "Epoch [11821/20000], Training Loss: 0.0767\n",
            "Epoch [11822/20000], Training Loss: 0.0786\n",
            "Epoch [11823/20000], Training Loss: 0.0867\n",
            "Epoch [11824/20000], Training Loss: 0.0752\n",
            "Epoch [11825/20000], Training Loss: 0.0793\n",
            "Epoch [11826/20000], Training Loss: 0.0787\n",
            "Epoch [11827/20000], Training Loss: 0.0756\n",
            "Epoch [11828/20000], Training Loss: 0.0793\n",
            "Epoch [11829/20000], Training Loss: 0.0829\n",
            "Epoch [11830/20000], Training Loss: 0.0818\n",
            "Epoch [11831/20000], Training Loss: 0.0879\n",
            "Epoch [11832/20000], Training Loss: 0.0832\n",
            "Epoch [11833/20000], Training Loss: 0.0747\n",
            "Epoch [11834/20000], Training Loss: 0.0783\n",
            "Epoch [11835/20000], Training Loss: 0.0798\n",
            "Epoch [11836/20000], Training Loss: 0.0848\n",
            "Epoch [11837/20000], Training Loss: 0.0761\n",
            "Epoch [11838/20000], Training Loss: 0.0802\n",
            "Epoch [11839/20000], Training Loss: 0.0772\n",
            "Epoch [11840/20000], Training Loss: 0.0825\n",
            "Epoch [11841/20000], Training Loss: 0.0816\n",
            "Epoch [11842/20000], Training Loss: 0.0841\n",
            "Epoch [11843/20000], Training Loss: 0.0772\n",
            "Epoch [11844/20000], Training Loss: 0.0762\n",
            "Epoch [11845/20000], Training Loss: 0.0763\n",
            "Epoch [11846/20000], Training Loss: 0.0750\n",
            "Epoch [11847/20000], Training Loss: 0.0815\n",
            "Epoch [11848/20000], Training Loss: 0.0880\n",
            "Epoch [11849/20000], Training Loss: 0.0816\n",
            "Epoch [11850/20000], Training Loss: 0.0767\n",
            "Epoch [11851/20000], Training Loss: 0.0833\n",
            "Epoch [11852/20000], Training Loss: 0.0804\n",
            "Epoch [11853/20000], Training Loss: 0.0737\n",
            "Epoch [11854/20000], Training Loss: 0.0855\n",
            "Epoch [11855/20000], Training Loss: 0.0836\n",
            "Epoch [11856/20000], Training Loss: 0.0771\n",
            "Epoch [11857/20000], Training Loss: 0.0785\n",
            "Epoch [11858/20000], Training Loss: 0.0805\n",
            "Epoch [11859/20000], Training Loss: 0.0737\n",
            "Epoch [11860/20000], Training Loss: 0.0722\n",
            "Epoch [11861/20000], Training Loss: 0.0788\n",
            "Epoch [11862/20000], Training Loss: 0.0748\n",
            "Epoch [11863/20000], Training Loss: 0.0795\n",
            "Epoch [11864/20000], Training Loss: 0.0849\n",
            "Epoch [11865/20000], Training Loss: 0.0761\n",
            "Epoch [11866/20000], Training Loss: 0.0862\n",
            "Epoch [11867/20000], Training Loss: 0.0812\n",
            "Epoch [11868/20000], Training Loss: 0.0740\n",
            "Epoch [11869/20000], Training Loss: 0.0763\n",
            "Epoch [11870/20000], Training Loss: 0.0880\n",
            "Epoch [11871/20000], Training Loss: 0.0788\n",
            "Epoch [11872/20000], Training Loss: 0.0828\n",
            "Epoch [11873/20000], Training Loss: 0.0814\n",
            "Epoch [11874/20000], Training Loss: 0.0753\n",
            "Epoch [11875/20000], Training Loss: 0.0861\n",
            "Epoch [11876/20000], Training Loss: 0.0785\n",
            "Epoch [11877/20000], Training Loss: 0.0751\n",
            "Epoch [11878/20000], Training Loss: 0.0815\n",
            "Epoch [11879/20000], Training Loss: 0.0818\n",
            "Epoch [11880/20000], Training Loss: 0.0777\n",
            "Epoch [11881/20000], Training Loss: 0.0822\n",
            "Epoch [11882/20000], Training Loss: 0.0782\n",
            "Epoch [11883/20000], Training Loss: 0.0770\n",
            "Epoch [11884/20000], Training Loss: 0.0807\n",
            "Epoch [11885/20000], Training Loss: 0.0751\n",
            "Epoch [11886/20000], Training Loss: 0.0764\n",
            "Epoch [11887/20000], Training Loss: 0.0814\n",
            "Epoch [11888/20000], Training Loss: 0.0749\n",
            "Epoch [11889/20000], Training Loss: 0.0789\n",
            "Epoch [11890/20000], Training Loss: 0.0838\n",
            "Epoch [11891/20000], Training Loss: 0.0833\n",
            "Epoch [11892/20000], Training Loss: 0.0780\n",
            "Epoch [11893/20000], Training Loss: 0.0800\n",
            "Epoch [11894/20000], Training Loss: 0.0843\n",
            "Epoch [11895/20000], Training Loss: 0.0815\n",
            "Epoch [11896/20000], Training Loss: 0.0728\n",
            "Epoch [11897/20000], Training Loss: 0.0757\n",
            "Epoch [11898/20000], Training Loss: 0.0834\n",
            "Epoch [11899/20000], Training Loss: 0.0807\n",
            "Epoch [11900/20000], Training Loss: 0.0772\n",
            "Epoch [11901/20000], Training Loss: 0.0738\n",
            "Epoch [11902/20000], Training Loss: 0.0770\n",
            "Epoch [11903/20000], Training Loss: 0.0773\n",
            "Epoch [11904/20000], Training Loss: 0.0739\n",
            "Epoch [11905/20000], Training Loss: 0.0804\n",
            "Epoch [11906/20000], Training Loss: 0.0748\n",
            "Epoch [11907/20000], Training Loss: 0.0757\n",
            "Epoch [11908/20000], Training Loss: 0.0775\n",
            "Epoch [11909/20000], Training Loss: 0.0791\n",
            "Epoch [11910/20000], Training Loss: 0.0755\n",
            "Epoch [11911/20000], Training Loss: 0.0743\n",
            "Epoch [11912/20000], Training Loss: 0.0833\n",
            "Epoch [11913/20000], Training Loss: 0.0813\n",
            "Epoch [11914/20000], Training Loss: 0.0799\n",
            "Epoch [11915/20000], Training Loss: 0.0830\n",
            "Epoch [11916/20000], Training Loss: 0.0821\n",
            "Epoch [11917/20000], Training Loss: 0.0776\n",
            "Epoch [11918/20000], Training Loss: 0.0774\n",
            "Epoch [11919/20000], Training Loss: 0.0794\n",
            "Epoch [11920/20000], Training Loss: 0.0770\n",
            "Epoch [11921/20000], Training Loss: 0.0817\n",
            "Epoch [11922/20000], Training Loss: 0.0747\n",
            "Epoch [11923/20000], Training Loss: 0.0812\n",
            "Epoch [11924/20000], Training Loss: 0.0771\n",
            "Epoch [11925/20000], Training Loss: 0.0767\n",
            "Epoch [11926/20000], Training Loss: 0.0774\n",
            "Epoch [11927/20000], Training Loss: 0.0762\n",
            "Epoch [11928/20000], Training Loss: 0.0798\n",
            "Epoch [11929/20000], Training Loss: 0.0857\n",
            "Epoch [11930/20000], Training Loss: 0.0784\n",
            "Epoch [11931/20000], Training Loss: 0.0739\n",
            "Epoch [11932/20000], Training Loss: 0.0809\n",
            "Epoch [11933/20000], Training Loss: 0.0796\n",
            "Epoch [11934/20000], Training Loss: 0.0779\n",
            "Epoch [11935/20000], Training Loss: 0.0864\n",
            "Epoch [11936/20000], Training Loss: 0.0767\n",
            "Epoch [11937/20000], Training Loss: 0.0770\n",
            "Epoch [11938/20000], Training Loss: 0.0794\n",
            "Epoch [11939/20000], Training Loss: 0.0849\n",
            "Epoch [11940/20000], Training Loss: 0.0773\n",
            "Epoch [11941/20000], Training Loss: 0.0760\n",
            "Epoch [11942/20000], Training Loss: 0.0802\n",
            "Epoch [11943/20000], Training Loss: 0.0797\n",
            "Epoch [11944/20000], Training Loss: 0.0746\n",
            "Epoch [11945/20000], Training Loss: 0.0846\n",
            "Epoch [11946/20000], Training Loss: 0.0794\n",
            "Epoch [11947/20000], Training Loss: 0.0859\n",
            "Epoch [11948/20000], Training Loss: 0.0788\n",
            "Epoch [11949/20000], Training Loss: 0.0821\n",
            "Epoch [11950/20000], Training Loss: 0.0804\n",
            "Epoch [11951/20000], Training Loss: 0.0803\n",
            "Epoch [11952/20000], Training Loss: 0.0790\n",
            "Epoch [11953/20000], Training Loss: 0.0794\n",
            "Epoch [11954/20000], Training Loss: 0.0807\n",
            "Epoch [11955/20000], Training Loss: 0.0787\n",
            "Epoch [11956/20000], Training Loss: 0.0744\n",
            "Epoch [11957/20000], Training Loss: 0.0867\n",
            "Epoch [11958/20000], Training Loss: 0.0826\n",
            "Epoch [11959/20000], Training Loss: 0.0795\n",
            "Epoch [11960/20000], Training Loss: 0.0846\n",
            "Epoch [11961/20000], Training Loss: 0.0775\n",
            "Epoch [11962/20000], Training Loss: 0.0833\n",
            "Epoch [11963/20000], Training Loss: 0.0846\n",
            "Epoch [11964/20000], Training Loss: 0.0846\n",
            "Epoch [11965/20000], Training Loss: 0.0850\n",
            "Epoch [11966/20000], Training Loss: 0.0805\n",
            "Epoch [11967/20000], Training Loss: 0.0807\n",
            "Epoch [11968/20000], Training Loss: 0.0805\n",
            "Epoch [11969/20000], Training Loss: 0.0787\n",
            "Epoch [11970/20000], Training Loss: 0.0820\n",
            "Epoch [11971/20000], Training Loss: 0.0836\n",
            "Epoch [11972/20000], Training Loss: 0.0740\n",
            "Epoch [11973/20000], Training Loss: 0.0793\n",
            "Epoch [11974/20000], Training Loss: 0.0816\n",
            "Epoch [11975/20000], Training Loss: 0.0799\n",
            "Epoch [11976/20000], Training Loss: 0.0829\n",
            "Epoch [11977/20000], Training Loss: 0.0786\n",
            "Epoch [11978/20000], Training Loss: 0.0787\n",
            "Epoch [11979/20000], Training Loss: 0.0737\n",
            "Epoch [11980/20000], Training Loss: 0.0788\n",
            "Epoch [11981/20000], Training Loss: 0.0775\n",
            "Epoch [11982/20000], Training Loss: 0.0814\n",
            "Epoch [11983/20000], Training Loss: 0.0773\n",
            "Epoch [11984/20000], Training Loss: 0.0769\n",
            "Epoch [11985/20000], Training Loss: 0.0830\n",
            "Epoch [11986/20000], Training Loss: 0.0811\n",
            "Epoch [11987/20000], Training Loss: 0.0722\n",
            "Epoch [11988/20000], Training Loss: 0.0728\n",
            "Epoch [11989/20000], Training Loss: 0.0748\n",
            "Epoch [11990/20000], Training Loss: 0.0883\n",
            "Epoch [11991/20000], Training Loss: 0.0817\n",
            "Epoch [11992/20000], Training Loss: 0.0859\n",
            "Epoch [11993/20000], Training Loss: 0.0737\n",
            "Epoch [11994/20000], Training Loss: 0.0734\n",
            "Epoch [11995/20000], Training Loss: 0.0788\n",
            "Epoch [11996/20000], Training Loss: 0.0769\n",
            "Epoch [11997/20000], Training Loss: 0.0802\n",
            "Epoch [11998/20000], Training Loss: 0.0793\n",
            "Epoch [11999/20000], Training Loss: 0.0843\n",
            "Epoch [12000/20000], Training Loss: 0.0800\n",
            "Epoch [12001/20000], Training Loss: 0.0788\n",
            "Epoch [12002/20000], Training Loss: 0.0794\n",
            "Epoch [12003/20000], Training Loss: 0.0831\n",
            "Epoch [12004/20000], Training Loss: 0.0812\n",
            "Epoch [12005/20000], Training Loss: 0.0786\n",
            "Epoch [12006/20000], Training Loss: 0.0855\n",
            "Epoch [12007/20000], Training Loss: 0.0745\n",
            "Epoch [12008/20000], Training Loss: 0.0811\n",
            "Epoch [12009/20000], Training Loss: 0.0794\n",
            "Epoch [12010/20000], Training Loss: 0.0818\n",
            "Epoch [12011/20000], Training Loss: 0.0744\n",
            "Epoch [12012/20000], Training Loss: 0.0851\n",
            "Epoch [12013/20000], Training Loss: 0.0756\n",
            "Epoch [12014/20000], Training Loss: 0.0832\n",
            "Epoch [12015/20000], Training Loss: 0.0852\n",
            "Epoch [12016/20000], Training Loss: 0.0810\n",
            "Epoch [12017/20000], Training Loss: 0.0812\n",
            "Epoch [12018/20000], Training Loss: 0.0788\n",
            "Epoch [12019/20000], Training Loss: 0.0746\n",
            "Epoch [12020/20000], Training Loss: 0.0828\n",
            "Epoch [12021/20000], Training Loss: 0.0822\n",
            "Epoch [12022/20000], Training Loss: 0.0810\n",
            "Epoch [12023/20000], Training Loss: 0.0805\n",
            "Epoch [12024/20000], Training Loss: 0.0813\n",
            "Epoch [12025/20000], Training Loss: 0.0814\n",
            "Epoch [12026/20000], Training Loss: 0.0810\n",
            "Epoch [12027/20000], Training Loss: 0.0801\n",
            "Epoch [12028/20000], Training Loss: 0.0793\n",
            "Epoch [12029/20000], Training Loss: 0.0811\n",
            "Epoch [12030/20000], Training Loss: 0.0739\n",
            "Epoch [12031/20000], Training Loss: 0.0799\n",
            "Epoch [12032/20000], Training Loss: 0.0848\n",
            "Epoch [12033/20000], Training Loss: 0.0770\n",
            "Epoch [12034/20000], Training Loss: 0.0792\n",
            "Epoch [12035/20000], Training Loss: 0.0798\n",
            "Epoch [12036/20000], Training Loss: 0.0764\n",
            "Epoch [12037/20000], Training Loss: 0.0800\n",
            "Epoch [12038/20000], Training Loss: 0.0851\n",
            "Epoch [12039/20000], Training Loss: 0.0831\n",
            "Epoch [12040/20000], Training Loss: 0.0874\n",
            "Epoch [12041/20000], Training Loss: 0.0794\n",
            "Epoch [12042/20000], Training Loss: 0.0790\n",
            "Epoch [12043/20000], Training Loss: 0.0784\n",
            "Epoch [12044/20000], Training Loss: 0.0828\n",
            "Epoch [12045/20000], Training Loss: 0.0738\n",
            "Epoch [12046/20000], Training Loss: 0.0839\n",
            "Epoch [12047/20000], Training Loss: 0.0816\n",
            "Epoch [12048/20000], Training Loss: 0.0797\n",
            "Epoch [12049/20000], Training Loss: 0.0865\n",
            "Epoch [12050/20000], Training Loss: 0.0766\n",
            "Epoch [12051/20000], Training Loss: 0.0777\n",
            "Epoch [12052/20000], Training Loss: 0.0763\n",
            "Epoch [12053/20000], Training Loss: 0.0833\n",
            "Epoch [12054/20000], Training Loss: 0.0858\n",
            "Epoch [12055/20000], Training Loss: 0.0808\n",
            "Epoch [12056/20000], Training Loss: 0.0828\n",
            "Epoch [12057/20000], Training Loss: 0.0796\n",
            "Epoch [12058/20000], Training Loss: 0.0730\n",
            "Epoch [12059/20000], Training Loss: 0.0796\n",
            "Epoch [12060/20000], Training Loss: 0.0800\n",
            "Epoch [12061/20000], Training Loss: 0.0746\n",
            "Epoch [12062/20000], Training Loss: 0.0799\n",
            "Epoch [12063/20000], Training Loss: 0.0786\n",
            "Epoch [12064/20000], Training Loss: 0.0769\n",
            "Epoch [12065/20000], Training Loss: 0.0809\n",
            "Epoch [12066/20000], Training Loss: 0.0780\n",
            "Epoch [12067/20000], Training Loss: 0.0773\n",
            "Epoch [12068/20000], Training Loss: 0.0805\n",
            "Epoch [12069/20000], Training Loss: 0.0765\n",
            "Epoch [12070/20000], Training Loss: 0.0792\n",
            "Epoch [12071/20000], Training Loss: 0.0809\n",
            "Epoch [12072/20000], Training Loss: 0.0756\n",
            "Epoch [12073/20000], Training Loss: 0.0763\n",
            "Epoch [12074/20000], Training Loss: 0.0766\n",
            "Epoch [12075/20000], Training Loss: 0.0711\n",
            "Epoch [12076/20000], Training Loss: 0.0770\n",
            "Epoch [12077/20000], Training Loss: 0.0785\n",
            "Epoch [12078/20000], Training Loss: 0.0841\n",
            "Epoch [12079/20000], Training Loss: 0.0799\n",
            "Epoch [12080/20000], Training Loss: 0.0772\n",
            "Epoch [12081/20000], Training Loss: 0.0838\n",
            "Epoch [12082/20000], Training Loss: 0.0805\n",
            "Epoch [12083/20000], Training Loss: 0.0758\n",
            "Epoch [12084/20000], Training Loss: 0.0746\n",
            "Epoch [12085/20000], Training Loss: 0.0811\n",
            "Epoch [12086/20000], Training Loss: 0.0797\n",
            "Epoch [12087/20000], Training Loss: 0.0803\n",
            "Epoch [12088/20000], Training Loss: 0.0753\n",
            "Epoch [12089/20000], Training Loss: 0.0783\n",
            "Epoch [12090/20000], Training Loss: 0.0774\n",
            "Epoch [12091/20000], Training Loss: 0.0743\n",
            "Epoch [12092/20000], Training Loss: 0.0796\n",
            "Epoch [12093/20000], Training Loss: 0.0806\n",
            "Epoch [12094/20000], Training Loss: 0.0794\n",
            "Epoch [12095/20000], Training Loss: 0.0847\n",
            "Epoch [12096/20000], Training Loss: 0.0819\n",
            "Epoch [12097/20000], Training Loss: 0.0738\n",
            "Epoch [12098/20000], Training Loss: 0.0754\n",
            "Epoch [12099/20000], Training Loss: 0.0759\n",
            "Epoch [12100/20000], Training Loss: 0.0816\n",
            "Epoch [12101/20000], Training Loss: 0.0823\n",
            "Epoch [12102/20000], Training Loss: 0.0854\n",
            "Epoch [12103/20000], Training Loss: 0.0801\n",
            "Epoch [12104/20000], Training Loss: 0.0833\n",
            "Epoch [12105/20000], Training Loss: 0.0818\n",
            "Epoch [12106/20000], Training Loss: 0.0762\n",
            "Epoch [12107/20000], Training Loss: 0.0802\n",
            "Epoch [12108/20000], Training Loss: 0.0802\n",
            "Epoch [12109/20000], Training Loss: 0.0854\n",
            "Epoch [12110/20000], Training Loss: 0.0811\n",
            "Epoch [12111/20000], Training Loss: 0.0755\n",
            "Epoch [12112/20000], Training Loss: 0.0805\n",
            "Epoch [12113/20000], Training Loss: 0.0791\n",
            "Epoch [12114/20000], Training Loss: 0.0787\n",
            "Epoch [12115/20000], Training Loss: 0.0792\n",
            "Epoch [12116/20000], Training Loss: 0.0773\n",
            "Epoch [12117/20000], Training Loss: 0.0817\n",
            "Epoch [12118/20000], Training Loss: 0.0829\n",
            "Epoch [12119/20000], Training Loss: 0.0798\n",
            "Epoch [12120/20000], Training Loss: 0.0794\n",
            "Epoch [12121/20000], Training Loss: 0.0776\n",
            "Epoch [12122/20000], Training Loss: 0.0758\n",
            "Epoch [12123/20000], Training Loss: 0.0809\n",
            "Epoch [12124/20000], Training Loss: 0.0814\n",
            "Epoch [12125/20000], Training Loss: 0.0796\n",
            "Epoch [12126/20000], Training Loss: 0.0850\n",
            "Epoch [12127/20000], Training Loss: 0.0824\n",
            "Epoch [12128/20000], Training Loss: 0.0764\n",
            "Epoch [12129/20000], Training Loss: 0.0791\n",
            "Epoch [12130/20000], Training Loss: 0.0846\n",
            "Epoch [12131/20000], Training Loss: 0.0811\n",
            "Epoch [12132/20000], Training Loss: 0.0790\n",
            "Epoch [12133/20000], Training Loss: 0.0803\n",
            "Epoch [12134/20000], Training Loss: 0.0742\n",
            "Epoch [12135/20000], Training Loss: 0.0854\n",
            "Epoch [12136/20000], Training Loss: 0.0760\n",
            "Epoch [12137/20000], Training Loss: 0.0848\n",
            "Epoch [12138/20000], Training Loss: 0.0784\n",
            "Epoch [12139/20000], Training Loss: 0.0772\n",
            "Epoch [12140/20000], Training Loss: 0.0781\n",
            "Epoch [12141/20000], Training Loss: 0.0784\n",
            "Epoch [12142/20000], Training Loss: 0.0760\n",
            "Epoch [12143/20000], Training Loss: 0.0877\n",
            "Epoch [12144/20000], Training Loss: 0.0855\n",
            "Epoch [12145/20000], Training Loss: 0.0762\n",
            "Epoch [12146/20000], Training Loss: 0.0798\n",
            "Epoch [12147/20000], Training Loss: 0.0865\n",
            "Epoch [12148/20000], Training Loss: 0.0805\n",
            "Epoch [12149/20000], Training Loss: 0.0800\n",
            "Epoch [12150/20000], Training Loss: 0.0768\n",
            "Epoch [12151/20000], Training Loss: 0.0837\n",
            "Epoch [12152/20000], Training Loss: 0.0794\n",
            "Epoch [12153/20000], Training Loss: 0.0833\n",
            "Epoch [12154/20000], Training Loss: 0.0846\n",
            "Epoch [12155/20000], Training Loss: 0.0812\n",
            "Epoch [12156/20000], Training Loss: 0.0855\n",
            "Epoch [12157/20000], Training Loss: 0.0822\n",
            "Epoch [12158/20000], Training Loss: 0.0809\n",
            "Epoch [12159/20000], Training Loss: 0.0871\n",
            "Epoch [12160/20000], Training Loss: 0.0775\n",
            "Epoch [12161/20000], Training Loss: 0.0764\n",
            "Epoch [12162/20000], Training Loss: 0.0851\n",
            "Epoch [12163/20000], Training Loss: 0.0739\n",
            "Epoch [12164/20000], Training Loss: 0.0818\n",
            "Epoch [12165/20000], Training Loss: 0.0815\n",
            "Epoch [12166/20000], Training Loss: 0.0746\n",
            "Epoch [12167/20000], Training Loss: 0.0838\n",
            "Epoch [12168/20000], Training Loss: 0.0787\n",
            "Epoch [12169/20000], Training Loss: 0.0814\n",
            "Epoch [12170/20000], Training Loss: 0.0796\n",
            "Epoch [12171/20000], Training Loss: 0.0844\n",
            "Epoch [12172/20000], Training Loss: 0.0834\n",
            "Epoch [12173/20000], Training Loss: 0.0811\n",
            "Epoch [12174/20000], Training Loss: 0.0763\n",
            "Epoch [12175/20000], Training Loss: 0.0799\n",
            "Epoch [12176/20000], Training Loss: 0.0756\n",
            "Epoch [12177/20000], Training Loss: 0.0779\n",
            "Epoch [12178/20000], Training Loss: 0.0770\n",
            "Epoch [12179/20000], Training Loss: 0.0766\n",
            "Epoch [12180/20000], Training Loss: 0.0814\n",
            "Epoch [12181/20000], Training Loss: 0.0758\n",
            "Epoch [12182/20000], Training Loss: 0.0803\n",
            "Epoch [12183/20000], Training Loss: 0.0776\n",
            "Epoch [12184/20000], Training Loss: 0.0796\n",
            "Epoch [12185/20000], Training Loss: 0.0811\n",
            "Epoch [12186/20000], Training Loss: 0.0874\n",
            "Epoch [12187/20000], Training Loss: 0.0757\n",
            "Epoch [12188/20000], Training Loss: 0.0837\n",
            "Epoch [12189/20000], Training Loss: 0.0776\n",
            "Epoch [12190/20000], Training Loss: 0.0758\n",
            "Epoch [12191/20000], Training Loss: 0.0795\n",
            "Epoch [12192/20000], Training Loss: 0.0752\n",
            "Epoch [12193/20000], Training Loss: 0.0858\n",
            "Epoch [12194/20000], Training Loss: 0.0779\n",
            "Epoch [12195/20000], Training Loss: 0.0795\n",
            "Epoch [12196/20000], Training Loss: 0.0806\n",
            "Epoch [12197/20000], Training Loss: 0.0818\n",
            "Epoch [12198/20000], Training Loss: 0.0796\n",
            "Epoch [12199/20000], Training Loss: 0.0818\n",
            "Epoch [12200/20000], Training Loss: 0.0791\n",
            "Epoch [12201/20000], Training Loss: 0.0737\n",
            "Epoch [12202/20000], Training Loss: 0.0785\n",
            "Epoch [12203/20000], Training Loss: 0.0748\n",
            "Epoch [12204/20000], Training Loss: 0.0745\n",
            "Epoch [12205/20000], Training Loss: 0.0846\n",
            "Epoch [12206/20000], Training Loss: 0.0839\n",
            "Epoch [12207/20000], Training Loss: 0.0785\n",
            "Epoch [12208/20000], Training Loss: 0.0814\n",
            "Epoch [12209/20000], Training Loss: 0.0823\n",
            "Epoch [12210/20000], Training Loss: 0.0834\n",
            "Epoch [12211/20000], Training Loss: 0.0797\n",
            "Epoch [12212/20000], Training Loss: 0.0834\n",
            "Epoch [12213/20000], Training Loss: 0.0742\n",
            "Epoch [12214/20000], Training Loss: 0.0843\n",
            "Epoch [12215/20000], Training Loss: 0.0808\n",
            "Epoch [12216/20000], Training Loss: 0.0795\n",
            "Epoch [12217/20000], Training Loss: 0.0873\n",
            "Epoch [12218/20000], Training Loss: 0.0817\n",
            "Epoch [12219/20000], Training Loss: 0.0807\n",
            "Epoch [12220/20000], Training Loss: 0.0786\n",
            "Epoch [12221/20000], Training Loss: 0.0762\n",
            "Epoch [12222/20000], Training Loss: 0.0739\n",
            "Epoch [12223/20000], Training Loss: 0.0810\n",
            "Epoch [12224/20000], Training Loss: 0.0868\n",
            "Epoch [12225/20000], Training Loss: 0.0819\n",
            "Epoch [12226/20000], Training Loss: 0.0737\n",
            "Epoch [12227/20000], Training Loss: 0.0762\n",
            "Epoch [12228/20000], Training Loss: 0.0794\n",
            "Epoch [12229/20000], Training Loss: 0.0786\n",
            "Epoch [12230/20000], Training Loss: 0.0834\n",
            "Epoch [12231/20000], Training Loss: 0.0842\n",
            "Epoch [12232/20000], Training Loss: 0.0834\n",
            "Epoch [12233/20000], Training Loss: 0.0839\n",
            "Epoch [12234/20000], Training Loss: 0.0756\n",
            "Epoch [12235/20000], Training Loss: 0.0832\n",
            "Epoch [12236/20000], Training Loss: 0.0800\n",
            "Epoch [12237/20000], Training Loss: 0.0814\n",
            "Epoch [12238/20000], Training Loss: 0.0743\n",
            "Epoch [12239/20000], Training Loss: 0.0768\n",
            "Epoch [12240/20000], Training Loss: 0.0843\n",
            "Epoch [12241/20000], Training Loss: 0.0816\n",
            "Epoch [12242/20000], Training Loss: 0.0808\n",
            "Epoch [12243/20000], Training Loss: 0.0843\n",
            "Epoch [12244/20000], Training Loss: 0.0849\n",
            "Epoch [12245/20000], Training Loss: 0.0806\n",
            "Epoch [12246/20000], Training Loss: 0.0826\n",
            "Epoch [12247/20000], Training Loss: 0.0779\n",
            "Epoch [12248/20000], Training Loss: 0.0781\n",
            "Epoch [12249/20000], Training Loss: 0.0754\n",
            "Epoch [12250/20000], Training Loss: 0.0776\n",
            "Epoch [12251/20000], Training Loss: 0.0799\n",
            "Epoch [12252/20000], Training Loss: 0.0788\n",
            "Epoch [12253/20000], Training Loss: 0.0850\n",
            "Epoch [12254/20000], Training Loss: 0.0836\n",
            "Epoch [12255/20000], Training Loss: 0.0815\n",
            "Epoch [12256/20000], Training Loss: 0.0816\n",
            "Epoch [12257/20000], Training Loss: 0.0749\n",
            "Epoch [12258/20000], Training Loss: 0.0815\n",
            "Epoch [12259/20000], Training Loss: 0.0790\n",
            "Epoch [12260/20000], Training Loss: 0.0773\n",
            "Epoch [12261/20000], Training Loss: 0.0799\n",
            "Epoch [12262/20000], Training Loss: 0.0772\n",
            "Epoch [12263/20000], Training Loss: 0.0834\n",
            "Epoch [12264/20000], Training Loss: 0.0817\n",
            "Epoch [12265/20000], Training Loss: 0.0766\n",
            "Epoch [12266/20000], Training Loss: 0.0786\n",
            "Epoch [12267/20000], Training Loss: 0.0791\n",
            "Epoch [12268/20000], Training Loss: 0.0783\n",
            "Epoch [12269/20000], Training Loss: 0.0780\n",
            "Epoch [12270/20000], Training Loss: 0.0764\n",
            "Epoch [12271/20000], Training Loss: 0.0796\n",
            "Epoch [12272/20000], Training Loss: 0.0812\n",
            "Epoch [12273/20000], Training Loss: 0.0811\n",
            "Epoch [12274/20000], Training Loss: 0.0745\n",
            "Epoch [12275/20000], Training Loss: 0.0743\n",
            "Epoch [12276/20000], Training Loss: 0.0851\n",
            "Epoch [12277/20000], Training Loss: 0.0775\n",
            "Epoch [12278/20000], Training Loss: 0.0792\n",
            "Epoch [12279/20000], Training Loss: 0.0819\n",
            "Epoch [12280/20000], Training Loss: 0.0845\n",
            "Epoch [12281/20000], Training Loss: 0.0767\n",
            "Epoch [12282/20000], Training Loss: 0.0719\n",
            "Epoch [12283/20000], Training Loss: 0.0752\n",
            "Epoch [12284/20000], Training Loss: 0.0799\n",
            "Epoch [12285/20000], Training Loss: 0.0843\n",
            "Epoch [12286/20000], Training Loss: 0.0806\n",
            "Epoch [12287/20000], Training Loss: 0.0846\n",
            "Epoch [12288/20000], Training Loss: 0.0755\n",
            "Epoch [12289/20000], Training Loss: 0.0756\n",
            "Epoch [12290/20000], Training Loss: 0.0760\n",
            "Epoch [12291/20000], Training Loss: 0.0737\n",
            "Epoch [12292/20000], Training Loss: 0.0851\n",
            "Epoch [12293/20000], Training Loss: 0.0826\n",
            "Epoch [12294/20000], Training Loss: 0.0752\n",
            "Epoch [12295/20000], Training Loss: 0.0766\n",
            "Epoch [12296/20000], Training Loss: 0.0729\n",
            "Epoch [12297/20000], Training Loss: 0.0808\n",
            "Epoch [12298/20000], Training Loss: 0.0800\n",
            "Epoch [12299/20000], Training Loss: 0.0835\n",
            "Epoch [12300/20000], Training Loss: 0.0817\n",
            "Epoch [12301/20000], Training Loss: 0.0847\n",
            "Epoch [12302/20000], Training Loss: 0.0730\n",
            "Epoch [12303/20000], Training Loss: 0.0765\n",
            "Epoch [12304/20000], Training Loss: 0.0769\n",
            "Epoch [12305/20000], Training Loss: 0.0786\n",
            "Epoch [12306/20000], Training Loss: 0.0811\n",
            "Epoch [12307/20000], Training Loss: 0.0768\n",
            "Epoch [12308/20000], Training Loss: 0.0793\n",
            "Epoch [12309/20000], Training Loss: 0.0746\n",
            "Epoch [12310/20000], Training Loss: 0.0832\n",
            "Epoch [12311/20000], Training Loss: 0.0856\n",
            "Epoch [12312/20000], Training Loss: 0.0808\n",
            "Epoch [12313/20000], Training Loss: 0.0820\n",
            "Epoch [12314/20000], Training Loss: 0.0802\n",
            "Epoch [12315/20000], Training Loss: 0.0770\n",
            "Epoch [12316/20000], Training Loss: 0.0764\n",
            "Epoch [12317/20000], Training Loss: 0.0725\n",
            "Epoch [12318/20000], Training Loss: 0.0799\n",
            "Epoch [12319/20000], Training Loss: 0.0761\n",
            "Epoch [12320/20000], Training Loss: 0.0740\n",
            "Epoch [12321/20000], Training Loss: 0.0811\n",
            "Epoch [12322/20000], Training Loss: 0.0809\n",
            "Epoch [12323/20000], Training Loss: 0.0810\n",
            "Epoch [12324/20000], Training Loss: 0.0759\n",
            "Epoch [12325/20000], Training Loss: 0.0888\n",
            "Epoch [12326/20000], Training Loss: 0.0758\n",
            "Epoch [12327/20000], Training Loss: 0.0732\n",
            "Epoch [12328/20000], Training Loss: 0.0756\n",
            "Epoch [12329/20000], Training Loss: 0.0840\n",
            "Epoch [12330/20000], Training Loss: 0.0731\n",
            "Epoch [12331/20000], Training Loss: 0.0840\n",
            "Epoch [12332/20000], Training Loss: 0.0782\n",
            "Epoch [12333/20000], Training Loss: 0.0798\n",
            "Epoch [12334/20000], Training Loss: 0.0732\n",
            "Epoch [12335/20000], Training Loss: 0.0842\n",
            "Epoch [12336/20000], Training Loss: 0.0776\n",
            "Epoch [12337/20000], Training Loss: 0.0769\n",
            "Epoch [12338/20000], Training Loss: 0.0793\n",
            "Epoch [12339/20000], Training Loss: 0.0801\n",
            "Epoch [12340/20000], Training Loss: 0.0839\n",
            "Epoch [12341/20000], Training Loss: 0.0806\n",
            "Epoch [12342/20000], Training Loss: 0.0817\n",
            "Epoch [12343/20000], Training Loss: 0.0805\n",
            "Epoch [12344/20000], Training Loss: 0.0862\n",
            "Epoch [12345/20000], Training Loss: 0.0770\n",
            "Epoch [12346/20000], Training Loss: 0.0752\n",
            "Epoch [12347/20000], Training Loss: 0.0809\n",
            "Epoch [12348/20000], Training Loss: 0.0743\n",
            "Epoch [12349/20000], Training Loss: 0.0742\n",
            "Epoch [12350/20000], Training Loss: 0.0832\n",
            "Epoch [12351/20000], Training Loss: 0.0779\n",
            "Epoch [12352/20000], Training Loss: 0.0821\n",
            "Epoch [12353/20000], Training Loss: 0.0829\n",
            "Epoch [12354/20000], Training Loss: 0.0812\n",
            "Epoch [12355/20000], Training Loss: 0.0810\n",
            "Epoch [12356/20000], Training Loss: 0.0809\n",
            "Epoch [12357/20000], Training Loss: 0.0774\n",
            "Epoch [12358/20000], Training Loss: 0.0758\n",
            "Epoch [12359/20000], Training Loss: 0.0795\n",
            "Epoch [12360/20000], Training Loss: 0.0747\n",
            "Epoch [12361/20000], Training Loss: 0.0775\n",
            "Epoch [12362/20000], Training Loss: 0.0863\n",
            "Epoch [12363/20000], Training Loss: 0.0801\n",
            "Epoch [12364/20000], Training Loss: 0.0792\n",
            "Epoch [12365/20000], Training Loss: 0.0815\n",
            "Epoch [12366/20000], Training Loss: 0.0824\n",
            "Epoch [12367/20000], Training Loss: 0.0777\n",
            "Epoch [12368/20000], Training Loss: 0.0766\n",
            "Epoch [12369/20000], Training Loss: 0.0791\n",
            "Epoch [12370/20000], Training Loss: 0.0798\n",
            "Epoch [12371/20000], Training Loss: 0.0833\n",
            "Epoch [12372/20000], Training Loss: 0.0769\n",
            "Epoch [12373/20000], Training Loss: 0.0837\n",
            "Epoch [12374/20000], Training Loss: 0.0868\n",
            "Epoch [12375/20000], Training Loss: 0.0781\n",
            "Epoch [12376/20000], Training Loss: 0.0755\n",
            "Epoch [12377/20000], Training Loss: 0.0828\n",
            "Epoch [12378/20000], Training Loss: 0.0790\n",
            "Epoch [12379/20000], Training Loss: 0.0808\n",
            "Epoch [12380/20000], Training Loss: 0.0788\n",
            "Epoch [12381/20000], Training Loss: 0.0788\n",
            "Epoch [12382/20000], Training Loss: 0.0839\n",
            "Epoch [12383/20000], Training Loss: 0.0842\n",
            "Epoch [12384/20000], Training Loss: 0.0815\n",
            "Epoch [12385/20000], Training Loss: 0.0831\n",
            "Epoch [12386/20000], Training Loss: 0.0770\n",
            "Epoch [12387/20000], Training Loss: 0.0823\n",
            "Epoch [12388/20000], Training Loss: 0.0739\n",
            "Epoch [12389/20000], Training Loss: 0.0768\n",
            "Epoch [12390/20000], Training Loss: 0.0787\n",
            "Epoch [12391/20000], Training Loss: 0.0892\n",
            "Epoch [12392/20000], Training Loss: 0.0786\n",
            "Epoch [12393/20000], Training Loss: 0.0806\n",
            "Epoch [12394/20000], Training Loss: 0.0858\n",
            "Epoch [12395/20000], Training Loss: 0.0776\n",
            "Epoch [12396/20000], Training Loss: 0.0801\n",
            "Epoch [12397/20000], Training Loss: 0.0798\n",
            "Epoch [12398/20000], Training Loss: 0.0813\n",
            "Epoch [12399/20000], Training Loss: 0.0848\n",
            "Epoch [12400/20000], Training Loss: 0.0748\n",
            "Epoch [12401/20000], Training Loss: 0.0751\n",
            "Epoch [12402/20000], Training Loss: 0.0767\n",
            "Epoch [12403/20000], Training Loss: 0.0754\n",
            "Epoch [12404/20000], Training Loss: 0.0766\n",
            "Epoch [12405/20000], Training Loss: 0.0793\n",
            "Epoch [12406/20000], Training Loss: 0.0786\n",
            "Epoch [12407/20000], Training Loss: 0.0743\n",
            "Epoch [12408/20000], Training Loss: 0.0751\n",
            "Epoch [12409/20000], Training Loss: 0.0798\n",
            "Epoch [12410/20000], Training Loss: 0.0794\n",
            "Epoch [12411/20000], Training Loss: 0.0811\n",
            "Epoch [12412/20000], Training Loss: 0.0839\n",
            "Epoch [12413/20000], Training Loss: 0.0765\n",
            "Epoch [12414/20000], Training Loss: 0.0781\n",
            "Epoch [12415/20000], Training Loss: 0.0738\n",
            "Epoch [12416/20000], Training Loss: 0.0854\n",
            "Epoch [12417/20000], Training Loss: 0.0801\n",
            "Epoch [12418/20000], Training Loss: 0.0780\n",
            "Epoch [12419/20000], Training Loss: 0.0781\n",
            "Epoch [12420/20000], Training Loss: 0.0791\n",
            "Epoch [12421/20000], Training Loss: 0.0831\n",
            "Epoch [12422/20000], Training Loss: 0.0808\n",
            "Epoch [12423/20000], Training Loss: 0.0743\n",
            "Epoch [12424/20000], Training Loss: 0.0767\n",
            "Epoch [12425/20000], Training Loss: 0.0821\n",
            "Epoch [12426/20000], Training Loss: 0.0742\n",
            "Epoch [12427/20000], Training Loss: 0.0816\n",
            "Epoch [12428/20000], Training Loss: 0.0848\n",
            "Epoch [12429/20000], Training Loss: 0.0781\n",
            "Epoch [12430/20000], Training Loss: 0.0747\n",
            "Epoch [12431/20000], Training Loss: 0.0753\n",
            "Epoch [12432/20000], Training Loss: 0.0806\n",
            "Epoch [12433/20000], Training Loss: 0.0833\n",
            "Epoch [12434/20000], Training Loss: 0.0801\n",
            "Epoch [12435/20000], Training Loss: 0.0780\n",
            "Epoch [12436/20000], Training Loss: 0.0874\n",
            "Epoch [12437/20000], Training Loss: 0.0838\n",
            "Epoch [12438/20000], Training Loss: 0.0833\n",
            "Epoch [12439/20000], Training Loss: 0.0806\n",
            "Epoch [12440/20000], Training Loss: 0.0842\n",
            "Epoch [12441/20000], Training Loss: 0.0802\n",
            "Epoch [12442/20000], Training Loss: 0.0785\n",
            "Epoch [12443/20000], Training Loss: 0.0815\n",
            "Epoch [12444/20000], Training Loss: 0.0785\n",
            "Epoch [12445/20000], Training Loss: 0.0761\n",
            "Epoch [12446/20000], Training Loss: 0.0830\n",
            "Epoch [12447/20000], Training Loss: 0.0861\n",
            "Epoch [12448/20000], Training Loss: 0.0754\n",
            "Epoch [12449/20000], Training Loss: 0.0794\n",
            "Epoch [12450/20000], Training Loss: 0.0782\n",
            "Epoch [12451/20000], Training Loss: 0.0807\n",
            "Epoch [12452/20000], Training Loss: 0.0787\n",
            "Epoch [12453/20000], Training Loss: 0.0865\n",
            "Epoch [12454/20000], Training Loss: 0.0824\n",
            "Epoch [12455/20000], Training Loss: 0.0873\n",
            "Epoch [12456/20000], Training Loss: 0.0785\n",
            "Epoch [12457/20000], Training Loss: 0.0820\n",
            "Epoch [12458/20000], Training Loss: 0.0754\n",
            "Epoch [12459/20000], Training Loss: 0.0819\n",
            "Epoch [12460/20000], Training Loss: 0.0825\n",
            "Epoch [12461/20000], Training Loss: 0.0849\n",
            "Epoch [12462/20000], Training Loss: 0.0808\n",
            "Epoch [12463/20000], Training Loss: 0.0752\n",
            "Epoch [12464/20000], Training Loss: 0.0746\n",
            "Epoch [12465/20000], Training Loss: 0.0802\n",
            "Epoch [12466/20000], Training Loss: 0.0784\n",
            "Epoch [12467/20000], Training Loss: 0.0770\n",
            "Epoch [12468/20000], Training Loss: 0.0819\n",
            "Epoch [12469/20000], Training Loss: 0.0804\n",
            "Epoch [12470/20000], Training Loss: 0.0884\n",
            "Epoch [12471/20000], Training Loss: 0.0856\n",
            "Epoch [12472/20000], Training Loss: 0.0787\n",
            "Epoch [12473/20000], Training Loss: 0.0797\n",
            "Epoch [12474/20000], Training Loss: 0.0787\n",
            "Epoch [12475/20000], Training Loss: 0.0770\n",
            "Epoch [12476/20000], Training Loss: 0.0742\n",
            "Epoch [12477/20000], Training Loss: 0.0830\n",
            "Epoch [12478/20000], Training Loss: 0.0862\n",
            "Epoch [12479/20000], Training Loss: 0.0793\n",
            "Epoch [12480/20000], Training Loss: 0.0775\n",
            "Epoch [12481/20000], Training Loss: 0.0750\n",
            "Epoch [12482/20000], Training Loss: 0.0824\n",
            "Epoch [12483/20000], Training Loss: 0.0832\n",
            "Epoch [12484/20000], Training Loss: 0.0753\n",
            "Epoch [12485/20000], Training Loss: 0.0836\n",
            "Epoch [12486/20000], Training Loss: 0.0859\n",
            "Epoch [12487/20000], Training Loss: 0.0761\n",
            "Epoch [12488/20000], Training Loss: 0.0798\n",
            "Epoch [12489/20000], Training Loss: 0.0870\n",
            "Epoch [12490/20000], Training Loss: 0.0814\n",
            "Epoch [12491/20000], Training Loss: 0.0767\n",
            "Epoch [12492/20000], Training Loss: 0.0813\n",
            "Epoch [12493/20000], Training Loss: 0.0778\n",
            "Epoch [12494/20000], Training Loss: 0.0802\n",
            "Epoch [12495/20000], Training Loss: 0.0743\n",
            "Epoch [12496/20000], Training Loss: 0.0786\n",
            "Epoch [12497/20000], Training Loss: 0.0846\n",
            "Epoch [12498/20000], Training Loss: 0.0831\n",
            "Epoch [12499/20000], Training Loss: 0.0832\n",
            "Epoch [12500/20000], Training Loss: 0.0853\n",
            "Epoch [12501/20000], Training Loss: 0.0753\n",
            "Epoch [12502/20000], Training Loss: 0.0782\n",
            "Epoch [12503/20000], Training Loss: 0.0763\n",
            "Epoch [12504/20000], Training Loss: 0.0761\n",
            "Epoch [12505/20000], Training Loss: 0.0789\n",
            "Epoch [12506/20000], Training Loss: 0.0748\n",
            "Epoch [12507/20000], Training Loss: 0.0823\n",
            "Epoch [12508/20000], Training Loss: 0.0871\n",
            "Epoch [12509/20000], Training Loss: 0.0824\n",
            "Epoch [12510/20000], Training Loss: 0.0801\n",
            "Epoch [12511/20000], Training Loss: 0.0761\n",
            "Epoch [12512/20000], Training Loss: 0.0775\n",
            "Epoch [12513/20000], Training Loss: 0.0848\n",
            "Epoch [12514/20000], Training Loss: 0.0845\n",
            "Epoch [12515/20000], Training Loss: 0.0809\n",
            "Epoch [12516/20000], Training Loss: 0.0736\n",
            "Epoch [12517/20000], Training Loss: 0.0816\n",
            "Epoch [12518/20000], Training Loss: 0.0819\n",
            "Epoch [12519/20000], Training Loss: 0.0796\n",
            "Epoch [12520/20000], Training Loss: 0.0850\n",
            "Epoch [12521/20000], Training Loss: 0.0813\n",
            "Epoch [12522/20000], Training Loss: 0.0795\n",
            "Epoch [12523/20000], Training Loss: 0.0773\n",
            "Epoch [12524/20000], Training Loss: 0.0790\n",
            "Epoch [12525/20000], Training Loss: 0.0817\n",
            "Epoch [12526/20000], Training Loss: 0.0785\n",
            "Epoch [12527/20000], Training Loss: 0.0804\n",
            "Epoch [12528/20000], Training Loss: 0.0836\n",
            "Epoch [12529/20000], Training Loss: 0.0820\n",
            "Epoch [12530/20000], Training Loss: 0.0768\n",
            "Epoch [12531/20000], Training Loss: 0.0786\n",
            "Epoch [12532/20000], Training Loss: 0.0773\n",
            "Epoch [12533/20000], Training Loss: 0.0756\n",
            "Epoch [12534/20000], Training Loss: 0.0785\n",
            "Epoch [12535/20000], Training Loss: 0.0761\n",
            "Epoch [12536/20000], Training Loss: 0.0777\n",
            "Epoch [12537/20000], Training Loss: 0.0858\n",
            "Epoch [12538/20000], Training Loss: 0.0739\n",
            "Epoch [12539/20000], Training Loss: 0.0810\n",
            "Epoch [12540/20000], Training Loss: 0.0824\n",
            "Epoch [12541/20000], Training Loss: 0.0786\n",
            "Epoch [12542/20000], Training Loss: 0.0728\n",
            "Epoch [12543/20000], Training Loss: 0.0785\n",
            "Epoch [12544/20000], Training Loss: 0.0796\n",
            "Epoch [12545/20000], Training Loss: 0.0845\n",
            "Epoch [12546/20000], Training Loss: 0.0839\n",
            "Epoch [12547/20000], Training Loss: 0.0822\n",
            "Epoch [12548/20000], Training Loss: 0.0830\n",
            "Epoch [12549/20000], Training Loss: 0.0817\n",
            "Epoch [12550/20000], Training Loss: 0.0805\n",
            "Epoch [12551/20000], Training Loss: 0.0745\n",
            "Epoch [12552/20000], Training Loss: 0.0763\n",
            "Epoch [12553/20000], Training Loss: 0.0788\n",
            "Epoch [12554/20000], Training Loss: 0.0837\n",
            "Epoch [12555/20000], Training Loss: 0.0797\n",
            "Epoch [12556/20000], Training Loss: 0.0792\n",
            "Epoch [12557/20000], Training Loss: 0.0843\n",
            "Epoch [12558/20000], Training Loss: 0.0789\n",
            "Epoch [12559/20000], Training Loss: 0.0729\n",
            "Epoch [12560/20000], Training Loss: 0.0848\n",
            "Epoch [12561/20000], Training Loss: 0.0815\n",
            "Epoch [12562/20000], Training Loss: 0.0809\n",
            "Epoch [12563/20000], Training Loss: 0.0848\n",
            "Epoch [12564/20000], Training Loss: 0.0788\n",
            "Epoch [12565/20000], Training Loss: 0.0747\n",
            "Epoch [12566/20000], Training Loss: 0.0771\n",
            "Epoch [12567/20000], Training Loss: 0.0764\n",
            "Epoch [12568/20000], Training Loss: 0.0781\n",
            "Epoch [12569/20000], Training Loss: 0.0733\n",
            "Epoch [12570/20000], Training Loss: 0.0812\n",
            "Epoch [12571/20000], Training Loss: 0.0862\n",
            "Epoch [12572/20000], Training Loss: 0.0823\n",
            "Epoch [12573/20000], Training Loss: 0.0849\n",
            "Epoch [12574/20000], Training Loss: 0.0857\n",
            "Epoch [12575/20000], Training Loss: 0.0749\n",
            "Epoch [12576/20000], Training Loss: 0.0793\n",
            "Epoch [12577/20000], Training Loss: 0.0795\n",
            "Epoch [12578/20000], Training Loss: 0.0812\n",
            "Epoch [12579/20000], Training Loss: 0.0849\n",
            "Epoch [12580/20000], Training Loss: 0.0755\n",
            "Epoch [12581/20000], Training Loss: 0.0795\n",
            "Epoch [12582/20000], Training Loss: 0.0788\n",
            "Epoch [12583/20000], Training Loss: 0.0816\n",
            "Epoch [12584/20000], Training Loss: 0.0787\n",
            "Epoch [12585/20000], Training Loss: 0.0774\n",
            "Epoch [12586/20000], Training Loss: 0.0768\n",
            "Epoch [12587/20000], Training Loss: 0.0843\n",
            "Epoch [12588/20000], Training Loss: 0.0835\n",
            "Epoch [12589/20000], Training Loss: 0.0791\n",
            "Epoch [12590/20000], Training Loss: 0.0867\n",
            "Epoch [12591/20000], Training Loss: 0.0775\n",
            "Epoch [12592/20000], Training Loss: 0.0837\n",
            "Epoch [12593/20000], Training Loss: 0.0768\n",
            "Epoch [12594/20000], Training Loss: 0.0754\n",
            "Epoch [12595/20000], Training Loss: 0.0793\n",
            "Epoch [12596/20000], Training Loss: 0.0845\n",
            "Epoch [12597/20000], Training Loss: 0.0780\n",
            "Epoch [12598/20000], Training Loss: 0.0791\n",
            "Epoch [12599/20000], Training Loss: 0.0795\n",
            "Epoch [12600/20000], Training Loss: 0.0796\n",
            "Epoch [12601/20000], Training Loss: 0.0832\n",
            "Epoch [12602/20000], Training Loss: 0.0728\n",
            "Epoch [12603/20000], Training Loss: 0.0796\n",
            "Epoch [12604/20000], Training Loss: 0.0777\n",
            "Epoch [12605/20000], Training Loss: 0.0871\n",
            "Epoch [12606/20000], Training Loss: 0.0789\n",
            "Epoch [12607/20000], Training Loss: 0.0728\n",
            "Epoch [12608/20000], Training Loss: 0.0848\n",
            "Epoch [12609/20000], Training Loss: 0.0779\n",
            "Epoch [12610/20000], Training Loss: 0.0803\n",
            "Epoch [12611/20000], Training Loss: 0.0772\n",
            "Epoch [12612/20000], Training Loss: 0.0834\n",
            "Epoch [12613/20000], Training Loss: 0.0742\n",
            "Epoch [12614/20000], Training Loss: 0.0739\n",
            "Epoch [12615/20000], Training Loss: 0.0797\n",
            "Epoch [12616/20000], Training Loss: 0.0802\n",
            "Epoch [12617/20000], Training Loss: 0.0839\n",
            "Epoch [12618/20000], Training Loss: 0.0791\n",
            "Epoch [12619/20000], Training Loss: 0.0795\n",
            "Epoch [12620/20000], Training Loss: 0.0834\n",
            "Epoch [12621/20000], Training Loss: 0.0758\n",
            "Epoch [12622/20000], Training Loss: 0.0797\n",
            "Epoch [12623/20000], Training Loss: 0.0810\n",
            "Epoch [12624/20000], Training Loss: 0.0858\n",
            "Epoch [12625/20000], Training Loss: 0.0783\n",
            "Epoch [12626/20000], Training Loss: 0.0762\n",
            "Epoch [12627/20000], Training Loss: 0.0749\n",
            "Epoch [12628/20000], Training Loss: 0.0802\n",
            "Epoch [12629/20000], Training Loss: 0.0821\n",
            "Epoch [12630/20000], Training Loss: 0.0835\n",
            "Epoch [12631/20000], Training Loss: 0.0790\n",
            "Epoch [12632/20000], Training Loss: 0.0786\n",
            "Epoch [12633/20000], Training Loss: 0.0816\n",
            "Epoch [12634/20000], Training Loss: 0.0789\n",
            "Epoch [12635/20000], Training Loss: 0.0853\n",
            "Epoch [12636/20000], Training Loss: 0.0745\n",
            "Epoch [12637/20000], Training Loss: 0.0854\n",
            "Epoch [12638/20000], Training Loss: 0.0744\n",
            "Epoch [12639/20000], Training Loss: 0.0832\n",
            "Epoch [12640/20000], Training Loss: 0.0757\n",
            "Epoch [12641/20000], Training Loss: 0.0833\n",
            "Epoch [12642/20000], Training Loss: 0.0765\n",
            "Epoch [12643/20000], Training Loss: 0.0798\n",
            "Epoch [12644/20000], Training Loss: 0.0813\n",
            "Epoch [12645/20000], Training Loss: 0.0828\n",
            "Epoch [12646/20000], Training Loss: 0.0788\n",
            "Epoch [12647/20000], Training Loss: 0.0739\n",
            "Epoch [12648/20000], Training Loss: 0.0764\n",
            "Epoch [12649/20000], Training Loss: 0.0794\n",
            "Epoch [12650/20000], Training Loss: 0.0806\n",
            "Epoch [12651/20000], Training Loss: 0.0728\n",
            "Epoch [12652/20000], Training Loss: 0.0763\n",
            "Epoch [12653/20000], Training Loss: 0.0747\n",
            "Epoch [12654/20000], Training Loss: 0.0863\n",
            "Epoch [12655/20000], Training Loss: 0.0765\n",
            "Epoch [12656/20000], Training Loss: 0.0760\n",
            "Epoch [12657/20000], Training Loss: 0.0760\n",
            "Epoch [12658/20000], Training Loss: 0.0806\n",
            "Epoch [12659/20000], Training Loss: 0.0730\n",
            "Epoch [12660/20000], Training Loss: 0.0796\n",
            "Epoch [12661/20000], Training Loss: 0.0816\n",
            "Epoch [12662/20000], Training Loss: 0.0808\n",
            "Epoch [12663/20000], Training Loss: 0.0775\n",
            "Epoch [12664/20000], Training Loss: 0.0786\n",
            "Epoch [12665/20000], Training Loss: 0.0837\n",
            "Epoch [12666/20000], Training Loss: 0.0852\n",
            "Epoch [12667/20000], Training Loss: 0.0740\n",
            "Epoch [12668/20000], Training Loss: 0.0757\n",
            "Epoch [12669/20000], Training Loss: 0.0787\n",
            "Epoch [12670/20000], Training Loss: 0.0787\n",
            "Epoch [12671/20000], Training Loss: 0.0804\n",
            "Epoch [12672/20000], Training Loss: 0.0792\n",
            "Epoch [12673/20000], Training Loss: 0.0846\n",
            "Epoch [12674/20000], Training Loss: 0.0818\n",
            "Epoch [12675/20000], Training Loss: 0.0803\n",
            "Epoch [12676/20000], Training Loss: 0.0758\n",
            "Epoch [12677/20000], Training Loss: 0.0781\n",
            "Epoch [12678/20000], Training Loss: 0.0766\n",
            "Epoch [12679/20000], Training Loss: 0.0832\n",
            "Epoch [12680/20000], Training Loss: 0.0749\n",
            "Epoch [12681/20000], Training Loss: 0.0827\n",
            "Epoch [12682/20000], Training Loss: 0.0737\n",
            "Epoch [12683/20000], Training Loss: 0.0812\n",
            "Epoch [12684/20000], Training Loss: 0.0782\n",
            "Epoch [12685/20000], Training Loss: 0.0832\n",
            "Epoch [12686/20000], Training Loss: 0.0770\n",
            "Epoch [12687/20000], Training Loss: 0.0757\n",
            "Epoch [12688/20000], Training Loss: 0.0760\n",
            "Epoch [12689/20000], Training Loss: 0.0798\n",
            "Epoch [12690/20000], Training Loss: 0.0729\n",
            "Epoch [12691/20000], Training Loss: 0.0807\n",
            "Epoch [12692/20000], Training Loss: 0.0802\n",
            "Epoch [12693/20000], Training Loss: 0.0763\n",
            "Epoch [12694/20000], Training Loss: 0.0869\n",
            "Epoch [12695/20000], Training Loss: 0.0812\n",
            "Epoch [12696/20000], Training Loss: 0.0801\n",
            "Epoch [12697/20000], Training Loss: 0.0805\n",
            "Epoch [12698/20000], Training Loss: 0.0801\n",
            "Epoch [12699/20000], Training Loss: 0.0778\n",
            "Epoch [12700/20000], Training Loss: 0.0818\n",
            "Epoch [12701/20000], Training Loss: 0.0802\n",
            "Epoch [12702/20000], Training Loss: 0.0812\n",
            "Epoch [12703/20000], Training Loss: 0.0882\n",
            "Epoch [12704/20000], Training Loss: 0.0812\n",
            "Epoch [12705/20000], Training Loss: 0.0811\n",
            "Epoch [12706/20000], Training Loss: 0.0836\n",
            "Epoch [12707/20000], Training Loss: 0.0733\n",
            "Epoch [12708/20000], Training Loss: 0.0836\n",
            "Epoch [12709/20000], Training Loss: 0.0820\n",
            "Epoch [12710/20000], Training Loss: 0.0795\n",
            "Epoch [12711/20000], Training Loss: 0.0765\n",
            "Epoch [12712/20000], Training Loss: 0.0822\n",
            "Epoch [12713/20000], Training Loss: 0.0736\n",
            "Epoch [12714/20000], Training Loss: 0.0790\n",
            "Epoch [12715/20000], Training Loss: 0.0798\n",
            "Epoch [12716/20000], Training Loss: 0.0773\n",
            "Epoch [12717/20000], Training Loss: 0.0831\n",
            "Epoch [12718/20000], Training Loss: 0.0723\n",
            "Epoch [12719/20000], Training Loss: 0.0796\n",
            "Epoch [12720/20000], Training Loss: 0.0757\n",
            "Epoch [12721/20000], Training Loss: 0.0864\n",
            "Epoch [12722/20000], Training Loss: 0.0788\n",
            "Epoch [12723/20000], Training Loss: 0.0761\n",
            "Epoch [12724/20000], Training Loss: 0.0809\n",
            "Epoch [12725/20000], Training Loss: 0.0776\n",
            "Epoch [12726/20000], Training Loss: 0.0825\n",
            "Epoch [12727/20000], Training Loss: 0.0812\n",
            "Epoch [12728/20000], Training Loss: 0.0791\n",
            "Epoch [12729/20000], Training Loss: 0.0749\n",
            "Epoch [12730/20000], Training Loss: 0.0763\n",
            "Epoch [12731/20000], Training Loss: 0.0788\n",
            "Epoch [12732/20000], Training Loss: 0.0821\n",
            "Epoch [12733/20000], Training Loss: 0.0778\n",
            "Epoch [12734/20000], Training Loss: 0.0799\n",
            "Epoch [12735/20000], Training Loss: 0.0800\n",
            "Epoch [12736/20000], Training Loss: 0.0781\n",
            "Epoch [12737/20000], Training Loss: 0.0789\n",
            "Epoch [12738/20000], Training Loss: 0.0800\n",
            "Epoch [12739/20000], Training Loss: 0.0867\n",
            "Epoch [12740/20000], Training Loss: 0.0852\n",
            "Epoch [12741/20000], Training Loss: 0.0745\n",
            "Epoch [12742/20000], Training Loss: 0.0872\n",
            "Epoch [12743/20000], Training Loss: 0.0750\n",
            "Epoch [12744/20000], Training Loss: 0.0802\n",
            "Epoch [12745/20000], Training Loss: 0.0784\n",
            "Epoch [12746/20000], Training Loss: 0.0779\n",
            "Epoch [12747/20000], Training Loss: 0.0829\n",
            "Epoch [12748/20000], Training Loss: 0.0794\n",
            "Epoch [12749/20000], Training Loss: 0.0804\n",
            "Epoch [12750/20000], Training Loss: 0.0794\n",
            "Epoch [12751/20000], Training Loss: 0.0869\n",
            "Epoch [12752/20000], Training Loss: 0.0735\n",
            "Epoch [12753/20000], Training Loss: 0.0863\n",
            "Epoch [12754/20000], Training Loss: 0.0809\n",
            "Epoch [12755/20000], Training Loss: 0.0821\n",
            "Epoch [12756/20000], Training Loss: 0.0763\n",
            "Epoch [12757/20000], Training Loss: 0.0740\n",
            "Epoch [12758/20000], Training Loss: 0.0815\n",
            "Epoch [12759/20000], Training Loss: 0.0790\n",
            "Epoch [12760/20000], Training Loss: 0.0790\n",
            "Epoch [12761/20000], Training Loss: 0.0846\n",
            "Epoch [12762/20000], Training Loss: 0.0809\n",
            "Epoch [12763/20000], Training Loss: 0.0808\n",
            "Epoch [12764/20000], Training Loss: 0.0786\n",
            "Epoch [12765/20000], Training Loss: 0.0760\n",
            "Epoch [12766/20000], Training Loss: 0.0751\n",
            "Epoch [12767/20000], Training Loss: 0.0822\n",
            "Epoch [12768/20000], Training Loss: 0.0790\n",
            "Epoch [12769/20000], Training Loss: 0.0825\n",
            "Epoch [12770/20000], Training Loss: 0.0763\n",
            "Epoch [12771/20000], Training Loss: 0.0855\n",
            "Epoch [12772/20000], Training Loss: 0.0751\n",
            "Epoch [12773/20000], Training Loss: 0.0781\n",
            "Epoch [12774/20000], Training Loss: 0.0814\n",
            "Epoch [12775/20000], Training Loss: 0.0843\n",
            "Epoch [12776/20000], Training Loss: 0.0810\n",
            "Epoch [12777/20000], Training Loss: 0.0751\n",
            "Epoch [12778/20000], Training Loss: 0.0816\n",
            "Epoch [12779/20000], Training Loss: 0.0793\n",
            "Epoch [12780/20000], Training Loss: 0.0794\n",
            "Epoch [12781/20000], Training Loss: 0.0783\n",
            "Epoch [12782/20000], Training Loss: 0.0767\n",
            "Epoch [12783/20000], Training Loss: 0.0771\n",
            "Epoch [12784/20000], Training Loss: 0.0818\n",
            "Epoch [12785/20000], Training Loss: 0.0823\n",
            "Epoch [12786/20000], Training Loss: 0.0811\n",
            "Epoch [12787/20000], Training Loss: 0.0801\n",
            "Epoch [12788/20000], Training Loss: 0.0761\n",
            "Epoch [12789/20000], Training Loss: 0.0774\n",
            "Epoch [12790/20000], Training Loss: 0.0780\n",
            "Epoch [12791/20000], Training Loss: 0.0786\n",
            "Epoch [12792/20000], Training Loss: 0.0842\n",
            "Epoch [12793/20000], Training Loss: 0.0786\n",
            "Epoch [12794/20000], Training Loss: 0.0857\n",
            "Epoch [12795/20000], Training Loss: 0.0750\n",
            "Epoch [12796/20000], Training Loss: 0.0739\n",
            "Epoch [12797/20000], Training Loss: 0.0752\n",
            "Epoch [12798/20000], Training Loss: 0.0838\n",
            "Epoch [12799/20000], Training Loss: 0.0792\n",
            "Epoch [12800/20000], Training Loss: 0.0798\n",
            "Epoch [12801/20000], Training Loss: 0.0807\n",
            "Epoch [12802/20000], Training Loss: 0.0758\n",
            "Epoch [12803/20000], Training Loss: 0.0827\n",
            "Epoch [12804/20000], Training Loss: 0.0866\n",
            "Epoch [12805/20000], Training Loss: 0.0792\n",
            "Epoch [12806/20000], Training Loss: 0.0770\n",
            "Epoch [12807/20000], Training Loss: 0.0802\n",
            "Epoch [12808/20000], Training Loss: 0.0835\n",
            "Epoch [12809/20000], Training Loss: 0.0771\n",
            "Epoch [12810/20000], Training Loss: 0.0744\n",
            "Epoch [12811/20000], Training Loss: 0.0811\n",
            "Epoch [12812/20000], Training Loss: 0.0738\n",
            "Epoch [12813/20000], Training Loss: 0.0805\n",
            "Epoch [12814/20000], Training Loss: 0.0797\n",
            "Epoch [12815/20000], Training Loss: 0.0798\n",
            "Epoch [12816/20000], Training Loss: 0.0762\n",
            "Epoch [12817/20000], Training Loss: 0.0863\n",
            "Epoch [12818/20000], Training Loss: 0.0826\n",
            "Epoch [12819/20000], Training Loss: 0.0799\n",
            "Epoch [12820/20000], Training Loss: 0.0769\n",
            "Epoch [12821/20000], Training Loss: 0.0801\n",
            "Epoch [12822/20000], Training Loss: 0.0790\n",
            "Epoch [12823/20000], Training Loss: 0.0807\n",
            "Epoch [12824/20000], Training Loss: 0.0786\n",
            "Epoch [12825/20000], Training Loss: 0.0769\n",
            "Epoch [12826/20000], Training Loss: 0.0760\n",
            "Epoch [12827/20000], Training Loss: 0.0798\n",
            "Epoch [12828/20000], Training Loss: 0.0808\n",
            "Epoch [12829/20000], Training Loss: 0.0769\n",
            "Epoch [12830/20000], Training Loss: 0.0752\n",
            "Epoch [12831/20000], Training Loss: 0.0785\n",
            "Epoch [12832/20000], Training Loss: 0.0852\n",
            "Epoch [12833/20000], Training Loss: 0.0794\n",
            "Epoch [12834/20000], Training Loss: 0.0816\n",
            "Epoch [12835/20000], Training Loss: 0.0758\n",
            "Epoch [12836/20000], Training Loss: 0.0849\n",
            "Epoch [12837/20000], Training Loss: 0.0745\n",
            "Epoch [12838/20000], Training Loss: 0.0794\n",
            "Epoch [12839/20000], Training Loss: 0.0812\n",
            "Epoch [12840/20000], Training Loss: 0.0783\n",
            "Epoch [12841/20000], Training Loss: 0.0789\n",
            "Epoch [12842/20000], Training Loss: 0.0799\n",
            "Epoch [12843/20000], Training Loss: 0.0817\n",
            "Epoch [12844/20000], Training Loss: 0.0859\n",
            "Epoch [12845/20000], Training Loss: 0.0812\n",
            "Epoch [12846/20000], Training Loss: 0.0843\n",
            "Epoch [12847/20000], Training Loss: 0.0807\n",
            "Epoch [12848/20000], Training Loss: 0.0754\n",
            "Epoch [12849/20000], Training Loss: 0.0785\n",
            "Epoch [12850/20000], Training Loss: 0.0815\n",
            "Epoch [12851/20000], Training Loss: 0.0792\n",
            "Epoch [12852/20000], Training Loss: 0.0829\n",
            "Epoch [12853/20000], Training Loss: 0.0833\n",
            "Epoch [12854/20000], Training Loss: 0.0804\n",
            "Epoch [12855/20000], Training Loss: 0.0818\n",
            "Epoch [12856/20000], Training Loss: 0.0762\n",
            "Epoch [12857/20000], Training Loss: 0.0772\n",
            "Epoch [12858/20000], Training Loss: 0.0843\n",
            "Epoch [12859/20000], Training Loss: 0.0797\n",
            "Epoch [12860/20000], Training Loss: 0.0772\n",
            "Epoch [12861/20000], Training Loss: 0.0826\n",
            "Epoch [12862/20000], Training Loss: 0.0806\n",
            "Epoch [12863/20000], Training Loss: 0.0813\n",
            "Epoch [12864/20000], Training Loss: 0.0808\n",
            "Epoch [12865/20000], Training Loss: 0.0771\n",
            "Epoch [12866/20000], Training Loss: 0.0803\n",
            "Epoch [12867/20000], Training Loss: 0.0752\n",
            "Epoch [12868/20000], Training Loss: 0.0868\n",
            "Epoch [12869/20000], Training Loss: 0.0842\n",
            "Epoch [12870/20000], Training Loss: 0.0821\n",
            "Epoch [12871/20000], Training Loss: 0.0787\n",
            "Epoch [12872/20000], Training Loss: 0.0801\n",
            "Epoch [12873/20000], Training Loss: 0.0836\n",
            "Epoch [12874/20000], Training Loss: 0.0798\n",
            "Epoch [12875/20000], Training Loss: 0.0751\n",
            "Epoch [12876/20000], Training Loss: 0.0761\n",
            "Epoch [12877/20000], Training Loss: 0.0792\n",
            "Epoch [12878/20000], Training Loss: 0.0815\n",
            "Epoch [12879/20000], Training Loss: 0.0800\n",
            "Epoch [12880/20000], Training Loss: 0.0746\n",
            "Epoch [12881/20000], Training Loss: 0.0764\n",
            "Epoch [12882/20000], Training Loss: 0.0816\n",
            "Epoch [12883/20000], Training Loss: 0.0802\n",
            "Epoch [12884/20000], Training Loss: 0.0747\n",
            "Epoch [12885/20000], Training Loss: 0.0855\n",
            "Epoch [12886/20000], Training Loss: 0.0752\n",
            "Epoch [12887/20000], Training Loss: 0.0754\n",
            "Epoch [12888/20000], Training Loss: 0.0806\n",
            "Epoch [12889/20000], Training Loss: 0.0778\n",
            "Epoch [12890/20000], Training Loss: 0.0785\n",
            "Epoch [12891/20000], Training Loss: 0.0825\n",
            "Epoch [12892/20000], Training Loss: 0.0772\n",
            "Epoch [12893/20000], Training Loss: 0.0782\n",
            "Epoch [12894/20000], Training Loss: 0.0798\n",
            "Epoch [12895/20000], Training Loss: 0.0848\n",
            "Epoch [12896/20000], Training Loss: 0.0770\n",
            "Epoch [12897/20000], Training Loss: 0.0731\n",
            "Epoch [12898/20000], Training Loss: 0.0853\n",
            "Epoch [12899/20000], Training Loss: 0.0793\n",
            "Epoch [12900/20000], Training Loss: 0.0795\n",
            "Epoch [12901/20000], Training Loss: 0.0798\n",
            "Epoch [12902/20000], Training Loss: 0.0748\n",
            "Epoch [12903/20000], Training Loss: 0.0882\n",
            "Epoch [12904/20000], Training Loss: 0.0796\n",
            "Epoch [12905/20000], Training Loss: 0.0826\n",
            "Epoch [12906/20000], Training Loss: 0.0774\n",
            "Epoch [12907/20000], Training Loss: 0.0771\n",
            "Epoch [12908/20000], Training Loss: 0.0749\n",
            "Epoch [12909/20000], Training Loss: 0.0810\n",
            "Epoch [12910/20000], Training Loss: 0.0794\n",
            "Epoch [12911/20000], Training Loss: 0.0799\n",
            "Epoch [12912/20000], Training Loss: 0.0807\n",
            "Epoch [12913/20000], Training Loss: 0.0820\n",
            "Epoch [12914/20000], Training Loss: 0.0774\n",
            "Epoch [12915/20000], Training Loss: 0.0861\n",
            "Epoch [12916/20000], Training Loss: 0.0851\n",
            "Epoch [12917/20000], Training Loss: 0.0782\n",
            "Epoch [12918/20000], Training Loss: 0.0780\n",
            "Epoch [12919/20000], Training Loss: 0.0826\n",
            "Epoch [12920/20000], Training Loss: 0.0817\n",
            "Epoch [12921/20000], Training Loss: 0.0795\n",
            "Epoch [12922/20000], Training Loss: 0.0786\n",
            "Epoch [12923/20000], Training Loss: 0.0762\n",
            "Epoch [12924/20000], Training Loss: 0.0780\n",
            "Epoch [12925/20000], Training Loss: 0.0761\n",
            "Epoch [12926/20000], Training Loss: 0.0822\n",
            "Epoch [12927/20000], Training Loss: 0.0817\n",
            "Epoch [12928/20000], Training Loss: 0.0817\n",
            "Epoch [12929/20000], Training Loss: 0.0821\n",
            "Epoch [12930/20000], Training Loss: 0.0747\n",
            "Epoch [12931/20000], Training Loss: 0.0775\n",
            "Epoch [12932/20000], Training Loss: 0.0752\n",
            "Epoch [12933/20000], Training Loss: 0.0842\n",
            "Epoch [12934/20000], Training Loss: 0.0788\n",
            "Epoch [12935/20000], Training Loss: 0.0747\n",
            "Epoch [12936/20000], Training Loss: 0.0802\n",
            "Epoch [12937/20000], Training Loss: 0.0778\n",
            "Epoch [12938/20000], Training Loss: 0.0774\n",
            "Epoch [12939/20000], Training Loss: 0.0797\n",
            "Epoch [12940/20000], Training Loss: 0.0810\n",
            "Epoch [12941/20000], Training Loss: 0.0767\n",
            "Epoch [12942/20000], Training Loss: 0.0769\n",
            "Epoch [12943/20000], Training Loss: 0.0751\n",
            "Epoch [12944/20000], Training Loss: 0.0818\n",
            "Epoch [12945/20000], Training Loss: 0.0747\n",
            "Epoch [12946/20000], Training Loss: 0.0831\n",
            "Epoch [12947/20000], Training Loss: 0.0816\n",
            "Epoch [12948/20000], Training Loss: 0.0856\n",
            "Epoch [12949/20000], Training Loss: 0.0797\n",
            "Epoch [12950/20000], Training Loss: 0.0871\n",
            "Epoch [12951/20000], Training Loss: 0.0746\n",
            "Epoch [12952/20000], Training Loss: 0.0791\n",
            "Epoch [12953/20000], Training Loss: 0.0836\n",
            "Epoch [12954/20000], Training Loss: 0.0804\n",
            "Epoch [12955/20000], Training Loss: 0.0872\n",
            "Epoch [12956/20000], Training Loss: 0.0778\n",
            "Epoch [12957/20000], Training Loss: 0.0801\n",
            "Epoch [12958/20000], Training Loss: 0.0799\n",
            "Epoch [12959/20000], Training Loss: 0.0816\n",
            "Epoch [12960/20000], Training Loss: 0.0735\n",
            "Epoch [12961/20000], Training Loss: 0.0803\n",
            "Epoch [12962/20000], Training Loss: 0.0800\n",
            "Epoch [12963/20000], Training Loss: 0.0845\n",
            "Epoch [12964/20000], Training Loss: 0.0745\n",
            "Epoch [12965/20000], Training Loss: 0.0759\n",
            "Epoch [12966/20000], Training Loss: 0.0810\n",
            "Epoch [12967/20000], Training Loss: 0.0767\n",
            "Epoch [12968/20000], Training Loss: 0.0781\n",
            "Epoch [12969/20000], Training Loss: 0.0753\n",
            "Epoch [12970/20000], Training Loss: 0.0800\n",
            "Epoch [12971/20000], Training Loss: 0.0776\n",
            "Epoch [12972/20000], Training Loss: 0.0773\n",
            "Epoch [12973/20000], Training Loss: 0.0745\n",
            "Epoch [12974/20000], Training Loss: 0.0766\n",
            "Epoch [12975/20000], Training Loss: 0.0740\n",
            "Epoch [12976/20000], Training Loss: 0.0781\n",
            "Epoch [12977/20000], Training Loss: 0.0803\n",
            "Epoch [12978/20000], Training Loss: 0.0799\n",
            "Epoch [12979/20000], Training Loss: 0.0841\n",
            "Epoch [12980/20000], Training Loss: 0.0759\n",
            "Epoch [12981/20000], Training Loss: 0.0765\n",
            "Epoch [12982/20000], Training Loss: 0.0791\n",
            "Epoch [12983/20000], Training Loss: 0.0757\n",
            "Epoch [12984/20000], Training Loss: 0.0753\n",
            "Epoch [12985/20000], Training Loss: 0.0770\n",
            "Epoch [12986/20000], Training Loss: 0.0779\n",
            "Epoch [12987/20000], Training Loss: 0.0777\n",
            "Epoch [12988/20000], Training Loss: 0.0784\n",
            "Epoch [12989/20000], Training Loss: 0.0787\n",
            "Epoch [12990/20000], Training Loss: 0.0798\n",
            "Epoch [12991/20000], Training Loss: 0.0847\n",
            "Epoch [12992/20000], Training Loss: 0.0811\n",
            "Epoch [12993/20000], Training Loss: 0.0744\n",
            "Epoch [12994/20000], Training Loss: 0.0826\n",
            "Epoch [12995/20000], Training Loss: 0.0808\n",
            "Epoch [12996/20000], Training Loss: 0.0845\n",
            "Epoch [12997/20000], Training Loss: 0.0791\n",
            "Epoch [12998/20000], Training Loss: 0.0794\n",
            "Epoch [12999/20000], Training Loss: 0.0802\n",
            "Epoch [13000/20000], Training Loss: 0.0781\n",
            "Epoch [13001/20000], Training Loss: 0.0802\n",
            "Epoch [13002/20000], Training Loss: 0.0739\n",
            "Epoch [13003/20000], Training Loss: 0.0782\n",
            "Epoch [13004/20000], Training Loss: 0.0824\n",
            "Epoch [13005/20000], Training Loss: 0.0800\n",
            "Epoch [13006/20000], Training Loss: 0.0834\n",
            "Epoch [13007/20000], Training Loss: 0.0848\n",
            "Epoch [13008/20000], Training Loss: 0.0787\n",
            "Epoch [13009/20000], Training Loss: 0.0748\n",
            "Epoch [13010/20000], Training Loss: 0.0809\n",
            "Epoch [13011/20000], Training Loss: 0.0810\n",
            "Epoch [13012/20000], Training Loss: 0.0759\n",
            "Epoch [13013/20000], Training Loss: 0.0837\n",
            "Epoch [13014/20000], Training Loss: 0.0859\n",
            "Epoch [13015/20000], Training Loss: 0.0852\n",
            "Epoch [13016/20000], Training Loss: 0.0826\n",
            "Epoch [13017/20000], Training Loss: 0.0855\n",
            "Epoch [13018/20000], Training Loss: 0.0821\n",
            "Epoch [13019/20000], Training Loss: 0.0811\n",
            "Epoch [13020/20000], Training Loss: 0.0753\n",
            "Epoch [13021/20000], Training Loss: 0.0842\n",
            "Epoch [13022/20000], Training Loss: 0.0776\n",
            "Epoch [13023/20000], Training Loss: 0.0815\n",
            "Epoch [13024/20000], Training Loss: 0.0785\n",
            "Epoch [13025/20000], Training Loss: 0.0815\n",
            "Epoch [13026/20000], Training Loss: 0.0747\n",
            "Epoch [13027/20000], Training Loss: 0.0806\n",
            "Epoch [13028/20000], Training Loss: 0.0795\n",
            "Epoch [13029/20000], Training Loss: 0.0817\n",
            "Epoch [13030/20000], Training Loss: 0.0749\n",
            "Epoch [13031/20000], Training Loss: 0.0779\n",
            "Epoch [13032/20000], Training Loss: 0.0771\n",
            "Epoch [13033/20000], Training Loss: 0.0823\n",
            "Epoch [13034/20000], Training Loss: 0.0794\n",
            "Epoch [13035/20000], Training Loss: 0.0822\n",
            "Epoch [13036/20000], Training Loss: 0.0792\n",
            "Epoch [13037/20000], Training Loss: 0.0772\n",
            "Epoch [13038/20000], Training Loss: 0.0873\n",
            "Epoch [13039/20000], Training Loss: 0.0723\n",
            "Epoch [13040/20000], Training Loss: 0.0784\n",
            "Epoch [13041/20000], Training Loss: 0.0789\n",
            "Epoch [13042/20000], Training Loss: 0.0798\n",
            "Epoch [13043/20000], Training Loss: 0.0785\n",
            "Epoch [13044/20000], Training Loss: 0.0848\n",
            "Epoch [13045/20000], Training Loss: 0.0796\n",
            "Epoch [13046/20000], Training Loss: 0.0808\n",
            "Epoch [13047/20000], Training Loss: 0.0775\n",
            "Epoch [13048/20000], Training Loss: 0.0788\n",
            "Epoch [13049/20000], Training Loss: 0.0773\n",
            "Epoch [13050/20000], Training Loss: 0.0786\n",
            "Epoch [13051/20000], Training Loss: 0.0748\n",
            "Epoch [13052/20000], Training Loss: 0.0816\n",
            "Epoch [13053/20000], Training Loss: 0.0781\n",
            "Epoch [13054/20000], Training Loss: 0.0809\n",
            "Epoch [13055/20000], Training Loss: 0.0807\n",
            "Epoch [13056/20000], Training Loss: 0.0756\n",
            "Epoch [13057/20000], Training Loss: 0.0781\n",
            "Epoch [13058/20000], Training Loss: 0.0855\n",
            "Epoch [13059/20000], Training Loss: 0.0834\n",
            "Epoch [13060/20000], Training Loss: 0.0803\n",
            "Epoch [13061/20000], Training Loss: 0.0776\n",
            "Epoch [13062/20000], Training Loss: 0.0795\n",
            "Epoch [13063/20000], Training Loss: 0.0808\n",
            "Epoch [13064/20000], Training Loss: 0.0746\n",
            "Epoch [13065/20000], Training Loss: 0.0788\n",
            "Epoch [13066/20000], Training Loss: 0.0792\n",
            "Epoch [13067/20000], Training Loss: 0.0803\n",
            "Epoch [13068/20000], Training Loss: 0.0811\n",
            "Epoch [13069/20000], Training Loss: 0.0822\n",
            "Epoch [13070/20000], Training Loss: 0.0793\n",
            "Epoch [13071/20000], Training Loss: 0.0785\n",
            "Epoch [13072/20000], Training Loss: 0.0789\n",
            "Epoch [13073/20000], Training Loss: 0.0811\n",
            "Epoch [13074/20000], Training Loss: 0.0822\n",
            "Epoch [13075/20000], Training Loss: 0.0852\n",
            "Epoch [13076/20000], Training Loss: 0.0748\n",
            "Epoch [13077/20000], Training Loss: 0.0827\n",
            "Epoch [13078/20000], Training Loss: 0.0760\n",
            "Epoch [13079/20000], Training Loss: 0.0827\n",
            "Epoch [13080/20000], Training Loss: 0.0778\n",
            "Epoch [13081/20000], Training Loss: 0.0769\n",
            "Epoch [13082/20000], Training Loss: 0.0801\n",
            "Epoch [13083/20000], Training Loss: 0.0843\n",
            "Epoch [13084/20000], Training Loss: 0.0794\n",
            "Epoch [13085/20000], Training Loss: 0.0781\n",
            "Epoch [13086/20000], Training Loss: 0.0801\n",
            "Epoch [13087/20000], Training Loss: 0.0788\n",
            "Epoch [13088/20000], Training Loss: 0.0788\n",
            "Epoch [13089/20000], Training Loss: 0.0817\n",
            "Epoch [13090/20000], Training Loss: 0.0783\n",
            "Epoch [13091/20000], Training Loss: 0.0805\n",
            "Epoch [13092/20000], Training Loss: 0.0822\n",
            "Epoch [13093/20000], Training Loss: 0.0855\n",
            "Epoch [13094/20000], Training Loss: 0.0760\n",
            "Epoch [13095/20000], Training Loss: 0.0807\n",
            "Epoch [13096/20000], Training Loss: 0.0788\n",
            "Epoch [13097/20000], Training Loss: 0.0822\n",
            "Epoch [13098/20000], Training Loss: 0.0792\n",
            "Epoch [13099/20000], Training Loss: 0.0760\n",
            "Epoch [13100/20000], Training Loss: 0.0782\n",
            "Epoch [13101/20000], Training Loss: 0.0742\n",
            "Epoch [13102/20000], Training Loss: 0.0796\n",
            "Epoch [13103/20000], Training Loss: 0.0779\n",
            "Epoch [13104/20000], Training Loss: 0.0787\n",
            "Epoch [13105/20000], Training Loss: 0.0814\n",
            "Epoch [13106/20000], Training Loss: 0.0843\n",
            "Epoch [13107/20000], Training Loss: 0.0795\n",
            "Epoch [13108/20000], Training Loss: 0.0758\n",
            "Epoch [13109/20000], Training Loss: 0.0757\n",
            "Epoch [13110/20000], Training Loss: 0.0807\n",
            "Epoch [13111/20000], Training Loss: 0.0757\n",
            "Epoch [13112/20000], Training Loss: 0.0827\n",
            "Epoch [13113/20000], Training Loss: 0.0799\n",
            "Epoch [13114/20000], Training Loss: 0.0801\n",
            "Epoch [13115/20000], Training Loss: 0.0795\n",
            "Epoch [13116/20000], Training Loss: 0.0756\n",
            "Epoch [13117/20000], Training Loss: 0.0796\n",
            "Epoch [13118/20000], Training Loss: 0.0787\n",
            "Epoch [13119/20000], Training Loss: 0.0794\n",
            "Epoch [13120/20000], Training Loss: 0.0828\n",
            "Epoch [13121/20000], Training Loss: 0.0775\n",
            "Epoch [13122/20000], Training Loss: 0.0813\n",
            "Epoch [13123/20000], Training Loss: 0.0831\n",
            "Epoch [13124/20000], Training Loss: 0.0775\n",
            "Epoch [13125/20000], Training Loss: 0.0841\n",
            "Epoch [13126/20000], Training Loss: 0.0857\n",
            "Epoch [13127/20000], Training Loss: 0.0803\n",
            "Epoch [13128/20000], Training Loss: 0.0738\n",
            "Epoch [13129/20000], Training Loss: 0.0768\n",
            "Epoch [13130/20000], Training Loss: 0.0745\n",
            "Epoch [13131/20000], Training Loss: 0.0815\n",
            "Epoch [13132/20000], Training Loss: 0.0769\n",
            "Epoch [13133/20000], Training Loss: 0.0736\n",
            "Epoch [13134/20000], Training Loss: 0.0868\n",
            "Epoch [13135/20000], Training Loss: 0.0806\n",
            "Epoch [13136/20000], Training Loss: 0.0871\n",
            "Epoch [13137/20000], Training Loss: 0.0790\n",
            "Epoch [13138/20000], Training Loss: 0.0780\n",
            "Epoch [13139/20000], Training Loss: 0.0800\n",
            "Epoch [13140/20000], Training Loss: 0.0736\n",
            "Epoch [13141/20000], Training Loss: 0.0792\n",
            "Epoch [13142/20000], Training Loss: 0.0790\n",
            "Epoch [13143/20000], Training Loss: 0.0805\n",
            "Epoch [13144/20000], Training Loss: 0.0726\n",
            "Epoch [13145/20000], Training Loss: 0.0786\n",
            "Epoch [13146/20000], Training Loss: 0.0747\n",
            "Epoch [13147/20000], Training Loss: 0.0805\n",
            "Epoch [13148/20000], Training Loss: 0.0862\n",
            "Epoch [13149/20000], Training Loss: 0.0829\n",
            "Epoch [13150/20000], Training Loss: 0.0846\n",
            "Epoch [13151/20000], Training Loss: 0.0856\n",
            "Epoch [13152/20000], Training Loss: 0.0745\n",
            "Epoch [13153/20000], Training Loss: 0.0866\n",
            "Epoch [13154/20000], Training Loss: 0.0808\n",
            "Epoch [13155/20000], Training Loss: 0.0836\n",
            "Epoch [13156/20000], Training Loss: 0.0748\n",
            "Epoch [13157/20000], Training Loss: 0.0743\n",
            "Epoch [13158/20000], Training Loss: 0.0818\n",
            "Epoch [13159/20000], Training Loss: 0.0779\n",
            "Epoch [13160/20000], Training Loss: 0.0779\n",
            "Epoch [13161/20000], Training Loss: 0.0755\n",
            "Epoch [13162/20000], Training Loss: 0.0798\n",
            "Epoch [13163/20000], Training Loss: 0.0813\n",
            "Epoch [13164/20000], Training Loss: 0.0784\n",
            "Epoch [13165/20000], Training Loss: 0.0755\n",
            "Epoch [13166/20000], Training Loss: 0.0767\n",
            "Epoch [13167/20000], Training Loss: 0.0794\n",
            "Epoch [13168/20000], Training Loss: 0.0753\n",
            "Epoch [13169/20000], Training Loss: 0.0766\n",
            "Epoch [13170/20000], Training Loss: 0.0853\n",
            "Epoch [13171/20000], Training Loss: 0.0819\n",
            "Epoch [13172/20000], Training Loss: 0.0788\n",
            "Epoch [13173/20000], Training Loss: 0.0752\n",
            "Epoch [13174/20000], Training Loss: 0.0791\n",
            "Epoch [13175/20000], Training Loss: 0.0813\n",
            "Epoch [13176/20000], Training Loss: 0.0778\n",
            "Epoch [13177/20000], Training Loss: 0.0843\n",
            "Epoch [13178/20000], Training Loss: 0.0809\n",
            "Epoch [13179/20000], Training Loss: 0.0805\n",
            "Epoch [13180/20000], Training Loss: 0.0811\n",
            "Epoch [13181/20000], Training Loss: 0.0802\n",
            "Epoch [13182/20000], Training Loss: 0.0808\n",
            "Epoch [13183/20000], Training Loss: 0.0777\n",
            "Epoch [13184/20000], Training Loss: 0.0815\n",
            "Epoch [13185/20000], Training Loss: 0.0758\n",
            "Epoch [13186/20000], Training Loss: 0.0804\n",
            "Epoch [13187/20000], Training Loss: 0.0819\n",
            "Epoch [13188/20000], Training Loss: 0.0815\n",
            "Epoch [13189/20000], Training Loss: 0.0785\n",
            "Epoch [13190/20000], Training Loss: 0.0729\n",
            "Epoch [13191/20000], Training Loss: 0.0766\n",
            "Epoch [13192/20000], Training Loss: 0.0799\n",
            "Epoch [13193/20000], Training Loss: 0.0796\n",
            "Epoch [13194/20000], Training Loss: 0.0826\n",
            "Epoch [13195/20000], Training Loss: 0.0853\n",
            "Epoch [13196/20000], Training Loss: 0.0783\n",
            "Epoch [13197/20000], Training Loss: 0.0852\n",
            "Epoch [13198/20000], Training Loss: 0.0844\n",
            "Epoch [13199/20000], Training Loss: 0.0791\n",
            "Epoch [13200/20000], Training Loss: 0.0806\n",
            "Epoch [13201/20000], Training Loss: 0.0733\n",
            "Epoch [13202/20000], Training Loss: 0.0816\n",
            "Epoch [13203/20000], Training Loss: 0.0764\n",
            "Epoch [13204/20000], Training Loss: 0.0815\n",
            "Epoch [13205/20000], Training Loss: 0.0797\n",
            "Epoch [13206/20000], Training Loss: 0.0781\n",
            "Epoch [13207/20000], Training Loss: 0.0728\n",
            "Epoch [13208/20000], Training Loss: 0.0841\n",
            "Epoch [13209/20000], Training Loss: 0.0778\n",
            "Epoch [13210/20000], Training Loss: 0.0833\n",
            "Epoch [13211/20000], Training Loss: 0.0876\n",
            "Epoch [13212/20000], Training Loss: 0.0790\n",
            "Epoch [13213/20000], Training Loss: 0.0832\n",
            "Epoch [13214/20000], Training Loss: 0.0737\n",
            "Epoch [13215/20000], Training Loss: 0.0828\n",
            "Epoch [13216/20000], Training Loss: 0.0772\n",
            "Epoch [13217/20000], Training Loss: 0.0809\n",
            "Epoch [13218/20000], Training Loss: 0.0749\n",
            "Epoch [13219/20000], Training Loss: 0.0752\n",
            "Epoch [13220/20000], Training Loss: 0.0791\n",
            "Epoch [13221/20000], Training Loss: 0.0829\n",
            "Epoch [13222/20000], Training Loss: 0.0846\n",
            "Epoch [13223/20000], Training Loss: 0.0839\n",
            "Epoch [13224/20000], Training Loss: 0.0823\n",
            "Epoch [13225/20000], Training Loss: 0.0832\n",
            "Epoch [13226/20000], Training Loss: 0.0727\n",
            "Epoch [13227/20000], Training Loss: 0.0795\n",
            "Epoch [13228/20000], Training Loss: 0.0731\n",
            "Epoch [13229/20000], Training Loss: 0.0782\n",
            "Epoch [13230/20000], Training Loss: 0.0756\n",
            "Epoch [13231/20000], Training Loss: 0.0811\n",
            "Epoch [13232/20000], Training Loss: 0.0760\n",
            "Epoch [13233/20000], Training Loss: 0.0794\n",
            "Epoch [13234/20000], Training Loss: 0.0866\n",
            "Epoch [13235/20000], Training Loss: 0.0773\n",
            "Epoch [13236/20000], Training Loss: 0.0828\n",
            "Epoch [13237/20000], Training Loss: 0.0770\n",
            "Epoch [13238/20000], Training Loss: 0.0804\n",
            "Epoch [13239/20000], Training Loss: 0.0877\n",
            "Epoch [13240/20000], Training Loss: 0.0768\n",
            "Epoch [13241/20000], Training Loss: 0.0746\n",
            "Epoch [13242/20000], Training Loss: 0.0845\n",
            "Epoch [13243/20000], Training Loss: 0.0739\n",
            "Epoch [13244/20000], Training Loss: 0.0816\n",
            "Epoch [13245/20000], Training Loss: 0.0763\n",
            "Epoch [13246/20000], Training Loss: 0.0842\n",
            "Epoch [13247/20000], Training Loss: 0.0848\n",
            "Epoch [13248/20000], Training Loss: 0.0844\n",
            "Epoch [13249/20000], Training Loss: 0.0798\n",
            "Epoch [13250/20000], Training Loss: 0.0858\n",
            "Epoch [13251/20000], Training Loss: 0.0772\n",
            "Epoch [13252/20000], Training Loss: 0.0749\n",
            "Epoch [13253/20000], Training Loss: 0.0825\n",
            "Epoch [13254/20000], Training Loss: 0.0799\n",
            "Epoch [13255/20000], Training Loss: 0.0801\n",
            "Epoch [13256/20000], Training Loss: 0.0824\n",
            "Epoch [13257/20000], Training Loss: 0.0752\n",
            "Epoch [13258/20000], Training Loss: 0.0759\n",
            "Epoch [13259/20000], Training Loss: 0.0794\n",
            "Epoch [13260/20000], Training Loss: 0.0832\n",
            "Epoch [13261/20000], Training Loss: 0.0756\n",
            "Epoch [13262/20000], Training Loss: 0.0743\n",
            "Epoch [13263/20000], Training Loss: 0.0820\n",
            "Epoch [13264/20000], Training Loss: 0.0715\n",
            "Epoch [13265/20000], Training Loss: 0.0811\n",
            "Epoch [13266/20000], Training Loss: 0.0829\n",
            "Epoch [13267/20000], Training Loss: 0.0854\n",
            "Epoch [13268/20000], Training Loss: 0.0793\n",
            "Epoch [13269/20000], Training Loss: 0.0745\n",
            "Epoch [13270/20000], Training Loss: 0.0739\n",
            "Epoch [13271/20000], Training Loss: 0.0813\n",
            "Epoch [13272/20000], Training Loss: 0.0795\n",
            "Epoch [13273/20000], Training Loss: 0.0767\n",
            "Epoch [13274/20000], Training Loss: 0.0809\n",
            "Epoch [13275/20000], Training Loss: 0.0742\n",
            "Epoch [13276/20000], Training Loss: 0.0753\n",
            "Epoch [13277/20000], Training Loss: 0.0749\n",
            "Epoch [13278/20000], Training Loss: 0.0838\n",
            "Epoch [13279/20000], Training Loss: 0.0737\n",
            "Epoch [13280/20000], Training Loss: 0.0808\n",
            "Epoch [13281/20000], Training Loss: 0.0866\n",
            "Epoch [13282/20000], Training Loss: 0.0767\n",
            "Epoch [13283/20000], Training Loss: 0.0834\n",
            "Epoch [13284/20000], Training Loss: 0.0794\n",
            "Epoch [13285/20000], Training Loss: 0.0773\n",
            "Epoch [13286/20000], Training Loss: 0.0744\n",
            "Epoch [13287/20000], Training Loss: 0.0758\n",
            "Epoch [13288/20000], Training Loss: 0.0809\n",
            "Epoch [13289/20000], Training Loss: 0.0729\n",
            "Epoch [13290/20000], Training Loss: 0.0853\n",
            "Epoch [13291/20000], Training Loss: 0.0805\n",
            "Epoch [13292/20000], Training Loss: 0.0728\n",
            "Epoch [13293/20000], Training Loss: 0.0787\n",
            "Epoch [13294/20000], Training Loss: 0.0797\n",
            "Epoch [13295/20000], Training Loss: 0.0803\n",
            "Epoch [13296/20000], Training Loss: 0.0790\n",
            "Epoch [13297/20000], Training Loss: 0.0750\n",
            "Epoch [13298/20000], Training Loss: 0.0861\n",
            "Epoch [13299/20000], Training Loss: 0.0818\n",
            "Epoch [13300/20000], Training Loss: 0.0777\n",
            "Epoch [13301/20000], Training Loss: 0.0784\n",
            "Epoch [13302/20000], Training Loss: 0.0767\n",
            "Epoch [13303/20000], Training Loss: 0.0783\n",
            "Epoch [13304/20000], Training Loss: 0.0736\n",
            "Epoch [13305/20000], Training Loss: 0.0811\n",
            "Epoch [13306/20000], Training Loss: 0.0750\n",
            "Epoch [13307/20000], Training Loss: 0.0760\n",
            "Epoch [13308/20000], Training Loss: 0.0726\n",
            "Epoch [13309/20000], Training Loss: 0.0758\n",
            "Epoch [13310/20000], Training Loss: 0.0780\n",
            "Epoch [13311/20000], Training Loss: 0.0789\n",
            "Epoch [13312/20000], Training Loss: 0.0835\n",
            "Epoch [13313/20000], Training Loss: 0.0737\n",
            "Epoch [13314/20000], Training Loss: 0.0787\n",
            "Epoch [13315/20000], Training Loss: 0.0782\n",
            "Epoch [13316/20000], Training Loss: 0.0778\n",
            "Epoch [13317/20000], Training Loss: 0.0736\n",
            "Epoch [13318/20000], Training Loss: 0.0808\n",
            "Epoch [13319/20000], Training Loss: 0.0803\n",
            "Epoch [13320/20000], Training Loss: 0.0768\n",
            "Epoch [13321/20000], Training Loss: 0.0804\n",
            "Epoch [13322/20000], Training Loss: 0.0751\n",
            "Epoch [13323/20000], Training Loss: 0.0751\n",
            "Epoch [13324/20000], Training Loss: 0.0783\n",
            "Epoch [13325/20000], Training Loss: 0.0843\n",
            "Epoch [13326/20000], Training Loss: 0.0739\n",
            "Epoch [13327/20000], Training Loss: 0.0841\n",
            "Epoch [13328/20000], Training Loss: 0.0792\n",
            "Epoch [13329/20000], Training Loss: 0.0836\n",
            "Epoch [13330/20000], Training Loss: 0.0815\n",
            "Epoch [13331/20000], Training Loss: 0.0855\n",
            "Epoch [13332/20000], Training Loss: 0.0769\n",
            "Epoch [13333/20000], Training Loss: 0.0843\n",
            "Epoch [13334/20000], Training Loss: 0.0726\n",
            "Epoch [13335/20000], Training Loss: 0.0807\n",
            "Epoch [13336/20000], Training Loss: 0.0819\n",
            "Epoch [13337/20000], Training Loss: 0.0785\n",
            "Epoch [13338/20000], Training Loss: 0.0806\n",
            "Epoch [13339/20000], Training Loss: 0.0790\n",
            "Epoch [13340/20000], Training Loss: 0.0801\n",
            "Epoch [13341/20000], Training Loss: 0.0839\n",
            "Epoch [13342/20000], Training Loss: 0.0850\n",
            "Epoch [13343/20000], Training Loss: 0.0841\n",
            "Epoch [13344/20000], Training Loss: 0.0743\n",
            "Epoch [13345/20000], Training Loss: 0.0855\n",
            "Epoch [13346/20000], Training Loss: 0.0797\n",
            "Epoch [13347/20000], Training Loss: 0.0826\n",
            "Epoch [13348/20000], Training Loss: 0.0853\n",
            "Epoch [13349/20000], Training Loss: 0.0753\n",
            "Epoch [13350/20000], Training Loss: 0.0797\n",
            "Epoch [13351/20000], Training Loss: 0.0778\n",
            "Epoch [13352/20000], Training Loss: 0.0772\n",
            "Epoch [13353/20000], Training Loss: 0.0816\n",
            "Epoch [13354/20000], Training Loss: 0.0775\n",
            "Epoch [13355/20000], Training Loss: 0.0736\n",
            "Epoch [13356/20000], Training Loss: 0.0749\n",
            "Epoch [13357/20000], Training Loss: 0.0737\n",
            "Epoch [13358/20000], Training Loss: 0.0817\n",
            "Epoch [13359/20000], Training Loss: 0.0769\n",
            "Epoch [13360/20000], Training Loss: 0.0844\n",
            "Epoch [13361/20000], Training Loss: 0.0799\n",
            "Epoch [13362/20000], Training Loss: 0.0776\n",
            "Epoch [13363/20000], Training Loss: 0.0829\n",
            "Epoch [13364/20000], Training Loss: 0.0808\n",
            "Epoch [13365/20000], Training Loss: 0.0754\n",
            "Epoch [13366/20000], Training Loss: 0.0841\n",
            "Epoch [13367/20000], Training Loss: 0.0760\n",
            "Epoch [13368/20000], Training Loss: 0.0806\n",
            "Epoch [13369/20000], Training Loss: 0.0805\n",
            "Epoch [13370/20000], Training Loss: 0.0789\n",
            "Epoch [13371/20000], Training Loss: 0.0800\n",
            "Epoch [13372/20000], Training Loss: 0.0778\n",
            "Epoch [13373/20000], Training Loss: 0.0812\n",
            "Epoch [13374/20000], Training Loss: 0.0780\n",
            "Epoch [13375/20000], Training Loss: 0.0801\n",
            "Epoch [13376/20000], Training Loss: 0.0831\n",
            "Epoch [13377/20000], Training Loss: 0.0784\n",
            "Epoch [13378/20000], Training Loss: 0.0856\n",
            "Epoch [13379/20000], Training Loss: 0.0784\n",
            "Epoch [13380/20000], Training Loss: 0.0741\n",
            "Epoch [13381/20000], Training Loss: 0.0794\n",
            "Epoch [13382/20000], Training Loss: 0.0839\n",
            "Epoch [13383/20000], Training Loss: 0.0803\n",
            "Epoch [13384/20000], Training Loss: 0.0784\n",
            "Epoch [13385/20000], Training Loss: 0.0781\n",
            "Epoch [13386/20000], Training Loss: 0.0800\n",
            "Epoch [13387/20000], Training Loss: 0.0842\n",
            "Epoch [13388/20000], Training Loss: 0.0796\n",
            "Epoch [13389/20000], Training Loss: 0.0744\n",
            "Epoch [13390/20000], Training Loss: 0.0769\n",
            "Epoch [13391/20000], Training Loss: 0.0808\n",
            "Epoch [13392/20000], Training Loss: 0.0836\n",
            "Epoch [13393/20000], Training Loss: 0.0858\n",
            "Epoch [13394/20000], Training Loss: 0.0813\n",
            "Epoch [13395/20000], Training Loss: 0.0748\n",
            "Epoch [13396/20000], Training Loss: 0.0827\n",
            "Epoch [13397/20000], Training Loss: 0.0801\n",
            "Epoch [13398/20000], Training Loss: 0.0827\n",
            "Epoch [13399/20000], Training Loss: 0.0811\n",
            "Epoch [13400/20000], Training Loss: 0.0841\n",
            "Epoch [13401/20000], Training Loss: 0.0747\n",
            "Epoch [13402/20000], Training Loss: 0.0774\n",
            "Epoch [13403/20000], Training Loss: 0.0777\n",
            "Epoch [13404/20000], Training Loss: 0.0749\n",
            "Epoch [13405/20000], Training Loss: 0.0766\n",
            "Epoch [13406/20000], Training Loss: 0.0771\n",
            "Epoch [13407/20000], Training Loss: 0.0795\n",
            "Epoch [13408/20000], Training Loss: 0.0759\n",
            "Epoch [13409/20000], Training Loss: 0.0867\n",
            "Epoch [13410/20000], Training Loss: 0.0785\n",
            "Epoch [13411/20000], Training Loss: 0.0845\n",
            "Epoch [13412/20000], Training Loss: 0.0756\n",
            "Epoch [13413/20000], Training Loss: 0.0776\n",
            "Epoch [13414/20000], Training Loss: 0.0813\n",
            "Epoch [13415/20000], Training Loss: 0.0790\n",
            "Epoch [13416/20000], Training Loss: 0.0759\n",
            "Epoch [13417/20000], Training Loss: 0.0736\n",
            "Epoch [13418/20000], Training Loss: 0.0800\n",
            "Epoch [13419/20000], Training Loss: 0.0827\n",
            "Epoch [13420/20000], Training Loss: 0.0815\n",
            "Epoch [13421/20000], Training Loss: 0.0776\n",
            "Epoch [13422/20000], Training Loss: 0.0784\n",
            "Epoch [13423/20000], Training Loss: 0.0817\n",
            "Epoch [13424/20000], Training Loss: 0.0785\n",
            "Epoch [13425/20000], Training Loss: 0.0857\n",
            "Epoch [13426/20000], Training Loss: 0.0780\n",
            "Epoch [13427/20000], Training Loss: 0.0740\n",
            "Epoch [13428/20000], Training Loss: 0.0805\n",
            "Epoch [13429/20000], Training Loss: 0.0742\n",
            "Epoch [13430/20000], Training Loss: 0.0850\n",
            "Epoch [13431/20000], Training Loss: 0.0771\n",
            "Epoch [13432/20000], Training Loss: 0.0818\n",
            "Epoch [13433/20000], Training Loss: 0.0818\n",
            "Epoch [13434/20000], Training Loss: 0.0757\n",
            "Epoch [13435/20000], Training Loss: 0.0811\n",
            "Epoch [13436/20000], Training Loss: 0.0809\n",
            "Epoch [13437/20000], Training Loss: 0.0848\n",
            "Epoch [13438/20000], Training Loss: 0.0796\n",
            "Epoch [13439/20000], Training Loss: 0.0857\n",
            "Epoch [13440/20000], Training Loss: 0.0836\n",
            "Epoch [13441/20000], Training Loss: 0.0799\n",
            "Epoch [13442/20000], Training Loss: 0.0800\n",
            "Epoch [13443/20000], Training Loss: 0.0806\n",
            "Epoch [13444/20000], Training Loss: 0.0797\n",
            "Epoch [13445/20000], Training Loss: 0.0822\n",
            "Epoch [13446/20000], Training Loss: 0.0777\n",
            "Epoch [13447/20000], Training Loss: 0.0817\n",
            "Epoch [13448/20000], Training Loss: 0.0837\n",
            "Epoch [13449/20000], Training Loss: 0.0753\n",
            "Epoch [13450/20000], Training Loss: 0.0823\n",
            "Epoch [13451/20000], Training Loss: 0.0796\n",
            "Epoch [13452/20000], Training Loss: 0.0779\n",
            "Epoch [13453/20000], Training Loss: 0.0800\n",
            "Epoch [13454/20000], Training Loss: 0.0762\n",
            "Epoch [13455/20000], Training Loss: 0.0752\n",
            "Epoch [13456/20000], Training Loss: 0.0814\n",
            "Epoch [13457/20000], Training Loss: 0.0815\n",
            "Epoch [13458/20000], Training Loss: 0.0765\n",
            "Epoch [13459/20000], Training Loss: 0.0757\n",
            "Epoch [13460/20000], Training Loss: 0.0799\n",
            "Epoch [13461/20000], Training Loss: 0.0781\n",
            "Epoch [13462/20000], Training Loss: 0.0757\n",
            "Epoch [13463/20000], Training Loss: 0.0728\n",
            "Epoch [13464/20000], Training Loss: 0.0782\n",
            "Epoch [13465/20000], Training Loss: 0.0782\n",
            "Epoch [13466/20000], Training Loss: 0.0765\n",
            "Epoch [13467/20000], Training Loss: 0.0834\n",
            "Epoch [13468/20000], Training Loss: 0.0797\n",
            "Epoch [13469/20000], Training Loss: 0.0777\n",
            "Epoch [13470/20000], Training Loss: 0.0827\n",
            "Epoch [13471/20000], Training Loss: 0.0772\n",
            "Epoch [13472/20000], Training Loss: 0.0784\n",
            "Epoch [13473/20000], Training Loss: 0.0802\n",
            "Epoch [13474/20000], Training Loss: 0.0823\n",
            "Epoch [13475/20000], Training Loss: 0.0804\n",
            "Epoch [13476/20000], Training Loss: 0.0790\n",
            "Epoch [13477/20000], Training Loss: 0.0796\n",
            "Epoch [13478/20000], Training Loss: 0.0791\n",
            "Epoch [13479/20000], Training Loss: 0.0794\n",
            "Epoch [13480/20000], Training Loss: 0.0802\n",
            "Epoch [13481/20000], Training Loss: 0.0805\n",
            "Epoch [13482/20000], Training Loss: 0.0753\n",
            "Epoch [13483/20000], Training Loss: 0.0818\n",
            "Epoch [13484/20000], Training Loss: 0.0754\n",
            "Epoch [13485/20000], Training Loss: 0.0816\n",
            "Epoch [13486/20000], Training Loss: 0.0840\n",
            "Epoch [13487/20000], Training Loss: 0.0763\n",
            "Epoch [13488/20000], Training Loss: 0.0793\n",
            "Epoch [13489/20000], Training Loss: 0.0800\n",
            "Epoch [13490/20000], Training Loss: 0.0750\n",
            "Epoch [13491/20000], Training Loss: 0.0797\n",
            "Epoch [13492/20000], Training Loss: 0.0754\n",
            "Epoch [13493/20000], Training Loss: 0.0833\n",
            "Epoch [13494/20000], Training Loss: 0.0718\n",
            "Epoch [13495/20000], Training Loss: 0.0770\n",
            "Epoch [13496/20000], Training Loss: 0.0811\n",
            "Epoch [13497/20000], Training Loss: 0.0784\n",
            "Epoch [13498/20000], Training Loss: 0.0793\n",
            "Epoch [13499/20000], Training Loss: 0.0797\n",
            "Epoch [13500/20000], Training Loss: 0.0749\n",
            "Epoch [13501/20000], Training Loss: 0.0811\n",
            "Epoch [13502/20000], Training Loss: 0.0758\n",
            "Epoch [13503/20000], Training Loss: 0.0741\n",
            "Epoch [13504/20000], Training Loss: 0.0806\n",
            "Epoch [13505/20000], Training Loss: 0.0758\n",
            "Epoch [13506/20000], Training Loss: 0.0796\n",
            "Epoch [13507/20000], Training Loss: 0.0813\n",
            "Epoch [13508/20000], Training Loss: 0.0737\n",
            "Epoch [13509/20000], Training Loss: 0.0809\n",
            "Epoch [13510/20000], Training Loss: 0.0779\n",
            "Epoch [13511/20000], Training Loss: 0.0777\n",
            "Epoch [13512/20000], Training Loss: 0.0851\n",
            "Epoch [13513/20000], Training Loss: 0.0793\n",
            "Epoch [13514/20000], Training Loss: 0.0814\n",
            "Epoch [13515/20000], Training Loss: 0.0838\n",
            "Epoch [13516/20000], Training Loss: 0.0813\n",
            "Epoch [13517/20000], Training Loss: 0.0747\n",
            "Epoch [13518/20000], Training Loss: 0.0808\n",
            "Epoch [13519/20000], Training Loss: 0.0740\n",
            "Epoch [13520/20000], Training Loss: 0.0794\n",
            "Epoch [13521/20000], Training Loss: 0.0799\n",
            "Epoch [13522/20000], Training Loss: 0.0785\n",
            "Epoch [13523/20000], Training Loss: 0.0854\n",
            "Epoch [13524/20000], Training Loss: 0.0794\n",
            "Epoch [13525/20000], Training Loss: 0.0778\n",
            "Epoch [13526/20000], Training Loss: 0.0745\n",
            "Epoch [13527/20000], Training Loss: 0.0848\n",
            "Epoch [13528/20000], Training Loss: 0.0822\n",
            "Epoch [13529/20000], Training Loss: 0.0798\n",
            "Epoch [13530/20000], Training Loss: 0.0801\n",
            "Epoch [13531/20000], Training Loss: 0.0797\n",
            "Epoch [13532/20000], Training Loss: 0.0849\n",
            "Epoch [13533/20000], Training Loss: 0.0768\n",
            "Epoch [13534/20000], Training Loss: 0.0747\n",
            "Epoch [13535/20000], Training Loss: 0.0843\n",
            "Epoch [13536/20000], Training Loss: 0.0852\n",
            "Epoch [13537/20000], Training Loss: 0.0774\n",
            "Epoch [13538/20000], Training Loss: 0.0731\n",
            "Epoch [13539/20000], Training Loss: 0.0770\n",
            "Epoch [13540/20000], Training Loss: 0.0810\n",
            "Epoch [13541/20000], Training Loss: 0.0799\n",
            "Epoch [13542/20000], Training Loss: 0.0777\n",
            "Epoch [13543/20000], Training Loss: 0.0846\n",
            "Epoch [13544/20000], Training Loss: 0.0799\n",
            "Epoch [13545/20000], Training Loss: 0.0850\n",
            "Epoch [13546/20000], Training Loss: 0.0825\n",
            "Epoch [13547/20000], Training Loss: 0.0760\n",
            "Epoch [13548/20000], Training Loss: 0.0771\n",
            "Epoch [13549/20000], Training Loss: 0.0809\n",
            "Epoch [13550/20000], Training Loss: 0.0799\n",
            "Epoch [13551/20000], Training Loss: 0.0809\n",
            "Epoch [13552/20000], Training Loss: 0.0819\n",
            "Epoch [13553/20000], Training Loss: 0.0799\n",
            "Epoch [13554/20000], Training Loss: 0.0754\n",
            "Epoch [13555/20000], Training Loss: 0.0767\n",
            "Epoch [13556/20000], Training Loss: 0.0765\n",
            "Epoch [13557/20000], Training Loss: 0.0814\n",
            "Epoch [13558/20000], Training Loss: 0.0839\n",
            "Epoch [13559/20000], Training Loss: 0.0758\n",
            "Epoch [13560/20000], Training Loss: 0.0739\n",
            "Epoch [13561/20000], Training Loss: 0.0883\n",
            "Epoch [13562/20000], Training Loss: 0.0812\n",
            "Epoch [13563/20000], Training Loss: 0.0774\n",
            "Epoch [13564/20000], Training Loss: 0.0756\n",
            "Epoch [13565/20000], Training Loss: 0.0766\n",
            "Epoch [13566/20000], Training Loss: 0.0761\n",
            "Epoch [13567/20000], Training Loss: 0.0780\n",
            "Epoch [13568/20000], Training Loss: 0.0822\n",
            "Epoch [13569/20000], Training Loss: 0.0827\n",
            "Epoch [13570/20000], Training Loss: 0.0845\n",
            "Epoch [13571/20000], Training Loss: 0.0818\n",
            "Epoch [13572/20000], Training Loss: 0.0875\n",
            "Epoch [13573/20000], Training Loss: 0.0797\n",
            "Epoch [13574/20000], Training Loss: 0.0808\n",
            "Epoch [13575/20000], Training Loss: 0.0762\n",
            "Epoch [13576/20000], Training Loss: 0.0801\n",
            "Epoch [13577/20000], Training Loss: 0.0810\n",
            "Epoch [13578/20000], Training Loss: 0.0857\n",
            "Epoch [13579/20000], Training Loss: 0.0818\n",
            "Epoch [13580/20000], Training Loss: 0.0762\n",
            "Epoch [13581/20000], Training Loss: 0.0786\n",
            "Epoch [13582/20000], Training Loss: 0.0812\n",
            "Epoch [13583/20000], Training Loss: 0.0798\n",
            "Epoch [13584/20000], Training Loss: 0.0853\n",
            "Epoch [13585/20000], Training Loss: 0.0739\n",
            "Epoch [13586/20000], Training Loss: 0.0825\n",
            "Epoch [13587/20000], Training Loss: 0.0830\n",
            "Epoch [13588/20000], Training Loss: 0.0792\n",
            "Epoch [13589/20000], Training Loss: 0.0823\n",
            "Epoch [13590/20000], Training Loss: 0.0777\n",
            "Epoch [13591/20000], Training Loss: 0.0761\n",
            "Epoch [13592/20000], Training Loss: 0.0754\n",
            "Epoch [13593/20000], Training Loss: 0.0791\n",
            "Epoch [13594/20000], Training Loss: 0.0824\n",
            "Epoch [13595/20000], Training Loss: 0.0842\n",
            "Epoch [13596/20000], Training Loss: 0.0790\n",
            "Epoch [13597/20000], Training Loss: 0.0780\n",
            "Epoch [13598/20000], Training Loss: 0.0736\n",
            "Epoch [13599/20000], Training Loss: 0.0737\n",
            "Epoch [13600/20000], Training Loss: 0.0867\n",
            "Epoch [13601/20000], Training Loss: 0.0879\n",
            "Epoch [13602/20000], Training Loss: 0.0807\n",
            "Epoch [13603/20000], Training Loss: 0.0735\n",
            "Epoch [13604/20000], Training Loss: 0.0790\n",
            "Epoch [13605/20000], Training Loss: 0.0730\n",
            "Epoch [13606/20000], Training Loss: 0.0794\n",
            "Epoch [13607/20000], Training Loss: 0.0776\n",
            "Epoch [13608/20000], Training Loss: 0.0774\n",
            "Epoch [13609/20000], Training Loss: 0.0845\n",
            "Epoch [13610/20000], Training Loss: 0.0830\n",
            "Epoch [13611/20000], Training Loss: 0.0786\n",
            "Epoch [13612/20000], Training Loss: 0.0803\n",
            "Epoch [13613/20000], Training Loss: 0.0832\n",
            "Epoch [13614/20000], Training Loss: 0.0762\n",
            "Epoch [13615/20000], Training Loss: 0.0779\n",
            "Epoch [13616/20000], Training Loss: 0.0871\n",
            "Epoch [13617/20000], Training Loss: 0.0780\n",
            "Epoch [13618/20000], Training Loss: 0.0815\n",
            "Epoch [13619/20000], Training Loss: 0.0850\n",
            "Epoch [13620/20000], Training Loss: 0.0774\n",
            "Epoch [13621/20000], Training Loss: 0.0833\n",
            "Epoch [13622/20000], Training Loss: 0.0738\n",
            "Epoch [13623/20000], Training Loss: 0.0772\n",
            "Epoch [13624/20000], Training Loss: 0.0813\n",
            "Epoch [13625/20000], Training Loss: 0.0774\n",
            "Epoch [13626/20000], Training Loss: 0.0765\n",
            "Epoch [13627/20000], Training Loss: 0.0770\n",
            "Epoch [13628/20000], Training Loss: 0.0804\n",
            "Epoch [13629/20000], Training Loss: 0.0783\n",
            "Epoch [13630/20000], Training Loss: 0.0863\n",
            "Epoch [13631/20000], Training Loss: 0.0812\n",
            "Epoch [13632/20000], Training Loss: 0.0773\n",
            "Epoch [13633/20000], Training Loss: 0.0757\n",
            "Epoch [13634/20000], Training Loss: 0.0861\n",
            "Epoch [13635/20000], Training Loss: 0.0832\n",
            "Epoch [13636/20000], Training Loss: 0.0736\n",
            "Epoch [13637/20000], Training Loss: 0.0817\n",
            "Epoch [13638/20000], Training Loss: 0.0809\n",
            "Epoch [13639/20000], Training Loss: 0.0770\n",
            "Epoch [13640/20000], Training Loss: 0.0794\n",
            "Epoch [13641/20000], Training Loss: 0.0789\n",
            "Epoch [13642/20000], Training Loss: 0.0808\n",
            "Epoch [13643/20000], Training Loss: 0.0780\n",
            "Epoch [13644/20000], Training Loss: 0.0781\n",
            "Epoch [13645/20000], Training Loss: 0.0739\n",
            "Epoch [13646/20000], Training Loss: 0.0750\n",
            "Epoch [13647/20000], Training Loss: 0.0777\n",
            "Epoch [13648/20000], Training Loss: 0.0737\n",
            "Epoch [13649/20000], Training Loss: 0.0739\n",
            "Epoch [13650/20000], Training Loss: 0.0807\n",
            "Epoch [13651/20000], Training Loss: 0.0770\n",
            "Epoch [13652/20000], Training Loss: 0.0803\n",
            "Epoch [13653/20000], Training Loss: 0.0751\n",
            "Epoch [13654/20000], Training Loss: 0.0862\n",
            "Epoch [13655/20000], Training Loss: 0.0771\n",
            "Epoch [13656/20000], Training Loss: 0.0752\n",
            "Epoch [13657/20000], Training Loss: 0.0802\n",
            "Epoch [13658/20000], Training Loss: 0.0794\n",
            "Epoch [13659/20000], Training Loss: 0.0805\n",
            "Epoch [13660/20000], Training Loss: 0.0778\n",
            "Epoch [13661/20000], Training Loss: 0.0867\n",
            "Epoch [13662/20000], Training Loss: 0.0775\n",
            "Epoch [13663/20000], Training Loss: 0.0859\n",
            "Epoch [13664/20000], Training Loss: 0.0793\n",
            "Epoch [13665/20000], Training Loss: 0.0841\n",
            "Epoch [13666/20000], Training Loss: 0.0878\n",
            "Epoch [13667/20000], Training Loss: 0.0792\n",
            "Epoch [13668/20000], Training Loss: 0.0735\n",
            "Epoch [13669/20000], Training Loss: 0.0759\n",
            "Epoch [13670/20000], Training Loss: 0.0804\n",
            "Epoch [13671/20000], Training Loss: 0.0782\n",
            "Epoch [13672/20000], Training Loss: 0.0865\n",
            "Epoch [13673/20000], Training Loss: 0.0851\n",
            "Epoch [13674/20000], Training Loss: 0.0757\n",
            "Epoch [13675/20000], Training Loss: 0.0781\n",
            "Epoch [13676/20000], Training Loss: 0.0799\n",
            "Epoch [13677/20000], Training Loss: 0.0825\n",
            "Epoch [13678/20000], Training Loss: 0.0847\n",
            "Epoch [13679/20000], Training Loss: 0.0841\n",
            "Epoch [13680/20000], Training Loss: 0.0790\n",
            "Epoch [13681/20000], Training Loss: 0.0781\n",
            "Epoch [13682/20000], Training Loss: 0.0754\n",
            "Epoch [13683/20000], Training Loss: 0.0738\n",
            "Epoch [13684/20000], Training Loss: 0.0779\n",
            "Epoch [13685/20000], Training Loss: 0.0812\n",
            "Epoch [13686/20000], Training Loss: 0.0771\n",
            "Epoch [13687/20000], Training Loss: 0.0825\n",
            "Epoch [13688/20000], Training Loss: 0.0803\n",
            "Epoch [13689/20000], Training Loss: 0.0811\n",
            "Epoch [13690/20000], Training Loss: 0.0835\n",
            "Epoch [13691/20000], Training Loss: 0.0773\n",
            "Epoch [13692/20000], Training Loss: 0.0789\n",
            "Epoch [13693/20000], Training Loss: 0.0818\n",
            "Epoch [13694/20000], Training Loss: 0.0776\n",
            "Epoch [13695/20000], Training Loss: 0.0812\n",
            "Epoch [13696/20000], Training Loss: 0.0771\n",
            "Epoch [13697/20000], Training Loss: 0.0838\n",
            "Epoch [13698/20000], Training Loss: 0.0766\n",
            "Epoch [13699/20000], Training Loss: 0.0791\n",
            "Epoch [13700/20000], Training Loss: 0.0810\n",
            "Epoch [13701/20000], Training Loss: 0.0837\n",
            "Epoch [13702/20000], Training Loss: 0.0822\n",
            "Epoch [13703/20000], Training Loss: 0.0783\n",
            "Epoch [13704/20000], Training Loss: 0.0815\n",
            "Epoch [13705/20000], Training Loss: 0.0826\n",
            "Epoch [13706/20000], Training Loss: 0.0758\n",
            "Epoch [13707/20000], Training Loss: 0.0796\n",
            "Epoch [13708/20000], Training Loss: 0.0840\n",
            "Epoch [13709/20000], Training Loss: 0.0853\n",
            "Epoch [13710/20000], Training Loss: 0.0770\n",
            "Epoch [13711/20000], Training Loss: 0.0883\n",
            "Epoch [13712/20000], Training Loss: 0.0793\n",
            "Epoch [13713/20000], Training Loss: 0.0813\n",
            "Epoch [13714/20000], Training Loss: 0.0738\n",
            "Epoch [13715/20000], Training Loss: 0.0803\n",
            "Epoch [13716/20000], Training Loss: 0.0738\n",
            "Epoch [13717/20000], Training Loss: 0.0802\n",
            "Epoch [13718/20000], Training Loss: 0.0738\n",
            "Epoch [13719/20000], Training Loss: 0.0893\n",
            "Epoch [13720/20000], Training Loss: 0.0812\n",
            "Epoch [13721/20000], Training Loss: 0.0780\n",
            "Epoch [13722/20000], Training Loss: 0.0819\n",
            "Epoch [13723/20000], Training Loss: 0.0761\n",
            "Epoch [13724/20000], Training Loss: 0.0750\n",
            "Epoch [13725/20000], Training Loss: 0.0810\n",
            "Epoch [13726/20000], Training Loss: 0.0822\n",
            "Epoch [13727/20000], Training Loss: 0.0838\n",
            "Epoch [13728/20000], Training Loss: 0.0746\n",
            "Epoch [13729/20000], Training Loss: 0.0791\n",
            "Epoch [13730/20000], Training Loss: 0.0804\n",
            "Epoch [13731/20000], Training Loss: 0.0792\n",
            "Epoch [13732/20000], Training Loss: 0.0832\n",
            "Epoch [13733/20000], Training Loss: 0.0778\n",
            "Epoch [13734/20000], Training Loss: 0.0771\n",
            "Epoch [13735/20000], Training Loss: 0.0787\n",
            "Epoch [13736/20000], Training Loss: 0.0762\n",
            "Epoch [13737/20000], Training Loss: 0.0844\n",
            "Epoch [13738/20000], Training Loss: 0.0784\n",
            "Epoch [13739/20000], Training Loss: 0.0786\n",
            "Epoch [13740/20000], Training Loss: 0.0788\n",
            "Epoch [13741/20000], Training Loss: 0.0769\n",
            "Epoch [13742/20000], Training Loss: 0.0802\n",
            "Epoch [13743/20000], Training Loss: 0.0754\n",
            "Epoch [13744/20000], Training Loss: 0.0755\n",
            "Epoch [13745/20000], Training Loss: 0.0802\n",
            "Epoch [13746/20000], Training Loss: 0.0763\n",
            "Epoch [13747/20000], Training Loss: 0.0794\n",
            "Epoch [13748/20000], Training Loss: 0.0848\n",
            "Epoch [13749/20000], Training Loss: 0.0808\n",
            "Epoch [13750/20000], Training Loss: 0.0824\n",
            "Epoch [13751/20000], Training Loss: 0.0764\n",
            "Epoch [13752/20000], Training Loss: 0.0770\n",
            "Epoch [13753/20000], Training Loss: 0.0823\n",
            "Epoch [13754/20000], Training Loss: 0.0778\n",
            "Epoch [13755/20000], Training Loss: 0.0797\n",
            "Epoch [13756/20000], Training Loss: 0.0857\n",
            "Epoch [13757/20000], Training Loss: 0.0815\n",
            "Epoch [13758/20000], Training Loss: 0.0767\n",
            "Epoch [13759/20000], Training Loss: 0.0801\n",
            "Epoch [13760/20000], Training Loss: 0.0861\n",
            "Epoch [13761/20000], Training Loss: 0.0829\n",
            "Epoch [13762/20000], Training Loss: 0.0808\n",
            "Epoch [13763/20000], Training Loss: 0.0790\n",
            "Epoch [13764/20000], Training Loss: 0.0817\n",
            "Epoch [13765/20000], Training Loss: 0.0776\n",
            "Epoch [13766/20000], Training Loss: 0.0767\n",
            "Epoch [13767/20000], Training Loss: 0.0815\n",
            "Epoch [13768/20000], Training Loss: 0.0800\n",
            "Epoch [13769/20000], Training Loss: 0.0800\n",
            "Epoch [13770/20000], Training Loss: 0.0785\n",
            "Epoch [13771/20000], Training Loss: 0.0769\n",
            "Epoch [13772/20000], Training Loss: 0.0826\n",
            "Epoch [13773/20000], Training Loss: 0.0827\n",
            "Epoch [13774/20000], Training Loss: 0.0789\n",
            "Epoch [13775/20000], Training Loss: 0.0743\n",
            "Epoch [13776/20000], Training Loss: 0.0835\n",
            "Epoch [13777/20000], Training Loss: 0.0773\n",
            "Epoch [13778/20000], Training Loss: 0.0747\n",
            "Epoch [13779/20000], Training Loss: 0.0805\n",
            "Epoch [13780/20000], Training Loss: 0.0826\n",
            "Epoch [13781/20000], Training Loss: 0.0787\n",
            "Epoch [13782/20000], Training Loss: 0.0814\n",
            "Epoch [13783/20000], Training Loss: 0.0788\n",
            "Epoch [13784/20000], Training Loss: 0.0766\n",
            "Epoch [13785/20000], Training Loss: 0.0817\n",
            "Epoch [13786/20000], Training Loss: 0.0813\n",
            "Epoch [13787/20000], Training Loss: 0.0764\n",
            "Epoch [13788/20000], Training Loss: 0.0780\n",
            "Epoch [13789/20000], Training Loss: 0.0774\n",
            "Epoch [13790/20000], Training Loss: 0.0781\n",
            "Epoch [13791/20000], Training Loss: 0.0813\n",
            "Epoch [13792/20000], Training Loss: 0.0807\n",
            "Epoch [13793/20000], Training Loss: 0.0754\n",
            "Epoch [13794/20000], Training Loss: 0.0814\n",
            "Epoch [13795/20000], Training Loss: 0.0783\n",
            "Epoch [13796/20000], Training Loss: 0.0809\n",
            "Epoch [13797/20000], Training Loss: 0.0811\n",
            "Epoch [13798/20000], Training Loss: 0.0805\n",
            "Epoch [13799/20000], Training Loss: 0.0756\n",
            "Epoch [13800/20000], Training Loss: 0.0811\n",
            "Epoch [13801/20000], Training Loss: 0.0814\n",
            "Epoch [13802/20000], Training Loss: 0.0736\n",
            "Epoch [13803/20000], Training Loss: 0.0816\n",
            "Epoch [13804/20000], Training Loss: 0.0779\n",
            "Epoch [13805/20000], Training Loss: 0.0764\n",
            "Epoch [13806/20000], Training Loss: 0.0804\n",
            "Epoch [13807/20000], Training Loss: 0.0825\n",
            "Epoch [13808/20000], Training Loss: 0.0740\n",
            "Epoch [13809/20000], Training Loss: 0.0755\n",
            "Epoch [13810/20000], Training Loss: 0.0750\n",
            "Epoch [13811/20000], Training Loss: 0.0840\n",
            "Epoch [13812/20000], Training Loss: 0.0793\n",
            "Epoch [13813/20000], Training Loss: 0.0745\n",
            "Epoch [13814/20000], Training Loss: 0.0840\n",
            "Epoch [13815/20000], Training Loss: 0.0774\n",
            "Epoch [13816/20000], Training Loss: 0.0814\n",
            "Epoch [13817/20000], Training Loss: 0.0782\n",
            "Epoch [13818/20000], Training Loss: 0.0759\n",
            "Epoch [13819/20000], Training Loss: 0.0776\n",
            "Epoch [13820/20000], Training Loss: 0.0810\n",
            "Epoch [13821/20000], Training Loss: 0.0766\n",
            "Epoch [13822/20000], Training Loss: 0.0874\n",
            "Epoch [13823/20000], Training Loss: 0.0806\n",
            "Epoch [13824/20000], Training Loss: 0.0809\n",
            "Epoch [13825/20000], Training Loss: 0.0804\n",
            "Epoch [13826/20000], Training Loss: 0.0765\n",
            "Epoch [13827/20000], Training Loss: 0.0766\n",
            "Epoch [13828/20000], Training Loss: 0.0811\n",
            "Epoch [13829/20000], Training Loss: 0.0830\n",
            "Epoch [13830/20000], Training Loss: 0.0760\n",
            "Epoch [13831/20000], Training Loss: 0.0805\n",
            "Epoch [13832/20000], Training Loss: 0.0798\n",
            "Epoch [13833/20000], Training Loss: 0.0784\n",
            "Epoch [13834/20000], Training Loss: 0.0763\n",
            "Epoch [13835/20000], Training Loss: 0.0786\n",
            "Epoch [13836/20000], Training Loss: 0.0788\n",
            "Epoch [13837/20000], Training Loss: 0.0802\n",
            "Epoch [13838/20000], Training Loss: 0.0780\n",
            "Epoch [13839/20000], Training Loss: 0.0794\n",
            "Epoch [13840/20000], Training Loss: 0.0746\n",
            "Epoch [13841/20000], Training Loss: 0.0769\n",
            "Epoch [13842/20000], Training Loss: 0.0786\n",
            "Epoch [13843/20000], Training Loss: 0.0806\n",
            "Epoch [13844/20000], Training Loss: 0.0825\n",
            "Epoch [13845/20000], Training Loss: 0.0806\n",
            "Epoch [13846/20000], Training Loss: 0.0862\n",
            "Epoch [13847/20000], Training Loss: 0.0780\n",
            "Epoch [13848/20000], Training Loss: 0.0875\n",
            "Epoch [13849/20000], Training Loss: 0.0829\n",
            "Epoch [13850/20000], Training Loss: 0.0798\n",
            "Epoch [13851/20000], Training Loss: 0.0766\n",
            "Epoch [13852/20000], Training Loss: 0.0791\n",
            "Epoch [13853/20000], Training Loss: 0.0865\n",
            "Epoch [13854/20000], Training Loss: 0.0793\n",
            "Epoch [13855/20000], Training Loss: 0.0775\n",
            "Epoch [13856/20000], Training Loss: 0.0785\n",
            "Epoch [13857/20000], Training Loss: 0.0830\n",
            "Epoch [13858/20000], Training Loss: 0.0783\n",
            "Epoch [13859/20000], Training Loss: 0.0863\n",
            "Epoch [13860/20000], Training Loss: 0.0793\n",
            "Epoch [13861/20000], Training Loss: 0.0799\n",
            "Epoch [13862/20000], Training Loss: 0.0842\n",
            "Epoch [13863/20000], Training Loss: 0.0797\n",
            "Epoch [13864/20000], Training Loss: 0.0794\n",
            "Epoch [13865/20000], Training Loss: 0.0829\n",
            "Epoch [13866/20000], Training Loss: 0.0755\n",
            "Epoch [13867/20000], Training Loss: 0.0819\n",
            "Epoch [13868/20000], Training Loss: 0.0785\n",
            "Epoch [13869/20000], Training Loss: 0.0782\n",
            "Epoch [13870/20000], Training Loss: 0.0747\n",
            "Epoch [13871/20000], Training Loss: 0.0749\n",
            "Epoch [13872/20000], Training Loss: 0.0800\n",
            "Epoch [13873/20000], Training Loss: 0.0793\n",
            "Epoch [13874/20000], Training Loss: 0.0745\n",
            "Epoch [13875/20000], Training Loss: 0.0837\n",
            "Epoch [13876/20000], Training Loss: 0.0849\n",
            "Epoch [13877/20000], Training Loss: 0.0843\n",
            "Epoch [13878/20000], Training Loss: 0.0820\n",
            "Epoch [13879/20000], Training Loss: 0.0781\n",
            "Epoch [13880/20000], Training Loss: 0.0753\n",
            "Epoch [13881/20000], Training Loss: 0.0800\n",
            "Epoch [13882/20000], Training Loss: 0.0787\n",
            "Epoch [13883/20000], Training Loss: 0.0755\n",
            "Epoch [13884/20000], Training Loss: 0.0777\n",
            "Epoch [13885/20000], Training Loss: 0.0832\n",
            "Epoch [13886/20000], Training Loss: 0.0796\n",
            "Epoch [13887/20000], Training Loss: 0.0810\n",
            "Epoch [13888/20000], Training Loss: 0.0775\n",
            "Epoch [13889/20000], Training Loss: 0.0728\n",
            "Epoch [13890/20000], Training Loss: 0.0832\n",
            "Epoch [13891/20000], Training Loss: 0.0803\n",
            "Epoch [13892/20000], Training Loss: 0.0818\n",
            "Epoch [13893/20000], Training Loss: 0.0808\n",
            "Epoch [13894/20000], Training Loss: 0.0808\n",
            "Epoch [13895/20000], Training Loss: 0.0779\n",
            "Epoch [13896/20000], Training Loss: 0.0736\n",
            "Epoch [13897/20000], Training Loss: 0.0826\n",
            "Epoch [13898/20000], Training Loss: 0.0740\n",
            "Epoch [13899/20000], Training Loss: 0.0754\n",
            "Epoch [13900/20000], Training Loss: 0.0758\n",
            "Epoch [13901/20000], Training Loss: 0.0766\n",
            "Epoch [13902/20000], Training Loss: 0.0742\n",
            "Epoch [13903/20000], Training Loss: 0.0784\n",
            "Epoch [13904/20000], Training Loss: 0.0763\n",
            "Epoch [13905/20000], Training Loss: 0.0784\n",
            "Epoch [13906/20000], Training Loss: 0.0799\n",
            "Epoch [13907/20000], Training Loss: 0.0848\n",
            "Epoch [13908/20000], Training Loss: 0.0781\n",
            "Epoch [13909/20000], Training Loss: 0.0788\n",
            "Epoch [13910/20000], Training Loss: 0.0789\n",
            "Epoch [13911/20000], Training Loss: 0.0777\n",
            "Epoch [13912/20000], Training Loss: 0.0798\n",
            "Epoch [13913/20000], Training Loss: 0.0732\n",
            "Epoch [13914/20000], Training Loss: 0.0831\n",
            "Epoch [13915/20000], Training Loss: 0.0793\n",
            "Epoch [13916/20000], Training Loss: 0.0804\n",
            "Epoch [13917/20000], Training Loss: 0.0814\n",
            "Epoch [13918/20000], Training Loss: 0.0810\n",
            "Epoch [13919/20000], Training Loss: 0.0738\n",
            "Epoch [13920/20000], Training Loss: 0.0820\n",
            "Epoch [13921/20000], Training Loss: 0.0777\n",
            "Epoch [13922/20000], Training Loss: 0.0828\n",
            "Epoch [13923/20000], Training Loss: 0.0739\n",
            "Epoch [13924/20000], Training Loss: 0.0843\n",
            "Epoch [13925/20000], Training Loss: 0.0761\n",
            "Epoch [13926/20000], Training Loss: 0.0788\n",
            "Epoch [13927/20000], Training Loss: 0.0817\n",
            "Epoch [13928/20000], Training Loss: 0.0775\n",
            "Epoch [13929/20000], Training Loss: 0.0855\n",
            "Epoch [13930/20000], Training Loss: 0.0750\n",
            "Epoch [13931/20000], Training Loss: 0.0833\n",
            "Epoch [13932/20000], Training Loss: 0.0824\n",
            "Epoch [13933/20000], Training Loss: 0.0818\n",
            "Epoch [13934/20000], Training Loss: 0.0778\n",
            "Epoch [13935/20000], Training Loss: 0.0803\n",
            "Epoch [13936/20000], Training Loss: 0.0813\n",
            "Epoch [13937/20000], Training Loss: 0.0738\n",
            "Epoch [13938/20000], Training Loss: 0.0755\n",
            "Epoch [13939/20000], Training Loss: 0.0822\n",
            "Epoch [13940/20000], Training Loss: 0.0776\n",
            "Epoch [13941/20000], Training Loss: 0.0831\n",
            "Epoch [13942/20000], Training Loss: 0.0777\n",
            "Epoch [13943/20000], Training Loss: 0.0767\n",
            "Epoch [13944/20000], Training Loss: 0.0835\n",
            "Epoch [13945/20000], Training Loss: 0.0777\n",
            "Epoch [13946/20000], Training Loss: 0.0769\n",
            "Epoch [13947/20000], Training Loss: 0.0772\n",
            "Epoch [13948/20000], Training Loss: 0.0822\n",
            "Epoch [13949/20000], Training Loss: 0.0775\n",
            "Epoch [13950/20000], Training Loss: 0.0826\n",
            "Epoch [13951/20000], Training Loss: 0.0763\n",
            "Epoch [13952/20000], Training Loss: 0.0777\n",
            "Epoch [13953/20000], Training Loss: 0.0829\n",
            "Epoch [13954/20000], Training Loss: 0.0792\n",
            "Epoch [13955/20000], Training Loss: 0.0784\n",
            "Epoch [13956/20000], Training Loss: 0.0816\n",
            "Epoch [13957/20000], Training Loss: 0.0745\n",
            "Epoch [13958/20000], Training Loss: 0.0845\n",
            "Epoch [13959/20000], Training Loss: 0.0854\n",
            "Epoch [13960/20000], Training Loss: 0.0812\n",
            "Epoch [13961/20000], Training Loss: 0.0787\n",
            "Epoch [13962/20000], Training Loss: 0.0837\n",
            "Epoch [13963/20000], Training Loss: 0.0789\n",
            "Epoch [13964/20000], Training Loss: 0.0749\n",
            "Epoch [13965/20000], Training Loss: 0.0793\n",
            "Epoch [13966/20000], Training Loss: 0.0840\n",
            "Epoch [13967/20000], Training Loss: 0.0864\n",
            "Epoch [13968/20000], Training Loss: 0.0735\n",
            "Epoch [13969/20000], Training Loss: 0.0795\n",
            "Epoch [13970/20000], Training Loss: 0.0818\n",
            "Epoch [13971/20000], Training Loss: 0.0803\n",
            "Epoch [13972/20000], Training Loss: 0.0767\n",
            "Epoch [13973/20000], Training Loss: 0.0808\n",
            "Epoch [13974/20000], Training Loss: 0.0824\n",
            "Epoch [13975/20000], Training Loss: 0.0795\n",
            "Epoch [13976/20000], Training Loss: 0.0784\n",
            "Epoch [13977/20000], Training Loss: 0.0837\n",
            "Epoch [13978/20000], Training Loss: 0.0744\n",
            "Epoch [13979/20000], Training Loss: 0.0845\n",
            "Epoch [13980/20000], Training Loss: 0.0778\n",
            "Epoch [13981/20000], Training Loss: 0.0799\n",
            "Epoch [13982/20000], Training Loss: 0.0847\n",
            "Epoch [13983/20000], Training Loss: 0.0806\n",
            "Epoch [13984/20000], Training Loss: 0.0750\n",
            "Epoch [13985/20000], Training Loss: 0.0813\n",
            "Epoch [13986/20000], Training Loss: 0.0854\n",
            "Epoch [13987/20000], Training Loss: 0.0833\n",
            "Epoch [13988/20000], Training Loss: 0.0785\n",
            "Epoch [13989/20000], Training Loss: 0.0809\n",
            "Epoch [13990/20000], Training Loss: 0.0802\n",
            "Epoch [13991/20000], Training Loss: 0.0799\n",
            "Epoch [13992/20000], Training Loss: 0.0759\n",
            "Epoch [13993/20000], Training Loss: 0.0824\n",
            "Epoch [13994/20000], Training Loss: 0.0798\n",
            "Epoch [13995/20000], Training Loss: 0.0818\n",
            "Epoch [13996/20000], Training Loss: 0.0760\n",
            "Epoch [13997/20000], Training Loss: 0.0794\n",
            "Epoch [13998/20000], Training Loss: 0.0755\n",
            "Epoch [13999/20000], Training Loss: 0.0840\n",
            "Epoch [14000/20000], Training Loss: 0.0740\n",
            "Epoch [14001/20000], Training Loss: 0.0849\n",
            "Epoch [14002/20000], Training Loss: 0.0828\n",
            "Epoch [14003/20000], Training Loss: 0.0818\n",
            "Epoch [14004/20000], Training Loss: 0.0832\n",
            "Epoch [14005/20000], Training Loss: 0.0787\n",
            "Epoch [14006/20000], Training Loss: 0.0842\n",
            "Epoch [14007/20000], Training Loss: 0.0790\n",
            "Epoch [14008/20000], Training Loss: 0.0827\n",
            "Epoch [14009/20000], Training Loss: 0.0750\n",
            "Epoch [14010/20000], Training Loss: 0.0804\n",
            "Epoch [14011/20000], Training Loss: 0.0796\n",
            "Epoch [14012/20000], Training Loss: 0.0819\n",
            "Epoch [14013/20000], Training Loss: 0.0783\n",
            "Epoch [14014/20000], Training Loss: 0.0775\n",
            "Epoch [14015/20000], Training Loss: 0.0800\n",
            "Epoch [14016/20000], Training Loss: 0.0836\n",
            "Epoch [14017/20000], Training Loss: 0.0778\n",
            "Epoch [14018/20000], Training Loss: 0.0868\n",
            "Epoch [14019/20000], Training Loss: 0.0788\n",
            "Epoch [14020/20000], Training Loss: 0.0841\n",
            "Epoch [14021/20000], Training Loss: 0.0781\n",
            "Epoch [14022/20000], Training Loss: 0.0846\n",
            "Epoch [14023/20000], Training Loss: 0.0761\n",
            "Epoch [14024/20000], Training Loss: 0.0764\n",
            "Epoch [14025/20000], Training Loss: 0.0865\n",
            "Epoch [14026/20000], Training Loss: 0.0769\n",
            "Epoch [14027/20000], Training Loss: 0.0785\n",
            "Epoch [14028/20000], Training Loss: 0.0828\n",
            "Epoch [14029/20000], Training Loss: 0.0769\n",
            "Epoch [14030/20000], Training Loss: 0.0739\n",
            "Epoch [14031/20000], Training Loss: 0.0827\n",
            "Epoch [14032/20000], Training Loss: 0.0814\n",
            "Epoch [14033/20000], Training Loss: 0.0786\n",
            "Epoch [14034/20000], Training Loss: 0.0841\n",
            "Epoch [14035/20000], Training Loss: 0.0788\n",
            "Epoch [14036/20000], Training Loss: 0.0815\n",
            "Epoch [14037/20000], Training Loss: 0.0792\n",
            "Epoch [14038/20000], Training Loss: 0.0790\n",
            "Epoch [14039/20000], Training Loss: 0.0762\n",
            "Epoch [14040/20000], Training Loss: 0.0808\n",
            "Epoch [14041/20000], Training Loss: 0.0829\n",
            "Epoch [14042/20000], Training Loss: 0.0822\n",
            "Epoch [14043/20000], Training Loss: 0.0828\n",
            "Epoch [14044/20000], Training Loss: 0.0829\n",
            "Epoch [14045/20000], Training Loss: 0.0803\n",
            "Epoch [14046/20000], Training Loss: 0.0750\n",
            "Epoch [14047/20000], Training Loss: 0.0849\n",
            "Epoch [14048/20000], Training Loss: 0.0820\n",
            "Epoch [14049/20000], Training Loss: 0.0791\n",
            "Epoch [14050/20000], Training Loss: 0.0810\n",
            "Epoch [14051/20000], Training Loss: 0.0816\n",
            "Epoch [14052/20000], Training Loss: 0.0803\n",
            "Epoch [14053/20000], Training Loss: 0.0776\n",
            "Epoch [14054/20000], Training Loss: 0.0810\n",
            "Epoch [14055/20000], Training Loss: 0.0832\n",
            "Epoch [14056/20000], Training Loss: 0.0810\n",
            "Epoch [14057/20000], Training Loss: 0.0752\n",
            "Epoch [14058/20000], Training Loss: 0.0828\n",
            "Epoch [14059/20000], Training Loss: 0.0797\n",
            "Epoch [14060/20000], Training Loss: 0.0779\n",
            "Epoch [14061/20000], Training Loss: 0.0768\n",
            "Epoch [14062/20000], Training Loss: 0.0743\n",
            "Epoch [14063/20000], Training Loss: 0.0858\n",
            "Epoch [14064/20000], Training Loss: 0.0789\n",
            "Epoch [14065/20000], Training Loss: 0.0809\n",
            "Epoch [14066/20000], Training Loss: 0.0767\n",
            "Epoch [14067/20000], Training Loss: 0.0801\n",
            "Epoch [14068/20000], Training Loss: 0.0801\n",
            "Epoch [14069/20000], Training Loss: 0.0835\n",
            "Epoch [14070/20000], Training Loss: 0.0793\n",
            "Epoch [14071/20000], Training Loss: 0.0762\n",
            "Epoch [14072/20000], Training Loss: 0.0794\n",
            "Epoch [14073/20000], Training Loss: 0.0790\n",
            "Epoch [14074/20000], Training Loss: 0.0787\n",
            "Epoch [14075/20000], Training Loss: 0.0802\n",
            "Epoch [14076/20000], Training Loss: 0.0752\n",
            "Epoch [14077/20000], Training Loss: 0.0800\n",
            "Epoch [14078/20000], Training Loss: 0.0744\n",
            "Epoch [14079/20000], Training Loss: 0.0809\n",
            "Epoch [14080/20000], Training Loss: 0.0750\n",
            "Epoch [14081/20000], Training Loss: 0.0813\n",
            "Epoch [14082/20000], Training Loss: 0.0824\n",
            "Epoch [14083/20000], Training Loss: 0.0795\n",
            "Epoch [14084/20000], Training Loss: 0.0759\n",
            "Epoch [14085/20000], Training Loss: 0.0793\n",
            "Epoch [14086/20000], Training Loss: 0.0824\n",
            "Epoch [14087/20000], Training Loss: 0.0847\n",
            "Epoch [14088/20000], Training Loss: 0.0781\n",
            "Epoch [14089/20000], Training Loss: 0.0752\n",
            "Epoch [14090/20000], Training Loss: 0.0804\n",
            "Epoch [14091/20000], Training Loss: 0.0830\n",
            "Epoch [14092/20000], Training Loss: 0.0785\n",
            "Epoch [14093/20000], Training Loss: 0.0860\n",
            "Epoch [14094/20000], Training Loss: 0.0808\n",
            "Epoch [14095/20000], Training Loss: 0.0752\n",
            "Epoch [14096/20000], Training Loss: 0.0813\n",
            "Epoch [14097/20000], Training Loss: 0.0761\n",
            "Epoch [14098/20000], Training Loss: 0.0833\n",
            "Epoch [14099/20000], Training Loss: 0.0803\n",
            "Epoch [14100/20000], Training Loss: 0.0756\n",
            "Epoch [14101/20000], Training Loss: 0.0765\n",
            "Epoch [14102/20000], Training Loss: 0.0791\n",
            "Epoch [14103/20000], Training Loss: 0.0737\n",
            "Epoch [14104/20000], Training Loss: 0.0814\n",
            "Epoch [14105/20000], Training Loss: 0.0792\n",
            "Epoch [14106/20000], Training Loss: 0.0808\n",
            "Epoch [14107/20000], Training Loss: 0.0768\n",
            "Epoch [14108/20000], Training Loss: 0.0834\n",
            "Epoch [14109/20000], Training Loss: 0.0820\n",
            "Epoch [14110/20000], Training Loss: 0.0817\n",
            "Epoch [14111/20000], Training Loss: 0.0802\n",
            "Epoch [14112/20000], Training Loss: 0.0788\n",
            "Epoch [14113/20000], Training Loss: 0.0834\n",
            "Epoch [14114/20000], Training Loss: 0.0820\n",
            "Epoch [14115/20000], Training Loss: 0.0800\n",
            "Epoch [14116/20000], Training Loss: 0.0738\n",
            "Epoch [14117/20000], Training Loss: 0.0751\n",
            "Epoch [14118/20000], Training Loss: 0.0833\n",
            "Epoch [14119/20000], Training Loss: 0.0826\n",
            "Epoch [14120/20000], Training Loss: 0.0742\n",
            "Epoch [14121/20000], Training Loss: 0.0759\n",
            "Epoch [14122/20000], Training Loss: 0.0737\n",
            "Epoch [14123/20000], Training Loss: 0.0808\n",
            "Epoch [14124/20000], Training Loss: 0.0744\n",
            "Epoch [14125/20000], Training Loss: 0.0826\n",
            "Epoch [14126/20000], Training Loss: 0.0831\n",
            "Epoch [14127/20000], Training Loss: 0.0803\n",
            "Epoch [14128/20000], Training Loss: 0.0829\n",
            "Epoch [14129/20000], Training Loss: 0.0757\n",
            "Epoch [14130/20000], Training Loss: 0.0837\n",
            "Epoch [14131/20000], Training Loss: 0.0785\n",
            "Epoch [14132/20000], Training Loss: 0.0776\n",
            "Epoch [14133/20000], Training Loss: 0.0848\n",
            "Epoch [14134/20000], Training Loss: 0.0785\n",
            "Epoch [14135/20000], Training Loss: 0.0834\n",
            "Epoch [14136/20000], Training Loss: 0.0786\n",
            "Epoch [14137/20000], Training Loss: 0.0834\n",
            "Epoch [14138/20000], Training Loss: 0.0803\n",
            "Epoch [14139/20000], Training Loss: 0.0804\n",
            "Epoch [14140/20000], Training Loss: 0.0839\n",
            "Epoch [14141/20000], Training Loss: 0.0813\n",
            "Epoch [14142/20000], Training Loss: 0.0853\n",
            "Epoch [14143/20000], Training Loss: 0.0756\n",
            "Epoch [14144/20000], Training Loss: 0.0779\n",
            "Epoch [14145/20000], Training Loss: 0.0808\n",
            "Epoch [14146/20000], Training Loss: 0.0831\n",
            "Epoch [14147/20000], Training Loss: 0.0866\n",
            "Epoch [14148/20000], Training Loss: 0.0761\n",
            "Epoch [14149/20000], Training Loss: 0.0816\n",
            "Epoch [14150/20000], Training Loss: 0.0801\n",
            "Epoch [14151/20000], Training Loss: 0.0830\n",
            "Epoch [14152/20000], Training Loss: 0.0801\n",
            "Epoch [14153/20000], Training Loss: 0.0791\n",
            "Epoch [14154/20000], Training Loss: 0.0795\n",
            "Epoch [14155/20000], Training Loss: 0.0840\n",
            "Epoch [14156/20000], Training Loss: 0.0838\n",
            "Epoch [14157/20000], Training Loss: 0.0831\n",
            "Epoch [14158/20000], Training Loss: 0.0797\n",
            "Epoch [14159/20000], Training Loss: 0.0847\n",
            "Epoch [14160/20000], Training Loss: 0.0829\n",
            "Epoch [14161/20000], Training Loss: 0.0747\n",
            "Epoch [14162/20000], Training Loss: 0.0771\n",
            "Epoch [14163/20000], Training Loss: 0.0761\n",
            "Epoch [14164/20000], Training Loss: 0.0735\n",
            "Epoch [14165/20000], Training Loss: 0.0790\n",
            "Epoch [14166/20000], Training Loss: 0.0752\n",
            "Epoch [14167/20000], Training Loss: 0.0768\n",
            "Epoch [14168/20000], Training Loss: 0.0851\n",
            "Epoch [14169/20000], Training Loss: 0.0786\n",
            "Epoch [14170/20000], Training Loss: 0.0872\n",
            "Epoch [14171/20000], Training Loss: 0.0761\n",
            "Epoch [14172/20000], Training Loss: 0.0847\n",
            "Epoch [14173/20000], Training Loss: 0.0842\n",
            "Epoch [14174/20000], Training Loss: 0.0771\n",
            "Epoch [14175/20000], Training Loss: 0.0809\n",
            "Epoch [14176/20000], Training Loss: 0.0806\n",
            "Epoch [14177/20000], Training Loss: 0.0794\n",
            "Epoch [14178/20000], Training Loss: 0.0772\n",
            "Epoch [14179/20000], Training Loss: 0.0744\n",
            "Epoch [14180/20000], Training Loss: 0.0840\n",
            "Epoch [14181/20000], Training Loss: 0.0770\n",
            "Epoch [14182/20000], Training Loss: 0.0798\n",
            "Epoch [14183/20000], Training Loss: 0.0776\n",
            "Epoch [14184/20000], Training Loss: 0.0741\n",
            "Epoch [14185/20000], Training Loss: 0.0781\n",
            "Epoch [14186/20000], Training Loss: 0.0798\n",
            "Epoch [14187/20000], Training Loss: 0.0856\n",
            "Epoch [14188/20000], Training Loss: 0.0760\n",
            "Epoch [14189/20000], Training Loss: 0.0768\n",
            "Epoch [14190/20000], Training Loss: 0.0742\n",
            "Epoch [14191/20000], Training Loss: 0.0786\n",
            "Epoch [14192/20000], Training Loss: 0.0807\n",
            "Epoch [14193/20000], Training Loss: 0.0794\n",
            "Epoch [14194/20000], Training Loss: 0.0840\n",
            "Epoch [14195/20000], Training Loss: 0.0799\n",
            "Epoch [14196/20000], Training Loss: 0.0834\n",
            "Epoch [14197/20000], Training Loss: 0.0815\n",
            "Epoch [14198/20000], Training Loss: 0.0768\n",
            "Epoch [14199/20000], Training Loss: 0.0738\n",
            "Epoch [14200/20000], Training Loss: 0.0827\n",
            "Epoch [14201/20000], Training Loss: 0.0764\n",
            "Epoch [14202/20000], Training Loss: 0.0795\n",
            "Epoch [14203/20000], Training Loss: 0.0844\n",
            "Epoch [14204/20000], Training Loss: 0.0769\n",
            "Epoch [14205/20000], Training Loss: 0.0793\n",
            "Epoch [14206/20000], Training Loss: 0.0766\n",
            "Epoch [14207/20000], Training Loss: 0.0832\n",
            "Epoch [14208/20000], Training Loss: 0.0837\n",
            "Epoch [14209/20000], Training Loss: 0.0832\n",
            "Epoch [14210/20000], Training Loss: 0.0851\n",
            "Epoch [14211/20000], Training Loss: 0.0778\n",
            "Epoch [14212/20000], Training Loss: 0.0736\n",
            "Epoch [14213/20000], Training Loss: 0.0799\n",
            "Epoch [14214/20000], Training Loss: 0.0812\n",
            "Epoch [14215/20000], Training Loss: 0.0826\n",
            "Epoch [14216/20000], Training Loss: 0.0742\n",
            "Epoch [14217/20000], Training Loss: 0.0787\n",
            "Epoch [14218/20000], Training Loss: 0.0779\n",
            "Epoch [14219/20000], Training Loss: 0.0781\n",
            "Epoch [14220/20000], Training Loss: 0.0805\n",
            "Epoch [14221/20000], Training Loss: 0.0745\n",
            "Epoch [14222/20000], Training Loss: 0.0772\n",
            "Epoch [14223/20000], Training Loss: 0.0820\n",
            "Epoch [14224/20000], Training Loss: 0.0803\n",
            "Epoch [14225/20000], Training Loss: 0.0782\n",
            "Epoch [14226/20000], Training Loss: 0.0772\n",
            "Epoch [14227/20000], Training Loss: 0.0856\n",
            "Epoch [14228/20000], Training Loss: 0.0854\n",
            "Epoch [14229/20000], Training Loss: 0.0828\n",
            "Epoch [14230/20000], Training Loss: 0.0846\n",
            "Epoch [14231/20000], Training Loss: 0.0834\n",
            "Epoch [14232/20000], Training Loss: 0.0772\n",
            "Epoch [14233/20000], Training Loss: 0.0809\n",
            "Epoch [14234/20000], Training Loss: 0.0854\n",
            "Epoch [14235/20000], Training Loss: 0.0835\n",
            "Epoch [14236/20000], Training Loss: 0.0750\n",
            "Epoch [14237/20000], Training Loss: 0.0839\n",
            "Epoch [14238/20000], Training Loss: 0.0813\n",
            "Epoch [14239/20000], Training Loss: 0.0788\n",
            "Epoch [14240/20000], Training Loss: 0.0767\n",
            "Epoch [14241/20000], Training Loss: 0.0785\n",
            "Epoch [14242/20000], Training Loss: 0.0789\n",
            "Epoch [14243/20000], Training Loss: 0.0825\n",
            "Epoch [14244/20000], Training Loss: 0.0778\n",
            "Epoch [14245/20000], Training Loss: 0.0849\n",
            "Epoch [14246/20000], Training Loss: 0.0848\n",
            "Epoch [14247/20000], Training Loss: 0.0793\n",
            "Epoch [14248/20000], Training Loss: 0.0785\n",
            "Epoch [14249/20000], Training Loss: 0.0791\n",
            "Epoch [14250/20000], Training Loss: 0.0826\n",
            "Epoch [14251/20000], Training Loss: 0.0816\n",
            "Epoch [14252/20000], Training Loss: 0.0787\n",
            "Epoch [14253/20000], Training Loss: 0.0814\n",
            "Epoch [14254/20000], Training Loss: 0.0850\n",
            "Epoch [14255/20000], Training Loss: 0.0774\n",
            "Epoch [14256/20000], Training Loss: 0.0760\n",
            "Epoch [14257/20000], Training Loss: 0.0806\n",
            "Epoch [14258/20000], Training Loss: 0.0802\n",
            "Epoch [14259/20000], Training Loss: 0.0752\n",
            "Epoch [14260/20000], Training Loss: 0.0730\n",
            "Epoch [14261/20000], Training Loss: 0.0801\n",
            "Epoch [14262/20000], Training Loss: 0.0811\n",
            "Epoch [14263/20000], Training Loss: 0.0810\n",
            "Epoch [14264/20000], Training Loss: 0.0854\n",
            "Epoch [14265/20000], Training Loss: 0.0749\n",
            "Epoch [14266/20000], Training Loss: 0.0787\n",
            "Epoch [14267/20000], Training Loss: 0.0759\n",
            "Epoch [14268/20000], Training Loss: 0.0748\n",
            "Epoch [14269/20000], Training Loss: 0.0791\n",
            "Epoch [14270/20000], Training Loss: 0.0848\n",
            "Epoch [14271/20000], Training Loss: 0.0827\n",
            "Epoch [14272/20000], Training Loss: 0.0803\n",
            "Epoch [14273/20000], Training Loss: 0.0760\n",
            "Epoch [14274/20000], Training Loss: 0.0805\n",
            "Epoch [14275/20000], Training Loss: 0.0785\n",
            "Epoch [14276/20000], Training Loss: 0.0736\n",
            "Epoch [14277/20000], Training Loss: 0.0788\n",
            "Epoch [14278/20000], Training Loss: 0.0831\n",
            "Epoch [14279/20000], Training Loss: 0.0849\n",
            "Epoch [14280/20000], Training Loss: 0.0756\n",
            "Epoch [14281/20000], Training Loss: 0.0808\n",
            "Epoch [14282/20000], Training Loss: 0.0801\n",
            "Epoch [14283/20000], Training Loss: 0.0793\n",
            "Epoch [14284/20000], Training Loss: 0.0803\n",
            "Epoch [14285/20000], Training Loss: 0.0843\n",
            "Epoch [14286/20000], Training Loss: 0.0841\n",
            "Epoch [14287/20000], Training Loss: 0.0778\n",
            "Epoch [14288/20000], Training Loss: 0.0738\n",
            "Epoch [14289/20000], Training Loss: 0.0786\n",
            "Epoch [14290/20000], Training Loss: 0.0796\n",
            "Epoch [14291/20000], Training Loss: 0.0863\n",
            "Epoch [14292/20000], Training Loss: 0.0834\n",
            "Epoch [14293/20000], Training Loss: 0.0808\n",
            "Epoch [14294/20000], Training Loss: 0.0817\n",
            "Epoch [14295/20000], Training Loss: 0.0803\n",
            "Epoch [14296/20000], Training Loss: 0.0772\n",
            "Epoch [14297/20000], Training Loss: 0.0755\n",
            "Epoch [14298/20000], Training Loss: 0.0760\n",
            "Epoch [14299/20000], Training Loss: 0.0825\n",
            "Epoch [14300/20000], Training Loss: 0.0796\n",
            "Epoch [14301/20000], Training Loss: 0.0738\n",
            "Epoch [14302/20000], Training Loss: 0.0778\n",
            "Epoch [14303/20000], Training Loss: 0.0800\n",
            "Epoch [14304/20000], Training Loss: 0.0822\n",
            "Epoch [14305/20000], Training Loss: 0.0842\n",
            "Epoch [14306/20000], Training Loss: 0.0795\n",
            "Epoch [14307/20000], Training Loss: 0.0864\n",
            "Epoch [14308/20000], Training Loss: 0.0771\n",
            "Epoch [14309/20000], Training Loss: 0.0755\n",
            "Epoch [14310/20000], Training Loss: 0.0762\n",
            "Epoch [14311/20000], Training Loss: 0.0803\n",
            "Epoch [14312/20000], Training Loss: 0.0798\n",
            "Epoch [14313/20000], Training Loss: 0.0736\n",
            "Epoch [14314/20000], Training Loss: 0.0751\n",
            "Epoch [14315/20000], Training Loss: 0.0769\n",
            "Epoch [14316/20000], Training Loss: 0.0748\n",
            "Epoch [14317/20000], Training Loss: 0.0835\n",
            "Epoch [14318/20000], Training Loss: 0.0797\n",
            "Epoch [14319/20000], Training Loss: 0.0792\n",
            "Epoch [14320/20000], Training Loss: 0.0767\n",
            "Epoch [14321/20000], Training Loss: 0.0797\n",
            "Epoch [14322/20000], Training Loss: 0.0840\n",
            "Epoch [14323/20000], Training Loss: 0.0835\n",
            "Epoch [14324/20000], Training Loss: 0.0814\n",
            "Epoch [14325/20000], Training Loss: 0.0773\n",
            "Epoch [14326/20000], Training Loss: 0.0819\n",
            "Epoch [14327/20000], Training Loss: 0.0832\n",
            "Epoch [14328/20000], Training Loss: 0.0819\n",
            "Epoch [14329/20000], Training Loss: 0.0727\n",
            "Epoch [14330/20000], Training Loss: 0.0772\n",
            "Epoch [14331/20000], Training Loss: 0.0868\n",
            "Epoch [14332/20000], Training Loss: 0.0769\n",
            "Epoch [14333/20000], Training Loss: 0.0807\n",
            "Epoch [14334/20000], Training Loss: 0.0787\n",
            "Epoch [14335/20000], Training Loss: 0.0791\n",
            "Epoch [14336/20000], Training Loss: 0.0791\n",
            "Epoch [14337/20000], Training Loss: 0.0793\n",
            "Epoch [14338/20000], Training Loss: 0.0795\n",
            "Epoch [14339/20000], Training Loss: 0.0837\n",
            "Epoch [14340/20000], Training Loss: 0.0789\n",
            "Epoch [14341/20000], Training Loss: 0.0807\n",
            "Epoch [14342/20000], Training Loss: 0.0813\n",
            "Epoch [14343/20000], Training Loss: 0.0781\n",
            "Epoch [14344/20000], Training Loss: 0.0739\n",
            "Epoch [14345/20000], Training Loss: 0.0865\n",
            "Epoch [14346/20000], Training Loss: 0.0809\n",
            "Epoch [14347/20000], Training Loss: 0.0762\n",
            "Epoch [14348/20000], Training Loss: 0.0755\n",
            "Epoch [14349/20000], Training Loss: 0.0765\n",
            "Epoch [14350/20000], Training Loss: 0.0833\n",
            "Epoch [14351/20000], Training Loss: 0.0825\n",
            "Epoch [14352/20000], Training Loss: 0.0755\n",
            "Epoch [14353/20000], Training Loss: 0.0739\n",
            "Epoch [14354/20000], Training Loss: 0.0792\n",
            "Epoch [14355/20000], Training Loss: 0.0787\n",
            "Epoch [14356/20000], Training Loss: 0.0819\n",
            "Epoch [14357/20000], Training Loss: 0.0826\n",
            "Epoch [14358/20000], Training Loss: 0.0790\n",
            "Epoch [14359/20000], Training Loss: 0.0771\n",
            "Epoch [14360/20000], Training Loss: 0.0753\n",
            "Epoch [14361/20000], Training Loss: 0.0755\n",
            "Epoch [14362/20000], Training Loss: 0.0824\n",
            "Epoch [14363/20000], Training Loss: 0.0840\n",
            "Epoch [14364/20000], Training Loss: 0.0876\n",
            "Epoch [14365/20000], Training Loss: 0.0822\n",
            "Epoch [14366/20000], Training Loss: 0.0720\n",
            "Epoch [14367/20000], Training Loss: 0.0773\n",
            "Epoch [14368/20000], Training Loss: 0.0752\n",
            "Epoch [14369/20000], Training Loss: 0.0748\n",
            "Epoch [14370/20000], Training Loss: 0.0800\n",
            "Epoch [14371/20000], Training Loss: 0.0817\n",
            "Epoch [14372/20000], Training Loss: 0.0780\n",
            "Epoch [14373/20000], Training Loss: 0.0865\n",
            "Epoch [14374/20000], Training Loss: 0.0858\n",
            "Epoch [14375/20000], Training Loss: 0.0808\n",
            "Epoch [14376/20000], Training Loss: 0.0831\n",
            "Epoch [14377/20000], Training Loss: 0.0769\n",
            "Epoch [14378/20000], Training Loss: 0.0785\n",
            "Epoch [14379/20000], Training Loss: 0.0788\n",
            "Epoch [14380/20000], Training Loss: 0.0762\n",
            "Epoch [14381/20000], Training Loss: 0.0810\n",
            "Epoch [14382/20000], Training Loss: 0.0878\n",
            "Epoch [14383/20000], Training Loss: 0.0780\n",
            "Epoch [14384/20000], Training Loss: 0.0793\n",
            "Epoch [14385/20000], Training Loss: 0.0834\n",
            "Epoch [14386/20000], Training Loss: 0.0827\n",
            "Epoch [14387/20000], Training Loss: 0.0850\n",
            "Epoch [14388/20000], Training Loss: 0.0771\n",
            "Epoch [14389/20000], Training Loss: 0.0801\n",
            "Epoch [14390/20000], Training Loss: 0.0841\n",
            "Epoch [14391/20000], Training Loss: 0.0841\n",
            "Epoch [14392/20000], Training Loss: 0.0807\n",
            "Epoch [14393/20000], Training Loss: 0.0759\n",
            "Epoch [14394/20000], Training Loss: 0.0736\n",
            "Epoch [14395/20000], Training Loss: 0.0827\n",
            "Epoch [14396/20000], Training Loss: 0.0843\n",
            "Epoch [14397/20000], Training Loss: 0.0801\n",
            "Epoch [14398/20000], Training Loss: 0.0794\n",
            "Epoch [14399/20000], Training Loss: 0.0842\n",
            "Epoch [14400/20000], Training Loss: 0.0771\n",
            "Epoch [14401/20000], Training Loss: 0.0841\n",
            "Epoch [14402/20000], Training Loss: 0.0804\n",
            "Epoch [14403/20000], Training Loss: 0.0851\n",
            "Epoch [14404/20000], Training Loss: 0.0865\n",
            "Epoch [14405/20000], Training Loss: 0.0794\n",
            "Epoch [14406/20000], Training Loss: 0.0745\n",
            "Epoch [14407/20000], Training Loss: 0.0817\n",
            "Epoch [14408/20000], Training Loss: 0.0778\n",
            "Epoch [14409/20000], Training Loss: 0.0739\n",
            "Epoch [14410/20000], Training Loss: 0.0827\n",
            "Epoch [14411/20000], Training Loss: 0.0809\n",
            "Epoch [14412/20000], Training Loss: 0.0832\n",
            "Epoch [14413/20000], Training Loss: 0.0830\n",
            "Epoch [14414/20000], Training Loss: 0.0764\n",
            "Epoch [14415/20000], Training Loss: 0.0860\n",
            "Epoch [14416/20000], Training Loss: 0.0810\n",
            "Epoch [14417/20000], Training Loss: 0.0824\n",
            "Epoch [14418/20000], Training Loss: 0.0767\n",
            "Epoch [14419/20000], Training Loss: 0.0857\n",
            "Epoch [14420/20000], Training Loss: 0.0820\n",
            "Epoch [14421/20000], Training Loss: 0.0800\n",
            "Epoch [14422/20000], Training Loss: 0.0773\n",
            "Epoch [14423/20000], Training Loss: 0.0831\n",
            "Epoch [14424/20000], Training Loss: 0.0800\n",
            "Epoch [14425/20000], Training Loss: 0.0844\n",
            "Epoch [14426/20000], Training Loss: 0.0809\n",
            "Epoch [14427/20000], Training Loss: 0.0809\n",
            "Epoch [14428/20000], Training Loss: 0.0770\n",
            "Epoch [14429/20000], Training Loss: 0.0793\n",
            "Epoch [14430/20000], Training Loss: 0.0788\n",
            "Epoch [14431/20000], Training Loss: 0.0834\n",
            "Epoch [14432/20000], Training Loss: 0.0796\n",
            "Epoch [14433/20000], Training Loss: 0.0752\n",
            "Epoch [14434/20000], Training Loss: 0.0720\n",
            "Epoch [14435/20000], Training Loss: 0.0817\n",
            "Epoch [14436/20000], Training Loss: 0.0807\n",
            "Epoch [14437/20000], Training Loss: 0.0842\n",
            "Epoch [14438/20000], Training Loss: 0.0790\n",
            "Epoch [14439/20000], Training Loss: 0.0846\n",
            "Epoch [14440/20000], Training Loss: 0.0818\n",
            "Epoch [14441/20000], Training Loss: 0.0760\n",
            "Epoch [14442/20000], Training Loss: 0.0816\n",
            "Epoch [14443/20000], Training Loss: 0.0789\n",
            "Epoch [14444/20000], Training Loss: 0.0747\n",
            "Epoch [14445/20000], Training Loss: 0.0786\n",
            "Epoch [14446/20000], Training Loss: 0.0797\n",
            "Epoch [14447/20000], Training Loss: 0.0772\n",
            "Epoch [14448/20000], Training Loss: 0.0809\n",
            "Epoch [14449/20000], Training Loss: 0.0784\n",
            "Epoch [14450/20000], Training Loss: 0.0772\n",
            "Epoch [14451/20000], Training Loss: 0.0798\n",
            "Epoch [14452/20000], Training Loss: 0.0871\n",
            "Epoch [14453/20000], Training Loss: 0.0833\n",
            "Epoch [14454/20000], Training Loss: 0.0764\n",
            "Epoch [14455/20000], Training Loss: 0.0789\n",
            "Epoch [14456/20000], Training Loss: 0.0781\n",
            "Epoch [14457/20000], Training Loss: 0.0779\n",
            "Epoch [14458/20000], Training Loss: 0.0803\n",
            "Epoch [14459/20000], Training Loss: 0.0783\n",
            "Epoch [14460/20000], Training Loss: 0.0798\n",
            "Epoch [14461/20000], Training Loss: 0.0838\n",
            "Epoch [14462/20000], Training Loss: 0.0750\n",
            "Epoch [14463/20000], Training Loss: 0.0796\n",
            "Epoch [14464/20000], Training Loss: 0.0850\n",
            "Epoch [14465/20000], Training Loss: 0.0787\n",
            "Epoch [14466/20000], Training Loss: 0.0785\n",
            "Epoch [14467/20000], Training Loss: 0.0780\n",
            "Epoch [14468/20000], Training Loss: 0.0817\n",
            "Epoch [14469/20000], Training Loss: 0.0737\n",
            "Epoch [14470/20000], Training Loss: 0.0848\n",
            "Epoch [14471/20000], Training Loss: 0.0898\n",
            "Epoch [14472/20000], Training Loss: 0.0800\n",
            "Epoch [14473/20000], Training Loss: 0.0836\n",
            "Epoch [14474/20000], Training Loss: 0.0857\n",
            "Epoch [14475/20000], Training Loss: 0.0819\n",
            "Epoch [14476/20000], Training Loss: 0.0824\n",
            "Epoch [14477/20000], Training Loss: 0.0851\n",
            "Epoch [14478/20000], Training Loss: 0.0826\n",
            "Epoch [14479/20000], Training Loss: 0.0726\n",
            "Epoch [14480/20000], Training Loss: 0.0792\n",
            "Epoch [14481/20000], Training Loss: 0.0809\n",
            "Epoch [14482/20000], Training Loss: 0.0779\n",
            "Epoch [14483/20000], Training Loss: 0.0815\n",
            "Epoch [14484/20000], Training Loss: 0.0809\n",
            "Epoch [14485/20000], Training Loss: 0.0780\n",
            "Epoch [14486/20000], Training Loss: 0.0760\n",
            "Epoch [14487/20000], Training Loss: 0.0778\n",
            "Epoch [14488/20000], Training Loss: 0.0836\n",
            "Epoch [14489/20000], Training Loss: 0.0808\n",
            "Epoch [14490/20000], Training Loss: 0.0791\n",
            "Epoch [14491/20000], Training Loss: 0.0769\n",
            "Epoch [14492/20000], Training Loss: 0.0828\n",
            "Epoch [14493/20000], Training Loss: 0.0762\n",
            "Epoch [14494/20000], Training Loss: 0.0762\n",
            "Epoch [14495/20000], Training Loss: 0.0744\n",
            "Epoch [14496/20000], Training Loss: 0.0809\n",
            "Epoch [14497/20000], Training Loss: 0.0797\n",
            "Epoch [14498/20000], Training Loss: 0.0726\n",
            "Epoch [14499/20000], Training Loss: 0.0812\n",
            "Epoch [14500/20000], Training Loss: 0.0778\n",
            "Epoch [14501/20000], Training Loss: 0.0817\n",
            "Epoch [14502/20000], Training Loss: 0.0817\n",
            "Epoch [14503/20000], Training Loss: 0.0744\n",
            "Epoch [14504/20000], Training Loss: 0.0776\n",
            "Epoch [14505/20000], Training Loss: 0.0787\n",
            "Epoch [14506/20000], Training Loss: 0.0794\n",
            "Epoch [14507/20000], Training Loss: 0.0816\n",
            "Epoch [14508/20000], Training Loss: 0.0784\n",
            "Epoch [14509/20000], Training Loss: 0.0830\n",
            "Epoch [14510/20000], Training Loss: 0.0764\n",
            "Epoch [14511/20000], Training Loss: 0.0782\n",
            "Epoch [14512/20000], Training Loss: 0.0876\n",
            "Epoch [14513/20000], Training Loss: 0.0756\n",
            "Epoch [14514/20000], Training Loss: 0.0827\n",
            "Epoch [14515/20000], Training Loss: 0.0786\n",
            "Epoch [14516/20000], Training Loss: 0.0779\n",
            "Epoch [14517/20000], Training Loss: 0.0799\n",
            "Epoch [14518/20000], Training Loss: 0.0751\n",
            "Epoch [14519/20000], Training Loss: 0.0800\n",
            "Epoch [14520/20000], Training Loss: 0.0752\n",
            "Epoch [14521/20000], Training Loss: 0.0769\n",
            "Epoch [14522/20000], Training Loss: 0.0801\n",
            "Epoch [14523/20000], Training Loss: 0.0821\n",
            "Epoch [14524/20000], Training Loss: 0.0764\n",
            "Epoch [14525/20000], Training Loss: 0.0826\n",
            "Epoch [14526/20000], Training Loss: 0.0773\n",
            "Epoch [14527/20000], Training Loss: 0.0783\n",
            "Epoch [14528/20000], Training Loss: 0.0774\n",
            "Epoch [14529/20000], Training Loss: 0.0761\n",
            "Epoch [14530/20000], Training Loss: 0.0793\n",
            "Epoch [14531/20000], Training Loss: 0.0809\n",
            "Epoch [14532/20000], Training Loss: 0.0786\n",
            "Epoch [14533/20000], Training Loss: 0.0802\n",
            "Epoch [14534/20000], Training Loss: 0.0739\n",
            "Epoch [14535/20000], Training Loss: 0.0804\n",
            "Epoch [14536/20000], Training Loss: 0.0782\n",
            "Epoch [14537/20000], Training Loss: 0.0843\n",
            "Epoch [14538/20000], Training Loss: 0.0859\n",
            "Epoch [14539/20000], Training Loss: 0.0781\n",
            "Epoch [14540/20000], Training Loss: 0.0742\n",
            "Epoch [14541/20000], Training Loss: 0.0852\n",
            "Epoch [14542/20000], Training Loss: 0.0858\n",
            "Epoch [14543/20000], Training Loss: 0.0783\n",
            "Epoch [14544/20000], Training Loss: 0.0799\n",
            "Epoch [14545/20000], Training Loss: 0.0752\n",
            "Epoch [14546/20000], Training Loss: 0.0829\n",
            "Epoch [14547/20000], Training Loss: 0.0755\n",
            "Epoch [14548/20000], Training Loss: 0.0792\n",
            "Epoch [14549/20000], Training Loss: 0.0780\n",
            "Epoch [14550/20000], Training Loss: 0.0771\n",
            "Epoch [14551/20000], Training Loss: 0.0734\n",
            "Epoch [14552/20000], Training Loss: 0.0743\n",
            "Epoch [14553/20000], Training Loss: 0.0771\n",
            "Epoch [14554/20000], Training Loss: 0.0788\n",
            "Epoch [14555/20000], Training Loss: 0.0850\n",
            "Epoch [14556/20000], Training Loss: 0.0799\n",
            "Epoch [14557/20000], Training Loss: 0.0850\n",
            "Epoch [14558/20000], Training Loss: 0.0858\n",
            "Epoch [14559/20000], Training Loss: 0.0777\n",
            "Epoch [14560/20000], Training Loss: 0.0766\n",
            "Epoch [14561/20000], Training Loss: 0.0858\n",
            "Epoch [14562/20000], Training Loss: 0.0816\n",
            "Epoch [14563/20000], Training Loss: 0.0739\n",
            "Epoch [14564/20000], Training Loss: 0.0800\n",
            "Epoch [14565/20000], Training Loss: 0.0854\n",
            "Epoch [14566/20000], Training Loss: 0.0763\n",
            "Epoch [14567/20000], Training Loss: 0.0790\n",
            "Epoch [14568/20000], Training Loss: 0.0807\n",
            "Epoch [14569/20000], Training Loss: 0.0802\n",
            "Epoch [14570/20000], Training Loss: 0.0798\n",
            "Epoch [14571/20000], Training Loss: 0.0770\n",
            "Epoch [14572/20000], Training Loss: 0.0826\n",
            "Epoch [14573/20000], Training Loss: 0.0833\n",
            "Epoch [14574/20000], Training Loss: 0.0755\n",
            "Epoch [14575/20000], Training Loss: 0.0795\n",
            "Epoch [14576/20000], Training Loss: 0.0846\n",
            "Epoch [14577/20000], Training Loss: 0.0827\n",
            "Epoch [14578/20000], Training Loss: 0.0835\n",
            "Epoch [14579/20000], Training Loss: 0.0778\n",
            "Epoch [14580/20000], Training Loss: 0.0849\n",
            "Epoch [14581/20000], Training Loss: 0.0792\n",
            "Epoch [14582/20000], Training Loss: 0.0813\n",
            "Epoch [14583/20000], Training Loss: 0.0760\n",
            "Epoch [14584/20000], Training Loss: 0.0818\n",
            "Epoch [14585/20000], Training Loss: 0.0742\n",
            "Epoch [14586/20000], Training Loss: 0.0800\n",
            "Epoch [14587/20000], Training Loss: 0.0752\n",
            "Epoch [14588/20000], Training Loss: 0.0829\n",
            "Epoch [14589/20000], Training Loss: 0.0745\n",
            "Epoch [14590/20000], Training Loss: 0.0819\n",
            "Epoch [14591/20000], Training Loss: 0.0785\n",
            "Epoch [14592/20000], Training Loss: 0.0859\n",
            "Epoch [14593/20000], Training Loss: 0.0814\n",
            "Epoch [14594/20000], Training Loss: 0.0808\n",
            "Epoch [14595/20000], Training Loss: 0.0812\n",
            "Epoch [14596/20000], Training Loss: 0.0754\n",
            "Epoch [14597/20000], Training Loss: 0.0779\n",
            "Epoch [14598/20000], Training Loss: 0.0847\n",
            "Epoch [14599/20000], Training Loss: 0.0857\n",
            "Epoch [14600/20000], Training Loss: 0.0783\n",
            "Epoch [14601/20000], Training Loss: 0.0839\n",
            "Epoch [14602/20000], Training Loss: 0.0755\n",
            "Epoch [14603/20000], Training Loss: 0.0762\n",
            "Epoch [14604/20000], Training Loss: 0.0765\n",
            "Epoch [14605/20000], Training Loss: 0.0811\n",
            "Epoch [14606/20000], Training Loss: 0.0831\n",
            "Epoch [14607/20000], Training Loss: 0.0786\n",
            "Epoch [14608/20000], Training Loss: 0.0797\n",
            "Epoch [14609/20000], Training Loss: 0.0763\n",
            "Epoch [14610/20000], Training Loss: 0.0816\n",
            "Epoch [14611/20000], Training Loss: 0.0834\n",
            "Epoch [14612/20000], Training Loss: 0.0752\n",
            "Epoch [14613/20000], Training Loss: 0.0769\n",
            "Epoch [14614/20000], Training Loss: 0.0810\n",
            "Epoch [14615/20000], Training Loss: 0.0813\n",
            "Epoch [14616/20000], Training Loss: 0.0868\n",
            "Epoch [14617/20000], Training Loss: 0.0820\n",
            "Epoch [14618/20000], Training Loss: 0.0814\n",
            "Epoch [14619/20000], Training Loss: 0.0777\n",
            "Epoch [14620/20000], Training Loss: 0.0802\n",
            "Epoch [14621/20000], Training Loss: 0.0871\n",
            "Epoch [14622/20000], Training Loss: 0.0824\n",
            "Epoch [14623/20000], Training Loss: 0.0798\n",
            "Epoch [14624/20000], Training Loss: 0.0766\n",
            "Epoch [14625/20000], Training Loss: 0.0834\n",
            "Epoch [14626/20000], Training Loss: 0.0787\n",
            "Epoch [14627/20000], Training Loss: 0.0798\n",
            "Epoch [14628/20000], Training Loss: 0.0838\n",
            "Epoch [14629/20000], Training Loss: 0.0876\n",
            "Epoch [14630/20000], Training Loss: 0.0769\n",
            "Epoch [14631/20000], Training Loss: 0.0803\n",
            "Epoch [14632/20000], Training Loss: 0.0811\n",
            "Epoch [14633/20000], Training Loss: 0.0759\n",
            "Epoch [14634/20000], Training Loss: 0.0795\n",
            "Epoch [14635/20000], Training Loss: 0.0747\n",
            "Epoch [14636/20000], Training Loss: 0.0809\n",
            "Epoch [14637/20000], Training Loss: 0.0785\n",
            "Epoch [14638/20000], Training Loss: 0.0868\n",
            "Epoch [14639/20000], Training Loss: 0.0754\n",
            "Epoch [14640/20000], Training Loss: 0.0798\n",
            "Epoch [14641/20000], Training Loss: 0.0784\n",
            "Epoch [14642/20000], Training Loss: 0.0821\n",
            "Epoch [14643/20000], Training Loss: 0.0821\n",
            "Epoch [14644/20000], Training Loss: 0.0791\n",
            "Epoch [14645/20000], Training Loss: 0.0774\n",
            "Epoch [14646/20000], Training Loss: 0.0761\n",
            "Epoch [14647/20000], Training Loss: 0.0812\n",
            "Epoch [14648/20000], Training Loss: 0.0752\n",
            "Epoch [14649/20000], Training Loss: 0.0778\n",
            "Epoch [14650/20000], Training Loss: 0.0785\n",
            "Epoch [14651/20000], Training Loss: 0.0769\n",
            "Epoch [14652/20000], Training Loss: 0.0764\n",
            "Epoch [14653/20000], Training Loss: 0.0851\n",
            "Epoch [14654/20000], Training Loss: 0.0758\n",
            "Epoch [14655/20000], Training Loss: 0.0853\n",
            "Epoch [14656/20000], Training Loss: 0.0814\n",
            "Epoch [14657/20000], Training Loss: 0.0803\n",
            "Epoch [14658/20000], Training Loss: 0.0858\n",
            "Epoch [14659/20000], Training Loss: 0.0745\n",
            "Epoch [14660/20000], Training Loss: 0.0856\n",
            "Epoch [14661/20000], Training Loss: 0.0768\n",
            "Epoch [14662/20000], Training Loss: 0.0861\n",
            "Epoch [14663/20000], Training Loss: 0.0812\n",
            "Epoch [14664/20000], Training Loss: 0.0787\n",
            "Epoch [14665/20000], Training Loss: 0.0852\n",
            "Epoch [14666/20000], Training Loss: 0.0748\n",
            "Epoch [14667/20000], Training Loss: 0.0773\n",
            "Epoch [14668/20000], Training Loss: 0.0793\n",
            "Epoch [14669/20000], Training Loss: 0.0801\n",
            "Epoch [14670/20000], Training Loss: 0.0843\n",
            "Epoch [14671/20000], Training Loss: 0.0757\n",
            "Epoch [14672/20000], Training Loss: 0.0776\n",
            "Epoch [14673/20000], Training Loss: 0.0773\n",
            "Epoch [14674/20000], Training Loss: 0.0768\n",
            "Epoch [14675/20000], Training Loss: 0.0806\n",
            "Epoch [14676/20000], Training Loss: 0.0805\n",
            "Epoch [14677/20000], Training Loss: 0.0858\n",
            "Epoch [14678/20000], Training Loss: 0.0732\n",
            "Epoch [14679/20000], Training Loss: 0.0809\n",
            "Epoch [14680/20000], Training Loss: 0.0717\n",
            "Epoch [14681/20000], Training Loss: 0.0760\n",
            "Epoch [14682/20000], Training Loss: 0.0793\n",
            "Epoch [14683/20000], Training Loss: 0.0828\n",
            "Epoch [14684/20000], Training Loss: 0.0795\n",
            "Epoch [14685/20000], Training Loss: 0.0813\n",
            "Epoch [14686/20000], Training Loss: 0.0752\n",
            "Epoch [14687/20000], Training Loss: 0.0779\n",
            "Epoch [14688/20000], Training Loss: 0.0727\n",
            "Epoch [14689/20000], Training Loss: 0.0821\n",
            "Epoch [14690/20000], Training Loss: 0.0804\n",
            "Epoch [14691/20000], Training Loss: 0.0858\n",
            "Epoch [14692/20000], Training Loss: 0.0770\n",
            "Epoch [14693/20000], Training Loss: 0.0787\n",
            "Epoch [14694/20000], Training Loss: 0.0745\n",
            "Epoch [14695/20000], Training Loss: 0.0749\n",
            "Epoch [14696/20000], Training Loss: 0.0816\n",
            "Epoch [14697/20000], Training Loss: 0.0761\n",
            "Epoch [14698/20000], Training Loss: 0.0781\n",
            "Epoch [14699/20000], Training Loss: 0.0825\n",
            "Epoch [14700/20000], Training Loss: 0.0900\n",
            "Epoch [14701/20000], Training Loss: 0.0799\n",
            "Epoch [14702/20000], Training Loss: 0.0730\n",
            "Epoch [14703/20000], Training Loss: 0.0797\n",
            "Epoch [14704/20000], Training Loss: 0.0797\n",
            "Epoch [14705/20000], Training Loss: 0.0774\n",
            "Epoch [14706/20000], Training Loss: 0.0763\n",
            "Epoch [14707/20000], Training Loss: 0.0815\n",
            "Epoch [14708/20000], Training Loss: 0.0847\n",
            "Epoch [14709/20000], Training Loss: 0.0784\n",
            "Epoch [14710/20000], Training Loss: 0.0830\n",
            "Epoch [14711/20000], Training Loss: 0.0856\n",
            "Epoch [14712/20000], Training Loss: 0.0773\n",
            "Epoch [14713/20000], Training Loss: 0.0799\n",
            "Epoch [14714/20000], Training Loss: 0.0816\n",
            "Epoch [14715/20000], Training Loss: 0.0832\n",
            "Epoch [14716/20000], Training Loss: 0.0794\n",
            "Epoch [14717/20000], Training Loss: 0.0750\n",
            "Epoch [14718/20000], Training Loss: 0.0818\n",
            "Epoch [14719/20000], Training Loss: 0.0766\n",
            "Epoch [14720/20000], Training Loss: 0.0810\n",
            "Epoch [14721/20000], Training Loss: 0.0787\n",
            "Epoch [14722/20000], Training Loss: 0.0788\n",
            "Epoch [14723/20000], Training Loss: 0.0798\n",
            "Epoch [14724/20000], Training Loss: 0.0834\n",
            "Epoch [14725/20000], Training Loss: 0.0829\n",
            "Epoch [14726/20000], Training Loss: 0.0882\n",
            "Epoch [14727/20000], Training Loss: 0.0787\n",
            "Epoch [14728/20000], Training Loss: 0.0799\n",
            "Epoch [14729/20000], Training Loss: 0.0799\n",
            "Epoch [14730/20000], Training Loss: 0.0814\n",
            "Epoch [14731/20000], Training Loss: 0.0813\n",
            "Epoch [14732/20000], Training Loss: 0.0776\n",
            "Epoch [14733/20000], Training Loss: 0.0746\n",
            "Epoch [14734/20000], Training Loss: 0.0831\n",
            "Epoch [14735/20000], Training Loss: 0.0787\n",
            "Epoch [14736/20000], Training Loss: 0.0802\n",
            "Epoch [14737/20000], Training Loss: 0.0835\n",
            "Epoch [14738/20000], Training Loss: 0.0840\n",
            "Epoch [14739/20000], Training Loss: 0.0794\n",
            "Epoch [14740/20000], Training Loss: 0.0801\n",
            "Epoch [14741/20000], Training Loss: 0.0803\n",
            "Epoch [14742/20000], Training Loss: 0.0774\n",
            "Epoch [14743/20000], Training Loss: 0.0842\n",
            "Epoch [14744/20000], Training Loss: 0.0758\n",
            "Epoch [14745/20000], Training Loss: 0.0787\n",
            "Epoch [14746/20000], Training Loss: 0.0819\n",
            "Epoch [14747/20000], Training Loss: 0.0767\n",
            "Epoch [14748/20000], Training Loss: 0.0829\n",
            "Epoch [14749/20000], Training Loss: 0.0787\n",
            "Epoch [14750/20000], Training Loss: 0.0815\n",
            "Epoch [14751/20000], Training Loss: 0.0740\n",
            "Epoch [14752/20000], Training Loss: 0.0747\n",
            "Epoch [14753/20000], Training Loss: 0.0776\n",
            "Epoch [14754/20000], Training Loss: 0.0781\n",
            "Epoch [14755/20000], Training Loss: 0.0771\n",
            "Epoch [14756/20000], Training Loss: 0.0822\n",
            "Epoch [14757/20000], Training Loss: 0.0791\n",
            "Epoch [14758/20000], Training Loss: 0.0805\n",
            "Epoch [14759/20000], Training Loss: 0.0796\n",
            "Epoch [14760/20000], Training Loss: 0.0760\n",
            "Epoch [14761/20000], Training Loss: 0.0731\n",
            "Epoch [14762/20000], Training Loss: 0.0800\n",
            "Epoch [14763/20000], Training Loss: 0.0810\n",
            "Epoch [14764/20000], Training Loss: 0.0768\n",
            "Epoch [14765/20000], Training Loss: 0.0783\n",
            "Epoch [14766/20000], Training Loss: 0.0784\n",
            "Epoch [14767/20000], Training Loss: 0.0819\n",
            "Epoch [14768/20000], Training Loss: 0.0844\n",
            "Epoch [14769/20000], Training Loss: 0.0793\n",
            "Epoch [14770/20000], Training Loss: 0.0847\n",
            "Epoch [14771/20000], Training Loss: 0.0779\n",
            "Epoch [14772/20000], Training Loss: 0.0850\n",
            "Epoch [14773/20000], Training Loss: 0.0795\n",
            "Epoch [14774/20000], Training Loss: 0.0827\n",
            "Epoch [14775/20000], Training Loss: 0.0827\n",
            "Epoch [14776/20000], Training Loss: 0.0769\n",
            "Epoch [14777/20000], Training Loss: 0.0842\n",
            "Epoch [14778/20000], Training Loss: 0.0860\n",
            "Epoch [14779/20000], Training Loss: 0.0872\n",
            "Epoch [14780/20000], Training Loss: 0.0750\n",
            "Epoch [14781/20000], Training Loss: 0.0734\n",
            "Epoch [14782/20000], Training Loss: 0.0785\n",
            "Epoch [14783/20000], Training Loss: 0.0753\n",
            "Epoch [14784/20000], Training Loss: 0.0766\n",
            "Epoch [14785/20000], Training Loss: 0.0783\n",
            "Epoch [14786/20000], Training Loss: 0.0806\n",
            "Epoch [14787/20000], Training Loss: 0.0825\n",
            "Epoch [14788/20000], Training Loss: 0.0856\n",
            "Epoch [14789/20000], Training Loss: 0.0813\n",
            "Epoch [14790/20000], Training Loss: 0.0831\n",
            "Epoch [14791/20000], Training Loss: 0.0859\n",
            "Epoch [14792/20000], Training Loss: 0.0819\n",
            "Epoch [14793/20000], Training Loss: 0.0801\n",
            "Epoch [14794/20000], Training Loss: 0.0784\n",
            "Epoch [14795/20000], Training Loss: 0.0754\n",
            "Epoch [14796/20000], Training Loss: 0.0826\n",
            "Epoch [14797/20000], Training Loss: 0.0731\n",
            "Epoch [14798/20000], Training Loss: 0.0793\n",
            "Epoch [14799/20000], Training Loss: 0.0775\n",
            "Epoch [14800/20000], Training Loss: 0.0777\n",
            "Epoch [14801/20000], Training Loss: 0.0830\n",
            "Epoch [14802/20000], Training Loss: 0.0844\n",
            "Epoch [14803/20000], Training Loss: 0.0736\n",
            "Epoch [14804/20000], Training Loss: 0.0807\n",
            "Epoch [14805/20000], Training Loss: 0.0789\n",
            "Epoch [14806/20000], Training Loss: 0.0767\n",
            "Epoch [14807/20000], Training Loss: 0.0731\n",
            "Epoch [14808/20000], Training Loss: 0.0830\n",
            "Epoch [14809/20000], Training Loss: 0.0773\n",
            "Epoch [14810/20000], Training Loss: 0.0749\n",
            "Epoch [14811/20000], Training Loss: 0.0867\n",
            "Epoch [14812/20000], Training Loss: 0.0738\n",
            "Epoch [14813/20000], Training Loss: 0.0811\n",
            "Epoch [14814/20000], Training Loss: 0.0849\n",
            "Epoch [14815/20000], Training Loss: 0.0803\n",
            "Epoch [14816/20000], Training Loss: 0.0736\n",
            "Epoch [14817/20000], Training Loss: 0.0823\n",
            "Epoch [14818/20000], Training Loss: 0.0788\n",
            "Epoch [14819/20000], Training Loss: 0.0836\n",
            "Epoch [14820/20000], Training Loss: 0.0806\n",
            "Epoch [14821/20000], Training Loss: 0.0791\n",
            "Epoch [14822/20000], Training Loss: 0.0852\n",
            "Epoch [14823/20000], Training Loss: 0.0770\n",
            "Epoch [14824/20000], Training Loss: 0.0787\n",
            "Epoch [14825/20000], Training Loss: 0.0806\n",
            "Epoch [14826/20000], Training Loss: 0.0767\n",
            "Epoch [14827/20000], Training Loss: 0.0758\n",
            "Epoch [14828/20000], Training Loss: 0.0766\n",
            "Epoch [14829/20000], Training Loss: 0.0744\n",
            "Epoch [14830/20000], Training Loss: 0.0788\n",
            "Epoch [14831/20000], Training Loss: 0.0800\n",
            "Epoch [14832/20000], Training Loss: 0.0830\n",
            "Epoch [14833/20000], Training Loss: 0.0812\n",
            "Epoch [14834/20000], Training Loss: 0.0818\n",
            "Epoch [14835/20000], Training Loss: 0.0714\n",
            "Epoch [14836/20000], Training Loss: 0.0731\n",
            "Epoch [14837/20000], Training Loss: 0.0780\n",
            "Epoch [14838/20000], Training Loss: 0.0769\n",
            "Epoch [14839/20000], Training Loss: 0.0835\n",
            "Epoch [14840/20000], Training Loss: 0.0823\n",
            "Epoch [14841/20000], Training Loss: 0.0793\n",
            "Epoch [14842/20000], Training Loss: 0.0855\n",
            "Epoch [14843/20000], Training Loss: 0.0747\n",
            "Epoch [14844/20000], Training Loss: 0.0819\n",
            "Epoch [14845/20000], Training Loss: 0.0769\n",
            "Epoch [14846/20000], Training Loss: 0.0774\n",
            "Epoch [14847/20000], Training Loss: 0.0836\n",
            "Epoch [14848/20000], Training Loss: 0.0839\n",
            "Epoch [14849/20000], Training Loss: 0.0831\n",
            "Epoch [14850/20000], Training Loss: 0.0739\n",
            "Epoch [14851/20000], Training Loss: 0.0809\n",
            "Epoch [14852/20000], Training Loss: 0.0815\n",
            "Epoch [14853/20000], Training Loss: 0.0793\n",
            "Epoch [14854/20000], Training Loss: 0.0859\n",
            "Epoch [14855/20000], Training Loss: 0.0748\n",
            "Epoch [14856/20000], Training Loss: 0.0771\n",
            "Epoch [14857/20000], Training Loss: 0.0811\n",
            "Epoch [14858/20000], Training Loss: 0.0739\n",
            "Epoch [14859/20000], Training Loss: 0.0778\n",
            "Epoch [14860/20000], Training Loss: 0.0725\n",
            "Epoch [14861/20000], Training Loss: 0.0839\n",
            "Epoch [14862/20000], Training Loss: 0.0785\n",
            "Epoch [14863/20000], Training Loss: 0.0775\n",
            "Epoch [14864/20000], Training Loss: 0.0804\n",
            "Epoch [14865/20000], Training Loss: 0.0872\n",
            "Epoch [14866/20000], Training Loss: 0.0762\n",
            "Epoch [14867/20000], Training Loss: 0.0801\n",
            "Epoch [14868/20000], Training Loss: 0.0853\n",
            "Epoch [14869/20000], Training Loss: 0.0774\n",
            "Epoch [14870/20000], Training Loss: 0.0730\n",
            "Epoch [14871/20000], Training Loss: 0.0738\n",
            "Epoch [14872/20000], Training Loss: 0.0786\n",
            "Epoch [14873/20000], Training Loss: 0.0830\n",
            "Epoch [14874/20000], Training Loss: 0.0754\n",
            "Epoch [14875/20000], Training Loss: 0.0827\n",
            "Epoch [14876/20000], Training Loss: 0.0850\n",
            "Epoch [14877/20000], Training Loss: 0.0724\n",
            "Epoch [14878/20000], Training Loss: 0.0781\n",
            "Epoch [14879/20000], Training Loss: 0.0780\n",
            "Epoch [14880/20000], Training Loss: 0.0823\n",
            "Epoch [14881/20000], Training Loss: 0.0811\n",
            "Epoch [14882/20000], Training Loss: 0.0829\n",
            "Epoch [14883/20000], Training Loss: 0.0788\n",
            "Epoch [14884/20000], Training Loss: 0.0748\n",
            "Epoch [14885/20000], Training Loss: 0.0741\n",
            "Epoch [14886/20000], Training Loss: 0.0772\n",
            "Epoch [14887/20000], Training Loss: 0.0842\n",
            "Epoch [14888/20000], Training Loss: 0.0859\n",
            "Epoch [14889/20000], Training Loss: 0.0805\n",
            "Epoch [14890/20000], Training Loss: 0.0761\n",
            "Epoch [14891/20000], Training Loss: 0.0768\n",
            "Epoch [14892/20000], Training Loss: 0.0837\n",
            "Epoch [14893/20000], Training Loss: 0.0769\n",
            "Epoch [14894/20000], Training Loss: 0.0808\n",
            "Epoch [14895/20000], Training Loss: 0.0805\n",
            "Epoch [14896/20000], Training Loss: 0.0851\n",
            "Epoch [14897/20000], Training Loss: 0.0748\n",
            "Epoch [14898/20000], Training Loss: 0.0798\n",
            "Epoch [14899/20000], Training Loss: 0.0783\n",
            "Epoch [14900/20000], Training Loss: 0.0804\n",
            "Epoch [14901/20000], Training Loss: 0.0762\n",
            "Epoch [14902/20000], Training Loss: 0.0773\n",
            "Epoch [14903/20000], Training Loss: 0.0768\n",
            "Epoch [14904/20000], Training Loss: 0.0748\n",
            "Epoch [14905/20000], Training Loss: 0.0829\n",
            "Epoch [14906/20000], Training Loss: 0.0750\n",
            "Epoch [14907/20000], Training Loss: 0.0806\n",
            "Epoch [14908/20000], Training Loss: 0.0810\n",
            "Epoch [14909/20000], Training Loss: 0.0743\n",
            "Epoch [14910/20000], Training Loss: 0.0751\n",
            "Epoch [14911/20000], Training Loss: 0.0782\n",
            "Epoch [14912/20000], Training Loss: 0.0793\n",
            "Epoch [14913/20000], Training Loss: 0.0823\n",
            "Epoch [14914/20000], Training Loss: 0.0783\n",
            "Epoch [14915/20000], Training Loss: 0.0850\n",
            "Epoch [14916/20000], Training Loss: 0.0776\n",
            "Epoch [14917/20000], Training Loss: 0.0804\n",
            "Epoch [14918/20000], Training Loss: 0.0782\n",
            "Epoch [14919/20000], Training Loss: 0.0782\n",
            "Epoch [14920/20000], Training Loss: 0.0741\n",
            "Epoch [14921/20000], Training Loss: 0.0798\n",
            "Epoch [14922/20000], Training Loss: 0.0833\n",
            "Epoch [14923/20000], Training Loss: 0.0809\n",
            "Epoch [14924/20000], Training Loss: 0.0815\n",
            "Epoch [14925/20000], Training Loss: 0.0763\n",
            "Epoch [14926/20000], Training Loss: 0.0771\n",
            "Epoch [14927/20000], Training Loss: 0.0723\n",
            "Epoch [14928/20000], Training Loss: 0.0791\n",
            "Epoch [14929/20000], Training Loss: 0.0733\n",
            "Epoch [14930/20000], Training Loss: 0.0751\n",
            "Epoch [14931/20000], Training Loss: 0.0863\n",
            "Epoch [14932/20000], Training Loss: 0.0793\n",
            "Epoch [14933/20000], Training Loss: 0.0853\n",
            "Epoch [14934/20000], Training Loss: 0.0817\n",
            "Epoch [14935/20000], Training Loss: 0.0827\n",
            "Epoch [14936/20000], Training Loss: 0.0843\n",
            "Epoch [14937/20000], Training Loss: 0.0739\n",
            "Epoch [14938/20000], Training Loss: 0.0848\n",
            "Epoch [14939/20000], Training Loss: 0.0776\n",
            "Epoch [14940/20000], Training Loss: 0.0876\n",
            "Epoch [14941/20000], Training Loss: 0.0855\n",
            "Epoch [14942/20000], Training Loss: 0.0822\n",
            "Epoch [14943/20000], Training Loss: 0.0802\n",
            "Epoch [14944/20000], Training Loss: 0.0828\n",
            "Epoch [14945/20000], Training Loss: 0.0772\n",
            "Epoch [14946/20000], Training Loss: 0.0804\n",
            "Epoch [14947/20000], Training Loss: 0.0875\n",
            "Epoch [14948/20000], Training Loss: 0.0760\n",
            "Epoch [14949/20000], Training Loss: 0.0754\n",
            "Epoch [14950/20000], Training Loss: 0.0768\n",
            "Epoch [14951/20000], Training Loss: 0.0763\n",
            "Epoch [14952/20000], Training Loss: 0.0776\n",
            "Epoch [14953/20000], Training Loss: 0.0811\n",
            "Epoch [14954/20000], Training Loss: 0.0832\n",
            "Epoch [14955/20000], Training Loss: 0.0739\n",
            "Epoch [14956/20000], Training Loss: 0.0834\n",
            "Epoch [14957/20000], Training Loss: 0.0858\n",
            "Epoch [14958/20000], Training Loss: 0.0795\n",
            "Epoch [14959/20000], Training Loss: 0.0856\n",
            "Epoch [14960/20000], Training Loss: 0.0844\n",
            "Epoch [14961/20000], Training Loss: 0.0736\n",
            "Epoch [14962/20000], Training Loss: 0.0808\n",
            "Epoch [14963/20000], Training Loss: 0.0807\n",
            "Epoch [14964/20000], Training Loss: 0.0801\n",
            "Epoch [14965/20000], Training Loss: 0.0811\n",
            "Epoch [14966/20000], Training Loss: 0.0823\n",
            "Epoch [14967/20000], Training Loss: 0.0865\n",
            "Epoch [14968/20000], Training Loss: 0.0816\n",
            "Epoch [14969/20000], Training Loss: 0.0719\n",
            "Epoch [14970/20000], Training Loss: 0.0853\n",
            "Epoch [14971/20000], Training Loss: 0.0790\n",
            "Epoch [14972/20000], Training Loss: 0.0772\n",
            "Epoch [14973/20000], Training Loss: 0.0810\n",
            "Epoch [14974/20000], Training Loss: 0.0791\n",
            "Epoch [14975/20000], Training Loss: 0.0830\n",
            "Epoch [14976/20000], Training Loss: 0.0844\n",
            "Epoch [14977/20000], Training Loss: 0.0796\n",
            "Epoch [14978/20000], Training Loss: 0.0807\n",
            "Epoch [14979/20000], Training Loss: 0.0821\n",
            "Epoch [14980/20000], Training Loss: 0.0843\n",
            "Epoch [14981/20000], Training Loss: 0.0802\n",
            "Epoch [14982/20000], Training Loss: 0.0735\n",
            "Epoch [14983/20000], Training Loss: 0.0840\n",
            "Epoch [14984/20000], Training Loss: 0.0778\n",
            "Epoch [14985/20000], Training Loss: 0.0866\n",
            "Epoch [14986/20000], Training Loss: 0.0794\n",
            "Epoch [14987/20000], Training Loss: 0.0840\n",
            "Epoch [14988/20000], Training Loss: 0.0833\n",
            "Epoch [14989/20000], Training Loss: 0.0780\n",
            "Epoch [14990/20000], Training Loss: 0.0835\n",
            "Epoch [14991/20000], Training Loss: 0.0785\n",
            "Epoch [14992/20000], Training Loss: 0.0775\n",
            "Epoch [14993/20000], Training Loss: 0.0773\n",
            "Epoch [14994/20000], Training Loss: 0.0818\n",
            "Epoch [14995/20000], Training Loss: 0.0781\n",
            "Epoch [14996/20000], Training Loss: 0.0803\n",
            "Epoch [14997/20000], Training Loss: 0.0726\n",
            "Epoch [14998/20000], Training Loss: 0.0818\n",
            "Epoch [14999/20000], Training Loss: 0.0747\n",
            "Epoch [15000/20000], Training Loss: 0.0810\n",
            "Epoch [15001/20000], Training Loss: 0.0769\n",
            "Epoch [15002/20000], Training Loss: 0.0804\n",
            "Epoch [15003/20000], Training Loss: 0.0836\n",
            "Epoch [15004/20000], Training Loss: 0.0787\n",
            "Epoch [15005/20000], Training Loss: 0.0800\n",
            "Epoch [15006/20000], Training Loss: 0.0792\n",
            "Epoch [15007/20000], Training Loss: 0.0733\n",
            "Epoch [15008/20000], Training Loss: 0.0794\n",
            "Epoch [15009/20000], Training Loss: 0.0738\n",
            "Epoch [15010/20000], Training Loss: 0.0807\n",
            "Epoch [15011/20000], Training Loss: 0.0818\n",
            "Epoch [15012/20000], Training Loss: 0.0751\n",
            "Epoch [15013/20000], Training Loss: 0.0864\n",
            "Epoch [15014/20000], Training Loss: 0.0838\n",
            "Epoch [15015/20000], Training Loss: 0.0790\n",
            "Epoch [15016/20000], Training Loss: 0.0855\n",
            "Epoch [15017/20000], Training Loss: 0.0742\n",
            "Epoch [15018/20000], Training Loss: 0.0850\n",
            "Epoch [15019/20000], Training Loss: 0.0776\n",
            "Epoch [15020/20000], Training Loss: 0.0755\n",
            "Epoch [15021/20000], Training Loss: 0.0794\n",
            "Epoch [15022/20000], Training Loss: 0.0748\n",
            "Epoch [15023/20000], Training Loss: 0.0830\n",
            "Epoch [15024/20000], Training Loss: 0.0787\n",
            "Epoch [15025/20000], Training Loss: 0.0802\n",
            "Epoch [15026/20000], Training Loss: 0.0840\n",
            "Epoch [15027/20000], Training Loss: 0.0804\n",
            "Epoch [15028/20000], Training Loss: 0.0794\n",
            "Epoch [15029/20000], Training Loss: 0.0775\n",
            "Epoch [15030/20000], Training Loss: 0.0758\n",
            "Epoch [15031/20000], Training Loss: 0.0792\n",
            "Epoch [15032/20000], Training Loss: 0.0787\n",
            "Epoch [15033/20000], Training Loss: 0.0768\n",
            "Epoch [15034/20000], Training Loss: 0.0798\n",
            "Epoch [15035/20000], Training Loss: 0.0759\n",
            "Epoch [15036/20000], Training Loss: 0.0785\n",
            "Epoch [15037/20000], Training Loss: 0.0843\n",
            "Epoch [15038/20000], Training Loss: 0.0756\n",
            "Epoch [15039/20000], Training Loss: 0.0780\n",
            "Epoch [15040/20000], Training Loss: 0.0801\n",
            "Epoch [15041/20000], Training Loss: 0.0795\n",
            "Epoch [15042/20000], Training Loss: 0.0758\n",
            "Epoch [15043/20000], Training Loss: 0.0812\n",
            "Epoch [15044/20000], Training Loss: 0.0818\n",
            "Epoch [15045/20000], Training Loss: 0.0729\n",
            "Epoch [15046/20000], Training Loss: 0.0752\n",
            "Epoch [15047/20000], Training Loss: 0.0785\n",
            "Epoch [15048/20000], Training Loss: 0.0812\n",
            "Epoch [15049/20000], Training Loss: 0.0757\n",
            "Epoch [15050/20000], Training Loss: 0.0823\n",
            "Epoch [15051/20000], Training Loss: 0.0755\n",
            "Epoch [15052/20000], Training Loss: 0.0806\n",
            "Epoch [15053/20000], Training Loss: 0.0758\n",
            "Epoch [15054/20000], Training Loss: 0.0811\n",
            "Epoch [15055/20000], Training Loss: 0.0740\n",
            "Epoch [15056/20000], Training Loss: 0.0869\n",
            "Epoch [15057/20000], Training Loss: 0.0740\n",
            "Epoch [15058/20000], Training Loss: 0.0747\n",
            "Epoch [15059/20000], Training Loss: 0.0787\n",
            "Epoch [15060/20000], Training Loss: 0.0824\n",
            "Epoch [15061/20000], Training Loss: 0.0764\n",
            "Epoch [15062/20000], Training Loss: 0.0743\n",
            "Epoch [15063/20000], Training Loss: 0.0815\n",
            "Epoch [15064/20000], Training Loss: 0.0823\n",
            "Epoch [15065/20000], Training Loss: 0.0862\n",
            "Epoch [15066/20000], Training Loss: 0.0803\n",
            "Epoch [15067/20000], Training Loss: 0.0805\n",
            "Epoch [15068/20000], Training Loss: 0.0738\n",
            "Epoch [15069/20000], Training Loss: 0.0745\n",
            "Epoch [15070/20000], Training Loss: 0.0769\n",
            "Epoch [15071/20000], Training Loss: 0.0790\n",
            "Epoch [15072/20000], Training Loss: 0.0828\n",
            "Epoch [15073/20000], Training Loss: 0.0750\n",
            "Epoch [15074/20000], Training Loss: 0.0828\n",
            "Epoch [15075/20000], Training Loss: 0.0820\n",
            "Epoch [15076/20000], Training Loss: 0.0766\n",
            "Epoch [15077/20000], Training Loss: 0.0754\n",
            "Epoch [15078/20000], Training Loss: 0.0769\n",
            "Epoch [15079/20000], Training Loss: 0.0829\n",
            "Epoch [15080/20000], Training Loss: 0.0853\n",
            "Epoch [15081/20000], Training Loss: 0.0758\n",
            "Epoch [15082/20000], Training Loss: 0.0751\n",
            "Epoch [15083/20000], Training Loss: 0.0859\n",
            "Epoch [15084/20000], Training Loss: 0.0856\n",
            "Epoch [15085/20000], Training Loss: 0.0823\n",
            "Epoch [15086/20000], Training Loss: 0.0815\n",
            "Epoch [15087/20000], Training Loss: 0.0825\n",
            "Epoch [15088/20000], Training Loss: 0.0746\n",
            "Epoch [15089/20000], Training Loss: 0.0815\n",
            "Epoch [15090/20000], Training Loss: 0.0808\n",
            "Epoch [15091/20000], Training Loss: 0.0852\n",
            "Epoch [15092/20000], Training Loss: 0.0861\n",
            "Epoch [15093/20000], Training Loss: 0.0827\n",
            "Epoch [15094/20000], Training Loss: 0.0771\n",
            "Epoch [15095/20000], Training Loss: 0.0841\n",
            "Epoch [15096/20000], Training Loss: 0.0808\n",
            "Epoch [15097/20000], Training Loss: 0.0751\n",
            "Epoch [15098/20000], Training Loss: 0.0771\n",
            "Epoch [15099/20000], Training Loss: 0.0784\n",
            "Epoch [15100/20000], Training Loss: 0.0739\n",
            "Epoch [15101/20000], Training Loss: 0.0722\n",
            "Epoch [15102/20000], Training Loss: 0.0755\n",
            "Epoch [15103/20000], Training Loss: 0.0783\n",
            "Epoch [15104/20000], Training Loss: 0.0781\n",
            "Epoch [15105/20000], Training Loss: 0.0823\n",
            "Epoch [15106/20000], Training Loss: 0.0819\n",
            "Epoch [15107/20000], Training Loss: 0.0777\n",
            "Epoch [15108/20000], Training Loss: 0.0806\n",
            "Epoch [15109/20000], Training Loss: 0.0794\n",
            "Epoch [15110/20000], Training Loss: 0.0794\n",
            "Epoch [15111/20000], Training Loss: 0.0795\n",
            "Epoch [15112/20000], Training Loss: 0.0847\n",
            "Epoch [15113/20000], Training Loss: 0.0826\n",
            "Epoch [15114/20000], Training Loss: 0.0805\n",
            "Epoch [15115/20000], Training Loss: 0.0822\n",
            "Epoch [15116/20000], Training Loss: 0.0758\n",
            "Epoch [15117/20000], Training Loss: 0.0845\n",
            "Epoch [15118/20000], Training Loss: 0.0779\n",
            "Epoch [15119/20000], Training Loss: 0.0751\n",
            "Epoch [15120/20000], Training Loss: 0.0763\n",
            "Epoch [15121/20000], Training Loss: 0.0844\n",
            "Epoch [15122/20000], Training Loss: 0.0800\n",
            "Epoch [15123/20000], Training Loss: 0.0789\n",
            "Epoch [15124/20000], Training Loss: 0.0771\n",
            "Epoch [15125/20000], Training Loss: 0.0755\n",
            "Epoch [15126/20000], Training Loss: 0.0811\n",
            "Epoch [15127/20000], Training Loss: 0.0785\n",
            "Epoch [15128/20000], Training Loss: 0.0786\n",
            "Epoch [15129/20000], Training Loss: 0.0846\n",
            "Epoch [15130/20000], Training Loss: 0.0766\n",
            "Epoch [15131/20000], Training Loss: 0.0822\n",
            "Epoch [15132/20000], Training Loss: 0.0800\n",
            "Epoch [15133/20000], Training Loss: 0.0823\n",
            "Epoch [15134/20000], Training Loss: 0.0759\n",
            "Epoch [15135/20000], Training Loss: 0.0757\n",
            "Epoch [15136/20000], Training Loss: 0.0783\n",
            "Epoch [15137/20000], Training Loss: 0.0834\n",
            "Epoch [15138/20000], Training Loss: 0.0761\n",
            "Epoch [15139/20000], Training Loss: 0.0800\n",
            "Epoch [15140/20000], Training Loss: 0.0790\n",
            "Epoch [15141/20000], Training Loss: 0.0824\n",
            "Epoch [15142/20000], Training Loss: 0.0740\n",
            "Epoch [15143/20000], Training Loss: 0.0852\n",
            "Epoch [15144/20000], Training Loss: 0.0784\n",
            "Epoch [15145/20000], Training Loss: 0.0813\n",
            "Epoch [15146/20000], Training Loss: 0.0787\n",
            "Epoch [15147/20000], Training Loss: 0.0803\n",
            "Epoch [15148/20000], Training Loss: 0.0745\n",
            "Epoch [15149/20000], Training Loss: 0.0801\n",
            "Epoch [15150/20000], Training Loss: 0.0750\n",
            "Epoch [15151/20000], Training Loss: 0.0844\n",
            "Epoch [15152/20000], Training Loss: 0.0783\n",
            "Epoch [15153/20000], Training Loss: 0.0765\n",
            "Epoch [15154/20000], Training Loss: 0.0873\n",
            "Epoch [15155/20000], Training Loss: 0.0798\n",
            "Epoch [15156/20000], Training Loss: 0.0836\n",
            "Epoch [15157/20000], Training Loss: 0.0813\n",
            "Epoch [15158/20000], Training Loss: 0.0859\n",
            "Epoch [15159/20000], Training Loss: 0.0762\n",
            "Epoch [15160/20000], Training Loss: 0.0755\n",
            "Epoch [15161/20000], Training Loss: 0.0812\n",
            "Epoch [15162/20000], Training Loss: 0.0750\n",
            "Epoch [15163/20000], Training Loss: 0.0851\n",
            "Epoch [15164/20000], Training Loss: 0.0833\n",
            "Epoch [15165/20000], Training Loss: 0.0795\n",
            "Epoch [15166/20000], Training Loss: 0.0824\n",
            "Epoch [15167/20000], Training Loss: 0.0746\n",
            "Epoch [15168/20000], Training Loss: 0.0794\n",
            "Epoch [15169/20000], Training Loss: 0.0796\n",
            "Epoch [15170/20000], Training Loss: 0.0791\n",
            "Epoch [15171/20000], Training Loss: 0.0831\n",
            "Epoch [15172/20000], Training Loss: 0.0823\n",
            "Epoch [15173/20000], Training Loss: 0.0862\n",
            "Epoch [15174/20000], Training Loss: 0.0779\n",
            "Epoch [15175/20000], Training Loss: 0.0774\n",
            "Epoch [15176/20000], Training Loss: 0.0799\n",
            "Epoch [15177/20000], Training Loss: 0.0802\n",
            "Epoch [15178/20000], Training Loss: 0.0797\n",
            "Epoch [15179/20000], Training Loss: 0.0837\n",
            "Epoch [15180/20000], Training Loss: 0.0788\n",
            "Epoch [15181/20000], Training Loss: 0.0770\n",
            "Epoch [15182/20000], Training Loss: 0.0830\n",
            "Epoch [15183/20000], Training Loss: 0.0762\n",
            "Epoch [15184/20000], Training Loss: 0.0887\n",
            "Epoch [15185/20000], Training Loss: 0.0865\n",
            "Epoch [15186/20000], Training Loss: 0.0849\n",
            "Epoch [15187/20000], Training Loss: 0.0794\n",
            "Epoch [15188/20000], Training Loss: 0.0739\n",
            "Epoch [15189/20000], Training Loss: 0.0779\n",
            "Epoch [15190/20000], Training Loss: 0.0764\n",
            "Epoch [15191/20000], Training Loss: 0.0769\n",
            "Epoch [15192/20000], Training Loss: 0.0818\n",
            "Epoch [15193/20000], Training Loss: 0.0772\n",
            "Epoch [15194/20000], Training Loss: 0.0838\n",
            "Epoch [15195/20000], Training Loss: 0.0812\n",
            "Epoch [15196/20000], Training Loss: 0.0787\n",
            "Epoch [15197/20000], Training Loss: 0.0828\n",
            "Epoch [15198/20000], Training Loss: 0.0804\n",
            "Epoch [15199/20000], Training Loss: 0.0794\n",
            "Epoch [15200/20000], Training Loss: 0.0786\n",
            "Epoch [15201/20000], Training Loss: 0.0822\n",
            "Epoch [15202/20000], Training Loss: 0.0771\n",
            "Epoch [15203/20000], Training Loss: 0.0808\n",
            "Epoch [15204/20000], Training Loss: 0.0821\n",
            "Epoch [15205/20000], Training Loss: 0.0827\n",
            "Epoch [15206/20000], Training Loss: 0.0797\n",
            "Epoch [15207/20000], Training Loss: 0.0825\n",
            "Epoch [15208/20000], Training Loss: 0.0776\n",
            "Epoch [15209/20000], Training Loss: 0.0824\n",
            "Epoch [15210/20000], Training Loss: 0.0848\n",
            "Epoch [15211/20000], Training Loss: 0.0747\n",
            "Epoch [15212/20000], Training Loss: 0.0828\n",
            "Epoch [15213/20000], Training Loss: 0.0738\n",
            "Epoch [15214/20000], Training Loss: 0.0748\n",
            "Epoch [15215/20000], Training Loss: 0.0872\n",
            "Epoch [15216/20000], Training Loss: 0.0762\n",
            "Epoch [15217/20000], Training Loss: 0.0788\n",
            "Epoch [15218/20000], Training Loss: 0.0785\n",
            "Epoch [15219/20000], Training Loss: 0.0815\n",
            "Epoch [15220/20000], Training Loss: 0.0778\n",
            "Epoch [15221/20000], Training Loss: 0.0798\n",
            "Epoch [15222/20000], Training Loss: 0.0746\n",
            "Epoch [15223/20000], Training Loss: 0.0738\n",
            "Epoch [15224/20000], Training Loss: 0.0762\n",
            "Epoch [15225/20000], Training Loss: 0.0804\n",
            "Epoch [15226/20000], Training Loss: 0.0852\n",
            "Epoch [15227/20000], Training Loss: 0.0777\n",
            "Epoch [15228/20000], Training Loss: 0.0759\n",
            "Epoch [15229/20000], Training Loss: 0.0755\n",
            "Epoch [15230/20000], Training Loss: 0.0810\n",
            "Epoch [15231/20000], Training Loss: 0.0772\n",
            "Epoch [15232/20000], Training Loss: 0.0847\n",
            "Epoch [15233/20000], Training Loss: 0.0780\n",
            "Epoch [15234/20000], Training Loss: 0.0815\n",
            "Epoch [15235/20000], Training Loss: 0.0797\n",
            "Epoch [15236/20000], Training Loss: 0.0763\n",
            "Epoch [15237/20000], Training Loss: 0.0799\n",
            "Epoch [15238/20000], Training Loss: 0.0796\n",
            "Epoch [15239/20000], Training Loss: 0.0760\n",
            "Epoch [15240/20000], Training Loss: 0.0767\n",
            "Epoch [15241/20000], Training Loss: 0.0844\n",
            "Epoch [15242/20000], Training Loss: 0.0748\n",
            "Epoch [15243/20000], Training Loss: 0.0797\n",
            "Epoch [15244/20000], Training Loss: 0.0875\n",
            "Epoch [15245/20000], Training Loss: 0.0839\n",
            "Epoch [15246/20000], Training Loss: 0.0737\n",
            "Epoch [15247/20000], Training Loss: 0.0823\n",
            "Epoch [15248/20000], Training Loss: 0.0744\n",
            "Epoch [15249/20000], Training Loss: 0.0804\n",
            "Epoch [15250/20000], Training Loss: 0.0808\n",
            "Epoch [15251/20000], Training Loss: 0.0839\n",
            "Epoch [15252/20000], Training Loss: 0.0815\n",
            "Epoch [15253/20000], Training Loss: 0.0748\n",
            "Epoch [15254/20000], Training Loss: 0.0759\n",
            "Epoch [15255/20000], Training Loss: 0.0798\n",
            "Epoch [15256/20000], Training Loss: 0.0772\n",
            "Epoch [15257/20000], Training Loss: 0.0771\n",
            "Epoch [15258/20000], Training Loss: 0.0836\n",
            "Epoch [15259/20000], Training Loss: 0.0778\n",
            "Epoch [15260/20000], Training Loss: 0.0757\n",
            "Epoch [15261/20000], Training Loss: 0.0831\n",
            "Epoch [15262/20000], Training Loss: 0.0801\n",
            "Epoch [15263/20000], Training Loss: 0.0814\n",
            "Epoch [15264/20000], Training Loss: 0.0791\n",
            "Epoch [15265/20000], Training Loss: 0.0813\n",
            "Epoch [15266/20000], Training Loss: 0.0804\n",
            "Epoch [15267/20000], Training Loss: 0.0854\n",
            "Epoch [15268/20000], Training Loss: 0.0819\n",
            "Epoch [15269/20000], Training Loss: 0.0861\n",
            "Epoch [15270/20000], Training Loss: 0.0778\n",
            "Epoch [15271/20000], Training Loss: 0.0749\n",
            "Epoch [15272/20000], Training Loss: 0.0861\n",
            "Epoch [15273/20000], Training Loss: 0.0876\n",
            "Epoch [15274/20000], Training Loss: 0.0798\n",
            "Epoch [15275/20000], Training Loss: 0.0793\n",
            "Epoch [15276/20000], Training Loss: 0.0777\n",
            "Epoch [15277/20000], Training Loss: 0.0801\n",
            "Epoch [15278/20000], Training Loss: 0.0808\n",
            "Epoch [15279/20000], Training Loss: 0.0821\n",
            "Epoch [15280/20000], Training Loss: 0.0830\n",
            "Epoch [15281/20000], Training Loss: 0.0789\n",
            "Epoch [15282/20000], Training Loss: 0.0797\n",
            "Epoch [15283/20000], Training Loss: 0.0783\n",
            "Epoch [15284/20000], Training Loss: 0.0773\n",
            "Epoch [15285/20000], Training Loss: 0.0750\n",
            "Epoch [15286/20000], Training Loss: 0.0732\n",
            "Epoch [15287/20000], Training Loss: 0.0815\n",
            "Epoch [15288/20000], Training Loss: 0.0756\n",
            "Epoch [15289/20000], Training Loss: 0.0816\n",
            "Epoch [15290/20000], Training Loss: 0.0796\n",
            "Epoch [15291/20000], Training Loss: 0.0880\n",
            "Epoch [15292/20000], Training Loss: 0.0844\n",
            "Epoch [15293/20000], Training Loss: 0.0739\n",
            "Epoch [15294/20000], Training Loss: 0.0802\n",
            "Epoch [15295/20000], Training Loss: 0.0784\n",
            "Epoch [15296/20000], Training Loss: 0.0807\n",
            "Epoch [15297/20000], Training Loss: 0.0747\n",
            "Epoch [15298/20000], Training Loss: 0.0824\n",
            "Epoch [15299/20000], Training Loss: 0.0786\n",
            "Epoch [15300/20000], Training Loss: 0.0824\n",
            "Epoch [15301/20000], Training Loss: 0.0798\n",
            "Epoch [15302/20000], Training Loss: 0.0832\n",
            "Epoch [15303/20000], Training Loss: 0.0734\n",
            "Epoch [15304/20000], Training Loss: 0.0762\n",
            "Epoch [15305/20000], Training Loss: 0.0743\n",
            "Epoch [15306/20000], Training Loss: 0.0775\n",
            "Epoch [15307/20000], Training Loss: 0.0736\n",
            "Epoch [15308/20000], Training Loss: 0.0743\n",
            "Epoch [15309/20000], Training Loss: 0.0826\n",
            "Epoch [15310/20000], Training Loss: 0.0811\n",
            "Epoch [15311/20000], Training Loss: 0.0883\n",
            "Epoch [15312/20000], Training Loss: 0.0817\n",
            "Epoch [15313/20000], Training Loss: 0.0819\n",
            "Epoch [15314/20000], Training Loss: 0.0735\n",
            "Epoch [15315/20000], Training Loss: 0.0785\n",
            "Epoch [15316/20000], Training Loss: 0.0833\n",
            "Epoch [15317/20000], Training Loss: 0.0842\n",
            "Epoch [15318/20000], Training Loss: 0.0858\n",
            "Epoch [15319/20000], Training Loss: 0.0814\n",
            "Epoch [15320/20000], Training Loss: 0.0826\n",
            "Epoch [15321/20000], Training Loss: 0.0795\n",
            "Epoch [15322/20000], Training Loss: 0.0809\n",
            "Epoch [15323/20000], Training Loss: 0.0806\n",
            "Epoch [15324/20000], Training Loss: 0.0826\n",
            "Epoch [15325/20000], Training Loss: 0.0771\n",
            "Epoch [15326/20000], Training Loss: 0.0773\n",
            "Epoch [15327/20000], Training Loss: 0.0790\n",
            "Epoch [15328/20000], Training Loss: 0.0817\n",
            "Epoch [15329/20000], Training Loss: 0.0751\n",
            "Epoch [15330/20000], Training Loss: 0.0786\n",
            "Epoch [15331/20000], Training Loss: 0.0759\n",
            "Epoch [15332/20000], Training Loss: 0.0817\n",
            "Epoch [15333/20000], Training Loss: 0.0865\n",
            "Epoch [15334/20000], Training Loss: 0.0752\n",
            "Epoch [15335/20000], Training Loss: 0.0793\n",
            "Epoch [15336/20000], Training Loss: 0.0819\n",
            "Epoch [15337/20000], Training Loss: 0.0804\n",
            "Epoch [15338/20000], Training Loss: 0.0877\n",
            "Epoch [15339/20000], Training Loss: 0.0786\n",
            "Epoch [15340/20000], Training Loss: 0.0770\n",
            "Epoch [15341/20000], Training Loss: 0.0750\n",
            "Epoch [15342/20000], Training Loss: 0.0851\n",
            "Epoch [15343/20000], Training Loss: 0.0854\n",
            "Epoch [15344/20000], Training Loss: 0.0799\n",
            "Epoch [15345/20000], Training Loss: 0.0799\n",
            "Epoch [15346/20000], Training Loss: 0.0758\n",
            "Epoch [15347/20000], Training Loss: 0.0784\n",
            "Epoch [15348/20000], Training Loss: 0.0756\n",
            "Epoch [15349/20000], Training Loss: 0.0806\n",
            "Epoch [15350/20000], Training Loss: 0.0805\n",
            "Epoch [15351/20000], Training Loss: 0.0802\n",
            "Epoch [15352/20000], Training Loss: 0.0798\n",
            "Epoch [15353/20000], Training Loss: 0.0812\n",
            "Epoch [15354/20000], Training Loss: 0.0784\n",
            "Epoch [15355/20000], Training Loss: 0.0800\n",
            "Epoch [15356/20000], Training Loss: 0.0807\n",
            "Epoch [15357/20000], Training Loss: 0.0793\n",
            "Epoch [15358/20000], Training Loss: 0.0769\n",
            "Epoch [15359/20000], Training Loss: 0.0859\n",
            "Epoch [15360/20000], Training Loss: 0.0779\n",
            "Epoch [15361/20000], Training Loss: 0.0765\n",
            "Epoch [15362/20000], Training Loss: 0.0802\n",
            "Epoch [15363/20000], Training Loss: 0.0808\n",
            "Epoch [15364/20000], Training Loss: 0.0798\n",
            "Epoch [15365/20000], Training Loss: 0.0801\n",
            "Epoch [15366/20000], Training Loss: 0.0732\n",
            "Epoch [15367/20000], Training Loss: 0.0794\n",
            "Epoch [15368/20000], Training Loss: 0.0759\n",
            "Epoch [15369/20000], Training Loss: 0.0827\n",
            "Epoch [15370/20000], Training Loss: 0.0846\n",
            "Epoch [15371/20000], Training Loss: 0.0776\n",
            "Epoch [15372/20000], Training Loss: 0.0812\n",
            "Epoch [15373/20000], Training Loss: 0.0767\n",
            "Epoch [15374/20000], Training Loss: 0.0836\n",
            "Epoch [15375/20000], Training Loss: 0.0761\n",
            "Epoch [15376/20000], Training Loss: 0.0760\n",
            "Epoch [15377/20000], Training Loss: 0.0768\n",
            "Epoch [15378/20000], Training Loss: 0.0796\n",
            "Epoch [15379/20000], Training Loss: 0.0735\n",
            "Epoch [15380/20000], Training Loss: 0.0849\n",
            "Epoch [15381/20000], Training Loss: 0.0840\n",
            "Epoch [15382/20000], Training Loss: 0.0796\n",
            "Epoch [15383/20000], Training Loss: 0.0836\n",
            "Epoch [15384/20000], Training Loss: 0.0780\n",
            "Epoch [15385/20000], Training Loss: 0.0763\n",
            "Epoch [15386/20000], Training Loss: 0.0783\n",
            "Epoch [15387/20000], Training Loss: 0.0780\n",
            "Epoch [15388/20000], Training Loss: 0.0751\n",
            "Epoch [15389/20000], Training Loss: 0.0825\n",
            "Epoch [15390/20000], Training Loss: 0.0768\n",
            "Epoch [15391/20000], Training Loss: 0.0758\n",
            "Epoch [15392/20000], Training Loss: 0.0775\n",
            "Epoch [15393/20000], Training Loss: 0.0828\n",
            "Epoch [15394/20000], Training Loss: 0.0801\n",
            "Epoch [15395/20000], Training Loss: 0.0825\n",
            "Epoch [15396/20000], Training Loss: 0.0819\n",
            "Epoch [15397/20000], Training Loss: 0.0842\n",
            "Epoch [15398/20000], Training Loss: 0.0738\n",
            "Epoch [15399/20000], Training Loss: 0.0766\n",
            "Epoch [15400/20000], Training Loss: 0.0812\n",
            "Epoch [15401/20000], Training Loss: 0.0767\n",
            "Epoch [15402/20000], Training Loss: 0.0788\n",
            "Epoch [15403/20000], Training Loss: 0.0762\n",
            "Epoch [15404/20000], Training Loss: 0.0791\n",
            "Epoch [15405/20000], Training Loss: 0.0807\n",
            "Epoch [15406/20000], Training Loss: 0.0841\n",
            "Epoch [15407/20000], Training Loss: 0.0788\n",
            "Epoch [15408/20000], Training Loss: 0.0799\n",
            "Epoch [15409/20000], Training Loss: 0.0771\n",
            "Epoch [15410/20000], Training Loss: 0.0878\n",
            "Epoch [15411/20000], Training Loss: 0.0815\n",
            "Epoch [15412/20000], Training Loss: 0.0784\n",
            "Epoch [15413/20000], Training Loss: 0.0864\n",
            "Epoch [15414/20000], Training Loss: 0.0807\n",
            "Epoch [15415/20000], Training Loss: 0.0730\n",
            "Epoch [15416/20000], Training Loss: 0.0734\n",
            "Epoch [15417/20000], Training Loss: 0.0781\n",
            "Epoch [15418/20000], Training Loss: 0.0750\n",
            "Epoch [15419/20000], Training Loss: 0.0787\n",
            "Epoch [15420/20000], Training Loss: 0.0808\n",
            "Epoch [15421/20000], Training Loss: 0.0758\n",
            "Epoch [15422/20000], Training Loss: 0.0814\n",
            "Epoch [15423/20000], Training Loss: 0.0838\n",
            "Epoch [15424/20000], Training Loss: 0.0743\n",
            "Epoch [15425/20000], Training Loss: 0.0816\n",
            "Epoch [15426/20000], Training Loss: 0.0741\n",
            "Epoch [15427/20000], Training Loss: 0.0813\n",
            "Epoch [15428/20000], Training Loss: 0.0830\n",
            "Epoch [15429/20000], Training Loss: 0.0859\n",
            "Epoch [15430/20000], Training Loss: 0.0737\n",
            "Epoch [15431/20000], Training Loss: 0.0769\n",
            "Epoch [15432/20000], Training Loss: 0.0791\n",
            "Epoch [15433/20000], Training Loss: 0.0789\n",
            "Epoch [15434/20000], Training Loss: 0.0803\n",
            "Epoch [15435/20000], Training Loss: 0.0838\n",
            "Epoch [15436/20000], Training Loss: 0.0834\n",
            "Epoch [15437/20000], Training Loss: 0.0799\n",
            "Epoch [15438/20000], Training Loss: 0.0828\n",
            "Epoch [15439/20000], Training Loss: 0.0828\n",
            "Epoch [15440/20000], Training Loss: 0.0793\n",
            "Epoch [15441/20000], Training Loss: 0.0853\n",
            "Epoch [15442/20000], Training Loss: 0.0779\n",
            "Epoch [15443/20000], Training Loss: 0.0783\n",
            "Epoch [15444/20000], Training Loss: 0.0736\n",
            "Epoch [15445/20000], Training Loss: 0.0854\n",
            "Epoch [15446/20000], Training Loss: 0.0834\n",
            "Epoch [15447/20000], Training Loss: 0.0840\n",
            "Epoch [15448/20000], Training Loss: 0.0801\n",
            "Epoch [15449/20000], Training Loss: 0.0810\n",
            "Epoch [15450/20000], Training Loss: 0.0831\n",
            "Epoch [15451/20000], Training Loss: 0.0790\n",
            "Epoch [15452/20000], Training Loss: 0.0772\n",
            "Epoch [15453/20000], Training Loss: 0.0803\n",
            "Epoch [15454/20000], Training Loss: 0.0814\n",
            "Epoch [15455/20000], Training Loss: 0.0838\n",
            "Epoch [15456/20000], Training Loss: 0.0806\n",
            "Epoch [15457/20000], Training Loss: 0.0767\n",
            "Epoch [15458/20000], Training Loss: 0.0823\n",
            "Epoch [15459/20000], Training Loss: 0.0778\n",
            "Epoch [15460/20000], Training Loss: 0.0854\n",
            "Epoch [15461/20000], Training Loss: 0.0768\n",
            "Epoch [15462/20000], Training Loss: 0.0801\n",
            "Epoch [15463/20000], Training Loss: 0.0786\n",
            "Epoch [15464/20000], Training Loss: 0.0863\n",
            "Epoch [15465/20000], Training Loss: 0.0851\n",
            "Epoch [15466/20000], Training Loss: 0.0794\n",
            "Epoch [15467/20000], Training Loss: 0.0761\n",
            "Epoch [15468/20000], Training Loss: 0.0830\n",
            "Epoch [15469/20000], Training Loss: 0.0858\n",
            "Epoch [15470/20000], Training Loss: 0.0799\n",
            "Epoch [15471/20000], Training Loss: 0.0788\n",
            "Epoch [15472/20000], Training Loss: 0.0780\n",
            "Epoch [15473/20000], Training Loss: 0.0808\n",
            "Epoch [15474/20000], Training Loss: 0.0856\n",
            "Epoch [15475/20000], Training Loss: 0.0745\n",
            "Epoch [15476/20000], Training Loss: 0.0793\n",
            "Epoch [15477/20000], Training Loss: 0.0763\n",
            "Epoch [15478/20000], Training Loss: 0.0765\n",
            "Epoch [15479/20000], Training Loss: 0.0777\n",
            "Epoch [15480/20000], Training Loss: 0.0803\n",
            "Epoch [15481/20000], Training Loss: 0.0808\n",
            "Epoch [15482/20000], Training Loss: 0.0844\n",
            "Epoch [15483/20000], Training Loss: 0.0834\n",
            "Epoch [15484/20000], Training Loss: 0.0785\n",
            "Epoch [15485/20000], Training Loss: 0.0746\n",
            "Epoch [15486/20000], Training Loss: 0.0819\n",
            "Epoch [15487/20000], Training Loss: 0.0816\n",
            "Epoch [15488/20000], Training Loss: 0.0736\n",
            "Epoch [15489/20000], Training Loss: 0.0816\n",
            "Epoch [15490/20000], Training Loss: 0.0796\n",
            "Epoch [15491/20000], Training Loss: 0.0846\n",
            "Epoch [15492/20000], Training Loss: 0.0855\n",
            "Epoch [15493/20000], Training Loss: 0.0799\n",
            "Epoch [15494/20000], Training Loss: 0.0815\n",
            "Epoch [15495/20000], Training Loss: 0.0826\n",
            "Epoch [15496/20000], Training Loss: 0.0799\n",
            "Epoch [15497/20000], Training Loss: 0.0815\n",
            "Epoch [15498/20000], Training Loss: 0.0824\n",
            "Epoch [15499/20000], Training Loss: 0.0807\n",
            "Epoch [15500/20000], Training Loss: 0.0854\n",
            "Epoch [15501/20000], Training Loss: 0.0792\n",
            "Epoch [15502/20000], Training Loss: 0.0825\n",
            "Epoch [15503/20000], Training Loss: 0.0777\n",
            "Epoch [15504/20000], Training Loss: 0.0761\n",
            "Epoch [15505/20000], Training Loss: 0.0849\n",
            "Epoch [15506/20000], Training Loss: 0.0801\n",
            "Epoch [15507/20000], Training Loss: 0.0758\n",
            "Epoch [15508/20000], Training Loss: 0.0785\n",
            "Epoch [15509/20000], Training Loss: 0.0804\n",
            "Epoch [15510/20000], Training Loss: 0.0732\n",
            "Epoch [15511/20000], Training Loss: 0.0761\n",
            "Epoch [15512/20000], Training Loss: 0.0769\n",
            "Epoch [15513/20000], Training Loss: 0.0801\n",
            "Epoch [15514/20000], Training Loss: 0.0750\n",
            "Epoch [15515/20000], Training Loss: 0.0858\n",
            "Epoch [15516/20000], Training Loss: 0.0838\n",
            "Epoch [15517/20000], Training Loss: 0.0845\n",
            "Epoch [15518/20000], Training Loss: 0.0776\n",
            "Epoch [15519/20000], Training Loss: 0.0847\n",
            "Epoch [15520/20000], Training Loss: 0.0809\n",
            "Epoch [15521/20000], Training Loss: 0.0839\n",
            "Epoch [15522/20000], Training Loss: 0.0753\n",
            "Epoch [15523/20000], Training Loss: 0.0812\n",
            "Epoch [15524/20000], Training Loss: 0.0830\n",
            "Epoch [15525/20000], Training Loss: 0.0802\n",
            "Epoch [15526/20000], Training Loss: 0.0831\n",
            "Epoch [15527/20000], Training Loss: 0.0787\n",
            "Epoch [15528/20000], Training Loss: 0.0790\n",
            "Epoch [15529/20000], Training Loss: 0.0739\n",
            "Epoch [15530/20000], Training Loss: 0.0835\n",
            "Epoch [15531/20000], Training Loss: 0.0757\n",
            "Epoch [15532/20000], Training Loss: 0.0737\n",
            "Epoch [15533/20000], Training Loss: 0.0757\n",
            "Epoch [15534/20000], Training Loss: 0.0847\n",
            "Epoch [15535/20000], Training Loss: 0.0803\n",
            "Epoch [15536/20000], Training Loss: 0.0782\n",
            "Epoch [15537/20000], Training Loss: 0.0826\n",
            "Epoch [15538/20000], Training Loss: 0.0816\n",
            "Epoch [15539/20000], Training Loss: 0.0763\n",
            "Epoch [15540/20000], Training Loss: 0.0797\n",
            "Epoch [15541/20000], Training Loss: 0.0786\n",
            "Epoch [15542/20000], Training Loss: 0.0776\n",
            "Epoch [15543/20000], Training Loss: 0.0743\n",
            "Epoch [15544/20000], Training Loss: 0.0786\n",
            "Epoch [15545/20000], Training Loss: 0.0811\n",
            "Epoch [15546/20000], Training Loss: 0.0847\n",
            "Epoch [15547/20000], Training Loss: 0.0820\n",
            "Epoch [15548/20000], Training Loss: 0.0831\n",
            "Epoch [15549/20000], Training Loss: 0.0800\n",
            "Epoch [15550/20000], Training Loss: 0.0789\n",
            "Epoch [15551/20000], Training Loss: 0.0785\n",
            "Epoch [15552/20000], Training Loss: 0.0826\n",
            "Epoch [15553/20000], Training Loss: 0.0875\n",
            "Epoch [15554/20000], Training Loss: 0.0806\n",
            "Epoch [15555/20000], Training Loss: 0.0768\n",
            "Epoch [15556/20000], Training Loss: 0.0745\n",
            "Epoch [15557/20000], Training Loss: 0.0783\n",
            "Epoch [15558/20000], Training Loss: 0.0781\n",
            "Epoch [15559/20000], Training Loss: 0.0858\n",
            "Epoch [15560/20000], Training Loss: 0.0783\n",
            "Epoch [15561/20000], Training Loss: 0.0832\n",
            "Epoch [15562/20000], Training Loss: 0.0798\n",
            "Epoch [15563/20000], Training Loss: 0.0810\n",
            "Epoch [15564/20000], Training Loss: 0.0761\n",
            "Epoch [15565/20000], Training Loss: 0.0810\n",
            "Epoch [15566/20000], Training Loss: 0.0751\n",
            "Epoch [15567/20000], Training Loss: 0.0728\n",
            "Epoch [15568/20000], Training Loss: 0.0803\n",
            "Epoch [15569/20000], Training Loss: 0.0728\n",
            "Epoch [15570/20000], Training Loss: 0.0769\n",
            "Epoch [15571/20000], Training Loss: 0.0797\n",
            "Epoch [15572/20000], Training Loss: 0.0744\n",
            "Epoch [15573/20000], Training Loss: 0.0822\n",
            "Epoch [15574/20000], Training Loss: 0.0833\n",
            "Epoch [15575/20000], Training Loss: 0.0803\n",
            "Epoch [15576/20000], Training Loss: 0.0848\n",
            "Epoch [15577/20000], Training Loss: 0.0802\n",
            "Epoch [15578/20000], Training Loss: 0.0876\n",
            "Epoch [15579/20000], Training Loss: 0.0762\n",
            "Epoch [15580/20000], Training Loss: 0.0798\n",
            "Epoch [15581/20000], Training Loss: 0.0728\n",
            "Epoch [15582/20000], Training Loss: 0.0739\n",
            "Epoch [15583/20000], Training Loss: 0.0794\n",
            "Epoch [15584/20000], Training Loss: 0.0753\n",
            "Epoch [15585/20000], Training Loss: 0.0805\n",
            "Epoch [15586/20000], Training Loss: 0.0775\n",
            "Epoch [15587/20000], Training Loss: 0.0842\n",
            "Epoch [15588/20000], Training Loss: 0.0815\n",
            "Epoch [15589/20000], Training Loss: 0.0744\n",
            "Epoch [15590/20000], Training Loss: 0.0798\n",
            "Epoch [15591/20000], Training Loss: 0.0831\n",
            "Epoch [15592/20000], Training Loss: 0.0819\n",
            "Epoch [15593/20000], Training Loss: 0.0804\n",
            "Epoch [15594/20000], Training Loss: 0.0812\n",
            "Epoch [15595/20000], Training Loss: 0.0770\n",
            "Epoch [15596/20000], Training Loss: 0.0788\n",
            "Epoch [15597/20000], Training Loss: 0.0787\n",
            "Epoch [15598/20000], Training Loss: 0.0763\n",
            "Epoch [15599/20000], Training Loss: 0.0839\n",
            "Epoch [15600/20000], Training Loss: 0.0789\n",
            "Epoch [15601/20000], Training Loss: 0.0824\n",
            "Epoch [15602/20000], Training Loss: 0.0835\n",
            "Epoch [15603/20000], Training Loss: 0.0772\n",
            "Epoch [15604/20000], Training Loss: 0.0767\n",
            "Epoch [15605/20000], Training Loss: 0.0805\n",
            "Epoch [15606/20000], Training Loss: 0.0837\n",
            "Epoch [15607/20000], Training Loss: 0.0764\n",
            "Epoch [15608/20000], Training Loss: 0.0807\n",
            "Epoch [15609/20000], Training Loss: 0.0747\n",
            "Epoch [15610/20000], Training Loss: 0.0738\n",
            "Epoch [15611/20000], Training Loss: 0.0814\n",
            "Epoch [15612/20000], Training Loss: 0.0857\n",
            "Epoch [15613/20000], Training Loss: 0.0773\n",
            "Epoch [15614/20000], Training Loss: 0.0824\n",
            "Epoch [15615/20000], Training Loss: 0.0786\n",
            "Epoch [15616/20000], Training Loss: 0.0813\n",
            "Epoch [15617/20000], Training Loss: 0.0768\n",
            "Epoch [15618/20000], Training Loss: 0.0806\n",
            "Epoch [15619/20000], Training Loss: 0.0810\n",
            "Epoch [15620/20000], Training Loss: 0.0751\n",
            "Epoch [15621/20000], Training Loss: 0.0791\n",
            "Epoch [15622/20000], Training Loss: 0.0843\n",
            "Epoch [15623/20000], Training Loss: 0.0777\n",
            "Epoch [15624/20000], Training Loss: 0.0800\n",
            "Epoch [15625/20000], Training Loss: 0.0795\n",
            "Epoch [15626/20000], Training Loss: 0.0801\n",
            "Epoch [15627/20000], Training Loss: 0.0780\n",
            "Epoch [15628/20000], Training Loss: 0.0856\n",
            "Epoch [15629/20000], Training Loss: 0.0779\n",
            "Epoch [15630/20000], Training Loss: 0.0816\n",
            "Epoch [15631/20000], Training Loss: 0.0783\n",
            "Epoch [15632/20000], Training Loss: 0.0791\n",
            "Epoch [15633/20000], Training Loss: 0.0740\n",
            "Epoch [15634/20000], Training Loss: 0.0751\n",
            "Epoch [15635/20000], Training Loss: 0.0762\n",
            "Epoch [15636/20000], Training Loss: 0.0786\n",
            "Epoch [15637/20000], Training Loss: 0.0747\n",
            "Epoch [15638/20000], Training Loss: 0.0799\n",
            "Epoch [15639/20000], Training Loss: 0.0799\n",
            "Epoch [15640/20000], Training Loss: 0.0750\n",
            "Epoch [15641/20000], Training Loss: 0.0872\n",
            "Epoch [15642/20000], Training Loss: 0.0804\n",
            "Epoch [15643/20000], Training Loss: 0.0791\n",
            "Epoch [15644/20000], Training Loss: 0.0800\n",
            "Epoch [15645/20000], Training Loss: 0.0784\n",
            "Epoch [15646/20000], Training Loss: 0.0775\n",
            "Epoch [15647/20000], Training Loss: 0.0754\n",
            "Epoch [15648/20000], Training Loss: 0.0828\n",
            "Epoch [15649/20000], Training Loss: 0.0743\n",
            "Epoch [15650/20000], Training Loss: 0.0819\n",
            "Epoch [15651/20000], Training Loss: 0.0823\n",
            "Epoch [15652/20000], Training Loss: 0.0869\n",
            "Epoch [15653/20000], Training Loss: 0.0744\n",
            "Epoch [15654/20000], Training Loss: 0.0824\n",
            "Epoch [15655/20000], Training Loss: 0.0790\n",
            "Epoch [15656/20000], Training Loss: 0.0813\n",
            "Epoch [15657/20000], Training Loss: 0.0826\n",
            "Epoch [15658/20000], Training Loss: 0.0747\n",
            "Epoch [15659/20000], Training Loss: 0.0794\n",
            "Epoch [15660/20000], Training Loss: 0.0758\n",
            "Epoch [15661/20000], Training Loss: 0.0822\n",
            "Epoch [15662/20000], Training Loss: 0.0778\n",
            "Epoch [15663/20000], Training Loss: 0.0821\n",
            "Epoch [15664/20000], Training Loss: 0.0767\n",
            "Epoch [15665/20000], Training Loss: 0.0768\n",
            "Epoch [15666/20000], Training Loss: 0.0733\n",
            "Epoch [15667/20000], Training Loss: 0.0772\n",
            "Epoch [15668/20000], Training Loss: 0.0816\n",
            "Epoch [15669/20000], Training Loss: 0.0794\n",
            "Epoch [15670/20000], Training Loss: 0.0794\n",
            "Epoch [15671/20000], Training Loss: 0.0751\n",
            "Epoch [15672/20000], Training Loss: 0.0796\n",
            "Epoch [15673/20000], Training Loss: 0.0808\n",
            "Epoch [15674/20000], Training Loss: 0.0800\n",
            "Epoch [15675/20000], Training Loss: 0.0823\n",
            "Epoch [15676/20000], Training Loss: 0.0785\n",
            "Epoch [15677/20000], Training Loss: 0.0763\n",
            "Epoch [15678/20000], Training Loss: 0.0763\n",
            "Epoch [15679/20000], Training Loss: 0.0810\n",
            "Epoch [15680/20000], Training Loss: 0.0843\n",
            "Epoch [15681/20000], Training Loss: 0.0815\n",
            "Epoch [15682/20000], Training Loss: 0.0806\n",
            "Epoch [15683/20000], Training Loss: 0.0875\n",
            "Epoch [15684/20000], Training Loss: 0.0782\n",
            "Epoch [15685/20000], Training Loss: 0.0825\n",
            "Epoch [15686/20000], Training Loss: 0.0849\n",
            "Epoch [15687/20000], Training Loss: 0.0832\n",
            "Epoch [15688/20000], Training Loss: 0.0815\n",
            "Epoch [15689/20000], Training Loss: 0.0789\n",
            "Epoch [15690/20000], Training Loss: 0.0806\n",
            "Epoch [15691/20000], Training Loss: 0.0777\n",
            "Epoch [15692/20000], Training Loss: 0.0818\n",
            "Epoch [15693/20000], Training Loss: 0.0778\n",
            "Epoch [15694/20000], Training Loss: 0.0846\n",
            "Epoch [15695/20000], Training Loss: 0.0791\n",
            "Epoch [15696/20000], Training Loss: 0.0786\n",
            "Epoch [15697/20000], Training Loss: 0.0832\n",
            "Epoch [15698/20000], Training Loss: 0.0775\n",
            "Epoch [15699/20000], Training Loss: 0.0821\n",
            "Epoch [15700/20000], Training Loss: 0.0786\n",
            "Epoch [15701/20000], Training Loss: 0.0745\n",
            "Epoch [15702/20000], Training Loss: 0.0853\n",
            "Epoch [15703/20000], Training Loss: 0.0809\n",
            "Epoch [15704/20000], Training Loss: 0.0763\n",
            "Epoch [15705/20000], Training Loss: 0.0803\n",
            "Epoch [15706/20000], Training Loss: 0.0854\n",
            "Epoch [15707/20000], Training Loss: 0.0799\n",
            "Epoch [15708/20000], Training Loss: 0.0824\n",
            "Epoch [15709/20000], Training Loss: 0.0799\n",
            "Epoch [15710/20000], Training Loss: 0.0814\n",
            "Epoch [15711/20000], Training Loss: 0.0783\n",
            "Epoch [15712/20000], Training Loss: 0.0783\n",
            "Epoch [15713/20000], Training Loss: 0.0834\n",
            "Epoch [15714/20000], Training Loss: 0.0858\n",
            "Epoch [15715/20000], Training Loss: 0.0823\n",
            "Epoch [15716/20000], Training Loss: 0.0818\n",
            "Epoch [15717/20000], Training Loss: 0.0784\n",
            "Epoch [15718/20000], Training Loss: 0.0832\n",
            "Epoch [15719/20000], Training Loss: 0.0770\n",
            "Epoch [15720/20000], Training Loss: 0.0821\n",
            "Epoch [15721/20000], Training Loss: 0.0836\n",
            "Epoch [15722/20000], Training Loss: 0.0801\n",
            "Epoch [15723/20000], Training Loss: 0.0767\n",
            "Epoch [15724/20000], Training Loss: 0.0799\n",
            "Epoch [15725/20000], Training Loss: 0.0801\n",
            "Epoch [15726/20000], Training Loss: 0.0754\n",
            "Epoch [15727/20000], Training Loss: 0.0862\n",
            "Epoch [15728/20000], Training Loss: 0.0809\n",
            "Epoch [15729/20000], Training Loss: 0.0805\n",
            "Epoch [15730/20000], Training Loss: 0.0828\n",
            "Epoch [15731/20000], Training Loss: 0.0743\n",
            "Epoch [15732/20000], Training Loss: 0.0770\n",
            "Epoch [15733/20000], Training Loss: 0.0764\n",
            "Epoch [15734/20000], Training Loss: 0.0774\n",
            "Epoch [15735/20000], Training Loss: 0.0757\n",
            "Epoch [15736/20000], Training Loss: 0.0736\n",
            "Epoch [15737/20000], Training Loss: 0.0824\n",
            "Epoch [15738/20000], Training Loss: 0.0780\n",
            "Epoch [15739/20000], Training Loss: 0.0857\n",
            "Epoch [15740/20000], Training Loss: 0.0872\n",
            "Epoch [15741/20000], Training Loss: 0.0752\n",
            "Epoch [15742/20000], Training Loss: 0.0808\n",
            "Epoch [15743/20000], Training Loss: 0.0724\n",
            "Epoch [15744/20000], Training Loss: 0.0807\n",
            "Epoch [15745/20000], Training Loss: 0.0809\n",
            "Epoch [15746/20000], Training Loss: 0.0827\n",
            "Epoch [15747/20000], Training Loss: 0.0821\n",
            "Epoch [15748/20000], Training Loss: 0.0788\n",
            "Epoch [15749/20000], Training Loss: 0.0843\n",
            "Epoch [15750/20000], Training Loss: 0.0739\n",
            "Epoch [15751/20000], Training Loss: 0.0849\n",
            "Epoch [15752/20000], Training Loss: 0.0805\n",
            "Epoch [15753/20000], Training Loss: 0.0750\n",
            "Epoch [15754/20000], Training Loss: 0.0788\n",
            "Epoch [15755/20000], Training Loss: 0.0820\n",
            "Epoch [15756/20000], Training Loss: 0.0829\n",
            "Epoch [15757/20000], Training Loss: 0.0844\n",
            "Epoch [15758/20000], Training Loss: 0.0771\n",
            "Epoch [15759/20000], Training Loss: 0.0815\n",
            "Epoch [15760/20000], Training Loss: 0.0788\n",
            "Epoch [15761/20000], Training Loss: 0.0729\n",
            "Epoch [15762/20000], Training Loss: 0.0777\n",
            "Epoch [15763/20000], Training Loss: 0.0774\n",
            "Epoch [15764/20000], Training Loss: 0.0846\n",
            "Epoch [15765/20000], Training Loss: 0.0832\n",
            "Epoch [15766/20000], Training Loss: 0.0800\n",
            "Epoch [15767/20000], Training Loss: 0.0779\n",
            "Epoch [15768/20000], Training Loss: 0.0830\n",
            "Epoch [15769/20000], Training Loss: 0.0779\n",
            "Epoch [15770/20000], Training Loss: 0.0848\n",
            "Epoch [15771/20000], Training Loss: 0.0789\n",
            "Epoch [15772/20000], Training Loss: 0.0742\n",
            "Epoch [15773/20000], Training Loss: 0.0793\n",
            "Epoch [15774/20000], Training Loss: 0.0788\n",
            "Epoch [15775/20000], Training Loss: 0.0780\n",
            "Epoch [15776/20000], Training Loss: 0.0765\n",
            "Epoch [15777/20000], Training Loss: 0.0807\n",
            "Epoch [15778/20000], Training Loss: 0.0792\n",
            "Epoch [15779/20000], Training Loss: 0.0800\n",
            "Epoch [15780/20000], Training Loss: 0.0847\n",
            "Epoch [15781/20000], Training Loss: 0.0765\n",
            "Epoch [15782/20000], Training Loss: 0.0850\n",
            "Epoch [15783/20000], Training Loss: 0.0746\n",
            "Epoch [15784/20000], Training Loss: 0.0729\n",
            "Epoch [15785/20000], Training Loss: 0.0820\n",
            "Epoch [15786/20000], Training Loss: 0.0807\n",
            "Epoch [15787/20000], Training Loss: 0.0877\n",
            "Epoch [15788/20000], Training Loss: 0.0801\n",
            "Epoch [15789/20000], Training Loss: 0.0794\n",
            "Epoch [15790/20000], Training Loss: 0.0778\n",
            "Epoch [15791/20000], Training Loss: 0.0759\n",
            "Epoch [15792/20000], Training Loss: 0.0744\n",
            "Epoch [15793/20000], Training Loss: 0.0764\n",
            "Epoch [15794/20000], Training Loss: 0.0807\n",
            "Epoch [15795/20000], Training Loss: 0.0754\n",
            "Epoch [15796/20000], Training Loss: 0.0839\n",
            "Epoch [15797/20000], Training Loss: 0.0740\n",
            "Epoch [15798/20000], Training Loss: 0.0767\n",
            "Epoch [15799/20000], Training Loss: 0.0791\n",
            "Epoch [15800/20000], Training Loss: 0.0800\n",
            "Epoch [15801/20000], Training Loss: 0.0739\n",
            "Epoch [15802/20000], Training Loss: 0.0786\n",
            "Epoch [15803/20000], Training Loss: 0.0778\n",
            "Epoch [15804/20000], Training Loss: 0.0824\n",
            "Epoch [15805/20000], Training Loss: 0.0858\n",
            "Epoch [15806/20000], Training Loss: 0.0803\n",
            "Epoch [15807/20000], Training Loss: 0.0761\n",
            "Epoch [15808/20000], Training Loss: 0.0744\n",
            "Epoch [15809/20000], Training Loss: 0.0784\n",
            "Epoch [15810/20000], Training Loss: 0.0821\n",
            "Epoch [15811/20000], Training Loss: 0.0776\n",
            "Epoch [15812/20000], Training Loss: 0.0786\n",
            "Epoch [15813/20000], Training Loss: 0.0767\n",
            "Epoch [15814/20000], Training Loss: 0.0841\n",
            "Epoch [15815/20000], Training Loss: 0.0824\n",
            "Epoch [15816/20000], Training Loss: 0.0871\n",
            "Epoch [15817/20000], Training Loss: 0.0811\n",
            "Epoch [15818/20000], Training Loss: 0.0759\n",
            "Epoch [15819/20000], Training Loss: 0.0849\n",
            "Epoch [15820/20000], Training Loss: 0.0809\n",
            "Epoch [15821/20000], Training Loss: 0.0721\n",
            "Epoch [15822/20000], Training Loss: 0.0786\n",
            "Epoch [15823/20000], Training Loss: 0.0813\n",
            "Epoch [15824/20000], Training Loss: 0.0776\n",
            "Epoch [15825/20000], Training Loss: 0.0787\n",
            "Epoch [15826/20000], Training Loss: 0.0776\n",
            "Epoch [15827/20000], Training Loss: 0.0819\n",
            "Epoch [15828/20000], Training Loss: 0.0824\n",
            "Epoch [15829/20000], Training Loss: 0.0740\n",
            "Epoch [15830/20000], Training Loss: 0.0820\n",
            "Epoch [15831/20000], Training Loss: 0.0742\n",
            "Epoch [15832/20000], Training Loss: 0.0829\n",
            "Epoch [15833/20000], Training Loss: 0.0810\n",
            "Epoch [15834/20000], Training Loss: 0.0743\n",
            "Epoch [15835/20000], Training Loss: 0.0824\n",
            "Epoch [15836/20000], Training Loss: 0.0803\n",
            "Epoch [15837/20000], Training Loss: 0.0809\n",
            "Epoch [15838/20000], Training Loss: 0.0788\n",
            "Epoch [15839/20000], Training Loss: 0.0783\n",
            "Epoch [15840/20000], Training Loss: 0.0745\n",
            "Epoch [15841/20000], Training Loss: 0.0811\n",
            "Epoch [15842/20000], Training Loss: 0.0805\n",
            "Epoch [15843/20000], Training Loss: 0.0814\n",
            "Epoch [15844/20000], Training Loss: 0.0772\n",
            "Epoch [15845/20000], Training Loss: 0.0758\n",
            "Epoch [15846/20000], Training Loss: 0.0808\n",
            "Epoch [15847/20000], Training Loss: 0.0799\n",
            "Epoch [15848/20000], Training Loss: 0.0756\n",
            "Epoch [15849/20000], Training Loss: 0.0831\n",
            "Epoch [15850/20000], Training Loss: 0.0802\n",
            "Epoch [15851/20000], Training Loss: 0.0844\n",
            "Epoch [15852/20000], Training Loss: 0.0791\n",
            "Epoch [15853/20000], Training Loss: 0.0769\n",
            "Epoch [15854/20000], Training Loss: 0.0813\n",
            "Epoch [15855/20000], Training Loss: 0.0737\n",
            "Epoch [15856/20000], Training Loss: 0.0847\n",
            "Epoch [15857/20000], Training Loss: 0.0814\n",
            "Epoch [15858/20000], Training Loss: 0.0812\n",
            "Epoch [15859/20000], Training Loss: 0.0821\n",
            "Epoch [15860/20000], Training Loss: 0.0859\n",
            "Epoch [15861/20000], Training Loss: 0.0810\n",
            "Epoch [15862/20000], Training Loss: 0.0764\n",
            "Epoch [15863/20000], Training Loss: 0.0750\n",
            "Epoch [15864/20000], Training Loss: 0.0757\n",
            "Epoch [15865/20000], Training Loss: 0.0817\n",
            "Epoch [15866/20000], Training Loss: 0.0859\n",
            "Epoch [15867/20000], Training Loss: 0.0799\n",
            "Epoch [15868/20000], Training Loss: 0.0834\n",
            "Epoch [15869/20000], Training Loss: 0.0804\n",
            "Epoch [15870/20000], Training Loss: 0.0875\n",
            "Epoch [15871/20000], Training Loss: 0.0778\n",
            "Epoch [15872/20000], Training Loss: 0.0803\n",
            "Epoch [15873/20000], Training Loss: 0.0760\n",
            "Epoch [15874/20000], Training Loss: 0.0809\n",
            "Epoch [15875/20000], Training Loss: 0.0826\n",
            "Epoch [15876/20000], Training Loss: 0.0792\n",
            "Epoch [15877/20000], Training Loss: 0.0733\n",
            "Epoch [15878/20000], Training Loss: 0.0799\n",
            "Epoch [15879/20000], Training Loss: 0.0808\n",
            "Epoch [15880/20000], Training Loss: 0.0793\n",
            "Epoch [15881/20000], Training Loss: 0.0762\n",
            "Epoch [15882/20000], Training Loss: 0.0768\n",
            "Epoch [15883/20000], Training Loss: 0.0791\n",
            "Epoch [15884/20000], Training Loss: 0.0741\n",
            "Epoch [15885/20000], Training Loss: 0.0791\n",
            "Epoch [15886/20000], Training Loss: 0.0816\n",
            "Epoch [15887/20000], Training Loss: 0.0810\n",
            "Epoch [15888/20000], Training Loss: 0.0735\n",
            "Epoch [15889/20000], Training Loss: 0.0801\n",
            "Epoch [15890/20000], Training Loss: 0.0753\n",
            "Epoch [15891/20000], Training Loss: 0.0761\n",
            "Epoch [15892/20000], Training Loss: 0.0784\n",
            "Epoch [15893/20000], Training Loss: 0.0843\n",
            "Epoch [15894/20000], Training Loss: 0.0756\n",
            "Epoch [15895/20000], Training Loss: 0.0814\n",
            "Epoch [15896/20000], Training Loss: 0.0743\n",
            "Epoch [15897/20000], Training Loss: 0.0813\n",
            "Epoch [15898/20000], Training Loss: 0.0799\n",
            "Epoch [15899/20000], Training Loss: 0.0791\n",
            "Epoch [15900/20000], Training Loss: 0.0787\n",
            "Epoch [15901/20000], Training Loss: 0.0779\n",
            "Epoch [15902/20000], Training Loss: 0.0754\n",
            "Epoch [15903/20000], Training Loss: 0.0801\n",
            "Epoch [15904/20000], Training Loss: 0.0768\n",
            "Epoch [15905/20000], Training Loss: 0.0736\n",
            "Epoch [15906/20000], Training Loss: 0.0795\n",
            "Epoch [15907/20000], Training Loss: 0.0807\n",
            "Epoch [15908/20000], Training Loss: 0.0770\n",
            "Epoch [15909/20000], Training Loss: 0.0781\n",
            "Epoch [15910/20000], Training Loss: 0.0830\n",
            "Epoch [15911/20000], Training Loss: 0.0746\n",
            "Epoch [15912/20000], Training Loss: 0.0777\n",
            "Epoch [15913/20000], Training Loss: 0.0842\n",
            "Epoch [15914/20000], Training Loss: 0.0801\n",
            "Epoch [15915/20000], Training Loss: 0.0757\n",
            "Epoch [15916/20000], Training Loss: 0.0850\n",
            "Epoch [15917/20000], Training Loss: 0.0784\n",
            "Epoch [15918/20000], Training Loss: 0.0810\n",
            "Epoch [15919/20000], Training Loss: 0.0864\n",
            "Epoch [15920/20000], Training Loss: 0.0795\n",
            "Epoch [15921/20000], Training Loss: 0.0759\n",
            "Epoch [15922/20000], Training Loss: 0.0848\n",
            "Epoch [15923/20000], Training Loss: 0.0768\n",
            "Epoch [15924/20000], Training Loss: 0.0780\n",
            "Epoch [15925/20000], Training Loss: 0.0738\n",
            "Epoch [15926/20000], Training Loss: 0.0810\n",
            "Epoch [15927/20000], Training Loss: 0.0807\n",
            "Epoch [15928/20000], Training Loss: 0.0799\n",
            "Epoch [15929/20000], Training Loss: 0.0778\n",
            "Epoch [15930/20000], Training Loss: 0.0805\n",
            "Epoch [15931/20000], Training Loss: 0.0831\n",
            "Epoch [15932/20000], Training Loss: 0.0748\n",
            "Epoch [15933/20000], Training Loss: 0.0846\n",
            "Epoch [15934/20000], Training Loss: 0.0820\n",
            "Epoch [15935/20000], Training Loss: 0.0840\n",
            "Epoch [15936/20000], Training Loss: 0.0758\n",
            "Epoch [15937/20000], Training Loss: 0.0731\n",
            "Epoch [15938/20000], Training Loss: 0.0851\n",
            "Epoch [15939/20000], Training Loss: 0.0769\n",
            "Epoch [15940/20000], Training Loss: 0.0816\n",
            "Epoch [15941/20000], Training Loss: 0.0812\n",
            "Epoch [15942/20000], Training Loss: 0.0818\n",
            "Epoch [15943/20000], Training Loss: 0.0758\n",
            "Epoch [15944/20000], Training Loss: 0.0820\n",
            "Epoch [15945/20000], Training Loss: 0.0768\n",
            "Epoch [15946/20000], Training Loss: 0.0796\n",
            "Epoch [15947/20000], Training Loss: 0.0766\n",
            "Epoch [15948/20000], Training Loss: 0.0837\n",
            "Epoch [15949/20000], Training Loss: 0.0812\n",
            "Epoch [15950/20000], Training Loss: 0.0769\n",
            "Epoch [15951/20000], Training Loss: 0.0830\n",
            "Epoch [15952/20000], Training Loss: 0.0802\n",
            "Epoch [15953/20000], Training Loss: 0.0816\n",
            "Epoch [15954/20000], Training Loss: 0.0812\n",
            "Epoch [15955/20000], Training Loss: 0.0815\n",
            "Epoch [15956/20000], Training Loss: 0.0784\n",
            "Epoch [15957/20000], Training Loss: 0.0737\n",
            "Epoch [15958/20000], Training Loss: 0.0824\n",
            "Epoch [15959/20000], Training Loss: 0.0761\n",
            "Epoch [15960/20000], Training Loss: 0.0805\n",
            "Epoch [15961/20000], Training Loss: 0.0784\n",
            "Epoch [15962/20000], Training Loss: 0.0789\n",
            "Epoch [15963/20000], Training Loss: 0.0784\n",
            "Epoch [15964/20000], Training Loss: 0.0759\n",
            "Epoch [15965/20000], Training Loss: 0.0812\n",
            "Epoch [15966/20000], Training Loss: 0.0794\n",
            "Epoch [15967/20000], Training Loss: 0.0843\n",
            "Epoch [15968/20000], Training Loss: 0.0739\n",
            "Epoch [15969/20000], Training Loss: 0.0763\n",
            "Epoch [15970/20000], Training Loss: 0.0794\n",
            "Epoch [15971/20000], Training Loss: 0.0778\n",
            "Epoch [15972/20000], Training Loss: 0.0783\n",
            "Epoch [15973/20000], Training Loss: 0.0751\n",
            "Epoch [15974/20000], Training Loss: 0.0826\n",
            "Epoch [15975/20000], Training Loss: 0.0752\n",
            "Epoch [15976/20000], Training Loss: 0.0770\n",
            "Epoch [15977/20000], Training Loss: 0.0798\n",
            "Epoch [15978/20000], Training Loss: 0.0726\n",
            "Epoch [15979/20000], Training Loss: 0.0765\n",
            "Epoch [15980/20000], Training Loss: 0.0730\n",
            "Epoch [15981/20000], Training Loss: 0.0810\n",
            "Epoch [15982/20000], Training Loss: 0.0787\n",
            "Epoch [15983/20000], Training Loss: 0.0785\n",
            "Epoch [15984/20000], Training Loss: 0.0771\n",
            "Epoch [15985/20000], Training Loss: 0.0751\n",
            "Epoch [15986/20000], Training Loss: 0.0851\n",
            "Epoch [15987/20000], Training Loss: 0.0816\n",
            "Epoch [15988/20000], Training Loss: 0.0764\n",
            "Epoch [15989/20000], Training Loss: 0.0841\n",
            "Epoch [15990/20000], Training Loss: 0.0814\n",
            "Epoch [15991/20000], Training Loss: 0.0855\n",
            "Epoch [15992/20000], Training Loss: 0.0756\n",
            "Epoch [15993/20000], Training Loss: 0.0808\n",
            "Epoch [15994/20000], Training Loss: 0.0764\n",
            "Epoch [15995/20000], Training Loss: 0.0795\n",
            "Epoch [15996/20000], Training Loss: 0.0807\n",
            "Epoch [15997/20000], Training Loss: 0.0765\n",
            "Epoch [15998/20000], Training Loss: 0.0846\n",
            "Epoch [15999/20000], Training Loss: 0.0775\n",
            "Epoch [16000/20000], Training Loss: 0.0806\n",
            "Epoch [16001/20000], Training Loss: 0.0842\n",
            "Epoch [16002/20000], Training Loss: 0.0781\n",
            "Epoch [16003/20000], Training Loss: 0.0767\n",
            "Epoch [16004/20000], Training Loss: 0.0769\n",
            "Epoch [16005/20000], Training Loss: 0.0810\n",
            "Epoch [16006/20000], Training Loss: 0.0791\n",
            "Epoch [16007/20000], Training Loss: 0.0798\n",
            "Epoch [16008/20000], Training Loss: 0.0825\n",
            "Epoch [16009/20000], Training Loss: 0.0750\n",
            "Epoch [16010/20000], Training Loss: 0.0860\n",
            "Epoch [16011/20000], Training Loss: 0.0730\n",
            "Epoch [16012/20000], Training Loss: 0.0808\n",
            "Epoch [16013/20000], Training Loss: 0.0820\n",
            "Epoch [16014/20000], Training Loss: 0.0737\n",
            "Epoch [16015/20000], Training Loss: 0.0756\n",
            "Epoch [16016/20000], Training Loss: 0.0785\n",
            "Epoch [16017/20000], Training Loss: 0.0740\n",
            "Epoch [16018/20000], Training Loss: 0.0854\n",
            "Epoch [16019/20000], Training Loss: 0.0763\n",
            "Epoch [16020/20000], Training Loss: 0.0810\n",
            "Epoch [16021/20000], Training Loss: 0.0812\n",
            "Epoch [16022/20000], Training Loss: 0.0793\n",
            "Epoch [16023/20000], Training Loss: 0.0774\n",
            "Epoch [16024/20000], Training Loss: 0.0753\n",
            "Epoch [16025/20000], Training Loss: 0.0789\n",
            "Epoch [16026/20000], Training Loss: 0.0827\n",
            "Epoch [16027/20000], Training Loss: 0.0843\n",
            "Epoch [16028/20000], Training Loss: 0.0821\n",
            "Epoch [16029/20000], Training Loss: 0.0784\n",
            "Epoch [16030/20000], Training Loss: 0.0811\n",
            "Epoch [16031/20000], Training Loss: 0.0820\n",
            "Epoch [16032/20000], Training Loss: 0.0793\n",
            "Epoch [16033/20000], Training Loss: 0.0738\n",
            "Epoch [16034/20000], Training Loss: 0.0764\n",
            "Epoch [16035/20000], Training Loss: 0.0758\n",
            "Epoch [16036/20000], Training Loss: 0.0815\n",
            "Epoch [16037/20000], Training Loss: 0.0826\n",
            "Epoch [16038/20000], Training Loss: 0.0743\n",
            "Epoch [16039/20000], Training Loss: 0.0812\n",
            "Epoch [16040/20000], Training Loss: 0.0846\n",
            "Epoch [16041/20000], Training Loss: 0.0750\n",
            "Epoch [16042/20000], Training Loss: 0.0807\n",
            "Epoch [16043/20000], Training Loss: 0.0760\n",
            "Epoch [16044/20000], Training Loss: 0.0783\n",
            "Epoch [16045/20000], Training Loss: 0.0812\n",
            "Epoch [16046/20000], Training Loss: 0.0758\n",
            "Epoch [16047/20000], Training Loss: 0.0767\n",
            "Epoch [16048/20000], Training Loss: 0.0785\n",
            "Epoch [16049/20000], Training Loss: 0.0797\n",
            "Epoch [16050/20000], Training Loss: 0.0807\n",
            "Epoch [16051/20000], Training Loss: 0.0826\n",
            "Epoch [16052/20000], Training Loss: 0.0799\n",
            "Epoch [16053/20000], Training Loss: 0.0753\n",
            "Epoch [16054/20000], Training Loss: 0.0791\n",
            "Epoch [16055/20000], Training Loss: 0.0843\n",
            "Epoch [16056/20000], Training Loss: 0.0748\n",
            "Epoch [16057/20000], Training Loss: 0.0871\n",
            "Epoch [16058/20000], Training Loss: 0.0793\n",
            "Epoch [16059/20000], Training Loss: 0.0805\n",
            "Epoch [16060/20000], Training Loss: 0.0826\n",
            "Epoch [16061/20000], Training Loss: 0.0791\n",
            "Epoch [16062/20000], Training Loss: 0.0774\n",
            "Epoch [16063/20000], Training Loss: 0.0753\n",
            "Epoch [16064/20000], Training Loss: 0.0806\n",
            "Epoch [16065/20000], Training Loss: 0.0842\n",
            "Epoch [16066/20000], Training Loss: 0.0819\n",
            "Epoch [16067/20000], Training Loss: 0.0839\n",
            "Epoch [16068/20000], Training Loss: 0.0746\n",
            "Epoch [16069/20000], Training Loss: 0.0752\n",
            "Epoch [16070/20000], Training Loss: 0.0792\n",
            "Epoch [16071/20000], Training Loss: 0.0804\n",
            "Epoch [16072/20000], Training Loss: 0.0811\n",
            "Epoch [16073/20000], Training Loss: 0.0754\n",
            "Epoch [16074/20000], Training Loss: 0.0844\n",
            "Epoch [16075/20000], Training Loss: 0.0798\n",
            "Epoch [16076/20000], Training Loss: 0.0816\n",
            "Epoch [16077/20000], Training Loss: 0.0828\n",
            "Epoch [16078/20000], Training Loss: 0.0780\n",
            "Epoch [16079/20000], Training Loss: 0.0745\n",
            "Epoch [16080/20000], Training Loss: 0.0759\n",
            "Epoch [16081/20000], Training Loss: 0.0734\n",
            "Epoch [16082/20000], Training Loss: 0.0800\n",
            "Epoch [16083/20000], Training Loss: 0.0816\n",
            "Epoch [16084/20000], Training Loss: 0.0724\n",
            "Epoch [16085/20000], Training Loss: 0.0746\n",
            "Epoch [16086/20000], Training Loss: 0.0818\n",
            "Epoch [16087/20000], Training Loss: 0.0814\n",
            "Epoch [16088/20000], Training Loss: 0.0808\n",
            "Epoch [16089/20000], Training Loss: 0.0770\n",
            "Epoch [16090/20000], Training Loss: 0.0864\n",
            "Epoch [16091/20000], Training Loss: 0.0782\n",
            "Epoch [16092/20000], Training Loss: 0.0832\n",
            "Epoch [16093/20000], Training Loss: 0.0767\n",
            "Epoch [16094/20000], Training Loss: 0.0842\n",
            "Epoch [16095/20000], Training Loss: 0.0765\n",
            "Epoch [16096/20000], Training Loss: 0.0866\n",
            "Epoch [16097/20000], Training Loss: 0.0797\n",
            "Epoch [16098/20000], Training Loss: 0.0780\n",
            "Epoch [16099/20000], Training Loss: 0.0841\n",
            "Epoch [16100/20000], Training Loss: 0.0817\n",
            "Epoch [16101/20000], Training Loss: 0.0747\n",
            "Epoch [16102/20000], Training Loss: 0.0781\n",
            "Epoch [16103/20000], Training Loss: 0.0766\n",
            "Epoch [16104/20000], Training Loss: 0.0767\n",
            "Epoch [16105/20000], Training Loss: 0.0748\n",
            "Epoch [16106/20000], Training Loss: 0.0760\n",
            "Epoch [16107/20000], Training Loss: 0.0792\n",
            "Epoch [16108/20000], Training Loss: 0.0787\n",
            "Epoch [16109/20000], Training Loss: 0.0873\n",
            "Epoch [16110/20000], Training Loss: 0.0774\n",
            "Epoch [16111/20000], Training Loss: 0.0796\n",
            "Epoch [16112/20000], Training Loss: 0.0800\n",
            "Epoch [16113/20000], Training Loss: 0.0722\n",
            "Epoch [16114/20000], Training Loss: 0.0761\n",
            "Epoch [16115/20000], Training Loss: 0.0835\n",
            "Epoch [16116/20000], Training Loss: 0.0759\n",
            "Epoch [16117/20000], Training Loss: 0.0757\n",
            "Epoch [16118/20000], Training Loss: 0.0746\n",
            "Epoch [16119/20000], Training Loss: 0.0758\n",
            "Epoch [16120/20000], Training Loss: 0.0779\n",
            "Epoch [16121/20000], Training Loss: 0.0794\n",
            "Epoch [16122/20000], Training Loss: 0.0843\n",
            "Epoch [16123/20000], Training Loss: 0.0808\n",
            "Epoch [16124/20000], Training Loss: 0.0825\n",
            "Epoch [16125/20000], Training Loss: 0.0797\n",
            "Epoch [16126/20000], Training Loss: 0.0763\n",
            "Epoch [16127/20000], Training Loss: 0.0755\n",
            "Epoch [16128/20000], Training Loss: 0.0750\n",
            "Epoch [16129/20000], Training Loss: 0.0804\n",
            "Epoch [16130/20000], Training Loss: 0.0788\n",
            "Epoch [16131/20000], Training Loss: 0.0733\n",
            "Epoch [16132/20000], Training Loss: 0.0756\n",
            "Epoch [16133/20000], Training Loss: 0.0840\n",
            "Epoch [16134/20000], Training Loss: 0.0794\n",
            "Epoch [16135/20000], Training Loss: 0.0734\n",
            "Epoch [16136/20000], Training Loss: 0.0816\n",
            "Epoch [16137/20000], Training Loss: 0.0854\n",
            "Epoch [16138/20000], Training Loss: 0.0826\n",
            "Epoch [16139/20000], Training Loss: 0.0760\n",
            "Epoch [16140/20000], Training Loss: 0.0761\n",
            "Epoch [16141/20000], Training Loss: 0.0851\n",
            "Epoch [16142/20000], Training Loss: 0.0804\n",
            "Epoch [16143/20000], Training Loss: 0.0788\n",
            "Epoch [16144/20000], Training Loss: 0.0795\n",
            "Epoch [16145/20000], Training Loss: 0.0807\n",
            "Epoch [16146/20000], Training Loss: 0.0795\n",
            "Epoch [16147/20000], Training Loss: 0.0778\n",
            "Epoch [16148/20000], Training Loss: 0.0790\n",
            "Epoch [16149/20000], Training Loss: 0.0745\n",
            "Epoch [16150/20000], Training Loss: 0.0857\n",
            "Epoch [16151/20000], Training Loss: 0.0792\n",
            "Epoch [16152/20000], Training Loss: 0.0840\n",
            "Epoch [16153/20000], Training Loss: 0.0797\n",
            "Epoch [16154/20000], Training Loss: 0.0828\n",
            "Epoch [16155/20000], Training Loss: 0.0792\n",
            "Epoch [16156/20000], Training Loss: 0.0818\n",
            "Epoch [16157/20000], Training Loss: 0.0739\n",
            "Epoch [16158/20000], Training Loss: 0.0773\n",
            "Epoch [16159/20000], Training Loss: 0.0794\n",
            "Epoch [16160/20000], Training Loss: 0.0850\n",
            "Epoch [16161/20000], Training Loss: 0.0854\n",
            "Epoch [16162/20000], Training Loss: 0.0753\n",
            "Epoch [16163/20000], Training Loss: 0.0787\n",
            "Epoch [16164/20000], Training Loss: 0.0808\n",
            "Epoch [16165/20000], Training Loss: 0.0772\n",
            "Epoch [16166/20000], Training Loss: 0.0787\n",
            "Epoch [16167/20000], Training Loss: 0.0792\n",
            "Epoch [16168/20000], Training Loss: 0.0838\n",
            "Epoch [16169/20000], Training Loss: 0.0833\n",
            "Epoch [16170/20000], Training Loss: 0.0806\n",
            "Epoch [16171/20000], Training Loss: 0.0759\n",
            "Epoch [16172/20000], Training Loss: 0.0751\n",
            "Epoch [16173/20000], Training Loss: 0.0761\n",
            "Epoch [16174/20000], Training Loss: 0.0805\n",
            "Epoch [16175/20000], Training Loss: 0.0800\n",
            "Epoch [16176/20000], Training Loss: 0.0830\n",
            "Epoch [16177/20000], Training Loss: 0.0793\n",
            "Epoch [16178/20000], Training Loss: 0.0746\n",
            "Epoch [16179/20000], Training Loss: 0.0776\n",
            "Epoch [16180/20000], Training Loss: 0.0802\n",
            "Epoch [16181/20000], Training Loss: 0.0853\n",
            "Epoch [16182/20000], Training Loss: 0.0797\n",
            "Epoch [16183/20000], Training Loss: 0.0729\n",
            "Epoch [16184/20000], Training Loss: 0.0798\n",
            "Epoch [16185/20000], Training Loss: 0.0836\n",
            "Epoch [16186/20000], Training Loss: 0.0812\n",
            "Epoch [16187/20000], Training Loss: 0.0819\n",
            "Epoch [16188/20000], Training Loss: 0.0769\n",
            "Epoch [16189/20000], Training Loss: 0.0835\n",
            "Epoch [16190/20000], Training Loss: 0.0761\n",
            "Epoch [16191/20000], Training Loss: 0.0813\n",
            "Epoch [16192/20000], Training Loss: 0.0778\n",
            "Epoch [16193/20000], Training Loss: 0.0792\n",
            "Epoch [16194/20000], Training Loss: 0.0772\n",
            "Epoch [16195/20000], Training Loss: 0.0881\n",
            "Epoch [16196/20000], Training Loss: 0.0831\n",
            "Epoch [16197/20000], Training Loss: 0.0858\n",
            "Epoch [16198/20000], Training Loss: 0.0809\n",
            "Epoch [16199/20000], Training Loss: 0.0798\n",
            "Epoch [16200/20000], Training Loss: 0.0762\n",
            "Epoch [16201/20000], Training Loss: 0.0799\n",
            "Epoch [16202/20000], Training Loss: 0.0772\n",
            "Epoch [16203/20000], Training Loss: 0.0835\n",
            "Epoch [16204/20000], Training Loss: 0.0801\n",
            "Epoch [16205/20000], Training Loss: 0.0834\n",
            "Epoch [16206/20000], Training Loss: 0.0835\n",
            "Epoch [16207/20000], Training Loss: 0.0800\n",
            "Epoch [16208/20000], Training Loss: 0.0799\n",
            "Epoch [16209/20000], Training Loss: 0.0751\n",
            "Epoch [16210/20000], Training Loss: 0.0840\n",
            "Epoch [16211/20000], Training Loss: 0.0868\n",
            "Epoch [16212/20000], Training Loss: 0.0765\n",
            "Epoch [16213/20000], Training Loss: 0.0846\n",
            "Epoch [16214/20000], Training Loss: 0.0849\n",
            "Epoch [16215/20000], Training Loss: 0.0742\n",
            "Epoch [16216/20000], Training Loss: 0.0781\n",
            "Epoch [16217/20000], Training Loss: 0.0809\n",
            "Epoch [16218/20000], Training Loss: 0.0811\n",
            "Epoch [16219/20000], Training Loss: 0.0805\n",
            "Epoch [16220/20000], Training Loss: 0.0750\n",
            "Epoch [16221/20000], Training Loss: 0.0774\n",
            "Epoch [16222/20000], Training Loss: 0.0827\n",
            "Epoch [16223/20000], Training Loss: 0.0813\n",
            "Epoch [16224/20000], Training Loss: 0.0759\n",
            "Epoch [16225/20000], Training Loss: 0.0862\n",
            "Epoch [16226/20000], Training Loss: 0.0748\n",
            "Epoch [16227/20000], Training Loss: 0.0842\n",
            "Epoch [16228/20000], Training Loss: 0.0797\n",
            "Epoch [16229/20000], Training Loss: 0.0749\n",
            "Epoch [16230/20000], Training Loss: 0.0805\n",
            "Epoch [16231/20000], Training Loss: 0.0764\n",
            "Epoch [16232/20000], Training Loss: 0.0858\n",
            "Epoch [16233/20000], Training Loss: 0.0859\n",
            "Epoch [16234/20000], Training Loss: 0.0768\n",
            "Epoch [16235/20000], Training Loss: 0.0892\n",
            "Epoch [16236/20000], Training Loss: 0.0794\n",
            "Epoch [16237/20000], Training Loss: 0.0808\n",
            "Epoch [16238/20000], Training Loss: 0.0847\n",
            "Epoch [16239/20000], Training Loss: 0.0775\n",
            "Epoch [16240/20000], Training Loss: 0.0814\n",
            "Epoch [16241/20000], Training Loss: 0.0756\n",
            "Epoch [16242/20000], Training Loss: 0.0747\n",
            "Epoch [16243/20000], Training Loss: 0.0759\n",
            "Epoch [16244/20000], Training Loss: 0.0736\n",
            "Epoch [16245/20000], Training Loss: 0.0816\n",
            "Epoch [16246/20000], Training Loss: 0.0752\n",
            "Epoch [16247/20000], Training Loss: 0.0769\n",
            "Epoch [16248/20000], Training Loss: 0.0785\n",
            "Epoch [16249/20000], Training Loss: 0.0799\n",
            "Epoch [16250/20000], Training Loss: 0.0808\n",
            "Epoch [16251/20000], Training Loss: 0.0745\n",
            "Epoch [16252/20000], Training Loss: 0.0801\n",
            "Epoch [16253/20000], Training Loss: 0.0808\n",
            "Epoch [16254/20000], Training Loss: 0.0860\n",
            "Epoch [16255/20000], Training Loss: 0.0737\n",
            "Epoch [16256/20000], Training Loss: 0.0809\n",
            "Epoch [16257/20000], Training Loss: 0.0740\n",
            "Epoch [16258/20000], Training Loss: 0.0860\n",
            "Epoch [16259/20000], Training Loss: 0.0828\n",
            "Epoch [16260/20000], Training Loss: 0.0735\n",
            "Epoch [16261/20000], Training Loss: 0.0845\n",
            "Epoch [16262/20000], Training Loss: 0.0806\n",
            "Epoch [16263/20000], Training Loss: 0.0769\n",
            "Epoch [16264/20000], Training Loss: 0.0827\n",
            "Epoch [16265/20000], Training Loss: 0.0754\n",
            "Epoch [16266/20000], Training Loss: 0.0774\n",
            "Epoch [16267/20000], Training Loss: 0.0786\n",
            "Epoch [16268/20000], Training Loss: 0.0853\n",
            "Epoch [16269/20000], Training Loss: 0.0744\n",
            "Epoch [16270/20000], Training Loss: 0.0808\n",
            "Epoch [16271/20000], Training Loss: 0.0827\n",
            "Epoch [16272/20000], Training Loss: 0.0748\n",
            "Epoch [16273/20000], Training Loss: 0.0792\n",
            "Epoch [16274/20000], Training Loss: 0.0752\n",
            "Epoch [16275/20000], Training Loss: 0.0826\n",
            "Epoch [16276/20000], Training Loss: 0.0835\n",
            "Epoch [16277/20000], Training Loss: 0.0789\n",
            "Epoch [16278/20000], Training Loss: 0.0821\n",
            "Epoch [16279/20000], Training Loss: 0.0770\n",
            "Epoch [16280/20000], Training Loss: 0.0809\n",
            "Epoch [16281/20000], Training Loss: 0.0793\n",
            "Epoch [16282/20000], Training Loss: 0.0770\n",
            "Epoch [16283/20000], Training Loss: 0.0743\n",
            "Epoch [16284/20000], Training Loss: 0.0850\n",
            "Epoch [16285/20000], Training Loss: 0.0749\n",
            "Epoch [16286/20000], Training Loss: 0.0780\n",
            "Epoch [16287/20000], Training Loss: 0.0798\n",
            "Epoch [16288/20000], Training Loss: 0.0773\n",
            "Epoch [16289/20000], Training Loss: 0.0791\n",
            "Epoch [16290/20000], Training Loss: 0.0796\n",
            "Epoch [16291/20000], Training Loss: 0.0779\n",
            "Epoch [16292/20000], Training Loss: 0.0779\n",
            "Epoch [16293/20000], Training Loss: 0.0835\n",
            "Epoch [16294/20000], Training Loss: 0.0860\n",
            "Epoch [16295/20000], Training Loss: 0.0769\n",
            "Epoch [16296/20000], Training Loss: 0.0815\n",
            "Epoch [16297/20000], Training Loss: 0.0756\n",
            "Epoch [16298/20000], Training Loss: 0.0862\n",
            "Epoch [16299/20000], Training Loss: 0.0754\n",
            "Epoch [16300/20000], Training Loss: 0.0742\n",
            "Epoch [16301/20000], Training Loss: 0.0828\n",
            "Epoch [16302/20000], Training Loss: 0.0763\n",
            "Epoch [16303/20000], Training Loss: 0.0744\n",
            "Epoch [16304/20000], Training Loss: 0.0784\n",
            "Epoch [16305/20000], Training Loss: 0.0829\n",
            "Epoch [16306/20000], Training Loss: 0.0805\n",
            "Epoch [16307/20000], Training Loss: 0.0743\n",
            "Epoch [16308/20000], Training Loss: 0.0835\n",
            "Epoch [16309/20000], Training Loss: 0.0780\n",
            "Epoch [16310/20000], Training Loss: 0.0811\n",
            "Epoch [16311/20000], Training Loss: 0.0780\n",
            "Epoch [16312/20000], Training Loss: 0.0825\n",
            "Epoch [16313/20000], Training Loss: 0.0803\n",
            "Epoch [16314/20000], Training Loss: 0.0780\n",
            "Epoch [16315/20000], Training Loss: 0.0800\n",
            "Epoch [16316/20000], Training Loss: 0.0847\n",
            "Epoch [16317/20000], Training Loss: 0.0788\n",
            "Epoch [16318/20000], Training Loss: 0.0774\n",
            "Epoch [16319/20000], Training Loss: 0.0737\n",
            "Epoch [16320/20000], Training Loss: 0.0742\n",
            "Epoch [16321/20000], Training Loss: 0.0828\n",
            "Epoch [16322/20000], Training Loss: 0.0743\n",
            "Epoch [16323/20000], Training Loss: 0.0789\n",
            "Epoch [16324/20000], Training Loss: 0.0795\n",
            "Epoch [16325/20000], Training Loss: 0.0752\n",
            "Epoch [16326/20000], Training Loss: 0.0853\n",
            "Epoch [16327/20000], Training Loss: 0.0836\n",
            "Epoch [16328/20000], Training Loss: 0.0833\n",
            "Epoch [16329/20000], Training Loss: 0.0771\n",
            "Epoch [16330/20000], Training Loss: 0.0769\n",
            "Epoch [16331/20000], Training Loss: 0.0850\n",
            "Epoch [16332/20000], Training Loss: 0.0797\n",
            "Epoch [16333/20000], Training Loss: 0.0749\n",
            "Epoch [16334/20000], Training Loss: 0.0828\n",
            "Epoch [16335/20000], Training Loss: 0.0776\n",
            "Epoch [16336/20000], Training Loss: 0.0847\n",
            "Epoch [16337/20000], Training Loss: 0.0810\n",
            "Epoch [16338/20000], Training Loss: 0.0824\n",
            "Epoch [16339/20000], Training Loss: 0.0785\n",
            "Epoch [16340/20000], Training Loss: 0.0784\n",
            "Epoch [16341/20000], Training Loss: 0.0842\n",
            "Epoch [16342/20000], Training Loss: 0.0807\n",
            "Epoch [16343/20000], Training Loss: 0.0877\n",
            "Epoch [16344/20000], Training Loss: 0.0800\n",
            "Epoch [16345/20000], Training Loss: 0.0816\n",
            "Epoch [16346/20000], Training Loss: 0.0759\n",
            "Epoch [16347/20000], Training Loss: 0.0776\n",
            "Epoch [16348/20000], Training Loss: 0.0762\n",
            "Epoch [16349/20000], Training Loss: 0.0803\n",
            "Epoch [16350/20000], Training Loss: 0.0800\n",
            "Epoch [16351/20000], Training Loss: 0.0748\n",
            "Epoch [16352/20000], Training Loss: 0.0749\n",
            "Epoch [16353/20000], Training Loss: 0.0828\n",
            "Epoch [16354/20000], Training Loss: 0.0798\n",
            "Epoch [16355/20000], Training Loss: 0.0771\n",
            "Epoch [16356/20000], Training Loss: 0.0719\n",
            "Epoch [16357/20000], Training Loss: 0.0769\n",
            "Epoch [16358/20000], Training Loss: 0.0737\n",
            "Epoch [16359/20000], Training Loss: 0.0756\n",
            "Epoch [16360/20000], Training Loss: 0.0793\n",
            "Epoch [16361/20000], Training Loss: 0.0765\n",
            "Epoch [16362/20000], Training Loss: 0.0789\n",
            "Epoch [16363/20000], Training Loss: 0.0845\n",
            "Epoch [16364/20000], Training Loss: 0.0807\n",
            "Epoch [16365/20000], Training Loss: 0.0815\n",
            "Epoch [16366/20000], Training Loss: 0.0841\n",
            "Epoch [16367/20000], Training Loss: 0.0820\n",
            "Epoch [16368/20000], Training Loss: 0.0844\n",
            "Epoch [16369/20000], Training Loss: 0.0729\n",
            "Epoch [16370/20000], Training Loss: 0.0813\n",
            "Epoch [16371/20000], Training Loss: 0.0818\n",
            "Epoch [16372/20000], Training Loss: 0.0744\n",
            "Epoch [16373/20000], Training Loss: 0.0792\n",
            "Epoch [16374/20000], Training Loss: 0.0849\n",
            "Epoch [16375/20000], Training Loss: 0.0768\n",
            "Epoch [16376/20000], Training Loss: 0.0720\n",
            "Epoch [16377/20000], Training Loss: 0.0752\n",
            "Epoch [16378/20000], Training Loss: 0.0791\n",
            "Epoch [16379/20000], Training Loss: 0.0798\n",
            "Epoch [16380/20000], Training Loss: 0.0785\n",
            "Epoch [16381/20000], Training Loss: 0.0854\n",
            "Epoch [16382/20000], Training Loss: 0.0841\n",
            "Epoch [16383/20000], Training Loss: 0.0809\n",
            "Epoch [16384/20000], Training Loss: 0.0802\n",
            "Epoch [16385/20000], Training Loss: 0.0798\n",
            "Epoch [16386/20000], Training Loss: 0.0751\n",
            "Epoch [16387/20000], Training Loss: 0.0838\n",
            "Epoch [16388/20000], Training Loss: 0.0862\n",
            "Epoch [16389/20000], Training Loss: 0.0865\n",
            "Epoch [16390/20000], Training Loss: 0.0749\n",
            "Epoch [16391/20000], Training Loss: 0.0782\n",
            "Epoch [16392/20000], Training Loss: 0.0756\n",
            "Epoch [16393/20000], Training Loss: 0.0784\n",
            "Epoch [16394/20000], Training Loss: 0.0755\n",
            "Epoch [16395/20000], Training Loss: 0.0771\n",
            "Epoch [16396/20000], Training Loss: 0.0801\n",
            "Epoch [16397/20000], Training Loss: 0.0761\n",
            "Epoch [16398/20000], Training Loss: 0.0817\n",
            "Epoch [16399/20000], Training Loss: 0.0768\n",
            "Epoch [16400/20000], Training Loss: 0.0837\n",
            "Epoch [16401/20000], Training Loss: 0.0776\n",
            "Epoch [16402/20000], Training Loss: 0.0741\n",
            "Epoch [16403/20000], Training Loss: 0.0741\n",
            "Epoch [16404/20000], Training Loss: 0.0761\n",
            "Epoch [16405/20000], Training Loss: 0.0788\n",
            "Epoch [16406/20000], Training Loss: 0.0835\n",
            "Epoch [16407/20000], Training Loss: 0.0757\n",
            "Epoch [16408/20000], Training Loss: 0.0793\n",
            "Epoch [16409/20000], Training Loss: 0.0818\n",
            "Epoch [16410/20000], Training Loss: 0.0875\n",
            "Epoch [16411/20000], Training Loss: 0.0854\n",
            "Epoch [16412/20000], Training Loss: 0.0795\n",
            "Epoch [16413/20000], Training Loss: 0.0834\n",
            "Epoch [16414/20000], Training Loss: 0.0801\n",
            "Epoch [16415/20000], Training Loss: 0.0761\n",
            "Epoch [16416/20000], Training Loss: 0.0765\n",
            "Epoch [16417/20000], Training Loss: 0.0875\n",
            "Epoch [16418/20000], Training Loss: 0.0791\n",
            "Epoch [16419/20000], Training Loss: 0.0772\n",
            "Epoch [16420/20000], Training Loss: 0.0801\n",
            "Epoch [16421/20000], Training Loss: 0.0856\n",
            "Epoch [16422/20000], Training Loss: 0.0776\n",
            "Epoch [16423/20000], Training Loss: 0.0795\n",
            "Epoch [16424/20000], Training Loss: 0.0751\n",
            "Epoch [16425/20000], Training Loss: 0.0751\n",
            "Epoch [16426/20000], Training Loss: 0.0820\n",
            "Epoch [16427/20000], Training Loss: 0.0735\n",
            "Epoch [16428/20000], Training Loss: 0.0798\n",
            "Epoch [16429/20000], Training Loss: 0.0780\n",
            "Epoch [16430/20000], Training Loss: 0.0817\n",
            "Epoch [16431/20000], Training Loss: 0.0821\n",
            "Epoch [16432/20000], Training Loss: 0.0722\n",
            "Epoch [16433/20000], Training Loss: 0.0792\n",
            "Epoch [16434/20000], Training Loss: 0.0795\n",
            "Epoch [16435/20000], Training Loss: 0.0721\n",
            "Epoch [16436/20000], Training Loss: 0.0738\n",
            "Epoch [16437/20000], Training Loss: 0.0830\n",
            "Epoch [16438/20000], Training Loss: 0.0846\n",
            "Epoch [16439/20000], Training Loss: 0.0751\n",
            "Epoch [16440/20000], Training Loss: 0.0821\n",
            "Epoch [16441/20000], Training Loss: 0.0769\n",
            "Epoch [16442/20000], Training Loss: 0.0849\n",
            "Epoch [16443/20000], Training Loss: 0.0852\n",
            "Epoch [16444/20000], Training Loss: 0.0743\n",
            "Epoch [16445/20000], Training Loss: 0.0839\n",
            "Epoch [16446/20000], Training Loss: 0.0818\n",
            "Epoch [16447/20000], Training Loss: 0.0786\n",
            "Epoch [16448/20000], Training Loss: 0.0804\n",
            "Epoch [16449/20000], Training Loss: 0.0793\n",
            "Epoch [16450/20000], Training Loss: 0.0795\n",
            "Epoch [16451/20000], Training Loss: 0.0777\n",
            "Epoch [16452/20000], Training Loss: 0.0769\n",
            "Epoch [16453/20000], Training Loss: 0.0793\n",
            "Epoch [16454/20000], Training Loss: 0.0751\n",
            "Epoch [16455/20000], Training Loss: 0.0754\n",
            "Epoch [16456/20000], Training Loss: 0.0800\n",
            "Epoch [16457/20000], Training Loss: 0.0786\n",
            "Epoch [16458/20000], Training Loss: 0.0820\n",
            "Epoch [16459/20000], Training Loss: 0.0787\n",
            "Epoch [16460/20000], Training Loss: 0.0844\n",
            "Epoch [16461/20000], Training Loss: 0.0773\n",
            "Epoch [16462/20000], Training Loss: 0.0827\n",
            "Epoch [16463/20000], Training Loss: 0.0809\n",
            "Epoch [16464/20000], Training Loss: 0.0805\n",
            "Epoch [16465/20000], Training Loss: 0.0815\n",
            "Epoch [16466/20000], Training Loss: 0.0746\n",
            "Epoch [16467/20000], Training Loss: 0.0792\n",
            "Epoch [16468/20000], Training Loss: 0.0775\n",
            "Epoch [16469/20000], Training Loss: 0.0796\n",
            "Epoch [16470/20000], Training Loss: 0.0747\n",
            "Epoch [16471/20000], Training Loss: 0.0853\n",
            "Epoch [16472/20000], Training Loss: 0.0859\n",
            "Epoch [16473/20000], Training Loss: 0.0855\n",
            "Epoch [16474/20000], Training Loss: 0.0770\n",
            "Epoch [16475/20000], Training Loss: 0.0808\n",
            "Epoch [16476/20000], Training Loss: 0.0758\n",
            "Epoch [16477/20000], Training Loss: 0.0823\n",
            "Epoch [16478/20000], Training Loss: 0.0835\n",
            "Epoch [16479/20000], Training Loss: 0.0795\n",
            "Epoch [16480/20000], Training Loss: 0.0751\n",
            "Epoch [16481/20000], Training Loss: 0.0721\n",
            "Epoch [16482/20000], Training Loss: 0.0775\n",
            "Epoch [16483/20000], Training Loss: 0.0826\n",
            "Epoch [16484/20000], Training Loss: 0.0807\n",
            "Epoch [16485/20000], Training Loss: 0.0787\n",
            "Epoch [16486/20000], Training Loss: 0.0828\n",
            "Epoch [16487/20000], Training Loss: 0.0744\n",
            "Epoch [16488/20000], Training Loss: 0.0754\n",
            "Epoch [16489/20000], Training Loss: 0.0839\n",
            "Epoch [16490/20000], Training Loss: 0.0832\n",
            "Epoch [16491/20000], Training Loss: 0.0865\n",
            "Epoch [16492/20000], Training Loss: 0.0794\n",
            "Epoch [16493/20000], Training Loss: 0.0787\n",
            "Epoch [16494/20000], Training Loss: 0.0770\n",
            "Epoch [16495/20000], Training Loss: 0.0792\n",
            "Epoch [16496/20000], Training Loss: 0.0751\n",
            "Epoch [16497/20000], Training Loss: 0.0789\n",
            "Epoch [16498/20000], Training Loss: 0.0848\n",
            "Epoch [16499/20000], Training Loss: 0.0851\n",
            "Epoch [16500/20000], Training Loss: 0.0811\n",
            "Epoch [16501/20000], Training Loss: 0.0748\n",
            "Epoch [16502/20000], Training Loss: 0.0766\n",
            "Epoch [16503/20000], Training Loss: 0.0729\n",
            "Epoch [16504/20000], Training Loss: 0.0844\n",
            "Epoch [16505/20000], Training Loss: 0.0807\n",
            "Epoch [16506/20000], Training Loss: 0.0813\n",
            "Epoch [16507/20000], Training Loss: 0.0778\n",
            "Epoch [16508/20000], Training Loss: 0.0856\n",
            "Epoch [16509/20000], Training Loss: 0.0797\n",
            "Epoch [16510/20000], Training Loss: 0.0762\n",
            "Epoch [16511/20000], Training Loss: 0.0767\n",
            "Epoch [16512/20000], Training Loss: 0.0778\n",
            "Epoch [16513/20000], Training Loss: 0.0826\n",
            "Epoch [16514/20000], Training Loss: 0.0767\n",
            "Epoch [16515/20000], Training Loss: 0.0785\n",
            "Epoch [16516/20000], Training Loss: 0.0737\n",
            "Epoch [16517/20000], Training Loss: 0.0757\n",
            "Epoch [16518/20000], Training Loss: 0.0794\n",
            "Epoch [16519/20000], Training Loss: 0.0761\n",
            "Epoch [16520/20000], Training Loss: 0.0787\n",
            "Epoch [16521/20000], Training Loss: 0.0756\n",
            "Epoch [16522/20000], Training Loss: 0.0788\n",
            "Epoch [16523/20000], Training Loss: 0.0851\n",
            "Epoch [16524/20000], Training Loss: 0.0782\n",
            "Epoch [16525/20000], Training Loss: 0.0800\n",
            "Epoch [16526/20000], Training Loss: 0.0850\n",
            "Epoch [16527/20000], Training Loss: 0.0786\n",
            "Epoch [16528/20000], Training Loss: 0.0858\n",
            "Epoch [16529/20000], Training Loss: 0.0736\n",
            "Epoch [16530/20000], Training Loss: 0.0840\n",
            "Epoch [16531/20000], Training Loss: 0.0784\n",
            "Epoch [16532/20000], Training Loss: 0.0850\n",
            "Epoch [16533/20000], Training Loss: 0.0764\n",
            "Epoch [16534/20000], Training Loss: 0.0808\n",
            "Epoch [16535/20000], Training Loss: 0.0746\n",
            "Epoch [16536/20000], Training Loss: 0.0843\n",
            "Epoch [16537/20000], Training Loss: 0.0749\n",
            "Epoch [16538/20000], Training Loss: 0.0824\n",
            "Epoch [16539/20000], Training Loss: 0.0815\n",
            "Epoch [16540/20000], Training Loss: 0.0758\n",
            "Epoch [16541/20000], Training Loss: 0.0765\n",
            "Epoch [16542/20000], Training Loss: 0.0784\n",
            "Epoch [16543/20000], Training Loss: 0.0750\n",
            "Epoch [16544/20000], Training Loss: 0.0840\n",
            "Epoch [16545/20000], Training Loss: 0.0771\n",
            "Epoch [16546/20000], Training Loss: 0.0870\n",
            "Epoch [16547/20000], Training Loss: 0.0779\n",
            "Epoch [16548/20000], Training Loss: 0.0840\n",
            "Epoch [16549/20000], Training Loss: 0.0780\n",
            "Epoch [16550/20000], Training Loss: 0.0751\n",
            "Epoch [16551/20000], Training Loss: 0.0749\n",
            "Epoch [16552/20000], Training Loss: 0.0828\n",
            "Epoch [16553/20000], Training Loss: 0.0785\n",
            "Epoch [16554/20000], Training Loss: 0.0841\n",
            "Epoch [16555/20000], Training Loss: 0.0824\n",
            "Epoch [16556/20000], Training Loss: 0.0778\n",
            "Epoch [16557/20000], Training Loss: 0.0756\n",
            "Epoch [16558/20000], Training Loss: 0.0793\n",
            "Epoch [16559/20000], Training Loss: 0.0737\n",
            "Epoch [16560/20000], Training Loss: 0.0848\n",
            "Epoch [16561/20000], Training Loss: 0.0736\n",
            "Epoch [16562/20000], Training Loss: 0.0799\n",
            "Epoch [16563/20000], Training Loss: 0.0810\n",
            "Epoch [16564/20000], Training Loss: 0.0819\n",
            "Epoch [16565/20000], Training Loss: 0.0772\n",
            "Epoch [16566/20000], Training Loss: 0.0843\n",
            "Epoch [16567/20000], Training Loss: 0.0769\n",
            "Epoch [16568/20000], Training Loss: 0.0816\n",
            "Epoch [16569/20000], Training Loss: 0.0741\n",
            "Epoch [16570/20000], Training Loss: 0.0863\n",
            "Epoch [16571/20000], Training Loss: 0.0843\n",
            "Epoch [16572/20000], Training Loss: 0.0786\n",
            "Epoch [16573/20000], Training Loss: 0.0766\n",
            "Epoch [16574/20000], Training Loss: 0.0797\n",
            "Epoch [16575/20000], Training Loss: 0.0859\n",
            "Epoch [16576/20000], Training Loss: 0.0754\n",
            "Epoch [16577/20000], Training Loss: 0.0764\n",
            "Epoch [16578/20000], Training Loss: 0.0770\n",
            "Epoch [16579/20000], Training Loss: 0.0765\n",
            "Epoch [16580/20000], Training Loss: 0.0859\n",
            "Epoch [16581/20000], Training Loss: 0.0826\n",
            "Epoch [16582/20000], Training Loss: 0.0773\n",
            "Epoch [16583/20000], Training Loss: 0.0884\n",
            "Epoch [16584/20000], Training Loss: 0.0751\n",
            "Epoch [16585/20000], Training Loss: 0.0719\n",
            "Epoch [16586/20000], Training Loss: 0.0809\n",
            "Epoch [16587/20000], Training Loss: 0.0827\n",
            "Epoch [16588/20000], Training Loss: 0.0790\n",
            "Epoch [16589/20000], Training Loss: 0.0869\n",
            "Epoch [16590/20000], Training Loss: 0.0831\n",
            "Epoch [16591/20000], Training Loss: 0.0770\n",
            "Epoch [16592/20000], Training Loss: 0.0752\n",
            "Epoch [16593/20000], Training Loss: 0.0796\n",
            "Epoch [16594/20000], Training Loss: 0.0813\n",
            "Epoch [16595/20000], Training Loss: 0.0790\n",
            "Epoch [16596/20000], Training Loss: 0.0790\n",
            "Epoch [16597/20000], Training Loss: 0.0786\n",
            "Epoch [16598/20000], Training Loss: 0.0729\n",
            "Epoch [16599/20000], Training Loss: 0.0789\n",
            "Epoch [16600/20000], Training Loss: 0.0844\n",
            "Epoch [16601/20000], Training Loss: 0.0754\n",
            "Epoch [16602/20000], Training Loss: 0.0826\n",
            "Epoch [16603/20000], Training Loss: 0.0871\n",
            "Epoch [16604/20000], Training Loss: 0.0729\n",
            "Epoch [16605/20000], Training Loss: 0.0800\n",
            "Epoch [16606/20000], Training Loss: 0.0775\n",
            "Epoch [16607/20000], Training Loss: 0.0792\n",
            "Epoch [16608/20000], Training Loss: 0.0761\n",
            "Epoch [16609/20000], Training Loss: 0.0798\n",
            "Epoch [16610/20000], Training Loss: 0.0845\n",
            "Epoch [16611/20000], Training Loss: 0.0783\n",
            "Epoch [16612/20000], Training Loss: 0.0744\n",
            "Epoch [16613/20000], Training Loss: 0.0803\n",
            "Epoch [16614/20000], Training Loss: 0.0862\n",
            "Epoch [16615/20000], Training Loss: 0.0813\n",
            "Epoch [16616/20000], Training Loss: 0.0794\n",
            "Epoch [16617/20000], Training Loss: 0.0851\n",
            "Epoch [16618/20000], Training Loss: 0.0851\n",
            "Epoch [16619/20000], Training Loss: 0.0827\n",
            "Epoch [16620/20000], Training Loss: 0.0854\n",
            "Epoch [16621/20000], Training Loss: 0.0808\n",
            "Epoch [16622/20000], Training Loss: 0.0764\n",
            "Epoch [16623/20000], Training Loss: 0.0807\n",
            "Epoch [16624/20000], Training Loss: 0.0810\n",
            "Epoch [16625/20000], Training Loss: 0.0782\n",
            "Epoch [16626/20000], Training Loss: 0.0793\n",
            "Epoch [16627/20000], Training Loss: 0.0850\n",
            "Epoch [16628/20000], Training Loss: 0.0820\n",
            "Epoch [16629/20000], Training Loss: 0.0766\n",
            "Epoch [16630/20000], Training Loss: 0.0862\n",
            "Epoch [16631/20000], Training Loss: 0.0840\n",
            "Epoch [16632/20000], Training Loss: 0.0811\n",
            "Epoch [16633/20000], Training Loss: 0.0823\n",
            "Epoch [16634/20000], Training Loss: 0.0798\n",
            "Epoch [16635/20000], Training Loss: 0.0804\n",
            "Epoch [16636/20000], Training Loss: 0.0809\n",
            "Epoch [16637/20000], Training Loss: 0.0784\n",
            "Epoch [16638/20000], Training Loss: 0.0804\n",
            "Epoch [16639/20000], Training Loss: 0.0788\n",
            "Epoch [16640/20000], Training Loss: 0.0795\n",
            "Epoch [16641/20000], Training Loss: 0.0876\n",
            "Epoch [16642/20000], Training Loss: 0.0778\n",
            "Epoch [16643/20000], Training Loss: 0.0793\n",
            "Epoch [16644/20000], Training Loss: 0.0789\n",
            "Epoch [16645/20000], Training Loss: 0.0839\n",
            "Epoch [16646/20000], Training Loss: 0.0860\n",
            "Epoch [16647/20000], Training Loss: 0.0797\n",
            "Epoch [16648/20000], Training Loss: 0.0730\n",
            "Epoch [16649/20000], Training Loss: 0.0807\n",
            "Epoch [16650/20000], Training Loss: 0.0801\n",
            "Epoch [16651/20000], Training Loss: 0.0832\n",
            "Epoch [16652/20000], Training Loss: 0.0761\n",
            "Epoch [16653/20000], Training Loss: 0.0797\n",
            "Epoch [16654/20000], Training Loss: 0.0801\n",
            "Epoch [16655/20000], Training Loss: 0.0772\n",
            "Epoch [16656/20000], Training Loss: 0.0739\n",
            "Epoch [16657/20000], Training Loss: 0.0826\n",
            "Epoch [16658/20000], Training Loss: 0.0790\n",
            "Epoch [16659/20000], Training Loss: 0.0766\n",
            "Epoch [16660/20000], Training Loss: 0.0783\n",
            "Epoch [16661/20000], Training Loss: 0.0745\n",
            "Epoch [16662/20000], Training Loss: 0.0785\n",
            "Epoch [16663/20000], Training Loss: 0.0860\n",
            "Epoch [16664/20000], Training Loss: 0.0842\n",
            "Epoch [16665/20000], Training Loss: 0.0783\n",
            "Epoch [16666/20000], Training Loss: 0.0809\n",
            "Epoch [16667/20000], Training Loss: 0.0804\n",
            "Epoch [16668/20000], Training Loss: 0.0790\n",
            "Epoch [16669/20000], Training Loss: 0.0871\n",
            "Epoch [16670/20000], Training Loss: 0.0760\n",
            "Epoch [16671/20000], Training Loss: 0.0833\n",
            "Epoch [16672/20000], Training Loss: 0.0768\n",
            "Epoch [16673/20000], Training Loss: 0.0747\n",
            "Epoch [16674/20000], Training Loss: 0.0744\n",
            "Epoch [16675/20000], Training Loss: 0.0825\n",
            "Epoch [16676/20000], Training Loss: 0.0798\n",
            "Epoch [16677/20000], Training Loss: 0.0766\n",
            "Epoch [16678/20000], Training Loss: 0.0775\n",
            "Epoch [16679/20000], Training Loss: 0.0811\n",
            "Epoch [16680/20000], Training Loss: 0.0741\n",
            "Epoch [16681/20000], Training Loss: 0.0810\n",
            "Epoch [16682/20000], Training Loss: 0.0781\n",
            "Epoch [16683/20000], Training Loss: 0.0792\n",
            "Epoch [16684/20000], Training Loss: 0.0807\n",
            "Epoch [16685/20000], Training Loss: 0.0783\n",
            "Epoch [16686/20000], Training Loss: 0.0769\n",
            "Epoch [16687/20000], Training Loss: 0.0753\n",
            "Epoch [16688/20000], Training Loss: 0.0816\n",
            "Epoch [16689/20000], Training Loss: 0.0866\n",
            "Epoch [16690/20000], Training Loss: 0.0760\n",
            "Epoch [16691/20000], Training Loss: 0.0810\n",
            "Epoch [16692/20000], Training Loss: 0.0746\n",
            "Epoch [16693/20000], Training Loss: 0.0814\n",
            "Epoch [16694/20000], Training Loss: 0.0822\n",
            "Epoch [16695/20000], Training Loss: 0.0735\n",
            "Epoch [16696/20000], Training Loss: 0.0783\n",
            "Epoch [16697/20000], Training Loss: 0.0849\n",
            "Epoch [16698/20000], Training Loss: 0.0797\n",
            "Epoch [16699/20000], Training Loss: 0.0786\n",
            "Epoch [16700/20000], Training Loss: 0.0748\n",
            "Epoch [16701/20000], Training Loss: 0.0855\n",
            "Epoch [16702/20000], Training Loss: 0.0767\n",
            "Epoch [16703/20000], Training Loss: 0.0755\n",
            "Epoch [16704/20000], Training Loss: 0.0825\n",
            "Epoch [16705/20000], Training Loss: 0.0770\n",
            "Epoch [16706/20000], Training Loss: 0.0810\n",
            "Epoch [16707/20000], Training Loss: 0.0845\n",
            "Epoch [16708/20000], Training Loss: 0.0803\n",
            "Epoch [16709/20000], Training Loss: 0.0765\n",
            "Epoch [16710/20000], Training Loss: 0.0784\n",
            "Epoch [16711/20000], Training Loss: 0.0787\n",
            "Epoch [16712/20000], Training Loss: 0.0812\n",
            "Epoch [16713/20000], Training Loss: 0.0841\n",
            "Epoch [16714/20000], Training Loss: 0.0825\n",
            "Epoch [16715/20000], Training Loss: 0.0796\n",
            "Epoch [16716/20000], Training Loss: 0.0837\n",
            "Epoch [16717/20000], Training Loss: 0.0745\n",
            "Epoch [16718/20000], Training Loss: 0.0809\n",
            "Epoch [16719/20000], Training Loss: 0.0835\n",
            "Epoch [16720/20000], Training Loss: 0.0778\n",
            "Epoch [16721/20000], Training Loss: 0.0804\n",
            "Epoch [16722/20000], Training Loss: 0.0820\n",
            "Epoch [16723/20000], Training Loss: 0.0782\n",
            "Epoch [16724/20000], Training Loss: 0.0813\n",
            "Epoch [16725/20000], Training Loss: 0.0769\n",
            "Epoch [16726/20000], Training Loss: 0.0743\n",
            "Epoch [16727/20000], Training Loss: 0.0787\n",
            "Epoch [16728/20000], Training Loss: 0.0747\n",
            "Epoch [16729/20000], Training Loss: 0.0830\n",
            "Epoch [16730/20000], Training Loss: 0.0808\n",
            "Epoch [16731/20000], Training Loss: 0.0785\n",
            "Epoch [16732/20000], Training Loss: 0.0743\n",
            "Epoch [16733/20000], Training Loss: 0.0797\n",
            "Epoch [16734/20000], Training Loss: 0.0823\n",
            "Epoch [16735/20000], Training Loss: 0.0800\n",
            "Epoch [16736/20000], Training Loss: 0.0819\n",
            "Epoch [16737/20000], Training Loss: 0.0783\n",
            "Epoch [16738/20000], Training Loss: 0.0767\n",
            "Epoch [16739/20000], Training Loss: 0.0743\n",
            "Epoch [16740/20000], Training Loss: 0.0789\n",
            "Epoch [16741/20000], Training Loss: 0.0846\n",
            "Epoch [16742/20000], Training Loss: 0.0800\n",
            "Epoch [16743/20000], Training Loss: 0.0849\n",
            "Epoch [16744/20000], Training Loss: 0.0840\n",
            "Epoch [16745/20000], Training Loss: 0.0816\n",
            "Epoch [16746/20000], Training Loss: 0.0851\n",
            "Epoch [16747/20000], Training Loss: 0.0819\n",
            "Epoch [16748/20000], Training Loss: 0.0861\n",
            "Epoch [16749/20000], Training Loss: 0.0736\n",
            "Epoch [16750/20000], Training Loss: 0.0744\n",
            "Epoch [16751/20000], Training Loss: 0.0777\n",
            "Epoch [16752/20000], Training Loss: 0.0797\n",
            "Epoch [16753/20000], Training Loss: 0.0779\n",
            "Epoch [16754/20000], Training Loss: 0.0767\n",
            "Epoch [16755/20000], Training Loss: 0.0807\n",
            "Epoch [16756/20000], Training Loss: 0.0822\n",
            "Epoch [16757/20000], Training Loss: 0.0752\n",
            "Epoch [16758/20000], Training Loss: 0.0798\n",
            "Epoch [16759/20000], Training Loss: 0.0796\n",
            "Epoch [16760/20000], Training Loss: 0.0832\n",
            "Epoch [16761/20000], Training Loss: 0.0801\n",
            "Epoch [16762/20000], Training Loss: 0.0758\n",
            "Epoch [16763/20000], Training Loss: 0.0830\n",
            "Epoch [16764/20000], Training Loss: 0.0818\n",
            "Epoch [16765/20000], Training Loss: 0.0767\n",
            "Epoch [16766/20000], Training Loss: 0.0814\n",
            "Epoch [16767/20000], Training Loss: 0.0784\n",
            "Epoch [16768/20000], Training Loss: 0.0834\n",
            "Epoch [16769/20000], Training Loss: 0.0827\n",
            "Epoch [16770/20000], Training Loss: 0.0737\n",
            "Epoch [16771/20000], Training Loss: 0.0841\n",
            "Epoch [16772/20000], Training Loss: 0.0737\n",
            "Epoch [16773/20000], Training Loss: 0.0801\n",
            "Epoch [16774/20000], Training Loss: 0.0815\n",
            "Epoch [16775/20000], Training Loss: 0.0856\n",
            "Epoch [16776/20000], Training Loss: 0.0841\n",
            "Epoch [16777/20000], Training Loss: 0.0837\n",
            "Epoch [16778/20000], Training Loss: 0.0745\n",
            "Epoch [16779/20000], Training Loss: 0.0836\n",
            "Epoch [16780/20000], Training Loss: 0.0798\n",
            "Epoch [16781/20000], Training Loss: 0.0790\n",
            "Epoch [16782/20000], Training Loss: 0.0786\n",
            "Epoch [16783/20000], Training Loss: 0.0812\n",
            "Epoch [16784/20000], Training Loss: 0.0815\n",
            "Epoch [16785/20000], Training Loss: 0.0786\n",
            "Epoch [16786/20000], Training Loss: 0.0770\n",
            "Epoch [16787/20000], Training Loss: 0.0783\n",
            "Epoch [16788/20000], Training Loss: 0.0862\n",
            "Epoch [16789/20000], Training Loss: 0.0749\n",
            "Epoch [16790/20000], Training Loss: 0.0799\n",
            "Epoch [16791/20000], Training Loss: 0.0787\n",
            "Epoch [16792/20000], Training Loss: 0.0809\n",
            "Epoch [16793/20000], Training Loss: 0.0805\n",
            "Epoch [16794/20000], Training Loss: 0.0809\n",
            "Epoch [16795/20000], Training Loss: 0.0802\n",
            "Epoch [16796/20000], Training Loss: 0.0789\n",
            "Epoch [16797/20000], Training Loss: 0.0773\n",
            "Epoch [16798/20000], Training Loss: 0.0822\n",
            "Epoch [16799/20000], Training Loss: 0.0798\n",
            "Epoch [16800/20000], Training Loss: 0.0794\n",
            "Epoch [16801/20000], Training Loss: 0.0810\n",
            "Epoch [16802/20000], Training Loss: 0.0841\n",
            "Epoch [16803/20000], Training Loss: 0.0739\n",
            "Epoch [16804/20000], Training Loss: 0.0794\n",
            "Epoch [16805/20000], Training Loss: 0.0806\n",
            "Epoch [16806/20000], Training Loss: 0.0807\n",
            "Epoch [16807/20000], Training Loss: 0.0857\n",
            "Epoch [16808/20000], Training Loss: 0.0848\n",
            "Epoch [16809/20000], Training Loss: 0.0756\n",
            "Epoch [16810/20000], Training Loss: 0.0834\n",
            "Epoch [16811/20000], Training Loss: 0.0743\n",
            "Epoch [16812/20000], Training Loss: 0.0778\n",
            "Epoch [16813/20000], Training Loss: 0.0818\n",
            "Epoch [16814/20000], Training Loss: 0.0850\n",
            "Epoch [16815/20000], Training Loss: 0.0793\n",
            "Epoch [16816/20000], Training Loss: 0.0833\n",
            "Epoch [16817/20000], Training Loss: 0.0793\n",
            "Epoch [16818/20000], Training Loss: 0.0857\n",
            "Epoch [16819/20000], Training Loss: 0.0828\n",
            "Epoch [16820/20000], Training Loss: 0.0780\n",
            "Epoch [16821/20000], Training Loss: 0.0743\n",
            "Epoch [16822/20000], Training Loss: 0.0782\n",
            "Epoch [16823/20000], Training Loss: 0.0810\n",
            "Epoch [16824/20000], Training Loss: 0.0850\n",
            "Epoch [16825/20000], Training Loss: 0.0792\n",
            "Epoch [16826/20000], Training Loss: 0.0791\n",
            "Epoch [16827/20000], Training Loss: 0.0731\n",
            "Epoch [16828/20000], Training Loss: 0.0787\n",
            "Epoch [16829/20000], Training Loss: 0.0790\n",
            "Epoch [16830/20000], Training Loss: 0.0773\n",
            "Epoch [16831/20000], Training Loss: 0.0823\n",
            "Epoch [16832/20000], Training Loss: 0.0794\n",
            "Epoch [16833/20000], Training Loss: 0.0761\n",
            "Epoch [16834/20000], Training Loss: 0.0793\n",
            "Epoch [16835/20000], Training Loss: 0.0837\n",
            "Epoch [16836/20000], Training Loss: 0.0756\n",
            "Epoch [16837/20000], Training Loss: 0.0782\n",
            "Epoch [16838/20000], Training Loss: 0.0827\n",
            "Epoch [16839/20000], Training Loss: 0.0824\n",
            "Epoch [16840/20000], Training Loss: 0.0855\n",
            "Epoch [16841/20000], Training Loss: 0.0764\n",
            "Epoch [16842/20000], Training Loss: 0.0819\n",
            "Epoch [16843/20000], Training Loss: 0.0801\n",
            "Epoch [16844/20000], Training Loss: 0.0773\n",
            "Epoch [16845/20000], Training Loss: 0.0800\n",
            "Epoch [16846/20000], Training Loss: 0.0777\n",
            "Epoch [16847/20000], Training Loss: 0.0863\n",
            "Epoch [16848/20000], Training Loss: 0.0852\n",
            "Epoch [16849/20000], Training Loss: 0.0843\n",
            "Epoch [16850/20000], Training Loss: 0.0785\n",
            "Epoch [16851/20000], Training Loss: 0.0824\n",
            "Epoch [16852/20000], Training Loss: 0.0840\n",
            "Epoch [16853/20000], Training Loss: 0.0801\n",
            "Epoch [16854/20000], Training Loss: 0.0859\n",
            "Epoch [16855/20000], Training Loss: 0.0841\n",
            "Epoch [16856/20000], Training Loss: 0.0749\n",
            "Epoch [16857/20000], Training Loss: 0.0800\n",
            "Epoch [16858/20000], Training Loss: 0.0848\n",
            "Epoch [16859/20000], Training Loss: 0.0739\n",
            "Epoch [16860/20000], Training Loss: 0.0741\n",
            "Epoch [16861/20000], Training Loss: 0.0740\n",
            "Epoch [16862/20000], Training Loss: 0.0765\n",
            "Epoch [16863/20000], Training Loss: 0.0808\n",
            "Epoch [16864/20000], Training Loss: 0.0729\n",
            "Epoch [16865/20000], Training Loss: 0.0797\n",
            "Epoch [16866/20000], Training Loss: 0.0798\n",
            "Epoch [16867/20000], Training Loss: 0.0809\n",
            "Epoch [16868/20000], Training Loss: 0.0768\n",
            "Epoch [16869/20000], Training Loss: 0.0811\n",
            "Epoch [16870/20000], Training Loss: 0.0823\n",
            "Epoch [16871/20000], Training Loss: 0.0774\n",
            "Epoch [16872/20000], Training Loss: 0.0813\n",
            "Epoch [16873/20000], Training Loss: 0.0786\n",
            "Epoch [16874/20000], Training Loss: 0.0765\n",
            "Epoch [16875/20000], Training Loss: 0.0754\n",
            "Epoch [16876/20000], Training Loss: 0.0808\n",
            "Epoch [16877/20000], Training Loss: 0.0852\n",
            "Epoch [16878/20000], Training Loss: 0.0819\n",
            "Epoch [16879/20000], Training Loss: 0.0797\n",
            "Epoch [16880/20000], Training Loss: 0.0758\n",
            "Epoch [16881/20000], Training Loss: 0.0776\n",
            "Epoch [16882/20000], Training Loss: 0.0754\n",
            "Epoch [16883/20000], Training Loss: 0.0802\n",
            "Epoch [16884/20000], Training Loss: 0.0734\n",
            "Epoch [16885/20000], Training Loss: 0.0810\n",
            "Epoch [16886/20000], Training Loss: 0.0812\n",
            "Epoch [16887/20000], Training Loss: 0.0782\n",
            "Epoch [16888/20000], Training Loss: 0.0820\n",
            "Epoch [16889/20000], Training Loss: 0.0836\n",
            "Epoch [16890/20000], Training Loss: 0.0811\n",
            "Epoch [16891/20000], Training Loss: 0.0799\n",
            "Epoch [16892/20000], Training Loss: 0.0756\n",
            "Epoch [16893/20000], Training Loss: 0.0739\n",
            "Epoch [16894/20000], Training Loss: 0.0791\n",
            "Epoch [16895/20000], Training Loss: 0.0826\n",
            "Epoch [16896/20000], Training Loss: 0.0831\n",
            "Epoch [16897/20000], Training Loss: 0.0793\n",
            "Epoch [16898/20000], Training Loss: 0.0837\n",
            "Epoch [16899/20000], Training Loss: 0.0810\n",
            "Epoch [16900/20000], Training Loss: 0.0840\n",
            "Epoch [16901/20000], Training Loss: 0.0828\n",
            "Epoch [16902/20000], Training Loss: 0.0772\n",
            "Epoch [16903/20000], Training Loss: 0.0790\n",
            "Epoch [16904/20000], Training Loss: 0.0807\n",
            "Epoch [16905/20000], Training Loss: 0.0826\n",
            "Epoch [16906/20000], Training Loss: 0.0793\n",
            "Epoch [16907/20000], Training Loss: 0.0769\n",
            "Epoch [16908/20000], Training Loss: 0.0726\n",
            "Epoch [16909/20000], Training Loss: 0.0774\n",
            "Epoch [16910/20000], Training Loss: 0.0797\n",
            "Epoch [16911/20000], Training Loss: 0.0767\n",
            "Epoch [16912/20000], Training Loss: 0.0805\n",
            "Epoch [16913/20000], Training Loss: 0.0766\n",
            "Epoch [16914/20000], Training Loss: 0.0750\n",
            "Epoch [16915/20000], Training Loss: 0.0748\n",
            "Epoch [16916/20000], Training Loss: 0.0837\n",
            "Epoch [16917/20000], Training Loss: 0.0760\n",
            "Epoch [16918/20000], Training Loss: 0.0838\n",
            "Epoch [16919/20000], Training Loss: 0.0832\n",
            "Epoch [16920/20000], Training Loss: 0.0809\n",
            "Epoch [16921/20000], Training Loss: 0.0776\n",
            "Epoch [16922/20000], Training Loss: 0.0781\n",
            "Epoch [16923/20000], Training Loss: 0.0844\n",
            "Epoch [16924/20000], Training Loss: 0.0746\n",
            "Epoch [16925/20000], Training Loss: 0.0788\n",
            "Epoch [16926/20000], Training Loss: 0.0800\n",
            "Epoch [16927/20000], Training Loss: 0.0770\n",
            "Epoch [16928/20000], Training Loss: 0.0727\n",
            "Epoch [16929/20000], Training Loss: 0.0738\n",
            "Epoch [16930/20000], Training Loss: 0.0740\n",
            "Epoch [16931/20000], Training Loss: 0.0847\n",
            "Epoch [16932/20000], Training Loss: 0.0835\n",
            "Epoch [16933/20000], Training Loss: 0.0794\n",
            "Epoch [16934/20000], Training Loss: 0.0803\n",
            "Epoch [16935/20000], Training Loss: 0.0858\n",
            "Epoch [16936/20000], Training Loss: 0.0803\n",
            "Epoch [16937/20000], Training Loss: 0.0828\n",
            "Epoch [16938/20000], Training Loss: 0.0844\n",
            "Epoch [16939/20000], Training Loss: 0.0799\n",
            "Epoch [16940/20000], Training Loss: 0.0784\n",
            "Epoch [16941/20000], Training Loss: 0.0797\n",
            "Epoch [16942/20000], Training Loss: 0.0770\n",
            "Epoch [16943/20000], Training Loss: 0.0795\n",
            "Epoch [16944/20000], Training Loss: 0.0820\n",
            "Epoch [16945/20000], Training Loss: 0.0861\n",
            "Epoch [16946/20000], Training Loss: 0.0775\n",
            "Epoch [16947/20000], Training Loss: 0.0772\n",
            "Epoch [16948/20000], Training Loss: 0.0819\n",
            "Epoch [16949/20000], Training Loss: 0.0787\n",
            "Epoch [16950/20000], Training Loss: 0.0841\n",
            "Epoch [16951/20000], Training Loss: 0.0778\n",
            "Epoch [16952/20000], Training Loss: 0.0792\n",
            "Epoch [16953/20000], Training Loss: 0.0733\n",
            "Epoch [16954/20000], Training Loss: 0.0784\n",
            "Epoch [16955/20000], Training Loss: 0.0777\n",
            "Epoch [16956/20000], Training Loss: 0.0797\n",
            "Epoch [16957/20000], Training Loss: 0.0791\n",
            "Epoch [16958/20000], Training Loss: 0.0745\n",
            "Epoch [16959/20000], Training Loss: 0.0811\n",
            "Epoch [16960/20000], Training Loss: 0.0783\n",
            "Epoch [16961/20000], Training Loss: 0.0786\n",
            "Epoch [16962/20000], Training Loss: 0.0823\n",
            "Epoch [16963/20000], Training Loss: 0.0839\n",
            "Epoch [16964/20000], Training Loss: 0.0814\n",
            "Epoch [16965/20000], Training Loss: 0.0779\n",
            "Epoch [16966/20000], Training Loss: 0.0748\n",
            "Epoch [16967/20000], Training Loss: 0.0829\n",
            "Epoch [16968/20000], Training Loss: 0.0793\n",
            "Epoch [16969/20000], Training Loss: 0.0799\n",
            "Epoch [16970/20000], Training Loss: 0.0849\n",
            "Epoch [16971/20000], Training Loss: 0.0874\n",
            "Epoch [16972/20000], Training Loss: 0.0784\n",
            "Epoch [16973/20000], Training Loss: 0.0843\n",
            "Epoch [16974/20000], Training Loss: 0.0844\n",
            "Epoch [16975/20000], Training Loss: 0.0794\n",
            "Epoch [16976/20000], Training Loss: 0.0853\n",
            "Epoch [16977/20000], Training Loss: 0.0839\n",
            "Epoch [16978/20000], Training Loss: 0.0745\n",
            "Epoch [16979/20000], Training Loss: 0.0791\n",
            "Epoch [16980/20000], Training Loss: 0.0779\n",
            "Epoch [16981/20000], Training Loss: 0.0826\n",
            "Epoch [16982/20000], Training Loss: 0.0779\n",
            "Epoch [16983/20000], Training Loss: 0.0789\n",
            "Epoch [16984/20000], Training Loss: 0.0793\n",
            "Epoch [16985/20000], Training Loss: 0.0837\n",
            "Epoch [16986/20000], Training Loss: 0.0752\n",
            "Epoch [16987/20000], Training Loss: 0.0711\n",
            "Epoch [16988/20000], Training Loss: 0.0728\n",
            "Epoch [16989/20000], Training Loss: 0.0771\n",
            "Epoch [16990/20000], Training Loss: 0.0852\n",
            "Epoch [16991/20000], Training Loss: 0.0776\n",
            "Epoch [16992/20000], Training Loss: 0.0799\n",
            "Epoch [16993/20000], Training Loss: 0.0807\n",
            "Epoch [16994/20000], Training Loss: 0.0856\n",
            "Epoch [16995/20000], Training Loss: 0.0786\n",
            "Epoch [16996/20000], Training Loss: 0.0736\n",
            "Epoch [16997/20000], Training Loss: 0.0799\n",
            "Epoch [16998/20000], Training Loss: 0.0843\n",
            "Epoch [16999/20000], Training Loss: 0.0758\n",
            "Epoch [17000/20000], Training Loss: 0.0789\n",
            "Epoch [17001/20000], Training Loss: 0.0765\n",
            "Epoch [17002/20000], Training Loss: 0.0803\n",
            "Epoch [17003/20000], Training Loss: 0.0795\n",
            "Epoch [17004/20000], Training Loss: 0.0760\n",
            "Epoch [17005/20000], Training Loss: 0.0851\n",
            "Epoch [17006/20000], Training Loss: 0.0752\n",
            "Epoch [17007/20000], Training Loss: 0.0810\n",
            "Epoch [17008/20000], Training Loss: 0.0786\n",
            "Epoch [17009/20000], Training Loss: 0.0848\n",
            "Epoch [17010/20000], Training Loss: 0.0775\n",
            "Epoch [17011/20000], Training Loss: 0.0749\n",
            "Epoch [17012/20000], Training Loss: 0.0811\n",
            "Epoch [17013/20000], Training Loss: 0.0795\n",
            "Epoch [17014/20000], Training Loss: 0.0746\n",
            "Epoch [17015/20000], Training Loss: 0.0827\n",
            "Epoch [17016/20000], Training Loss: 0.0811\n",
            "Epoch [17017/20000], Training Loss: 0.0740\n",
            "Epoch [17018/20000], Training Loss: 0.0787\n",
            "Epoch [17019/20000], Training Loss: 0.0828\n",
            "Epoch [17020/20000], Training Loss: 0.0805\n",
            "Epoch [17021/20000], Training Loss: 0.0795\n",
            "Epoch [17022/20000], Training Loss: 0.0839\n",
            "Epoch [17023/20000], Training Loss: 0.0805\n",
            "Epoch [17024/20000], Training Loss: 0.0839\n",
            "Epoch [17025/20000], Training Loss: 0.0789\n",
            "Epoch [17026/20000], Training Loss: 0.0795\n",
            "Epoch [17027/20000], Training Loss: 0.0871\n",
            "Epoch [17028/20000], Training Loss: 0.0822\n",
            "Epoch [17029/20000], Training Loss: 0.0766\n",
            "Epoch [17030/20000], Training Loss: 0.0773\n",
            "Epoch [17031/20000], Training Loss: 0.0769\n",
            "Epoch [17032/20000], Training Loss: 0.0742\n",
            "Epoch [17033/20000], Training Loss: 0.0811\n",
            "Epoch [17034/20000], Training Loss: 0.0757\n",
            "Epoch [17035/20000], Training Loss: 0.0787\n",
            "Epoch [17036/20000], Training Loss: 0.0772\n",
            "Epoch [17037/20000], Training Loss: 0.0749\n",
            "Epoch [17038/20000], Training Loss: 0.0818\n",
            "Epoch [17039/20000], Training Loss: 0.0807\n",
            "Epoch [17040/20000], Training Loss: 0.0806\n",
            "Epoch [17041/20000], Training Loss: 0.0754\n",
            "Epoch [17042/20000], Training Loss: 0.0814\n",
            "Epoch [17043/20000], Training Loss: 0.0817\n",
            "Epoch [17044/20000], Training Loss: 0.0806\n",
            "Epoch [17045/20000], Training Loss: 0.0802\n",
            "Epoch [17046/20000], Training Loss: 0.0824\n",
            "Epoch [17047/20000], Training Loss: 0.0797\n",
            "Epoch [17048/20000], Training Loss: 0.0774\n",
            "Epoch [17049/20000], Training Loss: 0.0792\n",
            "Epoch [17050/20000], Training Loss: 0.0886\n",
            "Epoch [17051/20000], Training Loss: 0.0809\n",
            "Epoch [17052/20000], Training Loss: 0.0788\n",
            "Epoch [17053/20000], Training Loss: 0.0784\n",
            "Epoch [17054/20000], Training Loss: 0.0826\n",
            "Epoch [17055/20000], Training Loss: 0.0807\n",
            "Epoch [17056/20000], Training Loss: 0.0805\n",
            "Epoch [17057/20000], Training Loss: 0.0769\n",
            "Epoch [17058/20000], Training Loss: 0.0816\n",
            "Epoch [17059/20000], Training Loss: 0.0776\n",
            "Epoch [17060/20000], Training Loss: 0.0841\n",
            "Epoch [17061/20000], Training Loss: 0.0889\n",
            "Epoch [17062/20000], Training Loss: 0.0784\n",
            "Epoch [17063/20000], Training Loss: 0.0771\n",
            "Epoch [17064/20000], Training Loss: 0.0812\n",
            "Epoch [17065/20000], Training Loss: 0.0771\n",
            "Epoch [17066/20000], Training Loss: 0.0794\n",
            "Epoch [17067/20000], Training Loss: 0.0827\n",
            "Epoch [17068/20000], Training Loss: 0.0809\n",
            "Epoch [17069/20000], Training Loss: 0.0777\n",
            "Epoch [17070/20000], Training Loss: 0.0759\n",
            "Epoch [17071/20000], Training Loss: 0.0845\n",
            "Epoch [17072/20000], Training Loss: 0.0818\n",
            "Epoch [17073/20000], Training Loss: 0.0818\n",
            "Epoch [17074/20000], Training Loss: 0.0820\n",
            "Epoch [17075/20000], Training Loss: 0.0794\n",
            "Epoch [17076/20000], Training Loss: 0.0800\n",
            "Epoch [17077/20000], Training Loss: 0.0803\n",
            "Epoch [17078/20000], Training Loss: 0.0769\n",
            "Epoch [17079/20000], Training Loss: 0.0798\n",
            "Epoch [17080/20000], Training Loss: 0.0761\n",
            "Epoch [17081/20000], Training Loss: 0.0802\n",
            "Epoch [17082/20000], Training Loss: 0.0820\n",
            "Epoch [17083/20000], Training Loss: 0.0759\n",
            "Epoch [17084/20000], Training Loss: 0.0834\n",
            "Epoch [17085/20000], Training Loss: 0.0805\n",
            "Epoch [17086/20000], Training Loss: 0.0743\n",
            "Epoch [17087/20000], Training Loss: 0.0745\n",
            "Epoch [17088/20000], Training Loss: 0.0821\n",
            "Epoch [17089/20000], Training Loss: 0.0839\n",
            "Epoch [17090/20000], Training Loss: 0.0756\n",
            "Epoch [17091/20000], Training Loss: 0.0813\n",
            "Epoch [17092/20000], Training Loss: 0.0780\n",
            "Epoch [17093/20000], Training Loss: 0.0809\n",
            "Epoch [17094/20000], Training Loss: 0.0822\n",
            "Epoch [17095/20000], Training Loss: 0.0800\n",
            "Epoch [17096/20000], Training Loss: 0.0786\n",
            "Epoch [17097/20000], Training Loss: 0.0780\n",
            "Epoch [17098/20000], Training Loss: 0.0815\n",
            "Epoch [17099/20000], Training Loss: 0.0782\n",
            "Epoch [17100/20000], Training Loss: 0.0816\n",
            "Epoch [17101/20000], Training Loss: 0.0794\n",
            "Epoch [17102/20000], Training Loss: 0.0758\n",
            "Epoch [17103/20000], Training Loss: 0.0778\n",
            "Epoch [17104/20000], Training Loss: 0.0807\n",
            "Epoch [17105/20000], Training Loss: 0.0771\n",
            "Epoch [17106/20000], Training Loss: 0.0780\n",
            "Epoch [17107/20000], Training Loss: 0.0772\n",
            "Epoch [17108/20000], Training Loss: 0.0773\n",
            "Epoch [17109/20000], Training Loss: 0.0827\n",
            "Epoch [17110/20000], Training Loss: 0.0797\n",
            "Epoch [17111/20000], Training Loss: 0.0854\n",
            "Epoch [17112/20000], Training Loss: 0.0825\n",
            "Epoch [17113/20000], Training Loss: 0.0794\n",
            "Epoch [17114/20000], Training Loss: 0.0810\n",
            "Epoch [17115/20000], Training Loss: 0.0798\n",
            "Epoch [17116/20000], Training Loss: 0.0785\n",
            "Epoch [17117/20000], Training Loss: 0.0747\n",
            "Epoch [17118/20000], Training Loss: 0.0791\n",
            "Epoch [17119/20000], Training Loss: 0.0805\n",
            "Epoch [17120/20000], Training Loss: 0.0797\n",
            "Epoch [17121/20000], Training Loss: 0.0780\n",
            "Epoch [17122/20000], Training Loss: 0.0783\n",
            "Epoch [17123/20000], Training Loss: 0.0772\n",
            "Epoch [17124/20000], Training Loss: 0.0835\n",
            "Epoch [17125/20000], Training Loss: 0.0781\n",
            "Epoch [17126/20000], Training Loss: 0.0784\n",
            "Epoch [17127/20000], Training Loss: 0.0801\n",
            "Epoch [17128/20000], Training Loss: 0.0834\n",
            "Epoch [17129/20000], Training Loss: 0.0842\n",
            "Epoch [17130/20000], Training Loss: 0.0768\n",
            "Epoch [17131/20000], Training Loss: 0.0795\n",
            "Epoch [17132/20000], Training Loss: 0.0809\n",
            "Epoch [17133/20000], Training Loss: 0.0754\n",
            "Epoch [17134/20000], Training Loss: 0.0833\n",
            "Epoch [17135/20000], Training Loss: 0.0848\n",
            "Epoch [17136/20000], Training Loss: 0.0778\n",
            "Epoch [17137/20000], Training Loss: 0.0778\n",
            "Epoch [17138/20000], Training Loss: 0.0813\n",
            "Epoch [17139/20000], Training Loss: 0.0791\n",
            "Epoch [17140/20000], Training Loss: 0.0828\n",
            "Epoch [17141/20000], Training Loss: 0.0806\n",
            "Epoch [17142/20000], Training Loss: 0.0754\n",
            "Epoch [17143/20000], Training Loss: 0.0735\n",
            "Epoch [17144/20000], Training Loss: 0.0815\n",
            "Epoch [17145/20000], Training Loss: 0.0748\n",
            "Epoch [17146/20000], Training Loss: 0.0739\n",
            "Epoch [17147/20000], Training Loss: 0.0773\n",
            "Epoch [17148/20000], Training Loss: 0.0797\n",
            "Epoch [17149/20000], Training Loss: 0.0789\n",
            "Epoch [17150/20000], Training Loss: 0.0790\n",
            "Epoch [17151/20000], Training Loss: 0.0786\n",
            "Epoch [17152/20000], Training Loss: 0.0799\n",
            "Epoch [17153/20000], Training Loss: 0.0782\n",
            "Epoch [17154/20000], Training Loss: 0.0778\n",
            "Epoch [17155/20000], Training Loss: 0.0806\n",
            "Epoch [17156/20000], Training Loss: 0.0816\n",
            "Epoch [17157/20000], Training Loss: 0.0739\n",
            "Epoch [17158/20000], Training Loss: 0.0752\n",
            "Epoch [17159/20000], Training Loss: 0.0756\n",
            "Epoch [17160/20000], Training Loss: 0.0823\n",
            "Epoch [17161/20000], Training Loss: 0.0763\n",
            "Epoch [17162/20000], Training Loss: 0.0768\n",
            "Epoch [17163/20000], Training Loss: 0.0799\n",
            "Epoch [17164/20000], Training Loss: 0.0809\n",
            "Epoch [17165/20000], Training Loss: 0.0829\n",
            "Epoch [17166/20000], Training Loss: 0.0857\n",
            "Epoch [17167/20000], Training Loss: 0.0788\n",
            "Epoch [17168/20000], Training Loss: 0.0768\n",
            "Epoch [17169/20000], Training Loss: 0.0781\n",
            "Epoch [17170/20000], Training Loss: 0.0789\n",
            "Epoch [17171/20000], Training Loss: 0.0813\n",
            "Epoch [17172/20000], Training Loss: 0.0845\n",
            "Epoch [17173/20000], Training Loss: 0.0807\n",
            "Epoch [17174/20000], Training Loss: 0.0800\n",
            "Epoch [17175/20000], Training Loss: 0.0793\n",
            "Epoch [17176/20000], Training Loss: 0.0756\n",
            "Epoch [17177/20000], Training Loss: 0.0757\n",
            "Epoch [17178/20000], Training Loss: 0.0821\n",
            "Epoch [17179/20000], Training Loss: 0.0794\n",
            "Epoch [17180/20000], Training Loss: 0.0824\n",
            "Epoch [17181/20000], Training Loss: 0.0808\n",
            "Epoch [17182/20000], Training Loss: 0.0737\n",
            "Epoch [17183/20000], Training Loss: 0.0785\n",
            "Epoch [17184/20000], Training Loss: 0.0801\n",
            "Epoch [17185/20000], Training Loss: 0.0782\n",
            "Epoch [17186/20000], Training Loss: 0.0812\n",
            "Epoch [17187/20000], Training Loss: 0.0751\n",
            "Epoch [17188/20000], Training Loss: 0.0806\n",
            "Epoch [17189/20000], Training Loss: 0.0800\n",
            "Epoch [17190/20000], Training Loss: 0.0818\n",
            "Epoch [17191/20000], Training Loss: 0.0852\n",
            "Epoch [17192/20000], Training Loss: 0.0797\n",
            "Epoch [17193/20000], Training Loss: 0.0769\n",
            "Epoch [17194/20000], Training Loss: 0.0737\n",
            "Epoch [17195/20000], Training Loss: 0.0745\n",
            "Epoch [17196/20000], Training Loss: 0.0804\n",
            "Epoch [17197/20000], Training Loss: 0.0783\n",
            "Epoch [17198/20000], Training Loss: 0.0803\n",
            "Epoch [17199/20000], Training Loss: 0.0812\n",
            "Epoch [17200/20000], Training Loss: 0.0820\n",
            "Epoch [17201/20000], Training Loss: 0.0786\n",
            "Epoch [17202/20000], Training Loss: 0.0806\n",
            "Epoch [17203/20000], Training Loss: 0.0768\n",
            "Epoch [17204/20000], Training Loss: 0.0776\n",
            "Epoch [17205/20000], Training Loss: 0.0851\n",
            "Epoch [17206/20000], Training Loss: 0.0816\n",
            "Epoch [17207/20000], Training Loss: 0.0811\n",
            "Epoch [17208/20000], Training Loss: 0.0809\n",
            "Epoch [17209/20000], Training Loss: 0.0798\n",
            "Epoch [17210/20000], Training Loss: 0.0801\n",
            "Epoch [17211/20000], Training Loss: 0.0806\n",
            "Epoch [17212/20000], Training Loss: 0.0803\n",
            "Epoch [17213/20000], Training Loss: 0.0812\n",
            "Epoch [17214/20000], Training Loss: 0.0785\n",
            "Epoch [17215/20000], Training Loss: 0.0797\n",
            "Epoch [17216/20000], Training Loss: 0.0790\n",
            "Epoch [17217/20000], Training Loss: 0.0770\n",
            "Epoch [17218/20000], Training Loss: 0.0835\n",
            "Epoch [17219/20000], Training Loss: 0.0805\n",
            "Epoch [17220/20000], Training Loss: 0.0829\n",
            "Epoch [17221/20000], Training Loss: 0.0859\n",
            "Epoch [17222/20000], Training Loss: 0.0748\n",
            "Epoch [17223/20000], Training Loss: 0.0782\n",
            "Epoch [17224/20000], Training Loss: 0.0766\n",
            "Epoch [17225/20000], Training Loss: 0.0847\n",
            "Epoch [17226/20000], Training Loss: 0.0826\n",
            "Epoch [17227/20000], Training Loss: 0.0792\n",
            "Epoch [17228/20000], Training Loss: 0.0755\n",
            "Epoch [17229/20000], Training Loss: 0.0832\n",
            "Epoch [17230/20000], Training Loss: 0.0786\n",
            "Epoch [17231/20000], Training Loss: 0.0762\n",
            "Epoch [17232/20000], Training Loss: 0.0801\n",
            "Epoch [17233/20000], Training Loss: 0.0790\n",
            "Epoch [17234/20000], Training Loss: 0.0832\n",
            "Epoch [17235/20000], Training Loss: 0.0779\n",
            "Epoch [17236/20000], Training Loss: 0.0797\n",
            "Epoch [17237/20000], Training Loss: 0.0860\n",
            "Epoch [17238/20000], Training Loss: 0.0794\n",
            "Epoch [17239/20000], Training Loss: 0.0807\n",
            "Epoch [17240/20000], Training Loss: 0.0774\n",
            "Epoch [17241/20000], Training Loss: 0.0749\n",
            "Epoch [17242/20000], Training Loss: 0.0855\n",
            "Epoch [17243/20000], Training Loss: 0.0820\n",
            "Epoch [17244/20000], Training Loss: 0.0808\n",
            "Epoch [17245/20000], Training Loss: 0.0808\n",
            "Epoch [17246/20000], Training Loss: 0.0726\n",
            "Epoch [17247/20000], Training Loss: 0.0849\n",
            "Epoch [17248/20000], Training Loss: 0.0795\n",
            "Epoch [17249/20000], Training Loss: 0.0825\n",
            "Epoch [17250/20000], Training Loss: 0.0794\n",
            "Epoch [17251/20000], Training Loss: 0.0799\n",
            "Epoch [17252/20000], Training Loss: 0.0825\n",
            "Epoch [17253/20000], Training Loss: 0.0762\n",
            "Epoch [17254/20000], Training Loss: 0.0766\n",
            "Epoch [17255/20000], Training Loss: 0.0828\n",
            "Epoch [17256/20000], Training Loss: 0.0806\n",
            "Epoch [17257/20000], Training Loss: 0.0786\n",
            "Epoch [17258/20000], Training Loss: 0.0746\n",
            "Epoch [17259/20000], Training Loss: 0.0739\n",
            "Epoch [17260/20000], Training Loss: 0.0811\n",
            "Epoch [17261/20000], Training Loss: 0.0768\n",
            "Epoch [17262/20000], Training Loss: 0.0823\n",
            "Epoch [17263/20000], Training Loss: 0.0832\n",
            "Epoch [17264/20000], Training Loss: 0.0802\n",
            "Epoch [17265/20000], Training Loss: 0.0810\n",
            "Epoch [17266/20000], Training Loss: 0.0747\n",
            "Epoch [17267/20000], Training Loss: 0.0769\n",
            "Epoch [17268/20000], Training Loss: 0.0810\n",
            "Epoch [17269/20000], Training Loss: 0.0732\n",
            "Epoch [17270/20000], Training Loss: 0.0750\n",
            "Epoch [17271/20000], Training Loss: 0.0774\n",
            "Epoch [17272/20000], Training Loss: 0.0812\n",
            "Epoch [17273/20000], Training Loss: 0.0774\n",
            "Epoch [17274/20000], Training Loss: 0.0808\n",
            "Epoch [17275/20000], Training Loss: 0.0784\n",
            "Epoch [17276/20000], Training Loss: 0.0767\n",
            "Epoch [17277/20000], Training Loss: 0.0788\n",
            "Epoch [17278/20000], Training Loss: 0.0804\n",
            "Epoch [17279/20000], Training Loss: 0.0771\n",
            "Epoch [17280/20000], Training Loss: 0.0827\n",
            "Epoch [17281/20000], Training Loss: 0.0833\n",
            "Epoch [17282/20000], Training Loss: 0.0860\n",
            "Epoch [17283/20000], Training Loss: 0.0758\n",
            "Epoch [17284/20000], Training Loss: 0.0777\n",
            "Epoch [17285/20000], Training Loss: 0.0773\n",
            "Epoch [17286/20000], Training Loss: 0.0777\n",
            "Epoch [17287/20000], Training Loss: 0.0835\n",
            "Epoch [17288/20000], Training Loss: 0.0805\n",
            "Epoch [17289/20000], Training Loss: 0.0804\n",
            "Epoch [17290/20000], Training Loss: 0.0797\n",
            "Epoch [17291/20000], Training Loss: 0.0848\n",
            "Epoch [17292/20000], Training Loss: 0.0807\n",
            "Epoch [17293/20000], Training Loss: 0.0815\n",
            "Epoch [17294/20000], Training Loss: 0.0773\n",
            "Epoch [17295/20000], Training Loss: 0.0767\n",
            "Epoch [17296/20000], Training Loss: 0.0787\n",
            "Epoch [17297/20000], Training Loss: 0.0746\n",
            "Epoch [17298/20000], Training Loss: 0.0826\n",
            "Epoch [17299/20000], Training Loss: 0.0830\n",
            "Epoch [17300/20000], Training Loss: 0.0760\n",
            "Epoch [17301/20000], Training Loss: 0.0846\n",
            "Epoch [17302/20000], Training Loss: 0.0802\n",
            "Epoch [17303/20000], Training Loss: 0.0816\n",
            "Epoch [17304/20000], Training Loss: 0.0756\n",
            "Epoch [17305/20000], Training Loss: 0.0737\n",
            "Epoch [17306/20000], Training Loss: 0.0808\n",
            "Epoch [17307/20000], Training Loss: 0.0736\n",
            "Epoch [17308/20000], Training Loss: 0.0798\n",
            "Epoch [17309/20000], Training Loss: 0.0763\n",
            "Epoch [17310/20000], Training Loss: 0.0805\n",
            "Epoch [17311/20000], Training Loss: 0.0803\n",
            "Epoch [17312/20000], Training Loss: 0.0759\n",
            "Epoch [17313/20000], Training Loss: 0.0781\n",
            "Epoch [17314/20000], Training Loss: 0.0752\n",
            "Epoch [17315/20000], Training Loss: 0.0744\n",
            "Epoch [17316/20000], Training Loss: 0.0802\n",
            "Epoch [17317/20000], Training Loss: 0.0788\n",
            "Epoch [17318/20000], Training Loss: 0.0765\n",
            "Epoch [17319/20000], Training Loss: 0.0753\n",
            "Epoch [17320/20000], Training Loss: 0.0762\n",
            "Epoch [17321/20000], Training Loss: 0.0835\n",
            "Epoch [17322/20000], Training Loss: 0.0789\n",
            "Epoch [17323/20000], Training Loss: 0.0860\n",
            "Epoch [17324/20000], Training Loss: 0.0834\n",
            "Epoch [17325/20000], Training Loss: 0.0751\n",
            "Epoch [17326/20000], Training Loss: 0.0756\n",
            "Epoch [17327/20000], Training Loss: 0.0815\n",
            "Epoch [17328/20000], Training Loss: 0.0850\n",
            "Epoch [17329/20000], Training Loss: 0.0793\n",
            "Epoch [17330/20000], Training Loss: 0.0771\n",
            "Epoch [17331/20000], Training Loss: 0.0747\n",
            "Epoch [17332/20000], Training Loss: 0.0741\n",
            "Epoch [17333/20000], Training Loss: 0.0780\n",
            "Epoch [17334/20000], Training Loss: 0.0737\n",
            "Epoch [17335/20000], Training Loss: 0.0783\n",
            "Epoch [17336/20000], Training Loss: 0.0814\n",
            "Epoch [17337/20000], Training Loss: 0.0851\n",
            "Epoch [17338/20000], Training Loss: 0.0791\n",
            "Epoch [17339/20000], Training Loss: 0.0749\n",
            "Epoch [17340/20000], Training Loss: 0.0728\n",
            "Epoch [17341/20000], Training Loss: 0.0825\n",
            "Epoch [17342/20000], Training Loss: 0.0836\n",
            "Epoch [17343/20000], Training Loss: 0.0795\n",
            "Epoch [17344/20000], Training Loss: 0.0792\n",
            "Epoch [17345/20000], Training Loss: 0.0772\n",
            "Epoch [17346/20000], Training Loss: 0.0872\n",
            "Epoch [17347/20000], Training Loss: 0.0754\n",
            "Epoch [17348/20000], Training Loss: 0.0795\n",
            "Epoch [17349/20000], Training Loss: 0.0768\n",
            "Epoch [17350/20000], Training Loss: 0.0773\n",
            "Epoch [17351/20000], Training Loss: 0.0839\n",
            "Epoch [17352/20000], Training Loss: 0.0857\n",
            "Epoch [17353/20000], Training Loss: 0.0822\n",
            "Epoch [17354/20000], Training Loss: 0.0851\n",
            "Epoch [17355/20000], Training Loss: 0.0842\n",
            "Epoch [17356/20000], Training Loss: 0.0821\n",
            "Epoch [17357/20000], Training Loss: 0.0751\n",
            "Epoch [17358/20000], Training Loss: 0.0798\n",
            "Epoch [17359/20000], Training Loss: 0.0774\n",
            "Epoch [17360/20000], Training Loss: 0.0788\n",
            "Epoch [17361/20000], Training Loss: 0.0880\n",
            "Epoch [17362/20000], Training Loss: 0.0768\n",
            "Epoch [17363/20000], Training Loss: 0.0797\n",
            "Epoch [17364/20000], Training Loss: 0.0762\n",
            "Epoch [17365/20000], Training Loss: 0.0741\n",
            "Epoch [17366/20000], Training Loss: 0.0785\n",
            "Epoch [17367/20000], Training Loss: 0.0817\n",
            "Epoch [17368/20000], Training Loss: 0.0742\n",
            "Epoch [17369/20000], Training Loss: 0.0801\n",
            "Epoch [17370/20000], Training Loss: 0.0791\n",
            "Epoch [17371/20000], Training Loss: 0.0769\n",
            "Epoch [17372/20000], Training Loss: 0.0744\n",
            "Epoch [17373/20000], Training Loss: 0.0837\n",
            "Epoch [17374/20000], Training Loss: 0.0840\n",
            "Epoch [17375/20000], Training Loss: 0.0784\n",
            "Epoch [17376/20000], Training Loss: 0.0816\n",
            "Epoch [17377/20000], Training Loss: 0.0814\n",
            "Epoch [17378/20000], Training Loss: 0.0856\n",
            "Epoch [17379/20000], Training Loss: 0.0747\n",
            "Epoch [17380/20000], Training Loss: 0.0798\n",
            "Epoch [17381/20000], Training Loss: 0.0854\n",
            "Epoch [17382/20000], Training Loss: 0.0775\n",
            "Epoch [17383/20000], Training Loss: 0.0746\n",
            "Epoch [17384/20000], Training Loss: 0.0802\n",
            "Epoch [17385/20000], Training Loss: 0.0765\n",
            "Epoch [17386/20000], Training Loss: 0.0795\n",
            "Epoch [17387/20000], Training Loss: 0.0813\n",
            "Epoch [17388/20000], Training Loss: 0.0786\n",
            "Epoch [17389/20000], Training Loss: 0.0788\n",
            "Epoch [17390/20000], Training Loss: 0.0743\n",
            "Epoch [17391/20000], Training Loss: 0.0822\n",
            "Epoch [17392/20000], Training Loss: 0.0781\n",
            "Epoch [17393/20000], Training Loss: 0.0823\n",
            "Epoch [17394/20000], Training Loss: 0.0794\n",
            "Epoch [17395/20000], Training Loss: 0.0804\n",
            "Epoch [17396/20000], Training Loss: 0.0806\n",
            "Epoch [17397/20000], Training Loss: 0.0802\n",
            "Epoch [17398/20000], Training Loss: 0.0788\n",
            "Epoch [17399/20000], Training Loss: 0.0798\n",
            "Epoch [17400/20000], Training Loss: 0.0811\n",
            "Epoch [17401/20000], Training Loss: 0.0740\n",
            "Epoch [17402/20000], Training Loss: 0.0825\n",
            "Epoch [17403/20000], Training Loss: 0.0811\n",
            "Epoch [17404/20000], Training Loss: 0.0833\n",
            "Epoch [17405/20000], Training Loss: 0.0783\n",
            "Epoch [17406/20000], Training Loss: 0.0722\n",
            "Epoch [17407/20000], Training Loss: 0.0799\n",
            "Epoch [17408/20000], Training Loss: 0.0778\n",
            "Epoch [17409/20000], Training Loss: 0.0855\n",
            "Epoch [17410/20000], Training Loss: 0.0788\n",
            "Epoch [17411/20000], Training Loss: 0.0844\n",
            "Epoch [17412/20000], Training Loss: 0.0849\n",
            "Epoch [17413/20000], Training Loss: 0.0835\n",
            "Epoch [17414/20000], Training Loss: 0.0783\n",
            "Epoch [17415/20000], Training Loss: 0.0796\n",
            "Epoch [17416/20000], Training Loss: 0.0728\n",
            "Epoch [17417/20000], Training Loss: 0.0749\n",
            "Epoch [17418/20000], Training Loss: 0.0785\n",
            "Epoch [17419/20000], Training Loss: 0.0869\n",
            "Epoch [17420/20000], Training Loss: 0.0786\n",
            "Epoch [17421/20000], Training Loss: 0.0821\n",
            "Epoch [17422/20000], Training Loss: 0.0828\n",
            "Epoch [17423/20000], Training Loss: 0.0771\n",
            "Epoch [17424/20000], Training Loss: 0.0813\n",
            "Epoch [17425/20000], Training Loss: 0.0788\n",
            "Epoch [17426/20000], Training Loss: 0.0813\n",
            "Epoch [17427/20000], Training Loss: 0.0835\n",
            "Epoch [17428/20000], Training Loss: 0.0775\n",
            "Epoch [17429/20000], Training Loss: 0.0814\n",
            "Epoch [17430/20000], Training Loss: 0.0789\n",
            "Epoch [17431/20000], Training Loss: 0.0787\n",
            "Epoch [17432/20000], Training Loss: 0.0785\n",
            "Epoch [17433/20000], Training Loss: 0.0796\n",
            "Epoch [17434/20000], Training Loss: 0.0848\n",
            "Epoch [17435/20000], Training Loss: 0.0793\n",
            "Epoch [17436/20000], Training Loss: 0.0814\n",
            "Epoch [17437/20000], Training Loss: 0.0783\n",
            "Epoch [17438/20000], Training Loss: 0.0824\n",
            "Epoch [17439/20000], Training Loss: 0.0794\n",
            "Epoch [17440/20000], Training Loss: 0.0722\n",
            "Epoch [17441/20000], Training Loss: 0.0765\n",
            "Epoch [17442/20000], Training Loss: 0.0787\n",
            "Epoch [17443/20000], Training Loss: 0.0759\n",
            "Epoch [17444/20000], Training Loss: 0.0802\n",
            "Epoch [17445/20000], Training Loss: 0.0810\n",
            "Epoch [17446/20000], Training Loss: 0.0768\n",
            "Epoch [17447/20000], Training Loss: 0.0789\n",
            "Epoch [17448/20000], Training Loss: 0.0738\n",
            "Epoch [17449/20000], Training Loss: 0.0803\n",
            "Epoch [17450/20000], Training Loss: 0.0786\n",
            "Epoch [17451/20000], Training Loss: 0.0810\n",
            "Epoch [17452/20000], Training Loss: 0.0769\n",
            "Epoch [17453/20000], Training Loss: 0.0748\n",
            "Epoch [17454/20000], Training Loss: 0.0766\n",
            "Epoch [17455/20000], Training Loss: 0.0802\n",
            "Epoch [17456/20000], Training Loss: 0.0850\n",
            "Epoch [17457/20000], Training Loss: 0.0746\n",
            "Epoch [17458/20000], Training Loss: 0.0858\n",
            "Epoch [17459/20000], Training Loss: 0.0807\n",
            "Epoch [17460/20000], Training Loss: 0.0842\n",
            "Epoch [17461/20000], Training Loss: 0.0825\n",
            "Epoch [17462/20000], Training Loss: 0.0753\n",
            "Epoch [17463/20000], Training Loss: 0.0820\n",
            "Epoch [17464/20000], Training Loss: 0.0779\n",
            "Epoch [17465/20000], Training Loss: 0.0754\n",
            "Epoch [17466/20000], Training Loss: 0.0836\n",
            "Epoch [17467/20000], Training Loss: 0.0778\n",
            "Epoch [17468/20000], Training Loss: 0.0810\n",
            "Epoch [17469/20000], Training Loss: 0.0809\n",
            "Epoch [17470/20000], Training Loss: 0.0749\n",
            "Epoch [17471/20000], Training Loss: 0.0780\n",
            "Epoch [17472/20000], Training Loss: 0.0771\n",
            "Epoch [17473/20000], Training Loss: 0.0776\n",
            "Epoch [17474/20000], Training Loss: 0.0791\n",
            "Epoch [17475/20000], Training Loss: 0.0827\n",
            "Epoch [17476/20000], Training Loss: 0.0767\n",
            "Epoch [17477/20000], Training Loss: 0.0752\n",
            "Epoch [17478/20000], Training Loss: 0.0762\n",
            "Epoch [17479/20000], Training Loss: 0.0756\n",
            "Epoch [17480/20000], Training Loss: 0.0820\n",
            "Epoch [17481/20000], Training Loss: 0.0804\n",
            "Epoch [17482/20000], Training Loss: 0.0769\n",
            "Epoch [17483/20000], Training Loss: 0.0806\n",
            "Epoch [17484/20000], Training Loss: 0.0806\n",
            "Epoch [17485/20000], Training Loss: 0.0774\n",
            "Epoch [17486/20000], Training Loss: 0.0746\n",
            "Epoch [17487/20000], Training Loss: 0.0838\n",
            "Epoch [17488/20000], Training Loss: 0.0758\n",
            "Epoch [17489/20000], Training Loss: 0.0792\n",
            "Epoch [17490/20000], Training Loss: 0.0759\n",
            "Epoch [17491/20000], Training Loss: 0.0785\n",
            "Epoch [17492/20000], Training Loss: 0.0790\n",
            "Epoch [17493/20000], Training Loss: 0.0822\n",
            "Epoch [17494/20000], Training Loss: 0.0808\n",
            "Epoch [17495/20000], Training Loss: 0.0806\n",
            "Epoch [17496/20000], Training Loss: 0.0815\n",
            "Epoch [17497/20000], Training Loss: 0.0785\n",
            "Epoch [17498/20000], Training Loss: 0.0798\n",
            "Epoch [17499/20000], Training Loss: 0.0795\n",
            "Epoch [17500/20000], Training Loss: 0.0789\n",
            "Epoch [17501/20000], Training Loss: 0.0786\n",
            "Epoch [17502/20000], Training Loss: 0.0757\n",
            "Epoch [17503/20000], Training Loss: 0.0753\n",
            "Epoch [17504/20000], Training Loss: 0.0831\n",
            "Epoch [17505/20000], Training Loss: 0.0776\n",
            "Epoch [17506/20000], Training Loss: 0.0784\n",
            "Epoch [17507/20000], Training Loss: 0.0778\n",
            "Epoch [17508/20000], Training Loss: 0.0830\n",
            "Epoch [17509/20000], Training Loss: 0.0837\n",
            "Epoch [17510/20000], Training Loss: 0.0765\n",
            "Epoch [17511/20000], Training Loss: 0.0752\n",
            "Epoch [17512/20000], Training Loss: 0.0807\n",
            "Epoch [17513/20000], Training Loss: 0.0793\n",
            "Epoch [17514/20000], Training Loss: 0.0764\n",
            "Epoch [17515/20000], Training Loss: 0.0847\n",
            "Epoch [17516/20000], Training Loss: 0.0757\n",
            "Epoch [17517/20000], Training Loss: 0.0780\n",
            "Epoch [17518/20000], Training Loss: 0.0803\n",
            "Epoch [17519/20000], Training Loss: 0.0845\n",
            "Epoch [17520/20000], Training Loss: 0.0848\n",
            "Epoch [17521/20000], Training Loss: 0.0812\n",
            "Epoch [17522/20000], Training Loss: 0.0847\n",
            "Epoch [17523/20000], Training Loss: 0.0813\n",
            "Epoch [17524/20000], Training Loss: 0.0787\n",
            "Epoch [17525/20000], Training Loss: 0.0819\n",
            "Epoch [17526/20000], Training Loss: 0.0805\n",
            "Epoch [17527/20000], Training Loss: 0.0856\n",
            "Epoch [17528/20000], Training Loss: 0.0808\n",
            "Epoch [17529/20000], Training Loss: 0.0808\n",
            "Epoch [17530/20000], Training Loss: 0.0801\n",
            "Epoch [17531/20000], Training Loss: 0.0775\n",
            "Epoch [17532/20000], Training Loss: 0.0757\n",
            "Epoch [17533/20000], Training Loss: 0.0833\n",
            "Epoch [17534/20000], Training Loss: 0.0729\n",
            "Epoch [17535/20000], Training Loss: 0.0725\n",
            "Epoch [17536/20000], Training Loss: 0.0757\n",
            "Epoch [17537/20000], Training Loss: 0.0794\n",
            "Epoch [17538/20000], Training Loss: 0.0821\n",
            "Epoch [17539/20000], Training Loss: 0.0782\n",
            "Epoch [17540/20000], Training Loss: 0.0796\n",
            "Epoch [17541/20000], Training Loss: 0.0797\n",
            "Epoch [17542/20000], Training Loss: 0.0758\n",
            "Epoch [17543/20000], Training Loss: 0.0810\n",
            "Epoch [17544/20000], Training Loss: 0.0781\n",
            "Epoch [17545/20000], Training Loss: 0.0747\n",
            "Epoch [17546/20000], Training Loss: 0.0758\n",
            "Epoch [17547/20000], Training Loss: 0.0809\n",
            "Epoch [17548/20000], Training Loss: 0.0763\n",
            "Epoch [17549/20000], Training Loss: 0.0801\n",
            "Epoch [17550/20000], Training Loss: 0.0840\n",
            "Epoch [17551/20000], Training Loss: 0.0846\n",
            "Epoch [17552/20000], Training Loss: 0.0793\n",
            "Epoch [17553/20000], Training Loss: 0.0839\n",
            "Epoch [17554/20000], Training Loss: 0.0756\n",
            "Epoch [17555/20000], Training Loss: 0.0740\n",
            "Epoch [17556/20000], Training Loss: 0.0768\n",
            "Epoch [17557/20000], Training Loss: 0.0802\n",
            "Epoch [17558/20000], Training Loss: 0.0759\n",
            "Epoch [17559/20000], Training Loss: 0.0801\n",
            "Epoch [17560/20000], Training Loss: 0.0798\n",
            "Epoch [17561/20000], Training Loss: 0.0827\n",
            "Epoch [17562/20000], Training Loss: 0.0818\n",
            "Epoch [17563/20000], Training Loss: 0.0747\n",
            "Epoch [17564/20000], Training Loss: 0.0825\n",
            "Epoch [17565/20000], Training Loss: 0.0795\n",
            "Epoch [17566/20000], Training Loss: 0.0818\n",
            "Epoch [17567/20000], Training Loss: 0.0830\n",
            "Epoch [17568/20000], Training Loss: 0.0868\n",
            "Epoch [17569/20000], Training Loss: 0.0797\n",
            "Epoch [17570/20000], Training Loss: 0.0807\n",
            "Epoch [17571/20000], Training Loss: 0.0763\n",
            "Epoch [17572/20000], Training Loss: 0.0833\n",
            "Epoch [17573/20000], Training Loss: 0.0825\n",
            "Epoch [17574/20000], Training Loss: 0.0807\n",
            "Epoch [17575/20000], Training Loss: 0.0808\n",
            "Epoch [17576/20000], Training Loss: 0.0816\n",
            "Epoch [17577/20000], Training Loss: 0.0809\n",
            "Epoch [17578/20000], Training Loss: 0.0821\n",
            "Epoch [17579/20000], Training Loss: 0.0736\n",
            "Epoch [17580/20000], Training Loss: 0.0798\n",
            "Epoch [17581/20000], Training Loss: 0.0807\n",
            "Epoch [17582/20000], Training Loss: 0.0833\n",
            "Epoch [17583/20000], Training Loss: 0.0818\n",
            "Epoch [17584/20000], Training Loss: 0.0750\n",
            "Epoch [17585/20000], Training Loss: 0.0856\n",
            "Epoch [17586/20000], Training Loss: 0.0805\n",
            "Epoch [17587/20000], Training Loss: 0.0750\n",
            "Epoch [17588/20000], Training Loss: 0.0744\n",
            "Epoch [17589/20000], Training Loss: 0.0776\n",
            "Epoch [17590/20000], Training Loss: 0.0787\n",
            "Epoch [17591/20000], Training Loss: 0.0784\n",
            "Epoch [17592/20000], Training Loss: 0.0742\n",
            "Epoch [17593/20000], Training Loss: 0.0731\n",
            "Epoch [17594/20000], Training Loss: 0.0748\n",
            "Epoch [17595/20000], Training Loss: 0.0818\n",
            "Epoch [17596/20000], Training Loss: 0.0747\n",
            "Epoch [17597/20000], Training Loss: 0.0804\n",
            "Epoch [17598/20000], Training Loss: 0.0820\n",
            "Epoch [17599/20000], Training Loss: 0.0776\n",
            "Epoch [17600/20000], Training Loss: 0.0853\n",
            "Epoch [17601/20000], Training Loss: 0.0802\n",
            "Epoch [17602/20000], Training Loss: 0.0777\n",
            "Epoch [17603/20000], Training Loss: 0.0828\n",
            "Epoch [17604/20000], Training Loss: 0.0807\n",
            "Epoch [17605/20000], Training Loss: 0.0777\n",
            "Epoch [17606/20000], Training Loss: 0.0744\n",
            "Epoch [17607/20000], Training Loss: 0.0838\n",
            "Epoch [17608/20000], Training Loss: 0.0805\n",
            "Epoch [17609/20000], Training Loss: 0.0768\n",
            "Epoch [17610/20000], Training Loss: 0.0803\n",
            "Epoch [17611/20000], Training Loss: 0.0875\n",
            "Epoch [17612/20000], Training Loss: 0.0751\n",
            "Epoch [17613/20000], Training Loss: 0.0813\n",
            "Epoch [17614/20000], Training Loss: 0.0769\n",
            "Epoch [17615/20000], Training Loss: 0.0806\n",
            "Epoch [17616/20000], Training Loss: 0.0809\n",
            "Epoch [17617/20000], Training Loss: 0.0812\n",
            "Epoch [17618/20000], Training Loss: 0.0852\n",
            "Epoch [17619/20000], Training Loss: 0.0764\n",
            "Epoch [17620/20000], Training Loss: 0.0787\n",
            "Epoch [17621/20000], Training Loss: 0.0781\n",
            "Epoch [17622/20000], Training Loss: 0.0771\n",
            "Epoch [17623/20000], Training Loss: 0.0804\n",
            "Epoch [17624/20000], Training Loss: 0.0745\n",
            "Epoch [17625/20000], Training Loss: 0.0797\n",
            "Epoch [17626/20000], Training Loss: 0.0778\n",
            "Epoch [17627/20000], Training Loss: 0.0815\n",
            "Epoch [17628/20000], Training Loss: 0.0796\n",
            "Epoch [17629/20000], Training Loss: 0.0856\n",
            "Epoch [17630/20000], Training Loss: 0.0803\n",
            "Epoch [17631/20000], Training Loss: 0.0776\n",
            "Epoch [17632/20000], Training Loss: 0.0829\n",
            "Epoch [17633/20000], Training Loss: 0.0791\n",
            "Epoch [17634/20000], Training Loss: 0.0784\n",
            "Epoch [17635/20000], Training Loss: 0.0836\n",
            "Epoch [17636/20000], Training Loss: 0.0785\n",
            "Epoch [17637/20000], Training Loss: 0.0797\n",
            "Epoch [17638/20000], Training Loss: 0.0798\n",
            "Epoch [17639/20000], Training Loss: 0.0738\n",
            "Epoch [17640/20000], Training Loss: 0.0791\n",
            "Epoch [17641/20000], Training Loss: 0.0862\n",
            "Epoch [17642/20000], Training Loss: 0.0849\n",
            "Epoch [17643/20000], Training Loss: 0.0783\n",
            "Epoch [17644/20000], Training Loss: 0.0794\n",
            "Epoch [17645/20000], Training Loss: 0.0858\n",
            "Epoch [17646/20000], Training Loss: 0.0747\n",
            "Epoch [17647/20000], Training Loss: 0.0844\n",
            "Epoch [17648/20000], Training Loss: 0.0761\n",
            "Epoch [17649/20000], Training Loss: 0.0750\n",
            "Epoch [17650/20000], Training Loss: 0.0788\n",
            "Epoch [17651/20000], Training Loss: 0.0811\n",
            "Epoch [17652/20000], Training Loss: 0.0788\n",
            "Epoch [17653/20000], Training Loss: 0.0805\n",
            "Epoch [17654/20000], Training Loss: 0.0778\n",
            "Epoch [17655/20000], Training Loss: 0.0746\n",
            "Epoch [17656/20000], Training Loss: 0.0801\n",
            "Epoch [17657/20000], Training Loss: 0.0764\n",
            "Epoch [17658/20000], Training Loss: 0.0776\n",
            "Epoch [17659/20000], Training Loss: 0.0845\n",
            "Epoch [17660/20000], Training Loss: 0.0835\n",
            "Epoch [17661/20000], Training Loss: 0.0844\n",
            "Epoch [17662/20000], Training Loss: 0.0779\n",
            "Epoch [17663/20000], Training Loss: 0.0763\n",
            "Epoch [17664/20000], Training Loss: 0.0806\n",
            "Epoch [17665/20000], Training Loss: 0.0787\n",
            "Epoch [17666/20000], Training Loss: 0.0810\n",
            "Epoch [17667/20000], Training Loss: 0.0750\n",
            "Epoch [17668/20000], Training Loss: 0.0814\n",
            "Epoch [17669/20000], Training Loss: 0.0852\n",
            "Epoch [17670/20000], Training Loss: 0.0802\n",
            "Epoch [17671/20000], Training Loss: 0.0786\n",
            "Epoch [17672/20000], Training Loss: 0.0818\n",
            "Epoch [17673/20000], Training Loss: 0.0801\n",
            "Epoch [17674/20000], Training Loss: 0.0794\n",
            "Epoch [17675/20000], Training Loss: 0.0805\n",
            "Epoch [17676/20000], Training Loss: 0.0834\n",
            "Epoch [17677/20000], Training Loss: 0.0883\n",
            "Epoch [17678/20000], Training Loss: 0.0751\n",
            "Epoch [17679/20000], Training Loss: 0.0736\n",
            "Epoch [17680/20000], Training Loss: 0.0758\n",
            "Epoch [17681/20000], Training Loss: 0.0778\n",
            "Epoch [17682/20000], Training Loss: 0.0787\n",
            "Epoch [17683/20000], Training Loss: 0.0776\n",
            "Epoch [17684/20000], Training Loss: 0.0805\n",
            "Epoch [17685/20000], Training Loss: 0.0758\n",
            "Epoch [17686/20000], Training Loss: 0.0848\n",
            "Epoch [17687/20000], Training Loss: 0.0744\n",
            "Epoch [17688/20000], Training Loss: 0.0770\n",
            "Epoch [17689/20000], Training Loss: 0.0766\n",
            "Epoch [17690/20000], Training Loss: 0.0804\n",
            "Epoch [17691/20000], Training Loss: 0.0837\n",
            "Epoch [17692/20000], Training Loss: 0.0791\n",
            "Epoch [17693/20000], Training Loss: 0.0832\n",
            "Epoch [17694/20000], Training Loss: 0.0787\n",
            "Epoch [17695/20000], Training Loss: 0.0854\n",
            "Epoch [17696/20000], Training Loss: 0.0838\n",
            "Epoch [17697/20000], Training Loss: 0.0842\n",
            "Epoch [17698/20000], Training Loss: 0.0804\n",
            "Epoch [17699/20000], Training Loss: 0.0864\n",
            "Epoch [17700/20000], Training Loss: 0.0756\n",
            "Epoch [17701/20000], Training Loss: 0.0797\n",
            "Epoch [17702/20000], Training Loss: 0.0853\n",
            "Epoch [17703/20000], Training Loss: 0.0819\n",
            "Epoch [17704/20000], Training Loss: 0.0755\n",
            "Epoch [17705/20000], Training Loss: 0.0742\n",
            "Epoch [17706/20000], Training Loss: 0.0793\n",
            "Epoch [17707/20000], Training Loss: 0.0774\n",
            "Epoch [17708/20000], Training Loss: 0.0812\n",
            "Epoch [17709/20000], Training Loss: 0.0778\n",
            "Epoch [17710/20000], Training Loss: 0.0833\n",
            "Epoch [17711/20000], Training Loss: 0.0787\n",
            "Epoch [17712/20000], Training Loss: 0.0827\n",
            "Epoch [17713/20000], Training Loss: 0.0808\n",
            "Epoch [17714/20000], Training Loss: 0.0798\n",
            "Epoch [17715/20000], Training Loss: 0.0763\n",
            "Epoch [17716/20000], Training Loss: 0.0803\n",
            "Epoch [17717/20000], Training Loss: 0.0824\n",
            "Epoch [17718/20000], Training Loss: 0.0793\n",
            "Epoch [17719/20000], Training Loss: 0.0738\n",
            "Epoch [17720/20000], Training Loss: 0.0843\n",
            "Epoch [17721/20000], Training Loss: 0.0737\n",
            "Epoch [17722/20000], Training Loss: 0.0779\n",
            "Epoch [17723/20000], Training Loss: 0.0791\n",
            "Epoch [17724/20000], Training Loss: 0.0820\n",
            "Epoch [17725/20000], Training Loss: 0.0817\n",
            "Epoch [17726/20000], Training Loss: 0.0817\n",
            "Epoch [17727/20000], Training Loss: 0.0782\n",
            "Epoch [17728/20000], Training Loss: 0.0782\n",
            "Epoch [17729/20000], Training Loss: 0.0862\n",
            "Epoch [17730/20000], Training Loss: 0.0736\n",
            "Epoch [17731/20000], Training Loss: 0.0802\n",
            "Epoch [17732/20000], Training Loss: 0.0835\n",
            "Epoch [17733/20000], Training Loss: 0.0812\n",
            "Epoch [17734/20000], Training Loss: 0.0741\n",
            "Epoch [17735/20000], Training Loss: 0.0802\n",
            "Epoch [17736/20000], Training Loss: 0.0782\n",
            "Epoch [17737/20000], Training Loss: 0.0757\n",
            "Epoch [17738/20000], Training Loss: 0.0776\n",
            "Epoch [17739/20000], Training Loss: 0.0753\n",
            "Epoch [17740/20000], Training Loss: 0.0811\n",
            "Epoch [17741/20000], Training Loss: 0.0848\n",
            "Epoch [17742/20000], Training Loss: 0.0749\n",
            "Epoch [17743/20000], Training Loss: 0.0808\n",
            "Epoch [17744/20000], Training Loss: 0.0830\n",
            "Epoch [17745/20000], Training Loss: 0.0831\n",
            "Epoch [17746/20000], Training Loss: 0.0784\n",
            "Epoch [17747/20000], Training Loss: 0.0753\n",
            "Epoch [17748/20000], Training Loss: 0.0754\n",
            "Epoch [17749/20000], Training Loss: 0.0770\n",
            "Epoch [17750/20000], Training Loss: 0.0789\n",
            "Epoch [17751/20000], Training Loss: 0.0781\n",
            "Epoch [17752/20000], Training Loss: 0.0836\n",
            "Epoch [17753/20000], Training Loss: 0.0762\n",
            "Epoch [17754/20000], Training Loss: 0.0844\n",
            "Epoch [17755/20000], Training Loss: 0.0828\n",
            "Epoch [17756/20000], Training Loss: 0.0795\n",
            "Epoch [17757/20000], Training Loss: 0.0767\n",
            "Epoch [17758/20000], Training Loss: 0.0824\n",
            "Epoch [17759/20000], Training Loss: 0.0818\n",
            "Epoch [17760/20000], Training Loss: 0.0815\n",
            "Epoch [17761/20000], Training Loss: 0.0775\n",
            "Epoch [17762/20000], Training Loss: 0.0852\n",
            "Epoch [17763/20000], Training Loss: 0.0795\n",
            "Epoch [17764/20000], Training Loss: 0.0785\n",
            "Epoch [17765/20000], Training Loss: 0.0763\n",
            "Epoch [17766/20000], Training Loss: 0.0748\n",
            "Epoch [17767/20000], Training Loss: 0.0781\n",
            "Epoch [17768/20000], Training Loss: 0.0790\n",
            "Epoch [17769/20000], Training Loss: 0.0758\n",
            "Epoch [17770/20000], Training Loss: 0.0832\n",
            "Epoch [17771/20000], Training Loss: 0.0826\n",
            "Epoch [17772/20000], Training Loss: 0.0800\n",
            "Epoch [17773/20000], Training Loss: 0.0794\n",
            "Epoch [17774/20000], Training Loss: 0.0782\n",
            "Epoch [17775/20000], Training Loss: 0.0813\n",
            "Epoch [17776/20000], Training Loss: 0.0764\n",
            "Epoch [17777/20000], Training Loss: 0.0868\n",
            "Epoch [17778/20000], Training Loss: 0.0771\n",
            "Epoch [17779/20000], Training Loss: 0.0782\n",
            "Epoch [17780/20000], Training Loss: 0.0815\n",
            "Epoch [17781/20000], Training Loss: 0.0862\n",
            "Epoch [17782/20000], Training Loss: 0.0750\n",
            "Epoch [17783/20000], Training Loss: 0.0803\n",
            "Epoch [17784/20000], Training Loss: 0.0880\n",
            "Epoch [17785/20000], Training Loss: 0.0795\n",
            "Epoch [17786/20000], Training Loss: 0.0824\n",
            "Epoch [17787/20000], Training Loss: 0.0792\n",
            "Epoch [17788/20000], Training Loss: 0.0766\n",
            "Epoch [17789/20000], Training Loss: 0.0803\n",
            "Epoch [17790/20000], Training Loss: 0.0836\n",
            "Epoch [17791/20000], Training Loss: 0.0790\n",
            "Epoch [17792/20000], Training Loss: 0.0768\n",
            "Epoch [17793/20000], Training Loss: 0.0833\n",
            "Epoch [17794/20000], Training Loss: 0.0749\n",
            "Epoch [17795/20000], Training Loss: 0.0842\n",
            "Epoch [17796/20000], Training Loss: 0.0826\n",
            "Epoch [17797/20000], Training Loss: 0.0757\n",
            "Epoch [17798/20000], Training Loss: 0.0747\n",
            "Epoch [17799/20000], Training Loss: 0.0857\n",
            "Epoch [17800/20000], Training Loss: 0.0761\n",
            "Epoch [17801/20000], Training Loss: 0.0804\n",
            "Epoch [17802/20000], Training Loss: 0.0849\n",
            "Epoch [17803/20000], Training Loss: 0.0843\n",
            "Epoch [17804/20000], Training Loss: 0.0743\n",
            "Epoch [17805/20000], Training Loss: 0.0808\n",
            "Epoch [17806/20000], Training Loss: 0.0816\n",
            "Epoch [17807/20000], Training Loss: 0.0758\n",
            "Epoch [17808/20000], Training Loss: 0.0809\n",
            "Epoch [17809/20000], Training Loss: 0.0743\n",
            "Epoch [17810/20000], Training Loss: 0.0807\n",
            "Epoch [17811/20000], Training Loss: 0.0786\n",
            "Epoch [17812/20000], Training Loss: 0.0764\n",
            "Epoch [17813/20000], Training Loss: 0.0820\n",
            "Epoch [17814/20000], Training Loss: 0.0755\n",
            "Epoch [17815/20000], Training Loss: 0.0840\n",
            "Epoch [17816/20000], Training Loss: 0.0797\n",
            "Epoch [17817/20000], Training Loss: 0.0797\n",
            "Epoch [17818/20000], Training Loss: 0.0741\n",
            "Epoch [17819/20000], Training Loss: 0.0795\n",
            "Epoch [17820/20000], Training Loss: 0.0797\n",
            "Epoch [17821/20000], Training Loss: 0.0834\n",
            "Epoch [17822/20000], Training Loss: 0.0857\n",
            "Epoch [17823/20000], Training Loss: 0.0801\n",
            "Epoch [17824/20000], Training Loss: 0.0862\n",
            "Epoch [17825/20000], Training Loss: 0.0778\n",
            "Epoch [17826/20000], Training Loss: 0.0821\n",
            "Epoch [17827/20000], Training Loss: 0.0775\n",
            "Epoch [17828/20000], Training Loss: 0.0762\n",
            "Epoch [17829/20000], Training Loss: 0.0816\n",
            "Epoch [17830/20000], Training Loss: 0.0750\n",
            "Epoch [17831/20000], Training Loss: 0.0750\n",
            "Epoch [17832/20000], Training Loss: 0.0821\n",
            "Epoch [17833/20000], Training Loss: 0.0810\n",
            "Epoch [17834/20000], Training Loss: 0.0794\n",
            "Epoch [17835/20000], Training Loss: 0.0755\n",
            "Epoch [17836/20000], Training Loss: 0.0800\n",
            "Epoch [17837/20000], Training Loss: 0.0760\n",
            "Epoch [17838/20000], Training Loss: 0.0787\n",
            "Epoch [17839/20000], Training Loss: 0.0800\n",
            "Epoch [17840/20000], Training Loss: 0.0729\n",
            "Epoch [17841/20000], Training Loss: 0.0752\n",
            "Epoch [17842/20000], Training Loss: 0.0786\n",
            "Epoch [17843/20000], Training Loss: 0.0776\n",
            "Epoch [17844/20000], Training Loss: 0.0832\n",
            "Epoch [17845/20000], Training Loss: 0.0822\n",
            "Epoch [17846/20000], Training Loss: 0.0737\n",
            "Epoch [17847/20000], Training Loss: 0.0744\n",
            "Epoch [17848/20000], Training Loss: 0.0739\n",
            "Epoch [17849/20000], Training Loss: 0.0843\n",
            "Epoch [17850/20000], Training Loss: 0.0801\n",
            "Epoch [17851/20000], Training Loss: 0.0845\n",
            "Epoch [17852/20000], Training Loss: 0.0838\n",
            "Epoch [17853/20000], Training Loss: 0.0806\n",
            "Epoch [17854/20000], Training Loss: 0.0805\n",
            "Epoch [17855/20000], Training Loss: 0.0868\n",
            "Epoch [17856/20000], Training Loss: 0.0765\n",
            "Epoch [17857/20000], Training Loss: 0.0793\n",
            "Epoch [17858/20000], Training Loss: 0.0798\n",
            "Epoch [17859/20000], Training Loss: 0.0781\n",
            "Epoch [17860/20000], Training Loss: 0.0787\n",
            "Epoch [17861/20000], Training Loss: 0.0741\n",
            "Epoch [17862/20000], Training Loss: 0.0791\n",
            "Epoch [17863/20000], Training Loss: 0.0873\n",
            "Epoch [17864/20000], Training Loss: 0.0831\n",
            "Epoch [17865/20000], Training Loss: 0.0804\n",
            "Epoch [17866/20000], Training Loss: 0.0738\n",
            "Epoch [17867/20000], Training Loss: 0.0807\n",
            "Epoch [17868/20000], Training Loss: 0.0856\n",
            "Epoch [17869/20000], Training Loss: 0.0807\n",
            "Epoch [17870/20000], Training Loss: 0.0756\n",
            "Epoch [17871/20000], Training Loss: 0.0798\n",
            "Epoch [17872/20000], Training Loss: 0.0742\n",
            "Epoch [17873/20000], Training Loss: 0.0784\n",
            "Epoch [17874/20000], Training Loss: 0.0814\n",
            "Epoch [17875/20000], Training Loss: 0.0797\n",
            "Epoch [17876/20000], Training Loss: 0.0808\n",
            "Epoch [17877/20000], Training Loss: 0.0794\n",
            "Epoch [17878/20000], Training Loss: 0.0779\n",
            "Epoch [17879/20000], Training Loss: 0.0766\n",
            "Epoch [17880/20000], Training Loss: 0.0844\n",
            "Epoch [17881/20000], Training Loss: 0.0730\n",
            "Epoch [17882/20000], Training Loss: 0.0807\n",
            "Epoch [17883/20000], Training Loss: 0.0806\n",
            "Epoch [17884/20000], Training Loss: 0.0752\n",
            "Epoch [17885/20000], Training Loss: 0.0794\n",
            "Epoch [17886/20000], Training Loss: 0.0743\n",
            "Epoch [17887/20000], Training Loss: 0.0826\n",
            "Epoch [17888/20000], Training Loss: 0.0806\n",
            "Epoch [17889/20000], Training Loss: 0.0802\n",
            "Epoch [17890/20000], Training Loss: 0.0743\n",
            "Epoch [17891/20000], Training Loss: 0.0739\n",
            "Epoch [17892/20000], Training Loss: 0.0828\n",
            "Epoch [17893/20000], Training Loss: 0.0819\n",
            "Epoch [17894/20000], Training Loss: 0.0757\n",
            "Epoch [17895/20000], Training Loss: 0.0821\n",
            "Epoch [17896/20000], Training Loss: 0.0828\n",
            "Epoch [17897/20000], Training Loss: 0.0794\n",
            "Epoch [17898/20000], Training Loss: 0.0800\n",
            "Epoch [17899/20000], Training Loss: 0.0792\n",
            "Epoch [17900/20000], Training Loss: 0.0767\n",
            "Epoch [17901/20000], Training Loss: 0.0740\n",
            "Epoch [17902/20000], Training Loss: 0.0802\n",
            "Epoch [17903/20000], Training Loss: 0.0784\n",
            "Epoch [17904/20000], Training Loss: 0.0788\n",
            "Epoch [17905/20000], Training Loss: 0.0736\n",
            "Epoch [17906/20000], Training Loss: 0.0789\n",
            "Epoch [17907/20000], Training Loss: 0.0787\n",
            "Epoch [17908/20000], Training Loss: 0.0732\n",
            "Epoch [17909/20000], Training Loss: 0.0790\n",
            "Epoch [17910/20000], Training Loss: 0.0800\n",
            "Epoch [17911/20000], Training Loss: 0.0807\n",
            "Epoch [17912/20000], Training Loss: 0.0825\n",
            "Epoch [17913/20000], Training Loss: 0.0741\n",
            "Epoch [17914/20000], Training Loss: 0.0787\n",
            "Epoch [17915/20000], Training Loss: 0.0768\n",
            "Epoch [17916/20000], Training Loss: 0.0793\n",
            "Epoch [17917/20000], Training Loss: 0.0769\n",
            "Epoch [17918/20000], Training Loss: 0.0773\n",
            "Epoch [17919/20000], Training Loss: 0.0813\n",
            "Epoch [17920/20000], Training Loss: 0.0797\n",
            "Epoch [17921/20000], Training Loss: 0.0816\n",
            "Epoch [17922/20000], Training Loss: 0.0790\n",
            "Epoch [17923/20000], Training Loss: 0.0809\n",
            "Epoch [17924/20000], Training Loss: 0.0745\n",
            "Epoch [17925/20000], Training Loss: 0.0828\n",
            "Epoch [17926/20000], Training Loss: 0.0834\n",
            "Epoch [17927/20000], Training Loss: 0.0763\n",
            "Epoch [17928/20000], Training Loss: 0.0831\n",
            "Epoch [17929/20000], Training Loss: 0.0789\n",
            "Epoch [17930/20000], Training Loss: 0.0804\n",
            "Epoch [17931/20000], Training Loss: 0.0814\n",
            "Epoch [17932/20000], Training Loss: 0.0794\n",
            "Epoch [17933/20000], Training Loss: 0.0738\n",
            "Epoch [17934/20000], Training Loss: 0.0804\n",
            "Epoch [17935/20000], Training Loss: 0.0814\n",
            "Epoch [17936/20000], Training Loss: 0.0811\n",
            "Epoch [17937/20000], Training Loss: 0.0765\n",
            "Epoch [17938/20000], Training Loss: 0.0848\n",
            "Epoch [17939/20000], Training Loss: 0.0818\n",
            "Epoch [17940/20000], Training Loss: 0.0774\n",
            "Epoch [17941/20000], Training Loss: 0.0795\n",
            "Epoch [17942/20000], Training Loss: 0.0790\n",
            "Epoch [17943/20000], Training Loss: 0.0780\n",
            "Epoch [17944/20000], Training Loss: 0.0789\n",
            "Epoch [17945/20000], Training Loss: 0.0761\n",
            "Epoch [17946/20000], Training Loss: 0.0801\n",
            "Epoch [17947/20000], Training Loss: 0.0795\n",
            "Epoch [17948/20000], Training Loss: 0.0836\n",
            "Epoch [17949/20000], Training Loss: 0.0771\n",
            "Epoch [17950/20000], Training Loss: 0.0802\n",
            "Epoch [17951/20000], Training Loss: 0.0781\n",
            "Epoch [17952/20000], Training Loss: 0.0765\n",
            "Epoch [17953/20000], Training Loss: 0.0817\n",
            "Epoch [17954/20000], Training Loss: 0.0790\n",
            "Epoch [17955/20000], Training Loss: 0.0742\n",
            "Epoch [17956/20000], Training Loss: 0.0819\n",
            "Epoch [17957/20000], Training Loss: 0.0819\n",
            "Epoch [17958/20000], Training Loss: 0.0779\n",
            "Epoch [17959/20000], Training Loss: 0.0836\n",
            "Epoch [17960/20000], Training Loss: 0.0824\n",
            "Epoch [17961/20000], Training Loss: 0.0798\n",
            "Epoch [17962/20000], Training Loss: 0.0804\n",
            "Epoch [17963/20000], Training Loss: 0.0746\n",
            "Epoch [17964/20000], Training Loss: 0.0734\n",
            "Epoch [17965/20000], Training Loss: 0.0776\n",
            "Epoch [17966/20000], Training Loss: 0.0808\n",
            "Epoch [17967/20000], Training Loss: 0.0813\n",
            "Epoch [17968/20000], Training Loss: 0.0798\n",
            "Epoch [17969/20000], Training Loss: 0.0802\n",
            "Epoch [17970/20000], Training Loss: 0.0753\n",
            "Epoch [17971/20000], Training Loss: 0.0766\n",
            "Epoch [17972/20000], Training Loss: 0.0802\n",
            "Epoch [17973/20000], Training Loss: 0.0835\n",
            "Epoch [17974/20000], Training Loss: 0.0783\n",
            "Epoch [17975/20000], Training Loss: 0.0781\n",
            "Epoch [17976/20000], Training Loss: 0.0805\n",
            "Epoch [17977/20000], Training Loss: 0.0801\n",
            "Epoch [17978/20000], Training Loss: 0.0755\n",
            "Epoch [17979/20000], Training Loss: 0.0755\n",
            "Epoch [17980/20000], Training Loss: 0.0834\n",
            "Epoch [17981/20000], Training Loss: 0.0809\n",
            "Epoch [17982/20000], Training Loss: 0.0844\n",
            "Epoch [17983/20000], Training Loss: 0.0781\n",
            "Epoch [17984/20000], Training Loss: 0.0816\n",
            "Epoch [17985/20000], Training Loss: 0.0806\n",
            "Epoch [17986/20000], Training Loss: 0.0769\n",
            "Epoch [17987/20000], Training Loss: 0.0800\n",
            "Epoch [17988/20000], Training Loss: 0.0757\n",
            "Epoch [17989/20000], Training Loss: 0.0845\n",
            "Epoch [17990/20000], Training Loss: 0.0819\n",
            "Epoch [17991/20000], Training Loss: 0.0795\n",
            "Epoch [17992/20000], Training Loss: 0.0871\n",
            "Epoch [17993/20000], Training Loss: 0.0793\n",
            "Epoch [17994/20000], Training Loss: 0.0747\n",
            "Epoch [17995/20000], Training Loss: 0.0843\n",
            "Epoch [17996/20000], Training Loss: 0.0855\n",
            "Epoch [17997/20000], Training Loss: 0.0822\n",
            "Epoch [17998/20000], Training Loss: 0.0785\n",
            "Epoch [17999/20000], Training Loss: 0.0788\n",
            "Epoch [18000/20000], Training Loss: 0.0828\n",
            "Epoch [18001/20000], Training Loss: 0.0800\n",
            "Epoch [18002/20000], Training Loss: 0.0804\n",
            "Epoch [18003/20000], Training Loss: 0.0794\n",
            "Epoch [18004/20000], Training Loss: 0.0755\n",
            "Epoch [18005/20000], Training Loss: 0.0757\n",
            "Epoch [18006/20000], Training Loss: 0.0770\n",
            "Epoch [18007/20000], Training Loss: 0.0755\n",
            "Epoch [18008/20000], Training Loss: 0.0746\n",
            "Epoch [18009/20000], Training Loss: 0.0719\n",
            "Epoch [18010/20000], Training Loss: 0.0842\n",
            "Epoch [18011/20000], Training Loss: 0.0812\n",
            "Epoch [18012/20000], Training Loss: 0.0799\n",
            "Epoch [18013/20000], Training Loss: 0.0828\n",
            "Epoch [18014/20000], Training Loss: 0.0810\n",
            "Epoch [18015/20000], Training Loss: 0.0767\n",
            "Epoch [18016/20000], Training Loss: 0.0784\n",
            "Epoch [18017/20000], Training Loss: 0.0721\n",
            "Epoch [18018/20000], Training Loss: 0.0752\n",
            "Epoch [18019/20000], Training Loss: 0.0757\n",
            "Epoch [18020/20000], Training Loss: 0.0790\n",
            "Epoch [18021/20000], Training Loss: 0.0792\n",
            "Epoch [18022/20000], Training Loss: 0.0746\n",
            "Epoch [18023/20000], Training Loss: 0.0757\n",
            "Epoch [18024/20000], Training Loss: 0.0781\n",
            "Epoch [18025/20000], Training Loss: 0.0763\n",
            "Epoch [18026/20000], Training Loss: 0.0817\n",
            "Epoch [18027/20000], Training Loss: 0.0778\n",
            "Epoch [18028/20000], Training Loss: 0.0787\n",
            "Epoch [18029/20000], Training Loss: 0.0761\n",
            "Epoch [18030/20000], Training Loss: 0.0840\n",
            "Epoch [18031/20000], Training Loss: 0.0808\n",
            "Epoch [18032/20000], Training Loss: 0.0757\n",
            "Epoch [18033/20000], Training Loss: 0.0809\n",
            "Epoch [18034/20000], Training Loss: 0.0785\n",
            "Epoch [18035/20000], Training Loss: 0.0813\n",
            "Epoch [18036/20000], Training Loss: 0.0827\n",
            "Epoch [18037/20000], Training Loss: 0.0826\n",
            "Epoch [18038/20000], Training Loss: 0.0828\n",
            "Epoch [18039/20000], Training Loss: 0.0819\n",
            "Epoch [18040/20000], Training Loss: 0.0781\n",
            "Epoch [18041/20000], Training Loss: 0.0808\n",
            "Epoch [18042/20000], Training Loss: 0.0826\n",
            "Epoch [18043/20000], Training Loss: 0.0736\n",
            "Epoch [18044/20000], Training Loss: 0.0818\n",
            "Epoch [18045/20000], Training Loss: 0.0816\n",
            "Epoch [18046/20000], Training Loss: 0.0748\n",
            "Epoch [18047/20000], Training Loss: 0.0750\n",
            "Epoch [18048/20000], Training Loss: 0.0771\n",
            "Epoch [18049/20000], Training Loss: 0.0751\n",
            "Epoch [18050/20000], Training Loss: 0.0756\n",
            "Epoch [18051/20000], Training Loss: 0.0811\n",
            "Epoch [18052/20000], Training Loss: 0.0789\n",
            "Epoch [18053/20000], Training Loss: 0.0745\n",
            "Epoch [18054/20000], Training Loss: 0.0859\n",
            "Epoch [18055/20000], Training Loss: 0.0860\n",
            "Epoch [18056/20000], Training Loss: 0.0799\n",
            "Epoch [18057/20000], Training Loss: 0.0760\n",
            "Epoch [18058/20000], Training Loss: 0.0762\n",
            "Epoch [18059/20000], Training Loss: 0.0754\n",
            "Epoch [18060/20000], Training Loss: 0.0752\n",
            "Epoch [18061/20000], Training Loss: 0.0817\n",
            "Epoch [18062/20000], Training Loss: 0.0795\n",
            "Epoch [18063/20000], Training Loss: 0.0834\n",
            "Epoch [18064/20000], Training Loss: 0.0767\n",
            "Epoch [18065/20000], Training Loss: 0.0786\n",
            "Epoch [18066/20000], Training Loss: 0.0793\n",
            "Epoch [18067/20000], Training Loss: 0.0872\n",
            "Epoch [18068/20000], Training Loss: 0.0756\n",
            "Epoch [18069/20000], Training Loss: 0.0782\n",
            "Epoch [18070/20000], Training Loss: 0.0794\n",
            "Epoch [18071/20000], Training Loss: 0.0810\n",
            "Epoch [18072/20000], Training Loss: 0.0798\n",
            "Epoch [18073/20000], Training Loss: 0.0773\n",
            "Epoch [18074/20000], Training Loss: 0.0763\n",
            "Epoch [18075/20000], Training Loss: 0.0815\n",
            "Epoch [18076/20000], Training Loss: 0.0819\n",
            "Epoch [18077/20000], Training Loss: 0.0852\n",
            "Epoch [18078/20000], Training Loss: 0.0741\n",
            "Epoch [18079/20000], Training Loss: 0.0870\n",
            "Epoch [18080/20000], Training Loss: 0.0874\n",
            "Epoch [18081/20000], Training Loss: 0.0826\n",
            "Epoch [18082/20000], Training Loss: 0.0767\n",
            "Epoch [18083/20000], Training Loss: 0.0790\n",
            "Epoch [18084/20000], Training Loss: 0.0775\n",
            "Epoch [18085/20000], Training Loss: 0.0780\n",
            "Epoch [18086/20000], Training Loss: 0.0756\n",
            "Epoch [18087/20000], Training Loss: 0.0804\n",
            "Epoch [18088/20000], Training Loss: 0.0796\n",
            "Epoch [18089/20000], Training Loss: 0.0768\n",
            "Epoch [18090/20000], Training Loss: 0.0780\n",
            "Epoch [18091/20000], Training Loss: 0.0795\n",
            "Epoch [18092/20000], Training Loss: 0.0772\n",
            "Epoch [18093/20000], Training Loss: 0.0807\n",
            "Epoch [18094/20000], Training Loss: 0.0732\n",
            "Epoch [18095/20000], Training Loss: 0.0804\n",
            "Epoch [18096/20000], Training Loss: 0.0795\n",
            "Epoch [18097/20000], Training Loss: 0.0799\n",
            "Epoch [18098/20000], Training Loss: 0.0819\n",
            "Epoch [18099/20000], Training Loss: 0.0807\n",
            "Epoch [18100/20000], Training Loss: 0.0814\n",
            "Epoch [18101/20000], Training Loss: 0.0824\n",
            "Epoch [18102/20000], Training Loss: 0.0764\n",
            "Epoch [18103/20000], Training Loss: 0.0789\n",
            "Epoch [18104/20000], Training Loss: 0.0815\n",
            "Epoch [18105/20000], Training Loss: 0.0798\n",
            "Epoch [18106/20000], Training Loss: 0.0747\n",
            "Epoch [18107/20000], Training Loss: 0.0842\n",
            "Epoch [18108/20000], Training Loss: 0.0777\n",
            "Epoch [18109/20000], Training Loss: 0.0811\n",
            "Epoch [18110/20000], Training Loss: 0.0834\n",
            "Epoch [18111/20000], Training Loss: 0.0792\n",
            "Epoch [18112/20000], Training Loss: 0.0760\n",
            "Epoch [18113/20000], Training Loss: 0.0799\n",
            "Epoch [18114/20000], Training Loss: 0.0819\n",
            "Epoch [18115/20000], Training Loss: 0.0810\n",
            "Epoch [18116/20000], Training Loss: 0.0843\n",
            "Epoch [18117/20000], Training Loss: 0.0809\n",
            "Epoch [18118/20000], Training Loss: 0.0814\n",
            "Epoch [18119/20000], Training Loss: 0.0769\n",
            "Epoch [18120/20000], Training Loss: 0.0802\n",
            "Epoch [18121/20000], Training Loss: 0.0826\n",
            "Epoch [18122/20000], Training Loss: 0.0766\n",
            "Epoch [18123/20000], Training Loss: 0.0762\n",
            "Epoch [18124/20000], Training Loss: 0.0771\n",
            "Epoch [18125/20000], Training Loss: 0.0834\n",
            "Epoch [18126/20000], Training Loss: 0.0819\n",
            "Epoch [18127/20000], Training Loss: 0.0812\n",
            "Epoch [18128/20000], Training Loss: 0.0777\n",
            "Epoch [18129/20000], Training Loss: 0.0812\n",
            "Epoch [18130/20000], Training Loss: 0.0837\n",
            "Epoch [18131/20000], Training Loss: 0.0758\n",
            "Epoch [18132/20000], Training Loss: 0.0734\n",
            "Epoch [18133/20000], Training Loss: 0.0753\n",
            "Epoch [18134/20000], Training Loss: 0.0745\n",
            "Epoch [18135/20000], Training Loss: 0.0790\n",
            "Epoch [18136/20000], Training Loss: 0.0800\n",
            "Epoch [18137/20000], Training Loss: 0.0785\n",
            "Epoch [18138/20000], Training Loss: 0.0786\n",
            "Epoch [18139/20000], Training Loss: 0.0811\n",
            "Epoch [18140/20000], Training Loss: 0.0746\n",
            "Epoch [18141/20000], Training Loss: 0.0836\n",
            "Epoch [18142/20000], Training Loss: 0.0733\n",
            "Epoch [18143/20000], Training Loss: 0.0776\n",
            "Epoch [18144/20000], Training Loss: 0.0889\n",
            "Epoch [18145/20000], Training Loss: 0.0802\n",
            "Epoch [18146/20000], Training Loss: 0.0801\n",
            "Epoch [18147/20000], Training Loss: 0.0767\n",
            "Epoch [18148/20000], Training Loss: 0.0758\n",
            "Epoch [18149/20000], Training Loss: 0.0844\n",
            "Epoch [18150/20000], Training Loss: 0.0817\n",
            "Epoch [18151/20000], Training Loss: 0.0794\n",
            "Epoch [18152/20000], Training Loss: 0.0810\n",
            "Epoch [18153/20000], Training Loss: 0.0765\n",
            "Epoch [18154/20000], Training Loss: 0.0740\n",
            "Epoch [18155/20000], Training Loss: 0.0825\n",
            "Epoch [18156/20000], Training Loss: 0.0795\n",
            "Epoch [18157/20000], Training Loss: 0.0781\n",
            "Epoch [18158/20000], Training Loss: 0.0752\n",
            "Epoch [18159/20000], Training Loss: 0.0842\n",
            "Epoch [18160/20000], Training Loss: 0.0749\n",
            "Epoch [18161/20000], Training Loss: 0.0745\n",
            "Epoch [18162/20000], Training Loss: 0.0806\n",
            "Epoch [18163/20000], Training Loss: 0.0812\n",
            "Epoch [18164/20000], Training Loss: 0.0764\n",
            "Epoch [18165/20000], Training Loss: 0.0841\n",
            "Epoch [18166/20000], Training Loss: 0.0758\n",
            "Epoch [18167/20000], Training Loss: 0.0790\n",
            "Epoch [18168/20000], Training Loss: 0.0760\n",
            "Epoch [18169/20000], Training Loss: 0.0803\n",
            "Epoch [18170/20000], Training Loss: 0.0757\n",
            "Epoch [18171/20000], Training Loss: 0.0756\n",
            "Epoch [18172/20000], Training Loss: 0.0812\n",
            "Epoch [18173/20000], Training Loss: 0.0760\n",
            "Epoch [18174/20000], Training Loss: 0.0813\n",
            "Epoch [18175/20000], Training Loss: 0.0805\n",
            "Epoch [18176/20000], Training Loss: 0.0827\n",
            "Epoch [18177/20000], Training Loss: 0.0736\n",
            "Epoch [18178/20000], Training Loss: 0.0811\n",
            "Epoch [18179/20000], Training Loss: 0.0799\n",
            "Epoch [18180/20000], Training Loss: 0.0844\n",
            "Epoch [18181/20000], Training Loss: 0.0810\n",
            "Epoch [18182/20000], Training Loss: 0.0796\n",
            "Epoch [18183/20000], Training Loss: 0.0734\n",
            "Epoch [18184/20000], Training Loss: 0.0834\n",
            "Epoch [18185/20000], Training Loss: 0.0793\n",
            "Epoch [18186/20000], Training Loss: 0.0807\n",
            "Epoch [18187/20000], Training Loss: 0.0759\n",
            "Epoch [18188/20000], Training Loss: 0.0813\n",
            "Epoch [18189/20000], Training Loss: 0.0738\n",
            "Epoch [18190/20000], Training Loss: 0.0801\n",
            "Epoch [18191/20000], Training Loss: 0.0843\n",
            "Epoch [18192/20000], Training Loss: 0.0755\n",
            "Epoch [18193/20000], Training Loss: 0.0748\n",
            "Epoch [18194/20000], Training Loss: 0.0745\n",
            "Epoch [18195/20000], Training Loss: 0.0752\n",
            "Epoch [18196/20000], Training Loss: 0.0762\n",
            "Epoch [18197/20000], Training Loss: 0.0864\n",
            "Epoch [18198/20000], Training Loss: 0.0782\n",
            "Epoch [18199/20000], Training Loss: 0.0743\n",
            "Epoch [18200/20000], Training Loss: 0.0792\n",
            "Epoch [18201/20000], Training Loss: 0.0796\n",
            "Epoch [18202/20000], Training Loss: 0.0814\n",
            "Epoch [18203/20000], Training Loss: 0.0816\n",
            "Epoch [18204/20000], Training Loss: 0.0753\n",
            "Epoch [18205/20000], Training Loss: 0.0854\n",
            "Epoch [18206/20000], Training Loss: 0.0783\n",
            "Epoch [18207/20000], Training Loss: 0.0869\n",
            "Epoch [18208/20000], Training Loss: 0.0772\n",
            "Epoch [18209/20000], Training Loss: 0.0780\n",
            "Epoch [18210/20000], Training Loss: 0.0791\n",
            "Epoch [18211/20000], Training Loss: 0.0807\n",
            "Epoch [18212/20000], Training Loss: 0.0797\n",
            "Epoch [18213/20000], Training Loss: 0.0801\n",
            "Epoch [18214/20000], Training Loss: 0.0810\n",
            "Epoch [18215/20000], Training Loss: 0.0801\n",
            "Epoch [18216/20000], Training Loss: 0.0775\n",
            "Epoch [18217/20000], Training Loss: 0.0798\n",
            "Epoch [18218/20000], Training Loss: 0.0787\n",
            "Epoch [18219/20000], Training Loss: 0.0837\n",
            "Epoch [18220/20000], Training Loss: 0.0815\n",
            "Epoch [18221/20000], Training Loss: 0.0813\n",
            "Epoch [18222/20000], Training Loss: 0.0865\n",
            "Epoch [18223/20000], Training Loss: 0.0805\n",
            "Epoch [18224/20000], Training Loss: 0.0805\n",
            "Epoch [18225/20000], Training Loss: 0.0796\n",
            "Epoch [18226/20000], Training Loss: 0.0754\n",
            "Epoch [18227/20000], Training Loss: 0.0790\n",
            "Epoch [18228/20000], Training Loss: 0.0773\n",
            "Epoch [18229/20000], Training Loss: 0.0861\n",
            "Epoch [18230/20000], Training Loss: 0.0795\n",
            "Epoch [18231/20000], Training Loss: 0.0763\n",
            "Epoch [18232/20000], Training Loss: 0.0834\n",
            "Epoch [18233/20000], Training Loss: 0.0808\n",
            "Epoch [18234/20000], Training Loss: 0.0752\n",
            "Epoch [18235/20000], Training Loss: 0.0864\n",
            "Epoch [18236/20000], Training Loss: 0.0845\n",
            "Epoch [18237/20000], Training Loss: 0.0786\n",
            "Epoch [18238/20000], Training Loss: 0.0783\n",
            "Epoch [18239/20000], Training Loss: 0.0803\n",
            "Epoch [18240/20000], Training Loss: 0.0764\n",
            "Epoch [18241/20000], Training Loss: 0.0792\n",
            "Epoch [18242/20000], Training Loss: 0.0817\n",
            "Epoch [18243/20000], Training Loss: 0.0806\n",
            "Epoch [18244/20000], Training Loss: 0.0786\n",
            "Epoch [18245/20000], Training Loss: 0.0774\n",
            "Epoch [18246/20000], Training Loss: 0.0815\n",
            "Epoch [18247/20000], Training Loss: 0.0851\n",
            "Epoch [18248/20000], Training Loss: 0.0812\n",
            "Epoch [18249/20000], Training Loss: 0.0843\n",
            "Epoch [18250/20000], Training Loss: 0.0791\n",
            "Epoch [18251/20000], Training Loss: 0.0804\n",
            "Epoch [18252/20000], Training Loss: 0.0790\n",
            "Epoch [18253/20000], Training Loss: 0.0842\n",
            "Epoch [18254/20000], Training Loss: 0.0803\n",
            "Epoch [18255/20000], Training Loss: 0.0871\n",
            "Epoch [18256/20000], Training Loss: 0.0806\n",
            "Epoch [18257/20000], Training Loss: 0.0741\n",
            "Epoch [18258/20000], Training Loss: 0.0798\n",
            "Epoch [18259/20000], Training Loss: 0.0815\n",
            "Epoch [18260/20000], Training Loss: 0.0789\n",
            "Epoch [18261/20000], Training Loss: 0.0763\n",
            "Epoch [18262/20000], Training Loss: 0.0826\n",
            "Epoch [18263/20000], Training Loss: 0.0753\n",
            "Epoch [18264/20000], Training Loss: 0.0814\n",
            "Epoch [18265/20000], Training Loss: 0.0818\n",
            "Epoch [18266/20000], Training Loss: 0.0768\n",
            "Epoch [18267/20000], Training Loss: 0.0858\n",
            "Epoch [18268/20000], Training Loss: 0.0793\n",
            "Epoch [18269/20000], Training Loss: 0.0792\n",
            "Epoch [18270/20000], Training Loss: 0.0804\n",
            "Epoch [18271/20000], Training Loss: 0.0860\n",
            "Epoch [18272/20000], Training Loss: 0.0777\n",
            "Epoch [18273/20000], Training Loss: 0.0791\n",
            "Epoch [18274/20000], Training Loss: 0.0804\n",
            "Epoch [18275/20000], Training Loss: 0.0862\n",
            "Epoch [18276/20000], Training Loss: 0.0792\n",
            "Epoch [18277/20000], Training Loss: 0.0776\n",
            "Epoch [18278/20000], Training Loss: 0.0785\n",
            "Epoch [18279/20000], Training Loss: 0.0741\n",
            "Epoch [18280/20000], Training Loss: 0.0764\n",
            "Epoch [18281/20000], Training Loss: 0.0748\n",
            "Epoch [18282/20000], Training Loss: 0.0864\n",
            "Epoch [18283/20000], Training Loss: 0.0808\n",
            "Epoch [18284/20000], Training Loss: 0.0793\n",
            "Epoch [18285/20000], Training Loss: 0.0804\n",
            "Epoch [18286/20000], Training Loss: 0.0765\n",
            "Epoch [18287/20000], Training Loss: 0.0814\n",
            "Epoch [18288/20000], Training Loss: 0.0757\n",
            "Epoch [18289/20000], Training Loss: 0.0807\n",
            "Epoch [18290/20000], Training Loss: 0.0813\n",
            "Epoch [18291/20000], Training Loss: 0.0744\n",
            "Epoch [18292/20000], Training Loss: 0.0778\n",
            "Epoch [18293/20000], Training Loss: 0.0799\n",
            "Epoch [18294/20000], Training Loss: 0.0785\n",
            "Epoch [18295/20000], Training Loss: 0.0771\n",
            "Epoch [18296/20000], Training Loss: 0.0754\n",
            "Epoch [18297/20000], Training Loss: 0.0847\n",
            "Epoch [18298/20000], Training Loss: 0.0806\n",
            "Epoch [18299/20000], Training Loss: 0.0819\n",
            "Epoch [18300/20000], Training Loss: 0.0787\n",
            "Epoch [18301/20000], Training Loss: 0.0850\n",
            "Epoch [18302/20000], Training Loss: 0.0815\n",
            "Epoch [18303/20000], Training Loss: 0.0795\n",
            "Epoch [18304/20000], Training Loss: 0.0745\n",
            "Epoch [18305/20000], Training Loss: 0.0830\n",
            "Epoch [18306/20000], Training Loss: 0.0801\n",
            "Epoch [18307/20000], Training Loss: 0.0813\n",
            "Epoch [18308/20000], Training Loss: 0.0856\n",
            "Epoch [18309/20000], Training Loss: 0.0786\n",
            "Epoch [18310/20000], Training Loss: 0.0756\n",
            "Epoch [18311/20000], Training Loss: 0.0823\n",
            "Epoch [18312/20000], Training Loss: 0.0757\n",
            "Epoch [18313/20000], Training Loss: 0.0803\n",
            "Epoch [18314/20000], Training Loss: 0.0817\n",
            "Epoch [18315/20000], Training Loss: 0.0829\n",
            "Epoch [18316/20000], Training Loss: 0.0776\n",
            "Epoch [18317/20000], Training Loss: 0.0809\n",
            "Epoch [18318/20000], Training Loss: 0.0763\n",
            "Epoch [18319/20000], Training Loss: 0.0800\n",
            "Epoch [18320/20000], Training Loss: 0.0801\n",
            "Epoch [18321/20000], Training Loss: 0.0788\n",
            "Epoch [18322/20000], Training Loss: 0.0803\n",
            "Epoch [18323/20000], Training Loss: 0.0785\n",
            "Epoch [18324/20000], Training Loss: 0.0759\n",
            "Epoch [18325/20000], Training Loss: 0.0787\n",
            "Epoch [18326/20000], Training Loss: 0.0785\n",
            "Epoch [18327/20000], Training Loss: 0.0855\n",
            "Epoch [18328/20000], Training Loss: 0.0747\n",
            "Epoch [18329/20000], Training Loss: 0.0833\n",
            "Epoch [18330/20000], Training Loss: 0.0847\n",
            "Epoch [18331/20000], Training Loss: 0.0750\n",
            "Epoch [18332/20000], Training Loss: 0.0815\n",
            "Epoch [18333/20000], Training Loss: 0.0849\n",
            "Epoch [18334/20000], Training Loss: 0.0768\n",
            "Epoch [18335/20000], Training Loss: 0.0819\n",
            "Epoch [18336/20000], Training Loss: 0.0774\n",
            "Epoch [18337/20000], Training Loss: 0.0805\n",
            "Epoch [18338/20000], Training Loss: 0.0795\n",
            "Epoch [18339/20000], Training Loss: 0.0825\n",
            "Epoch [18340/20000], Training Loss: 0.0850\n",
            "Epoch [18341/20000], Training Loss: 0.0777\n",
            "Epoch [18342/20000], Training Loss: 0.0824\n",
            "Epoch [18343/20000], Training Loss: 0.0759\n",
            "Epoch [18344/20000], Training Loss: 0.0824\n",
            "Epoch [18345/20000], Training Loss: 0.0754\n",
            "Epoch [18346/20000], Training Loss: 0.0817\n",
            "Epoch [18347/20000], Training Loss: 0.0808\n",
            "Epoch [18348/20000], Training Loss: 0.0751\n",
            "Epoch [18349/20000], Training Loss: 0.0770\n",
            "Epoch [18350/20000], Training Loss: 0.0821\n",
            "Epoch [18351/20000], Training Loss: 0.0733\n",
            "Epoch [18352/20000], Training Loss: 0.0859\n",
            "Epoch [18353/20000], Training Loss: 0.0799\n",
            "Epoch [18354/20000], Training Loss: 0.0853\n",
            "Epoch [18355/20000], Training Loss: 0.0799\n",
            "Epoch [18356/20000], Training Loss: 0.0857\n",
            "Epoch [18357/20000], Training Loss: 0.0801\n",
            "Epoch [18358/20000], Training Loss: 0.0767\n",
            "Epoch [18359/20000], Training Loss: 0.0818\n",
            "Epoch [18360/20000], Training Loss: 0.0840\n",
            "Epoch [18361/20000], Training Loss: 0.0747\n",
            "Epoch [18362/20000], Training Loss: 0.0815\n",
            "Epoch [18363/20000], Training Loss: 0.0803\n",
            "Epoch [18364/20000], Training Loss: 0.0790\n",
            "Epoch [18365/20000], Training Loss: 0.0741\n",
            "Epoch [18366/20000], Training Loss: 0.0748\n",
            "Epoch [18367/20000], Training Loss: 0.0793\n",
            "Epoch [18368/20000], Training Loss: 0.0749\n",
            "Epoch [18369/20000], Training Loss: 0.0789\n",
            "Epoch [18370/20000], Training Loss: 0.0777\n",
            "Epoch [18371/20000], Training Loss: 0.0718\n",
            "Epoch [18372/20000], Training Loss: 0.0803\n",
            "Epoch [18373/20000], Training Loss: 0.0828\n",
            "Epoch [18374/20000], Training Loss: 0.0845\n",
            "Epoch [18375/20000], Training Loss: 0.0854\n",
            "Epoch [18376/20000], Training Loss: 0.0795\n",
            "Epoch [18377/20000], Training Loss: 0.0754\n",
            "Epoch [18378/20000], Training Loss: 0.0755\n",
            "Epoch [18379/20000], Training Loss: 0.0821\n",
            "Epoch [18380/20000], Training Loss: 0.0723\n",
            "Epoch [18381/20000], Training Loss: 0.0807\n",
            "Epoch [18382/20000], Training Loss: 0.0767\n",
            "Epoch [18383/20000], Training Loss: 0.0870\n",
            "Epoch [18384/20000], Training Loss: 0.0820\n",
            "Epoch [18385/20000], Training Loss: 0.0851\n",
            "Epoch [18386/20000], Training Loss: 0.0729\n",
            "Epoch [18387/20000], Training Loss: 0.0821\n",
            "Epoch [18388/20000], Training Loss: 0.0817\n",
            "Epoch [18389/20000], Training Loss: 0.0742\n",
            "Epoch [18390/20000], Training Loss: 0.0824\n",
            "Epoch [18391/20000], Training Loss: 0.0773\n",
            "Epoch [18392/20000], Training Loss: 0.0786\n",
            "Epoch [18393/20000], Training Loss: 0.0814\n",
            "Epoch [18394/20000], Training Loss: 0.0767\n",
            "Epoch [18395/20000], Training Loss: 0.0796\n",
            "Epoch [18396/20000], Training Loss: 0.0854\n",
            "Epoch [18397/20000], Training Loss: 0.0866\n",
            "Epoch [18398/20000], Training Loss: 0.0787\n",
            "Epoch [18399/20000], Training Loss: 0.0746\n",
            "Epoch [18400/20000], Training Loss: 0.0799\n",
            "Epoch [18401/20000], Training Loss: 0.0758\n",
            "Epoch [18402/20000], Training Loss: 0.0805\n",
            "Epoch [18403/20000], Training Loss: 0.0770\n",
            "Epoch [18404/20000], Training Loss: 0.0798\n",
            "Epoch [18405/20000], Training Loss: 0.0792\n",
            "Epoch [18406/20000], Training Loss: 0.0726\n",
            "Epoch [18407/20000], Training Loss: 0.0812\n",
            "Epoch [18408/20000], Training Loss: 0.0725\n",
            "Epoch [18409/20000], Training Loss: 0.0735\n",
            "Epoch [18410/20000], Training Loss: 0.0819\n",
            "Epoch [18411/20000], Training Loss: 0.0750\n",
            "Epoch [18412/20000], Training Loss: 0.0802\n",
            "Epoch [18413/20000], Training Loss: 0.0726\n",
            "Epoch [18414/20000], Training Loss: 0.0767\n",
            "Epoch [18415/20000], Training Loss: 0.0750\n",
            "Epoch [18416/20000], Training Loss: 0.0757\n",
            "Epoch [18417/20000], Training Loss: 0.0797\n",
            "Epoch [18418/20000], Training Loss: 0.0795\n",
            "Epoch [18419/20000], Training Loss: 0.0803\n",
            "Epoch [18420/20000], Training Loss: 0.0859\n",
            "Epoch [18421/20000], Training Loss: 0.0778\n",
            "Epoch [18422/20000], Training Loss: 0.0814\n",
            "Epoch [18423/20000], Training Loss: 0.0755\n",
            "Epoch [18424/20000], Training Loss: 0.0783\n",
            "Epoch [18425/20000], Training Loss: 0.0788\n",
            "Epoch [18426/20000], Training Loss: 0.0809\n",
            "Epoch [18427/20000], Training Loss: 0.0802\n",
            "Epoch [18428/20000], Training Loss: 0.0751\n",
            "Epoch [18429/20000], Training Loss: 0.0743\n",
            "Epoch [18430/20000], Training Loss: 0.0807\n",
            "Epoch [18431/20000], Training Loss: 0.0810\n",
            "Epoch [18432/20000], Training Loss: 0.0808\n",
            "Epoch [18433/20000], Training Loss: 0.0809\n",
            "Epoch [18434/20000], Training Loss: 0.0849\n",
            "Epoch [18435/20000], Training Loss: 0.0859\n",
            "Epoch [18436/20000], Training Loss: 0.0776\n",
            "Epoch [18437/20000], Training Loss: 0.0810\n",
            "Epoch [18438/20000], Training Loss: 0.0864\n",
            "Epoch [18439/20000], Training Loss: 0.0835\n",
            "Epoch [18440/20000], Training Loss: 0.0803\n",
            "Epoch [18441/20000], Training Loss: 0.0790\n",
            "Epoch [18442/20000], Training Loss: 0.0808\n",
            "Epoch [18443/20000], Training Loss: 0.0783\n",
            "Epoch [18444/20000], Training Loss: 0.0870\n",
            "Epoch [18445/20000], Training Loss: 0.0794\n",
            "Epoch [18446/20000], Training Loss: 0.0775\n",
            "Epoch [18447/20000], Training Loss: 0.0793\n",
            "Epoch [18448/20000], Training Loss: 0.0804\n",
            "Epoch [18449/20000], Training Loss: 0.0806\n",
            "Epoch [18450/20000], Training Loss: 0.0772\n",
            "Epoch [18451/20000], Training Loss: 0.0869\n",
            "Epoch [18452/20000], Training Loss: 0.0773\n",
            "Epoch [18453/20000], Training Loss: 0.0778\n",
            "Epoch [18454/20000], Training Loss: 0.0764\n",
            "Epoch [18455/20000], Training Loss: 0.0775\n",
            "Epoch [18456/20000], Training Loss: 0.0795\n",
            "Epoch [18457/20000], Training Loss: 0.0828\n",
            "Epoch [18458/20000], Training Loss: 0.0791\n",
            "Epoch [18459/20000], Training Loss: 0.0837\n",
            "Epoch [18460/20000], Training Loss: 0.0801\n",
            "Epoch [18461/20000], Training Loss: 0.0805\n",
            "Epoch [18462/20000], Training Loss: 0.0800\n",
            "Epoch [18463/20000], Training Loss: 0.0867\n",
            "Epoch [18464/20000], Training Loss: 0.0814\n",
            "Epoch [18465/20000], Training Loss: 0.0782\n",
            "Epoch [18466/20000], Training Loss: 0.0840\n",
            "Epoch [18467/20000], Training Loss: 0.0790\n",
            "Epoch [18468/20000], Training Loss: 0.0849\n",
            "Epoch [18469/20000], Training Loss: 0.0733\n",
            "Epoch [18470/20000], Training Loss: 0.0830\n",
            "Epoch [18471/20000], Training Loss: 0.0789\n",
            "Epoch [18472/20000], Training Loss: 0.0762\n",
            "Epoch [18473/20000], Training Loss: 0.0877\n",
            "Epoch [18474/20000], Training Loss: 0.0819\n",
            "Epoch [18475/20000], Training Loss: 0.0804\n",
            "Epoch [18476/20000], Training Loss: 0.0804\n",
            "Epoch [18477/20000], Training Loss: 0.0740\n",
            "Epoch [18478/20000], Training Loss: 0.0735\n",
            "Epoch [18479/20000], Training Loss: 0.0786\n",
            "Epoch [18480/20000], Training Loss: 0.0724\n",
            "Epoch [18481/20000], Training Loss: 0.0760\n",
            "Epoch [18482/20000], Training Loss: 0.0796\n",
            "Epoch [18483/20000], Training Loss: 0.0810\n",
            "Epoch [18484/20000], Training Loss: 0.0827\n",
            "Epoch [18485/20000], Training Loss: 0.0765\n",
            "Epoch [18486/20000], Training Loss: 0.0759\n",
            "Epoch [18487/20000], Training Loss: 0.0787\n",
            "Epoch [18488/20000], Training Loss: 0.0809\n",
            "Epoch [18489/20000], Training Loss: 0.0808\n",
            "Epoch [18490/20000], Training Loss: 0.0779\n",
            "Epoch [18491/20000], Training Loss: 0.0827\n",
            "Epoch [18492/20000], Training Loss: 0.0762\n",
            "Epoch [18493/20000], Training Loss: 0.0810\n",
            "Epoch [18494/20000], Training Loss: 0.0857\n",
            "Epoch [18495/20000], Training Loss: 0.0802\n",
            "Epoch [18496/20000], Training Loss: 0.0800\n",
            "Epoch [18497/20000], Training Loss: 0.0822\n",
            "Epoch [18498/20000], Training Loss: 0.0827\n",
            "Epoch [18499/20000], Training Loss: 0.0860\n",
            "Epoch [18500/20000], Training Loss: 0.0744\n",
            "Epoch [18501/20000], Training Loss: 0.0755\n",
            "Epoch [18502/20000], Training Loss: 0.0812\n",
            "Epoch [18503/20000], Training Loss: 0.0855\n",
            "Epoch [18504/20000], Training Loss: 0.0798\n",
            "Epoch [18505/20000], Training Loss: 0.0842\n",
            "Epoch [18506/20000], Training Loss: 0.0819\n",
            "Epoch [18507/20000], Training Loss: 0.0829\n",
            "Epoch [18508/20000], Training Loss: 0.0782\n",
            "Epoch [18509/20000], Training Loss: 0.0758\n",
            "Epoch [18510/20000], Training Loss: 0.0820\n",
            "Epoch [18511/20000], Training Loss: 0.0791\n",
            "Epoch [18512/20000], Training Loss: 0.0768\n",
            "Epoch [18513/20000], Training Loss: 0.0835\n",
            "Epoch [18514/20000], Training Loss: 0.0857\n",
            "Epoch [18515/20000], Training Loss: 0.0756\n",
            "Epoch [18516/20000], Training Loss: 0.0817\n",
            "Epoch [18517/20000], Training Loss: 0.0815\n",
            "Epoch [18518/20000], Training Loss: 0.0817\n",
            "Epoch [18519/20000], Training Loss: 0.0754\n",
            "Epoch [18520/20000], Training Loss: 0.0780\n",
            "Epoch [18521/20000], Training Loss: 0.0738\n",
            "Epoch [18522/20000], Training Loss: 0.0807\n",
            "Epoch [18523/20000], Training Loss: 0.0825\n",
            "Epoch [18524/20000], Training Loss: 0.0855\n",
            "Epoch [18525/20000], Training Loss: 0.0802\n",
            "Epoch [18526/20000], Training Loss: 0.0755\n",
            "Epoch [18527/20000], Training Loss: 0.0785\n",
            "Epoch [18528/20000], Training Loss: 0.0771\n",
            "Epoch [18529/20000], Training Loss: 0.0742\n",
            "Epoch [18530/20000], Training Loss: 0.0752\n",
            "Epoch [18531/20000], Training Loss: 0.0826\n",
            "Epoch [18532/20000], Training Loss: 0.0838\n",
            "Epoch [18533/20000], Training Loss: 0.0808\n",
            "Epoch [18534/20000], Training Loss: 0.0746\n",
            "Epoch [18535/20000], Training Loss: 0.0782\n",
            "Epoch [18536/20000], Training Loss: 0.0754\n",
            "Epoch [18537/20000], Training Loss: 0.0804\n",
            "Epoch [18538/20000], Training Loss: 0.0883\n",
            "Epoch [18539/20000], Training Loss: 0.0765\n",
            "Epoch [18540/20000], Training Loss: 0.0788\n",
            "Epoch [18541/20000], Training Loss: 0.0857\n",
            "Epoch [18542/20000], Training Loss: 0.0758\n",
            "Epoch [18543/20000], Training Loss: 0.0850\n",
            "Epoch [18544/20000], Training Loss: 0.0724\n",
            "Epoch [18545/20000], Training Loss: 0.0820\n",
            "Epoch [18546/20000], Training Loss: 0.0810\n",
            "Epoch [18547/20000], Training Loss: 0.0772\n",
            "Epoch [18548/20000], Training Loss: 0.0808\n",
            "Epoch [18549/20000], Training Loss: 0.0759\n",
            "Epoch [18550/20000], Training Loss: 0.0802\n",
            "Epoch [18551/20000], Training Loss: 0.0756\n",
            "Epoch [18552/20000], Training Loss: 0.0753\n",
            "Epoch [18553/20000], Training Loss: 0.0821\n",
            "Epoch [18554/20000], Training Loss: 0.0736\n",
            "Epoch [18555/20000], Training Loss: 0.0738\n",
            "Epoch [18556/20000], Training Loss: 0.0828\n",
            "Epoch [18557/20000], Training Loss: 0.0810\n",
            "Epoch [18558/20000], Training Loss: 0.0786\n",
            "Epoch [18559/20000], Training Loss: 0.0790\n",
            "Epoch [18560/20000], Training Loss: 0.0816\n",
            "Epoch [18561/20000], Training Loss: 0.0795\n",
            "Epoch [18562/20000], Training Loss: 0.0760\n",
            "Epoch [18563/20000], Training Loss: 0.0790\n",
            "Epoch [18564/20000], Training Loss: 0.0839\n",
            "Epoch [18565/20000], Training Loss: 0.0824\n",
            "Epoch [18566/20000], Training Loss: 0.0800\n",
            "Epoch [18567/20000], Training Loss: 0.0778\n",
            "Epoch [18568/20000], Training Loss: 0.0834\n",
            "Epoch [18569/20000], Training Loss: 0.0732\n",
            "Epoch [18570/20000], Training Loss: 0.0826\n",
            "Epoch [18571/20000], Training Loss: 0.0780\n",
            "Epoch [18572/20000], Training Loss: 0.0821\n",
            "Epoch [18573/20000], Training Loss: 0.0763\n",
            "Epoch [18574/20000], Training Loss: 0.0862\n",
            "Epoch [18575/20000], Training Loss: 0.0796\n",
            "Epoch [18576/20000], Training Loss: 0.0782\n",
            "Epoch [18577/20000], Training Loss: 0.0814\n",
            "Epoch [18578/20000], Training Loss: 0.0822\n",
            "Epoch [18579/20000], Training Loss: 0.0853\n",
            "Epoch [18580/20000], Training Loss: 0.0841\n",
            "Epoch [18581/20000], Training Loss: 0.0768\n",
            "Epoch [18582/20000], Training Loss: 0.0820\n",
            "Epoch [18583/20000], Training Loss: 0.0819\n",
            "Epoch [18584/20000], Training Loss: 0.0777\n",
            "Epoch [18585/20000], Training Loss: 0.0751\n",
            "Epoch [18586/20000], Training Loss: 0.0766\n",
            "Epoch [18587/20000], Training Loss: 0.0793\n",
            "Epoch [18588/20000], Training Loss: 0.0796\n",
            "Epoch [18589/20000], Training Loss: 0.0771\n",
            "Epoch [18590/20000], Training Loss: 0.0780\n",
            "Epoch [18591/20000], Training Loss: 0.0792\n",
            "Epoch [18592/20000], Training Loss: 0.0837\n",
            "Epoch [18593/20000], Training Loss: 0.0774\n",
            "Epoch [18594/20000], Training Loss: 0.0800\n",
            "Epoch [18595/20000], Training Loss: 0.0811\n",
            "Epoch [18596/20000], Training Loss: 0.0769\n",
            "Epoch [18597/20000], Training Loss: 0.0750\n",
            "Epoch [18598/20000], Training Loss: 0.0813\n",
            "Epoch [18599/20000], Training Loss: 0.0805\n",
            "Epoch [18600/20000], Training Loss: 0.0815\n",
            "Epoch [18601/20000], Training Loss: 0.0827\n",
            "Epoch [18602/20000], Training Loss: 0.0848\n",
            "Epoch [18603/20000], Training Loss: 0.0771\n",
            "Epoch [18604/20000], Training Loss: 0.0837\n",
            "Epoch [18605/20000], Training Loss: 0.0789\n",
            "Epoch [18606/20000], Training Loss: 0.0764\n",
            "Epoch [18607/20000], Training Loss: 0.0749\n",
            "Epoch [18608/20000], Training Loss: 0.0809\n",
            "Epoch [18609/20000], Training Loss: 0.0797\n",
            "Epoch [18610/20000], Training Loss: 0.0855\n",
            "Epoch [18611/20000], Training Loss: 0.0770\n",
            "Epoch [18612/20000], Training Loss: 0.0747\n",
            "Epoch [18613/20000], Training Loss: 0.0843\n",
            "Epoch [18614/20000], Training Loss: 0.0786\n",
            "Epoch [18615/20000], Training Loss: 0.0814\n",
            "Epoch [18616/20000], Training Loss: 0.0776\n",
            "Epoch [18617/20000], Training Loss: 0.0760\n",
            "Epoch [18618/20000], Training Loss: 0.0731\n",
            "Epoch [18619/20000], Training Loss: 0.0823\n",
            "Epoch [18620/20000], Training Loss: 0.0819\n",
            "Epoch [18621/20000], Training Loss: 0.0733\n",
            "Epoch [18622/20000], Training Loss: 0.0810\n",
            "Epoch [18623/20000], Training Loss: 0.0783\n",
            "Epoch [18624/20000], Training Loss: 0.0777\n",
            "Epoch [18625/20000], Training Loss: 0.0780\n",
            "Epoch [18626/20000], Training Loss: 0.0798\n",
            "Epoch [18627/20000], Training Loss: 0.0771\n",
            "Epoch [18628/20000], Training Loss: 0.0800\n",
            "Epoch [18629/20000], Training Loss: 0.0745\n",
            "Epoch [18630/20000], Training Loss: 0.0842\n",
            "Epoch [18631/20000], Training Loss: 0.0875\n",
            "Epoch [18632/20000], Training Loss: 0.0809\n",
            "Epoch [18633/20000], Training Loss: 0.0777\n",
            "Epoch [18634/20000], Training Loss: 0.0777\n",
            "Epoch [18635/20000], Training Loss: 0.0800\n",
            "Epoch [18636/20000], Training Loss: 0.0812\n",
            "Epoch [18637/20000], Training Loss: 0.0755\n",
            "Epoch [18638/20000], Training Loss: 0.0832\n",
            "Epoch [18639/20000], Training Loss: 0.0837\n",
            "Epoch [18640/20000], Training Loss: 0.0835\n",
            "Epoch [18641/20000], Training Loss: 0.0834\n",
            "Epoch [18642/20000], Training Loss: 0.0778\n",
            "Epoch [18643/20000], Training Loss: 0.0810\n",
            "Epoch [18644/20000], Training Loss: 0.0798\n",
            "Epoch [18645/20000], Training Loss: 0.0788\n",
            "Epoch [18646/20000], Training Loss: 0.0860\n",
            "Epoch [18647/20000], Training Loss: 0.0796\n",
            "Epoch [18648/20000], Training Loss: 0.0751\n",
            "Epoch [18649/20000], Training Loss: 0.0809\n",
            "Epoch [18650/20000], Training Loss: 0.0754\n",
            "Epoch [18651/20000], Training Loss: 0.0754\n",
            "Epoch [18652/20000], Training Loss: 0.0851\n",
            "Epoch [18653/20000], Training Loss: 0.0847\n",
            "Epoch [18654/20000], Training Loss: 0.0819\n",
            "Epoch [18655/20000], Training Loss: 0.0779\n",
            "Epoch [18656/20000], Training Loss: 0.0813\n",
            "Epoch [18657/20000], Training Loss: 0.0804\n",
            "Epoch [18658/20000], Training Loss: 0.0794\n",
            "Epoch [18659/20000], Training Loss: 0.0785\n",
            "Epoch [18660/20000], Training Loss: 0.0780\n",
            "Epoch [18661/20000], Training Loss: 0.0843\n",
            "Epoch [18662/20000], Training Loss: 0.0756\n",
            "Epoch [18663/20000], Training Loss: 0.0773\n",
            "Epoch [18664/20000], Training Loss: 0.0803\n",
            "Epoch [18665/20000], Training Loss: 0.0734\n",
            "Epoch [18666/20000], Training Loss: 0.0792\n",
            "Epoch [18667/20000], Training Loss: 0.0752\n",
            "Epoch [18668/20000], Training Loss: 0.0794\n",
            "Epoch [18669/20000], Training Loss: 0.0774\n",
            "Epoch [18670/20000], Training Loss: 0.0785\n",
            "Epoch [18671/20000], Training Loss: 0.0792\n",
            "Epoch [18672/20000], Training Loss: 0.0823\n",
            "Epoch [18673/20000], Training Loss: 0.0796\n",
            "Epoch [18674/20000], Training Loss: 0.0771\n",
            "Epoch [18675/20000], Training Loss: 0.0857\n",
            "Epoch [18676/20000], Training Loss: 0.0779\n",
            "Epoch [18677/20000], Training Loss: 0.0848\n",
            "Epoch [18678/20000], Training Loss: 0.0839\n",
            "Epoch [18679/20000], Training Loss: 0.0810\n",
            "Epoch [18680/20000], Training Loss: 0.0800\n",
            "Epoch [18681/20000], Training Loss: 0.0839\n",
            "Epoch [18682/20000], Training Loss: 0.0773\n",
            "Epoch [18683/20000], Training Loss: 0.0806\n",
            "Epoch [18684/20000], Training Loss: 0.0778\n",
            "Epoch [18685/20000], Training Loss: 0.0760\n",
            "Epoch [18686/20000], Training Loss: 0.0802\n",
            "Epoch [18687/20000], Training Loss: 0.0802\n",
            "Epoch [18688/20000], Training Loss: 0.0812\n",
            "Epoch [18689/20000], Training Loss: 0.0773\n",
            "Epoch [18690/20000], Training Loss: 0.0787\n",
            "Epoch [18691/20000], Training Loss: 0.0793\n",
            "Epoch [18692/20000], Training Loss: 0.0781\n",
            "Epoch [18693/20000], Training Loss: 0.0779\n",
            "Epoch [18694/20000], Training Loss: 0.0796\n",
            "Epoch [18695/20000], Training Loss: 0.0798\n",
            "Epoch [18696/20000], Training Loss: 0.0791\n",
            "Epoch [18697/20000], Training Loss: 0.0833\n",
            "Epoch [18698/20000], Training Loss: 0.0767\n",
            "Epoch [18699/20000], Training Loss: 0.0771\n",
            "Epoch [18700/20000], Training Loss: 0.0819\n",
            "Epoch [18701/20000], Training Loss: 0.0809\n",
            "Epoch [18702/20000], Training Loss: 0.0873\n",
            "Epoch [18703/20000], Training Loss: 0.0833\n",
            "Epoch [18704/20000], Training Loss: 0.0803\n",
            "Epoch [18705/20000], Training Loss: 0.0793\n",
            "Epoch [18706/20000], Training Loss: 0.0841\n",
            "Epoch [18707/20000], Training Loss: 0.0762\n",
            "Epoch [18708/20000], Training Loss: 0.0782\n",
            "Epoch [18709/20000], Training Loss: 0.0852\n",
            "Epoch [18710/20000], Training Loss: 0.0801\n",
            "Epoch [18711/20000], Training Loss: 0.0827\n",
            "Epoch [18712/20000], Training Loss: 0.0828\n",
            "Epoch [18713/20000], Training Loss: 0.0801\n",
            "Epoch [18714/20000], Training Loss: 0.0813\n",
            "Epoch [18715/20000], Training Loss: 0.0734\n",
            "Epoch [18716/20000], Training Loss: 0.0790\n",
            "Epoch [18717/20000], Training Loss: 0.0811\n",
            "Epoch [18718/20000], Training Loss: 0.0791\n",
            "Epoch [18719/20000], Training Loss: 0.0779\n",
            "Epoch [18720/20000], Training Loss: 0.0799\n",
            "Epoch [18721/20000], Training Loss: 0.0817\n",
            "Epoch [18722/20000], Training Loss: 0.0846\n",
            "Epoch [18723/20000], Training Loss: 0.0769\n",
            "Epoch [18724/20000], Training Loss: 0.0768\n",
            "Epoch [18725/20000], Training Loss: 0.0848\n",
            "Epoch [18726/20000], Training Loss: 0.0786\n",
            "Epoch [18727/20000], Training Loss: 0.0774\n",
            "Epoch [18728/20000], Training Loss: 0.0795\n",
            "Epoch [18729/20000], Training Loss: 0.0810\n",
            "Epoch [18730/20000], Training Loss: 0.0786\n",
            "Epoch [18731/20000], Training Loss: 0.0840\n",
            "Epoch [18732/20000], Training Loss: 0.0787\n",
            "Epoch [18733/20000], Training Loss: 0.0812\n",
            "Epoch [18734/20000], Training Loss: 0.0783\n",
            "Epoch [18735/20000], Training Loss: 0.0794\n",
            "Epoch [18736/20000], Training Loss: 0.0815\n",
            "Epoch [18737/20000], Training Loss: 0.0744\n",
            "Epoch [18738/20000], Training Loss: 0.0757\n",
            "Epoch [18739/20000], Training Loss: 0.0811\n",
            "Epoch [18740/20000], Training Loss: 0.0838\n",
            "Epoch [18741/20000], Training Loss: 0.0811\n",
            "Epoch [18742/20000], Training Loss: 0.0723\n",
            "Epoch [18743/20000], Training Loss: 0.0788\n",
            "Epoch [18744/20000], Training Loss: 0.0852\n",
            "Epoch [18745/20000], Training Loss: 0.0818\n",
            "Epoch [18746/20000], Training Loss: 0.0838\n",
            "Epoch [18747/20000], Training Loss: 0.0754\n",
            "Epoch [18748/20000], Training Loss: 0.0849\n",
            "Epoch [18749/20000], Training Loss: 0.0771\n",
            "Epoch [18750/20000], Training Loss: 0.0794\n",
            "Epoch [18751/20000], Training Loss: 0.0793\n",
            "Epoch [18752/20000], Training Loss: 0.0779\n",
            "Epoch [18753/20000], Training Loss: 0.0806\n",
            "Epoch [18754/20000], Training Loss: 0.0812\n",
            "Epoch [18755/20000], Training Loss: 0.0846\n",
            "Epoch [18756/20000], Training Loss: 0.0754\n",
            "Epoch [18757/20000], Training Loss: 0.0750\n",
            "Epoch [18758/20000], Training Loss: 0.0825\n",
            "Epoch [18759/20000], Training Loss: 0.0826\n",
            "Epoch [18760/20000], Training Loss: 0.0801\n",
            "Epoch [18761/20000], Training Loss: 0.0823\n",
            "Epoch [18762/20000], Training Loss: 0.0784\n",
            "Epoch [18763/20000], Training Loss: 0.0804\n",
            "Epoch [18764/20000], Training Loss: 0.0794\n",
            "Epoch [18765/20000], Training Loss: 0.0794\n",
            "Epoch [18766/20000], Training Loss: 0.0750\n",
            "Epoch [18767/20000], Training Loss: 0.0785\n",
            "Epoch [18768/20000], Training Loss: 0.0825\n",
            "Epoch [18769/20000], Training Loss: 0.0815\n",
            "Epoch [18770/20000], Training Loss: 0.0746\n",
            "Epoch [18771/20000], Training Loss: 0.0807\n",
            "Epoch [18772/20000], Training Loss: 0.0747\n",
            "Epoch [18773/20000], Training Loss: 0.0762\n",
            "Epoch [18774/20000], Training Loss: 0.0843\n",
            "Epoch [18775/20000], Training Loss: 0.0833\n",
            "Epoch [18776/20000], Training Loss: 0.0737\n",
            "Epoch [18777/20000], Training Loss: 0.0837\n",
            "Epoch [18778/20000], Training Loss: 0.0804\n",
            "Epoch [18779/20000], Training Loss: 0.0822\n",
            "Epoch [18780/20000], Training Loss: 0.0800\n",
            "Epoch [18781/20000], Training Loss: 0.0809\n",
            "Epoch [18782/20000], Training Loss: 0.0757\n",
            "Epoch [18783/20000], Training Loss: 0.0787\n",
            "Epoch [18784/20000], Training Loss: 0.0764\n",
            "Epoch [18785/20000], Training Loss: 0.0729\n",
            "Epoch [18786/20000], Training Loss: 0.0805\n",
            "Epoch [18787/20000], Training Loss: 0.0767\n",
            "Epoch [18788/20000], Training Loss: 0.0765\n",
            "Epoch [18789/20000], Training Loss: 0.0807\n",
            "Epoch [18790/20000], Training Loss: 0.0736\n",
            "Epoch [18791/20000], Training Loss: 0.0760\n",
            "Epoch [18792/20000], Training Loss: 0.0767\n",
            "Epoch [18793/20000], Training Loss: 0.0823\n",
            "Epoch [18794/20000], Training Loss: 0.0786\n",
            "Epoch [18795/20000], Training Loss: 0.0734\n",
            "Epoch [18796/20000], Training Loss: 0.0781\n",
            "Epoch [18797/20000], Training Loss: 0.0739\n",
            "Epoch [18798/20000], Training Loss: 0.0742\n",
            "Epoch [18799/20000], Training Loss: 0.0729\n",
            "Epoch [18800/20000], Training Loss: 0.0738\n",
            "Epoch [18801/20000], Training Loss: 0.0810\n",
            "Epoch [18802/20000], Training Loss: 0.0804\n",
            "Epoch [18803/20000], Training Loss: 0.0779\n",
            "Epoch [18804/20000], Training Loss: 0.0760\n",
            "Epoch [18805/20000], Training Loss: 0.0786\n",
            "Epoch [18806/20000], Training Loss: 0.0766\n",
            "Epoch [18807/20000], Training Loss: 0.0791\n",
            "Epoch [18808/20000], Training Loss: 0.0807\n",
            "Epoch [18809/20000], Training Loss: 0.0753\n",
            "Epoch [18810/20000], Training Loss: 0.0791\n",
            "Epoch [18811/20000], Training Loss: 0.0743\n",
            "Epoch [18812/20000], Training Loss: 0.0792\n",
            "Epoch [18813/20000], Training Loss: 0.0808\n",
            "Epoch [18814/20000], Training Loss: 0.0824\n",
            "Epoch [18815/20000], Training Loss: 0.0865\n",
            "Epoch [18816/20000], Training Loss: 0.0770\n",
            "Epoch [18817/20000], Training Loss: 0.0806\n",
            "Epoch [18818/20000], Training Loss: 0.0783\n",
            "Epoch [18819/20000], Training Loss: 0.0751\n",
            "Epoch [18820/20000], Training Loss: 0.0809\n",
            "Epoch [18821/20000], Training Loss: 0.0791\n",
            "Epoch [18822/20000], Training Loss: 0.0730\n",
            "Epoch [18823/20000], Training Loss: 0.0855\n",
            "Epoch [18824/20000], Training Loss: 0.0735\n",
            "Epoch [18825/20000], Training Loss: 0.0812\n",
            "Epoch [18826/20000], Training Loss: 0.0846\n",
            "Epoch [18827/20000], Training Loss: 0.0816\n",
            "Epoch [18828/20000], Training Loss: 0.0808\n",
            "Epoch [18829/20000], Training Loss: 0.0824\n",
            "Epoch [18830/20000], Training Loss: 0.0750\n",
            "Epoch [18831/20000], Training Loss: 0.0777\n",
            "Epoch [18832/20000], Training Loss: 0.0818\n",
            "Epoch [18833/20000], Training Loss: 0.0750\n",
            "Epoch [18834/20000], Training Loss: 0.0787\n",
            "Epoch [18835/20000], Training Loss: 0.0830\n",
            "Epoch [18836/20000], Training Loss: 0.0764\n",
            "Epoch [18837/20000], Training Loss: 0.0818\n",
            "Epoch [18838/20000], Training Loss: 0.0758\n",
            "Epoch [18839/20000], Training Loss: 0.0791\n",
            "Epoch [18840/20000], Training Loss: 0.0820\n",
            "Epoch [18841/20000], Training Loss: 0.0773\n",
            "Epoch [18842/20000], Training Loss: 0.0801\n",
            "Epoch [18843/20000], Training Loss: 0.0791\n",
            "Epoch [18844/20000], Training Loss: 0.0782\n",
            "Epoch [18845/20000], Training Loss: 0.0798\n",
            "Epoch [18846/20000], Training Loss: 0.0797\n",
            "Epoch [18847/20000], Training Loss: 0.0857\n",
            "Epoch [18848/20000], Training Loss: 0.0777\n",
            "Epoch [18849/20000], Training Loss: 0.0834\n",
            "Epoch [18850/20000], Training Loss: 0.0739\n",
            "Epoch [18851/20000], Training Loss: 0.0842\n",
            "Epoch [18852/20000], Training Loss: 0.0794\n",
            "Epoch [18853/20000], Training Loss: 0.0877\n",
            "Epoch [18854/20000], Training Loss: 0.0733\n",
            "Epoch [18855/20000], Training Loss: 0.0802\n",
            "Epoch [18856/20000], Training Loss: 0.0786\n",
            "Epoch [18857/20000], Training Loss: 0.0745\n",
            "Epoch [18858/20000], Training Loss: 0.0737\n",
            "Epoch [18859/20000], Training Loss: 0.0754\n",
            "Epoch [18860/20000], Training Loss: 0.0859\n",
            "Epoch [18861/20000], Training Loss: 0.0823\n",
            "Epoch [18862/20000], Training Loss: 0.0780\n",
            "Epoch [18863/20000], Training Loss: 0.0787\n",
            "Epoch [18864/20000], Training Loss: 0.0784\n",
            "Epoch [18865/20000], Training Loss: 0.0821\n",
            "Epoch [18866/20000], Training Loss: 0.0759\n",
            "Epoch [18867/20000], Training Loss: 0.0745\n",
            "Epoch [18868/20000], Training Loss: 0.0763\n",
            "Epoch [18869/20000], Training Loss: 0.0759\n",
            "Epoch [18870/20000], Training Loss: 0.0759\n",
            "Epoch [18871/20000], Training Loss: 0.0793\n",
            "Epoch [18872/20000], Training Loss: 0.0812\n",
            "Epoch [18873/20000], Training Loss: 0.0824\n",
            "Epoch [18874/20000], Training Loss: 0.0785\n",
            "Epoch [18875/20000], Training Loss: 0.0767\n",
            "Epoch [18876/20000], Training Loss: 0.0802\n",
            "Epoch [18877/20000], Training Loss: 0.0789\n",
            "Epoch [18878/20000], Training Loss: 0.0819\n",
            "Epoch [18879/20000], Training Loss: 0.0819\n",
            "Epoch [18880/20000], Training Loss: 0.0808\n",
            "Epoch [18881/20000], Training Loss: 0.0850\n",
            "Epoch [18882/20000], Training Loss: 0.0785\n",
            "Epoch [18883/20000], Training Loss: 0.0802\n",
            "Epoch [18884/20000], Training Loss: 0.0783\n",
            "Epoch [18885/20000], Training Loss: 0.0762\n",
            "Epoch [18886/20000], Training Loss: 0.0848\n",
            "Epoch [18887/20000], Training Loss: 0.0838\n",
            "Epoch [18888/20000], Training Loss: 0.0782\n",
            "Epoch [18889/20000], Training Loss: 0.0814\n",
            "Epoch [18890/20000], Training Loss: 0.0760\n",
            "Epoch [18891/20000], Training Loss: 0.0824\n",
            "Epoch [18892/20000], Training Loss: 0.0779\n",
            "Epoch [18893/20000], Training Loss: 0.0839\n",
            "Epoch [18894/20000], Training Loss: 0.0765\n",
            "Epoch [18895/20000], Training Loss: 0.0747\n",
            "Epoch [18896/20000], Training Loss: 0.0769\n",
            "Epoch [18897/20000], Training Loss: 0.0814\n",
            "Epoch [18898/20000], Training Loss: 0.0777\n",
            "Epoch [18899/20000], Training Loss: 0.0821\n",
            "Epoch [18900/20000], Training Loss: 0.0836\n",
            "Epoch [18901/20000], Training Loss: 0.0753\n",
            "Epoch [18902/20000], Training Loss: 0.0852\n",
            "Epoch [18903/20000], Training Loss: 0.0739\n",
            "Epoch [18904/20000], Training Loss: 0.0794\n",
            "Epoch [18905/20000], Training Loss: 0.0776\n",
            "Epoch [18906/20000], Training Loss: 0.0769\n",
            "Epoch [18907/20000], Training Loss: 0.0759\n",
            "Epoch [18908/20000], Training Loss: 0.0801\n",
            "Epoch [18909/20000], Training Loss: 0.0781\n",
            "Epoch [18910/20000], Training Loss: 0.0785\n",
            "Epoch [18911/20000], Training Loss: 0.0788\n",
            "Epoch [18912/20000], Training Loss: 0.0867\n",
            "Epoch [18913/20000], Training Loss: 0.0887\n",
            "Epoch [18914/20000], Training Loss: 0.0742\n",
            "Epoch [18915/20000], Training Loss: 0.0805\n",
            "Epoch [18916/20000], Training Loss: 0.0795\n",
            "Epoch [18917/20000], Training Loss: 0.0776\n",
            "Epoch [18918/20000], Training Loss: 0.0772\n",
            "Epoch [18919/20000], Training Loss: 0.0767\n",
            "Epoch [18920/20000], Training Loss: 0.0743\n",
            "Epoch [18921/20000], Training Loss: 0.0804\n",
            "Epoch [18922/20000], Training Loss: 0.0746\n",
            "Epoch [18923/20000], Training Loss: 0.0786\n",
            "Epoch [18924/20000], Training Loss: 0.0788\n",
            "Epoch [18925/20000], Training Loss: 0.0822\n",
            "Epoch [18926/20000], Training Loss: 0.0765\n",
            "Epoch [18927/20000], Training Loss: 0.0874\n",
            "Epoch [18928/20000], Training Loss: 0.0811\n",
            "Epoch [18929/20000], Training Loss: 0.0746\n",
            "Epoch [18930/20000], Training Loss: 0.0786\n",
            "Epoch [18931/20000], Training Loss: 0.0762\n",
            "Epoch [18932/20000], Training Loss: 0.0760\n",
            "Epoch [18933/20000], Training Loss: 0.0788\n",
            "Epoch [18934/20000], Training Loss: 0.0822\n",
            "Epoch [18935/20000], Training Loss: 0.0822\n",
            "Epoch [18936/20000], Training Loss: 0.0787\n",
            "Epoch [18937/20000], Training Loss: 0.0810\n",
            "Epoch [18938/20000], Training Loss: 0.0810\n",
            "Epoch [18939/20000], Training Loss: 0.0812\n",
            "Epoch [18940/20000], Training Loss: 0.0821\n",
            "Epoch [18941/20000], Training Loss: 0.0869\n",
            "Epoch [18942/20000], Training Loss: 0.0816\n",
            "Epoch [18943/20000], Training Loss: 0.0863\n",
            "Epoch [18944/20000], Training Loss: 0.0832\n",
            "Epoch [18945/20000], Training Loss: 0.0796\n",
            "Epoch [18946/20000], Training Loss: 0.0840\n",
            "Epoch [18947/20000], Training Loss: 0.0746\n",
            "Epoch [18948/20000], Training Loss: 0.0793\n",
            "Epoch [18949/20000], Training Loss: 0.0784\n",
            "Epoch [18950/20000], Training Loss: 0.0746\n",
            "Epoch [18951/20000], Training Loss: 0.0804\n",
            "Epoch [18952/20000], Training Loss: 0.0818\n",
            "Epoch [18953/20000], Training Loss: 0.0763\n",
            "Epoch [18954/20000], Training Loss: 0.0805\n",
            "Epoch [18955/20000], Training Loss: 0.0820\n",
            "Epoch [18956/20000], Training Loss: 0.0817\n",
            "Epoch [18957/20000], Training Loss: 0.0817\n",
            "Epoch [18958/20000], Training Loss: 0.0804\n",
            "Epoch [18959/20000], Training Loss: 0.0775\n",
            "Epoch [18960/20000], Training Loss: 0.0826\n",
            "Epoch [18961/20000], Training Loss: 0.0761\n",
            "Epoch [18962/20000], Training Loss: 0.0732\n",
            "Epoch [18963/20000], Training Loss: 0.0758\n",
            "Epoch [18964/20000], Training Loss: 0.0771\n",
            "Epoch [18965/20000], Training Loss: 0.0791\n",
            "Epoch [18966/20000], Training Loss: 0.0861\n",
            "Epoch [18967/20000], Training Loss: 0.0817\n",
            "Epoch [18968/20000], Training Loss: 0.0865\n",
            "Epoch [18969/20000], Training Loss: 0.0833\n",
            "Epoch [18970/20000], Training Loss: 0.0803\n",
            "Epoch [18971/20000], Training Loss: 0.0832\n",
            "Epoch [18972/20000], Training Loss: 0.0795\n",
            "Epoch [18973/20000], Training Loss: 0.0740\n",
            "Epoch [18974/20000], Training Loss: 0.0780\n",
            "Epoch [18975/20000], Training Loss: 0.0796\n",
            "Epoch [18976/20000], Training Loss: 0.0808\n",
            "Epoch [18977/20000], Training Loss: 0.0764\n",
            "Epoch [18978/20000], Training Loss: 0.0782\n",
            "Epoch [18979/20000], Training Loss: 0.0764\n",
            "Epoch [18980/20000], Training Loss: 0.0811\n",
            "Epoch [18981/20000], Training Loss: 0.0848\n",
            "Epoch [18982/20000], Training Loss: 0.0791\n",
            "Epoch [18983/20000], Training Loss: 0.0759\n",
            "Epoch [18984/20000], Training Loss: 0.0841\n",
            "Epoch [18985/20000], Training Loss: 0.0847\n",
            "Epoch [18986/20000], Training Loss: 0.0768\n",
            "Epoch [18987/20000], Training Loss: 0.0777\n",
            "Epoch [18988/20000], Training Loss: 0.0809\n",
            "Epoch [18989/20000], Training Loss: 0.0797\n",
            "Epoch [18990/20000], Training Loss: 0.0801\n",
            "Epoch [18991/20000], Training Loss: 0.0823\n",
            "Epoch [18992/20000], Training Loss: 0.0850\n",
            "Epoch [18993/20000], Training Loss: 0.0808\n",
            "Epoch [18994/20000], Training Loss: 0.0783\n",
            "Epoch [18995/20000], Training Loss: 0.0886\n",
            "Epoch [18996/20000], Training Loss: 0.0791\n",
            "Epoch [18997/20000], Training Loss: 0.0731\n",
            "Epoch [18998/20000], Training Loss: 0.0753\n",
            "Epoch [18999/20000], Training Loss: 0.0747\n",
            "Epoch [19000/20000], Training Loss: 0.0835\n",
            "Epoch [19001/20000], Training Loss: 0.0751\n",
            "Epoch [19002/20000], Training Loss: 0.0795\n",
            "Epoch [19003/20000], Training Loss: 0.0868\n",
            "Epoch [19004/20000], Training Loss: 0.0767\n",
            "Epoch [19005/20000], Training Loss: 0.0826\n",
            "Epoch [19006/20000], Training Loss: 0.0796\n",
            "Epoch [19007/20000], Training Loss: 0.0779\n",
            "Epoch [19008/20000], Training Loss: 0.0802\n",
            "Epoch [19009/20000], Training Loss: 0.0807\n",
            "Epoch [19010/20000], Training Loss: 0.0776\n",
            "Epoch [19011/20000], Training Loss: 0.0805\n",
            "Epoch [19012/20000], Training Loss: 0.0792\n",
            "Epoch [19013/20000], Training Loss: 0.0837\n",
            "Epoch [19014/20000], Training Loss: 0.0769\n",
            "Epoch [19015/20000], Training Loss: 0.0772\n",
            "Epoch [19016/20000], Training Loss: 0.0804\n",
            "Epoch [19017/20000], Training Loss: 0.0778\n",
            "Epoch [19018/20000], Training Loss: 0.0770\n",
            "Epoch [19019/20000], Training Loss: 0.0858\n",
            "Epoch [19020/20000], Training Loss: 0.0767\n",
            "Epoch [19021/20000], Training Loss: 0.0839\n",
            "Epoch [19022/20000], Training Loss: 0.0837\n",
            "Epoch [19023/20000], Training Loss: 0.0776\n",
            "Epoch [19024/20000], Training Loss: 0.0847\n",
            "Epoch [19025/20000], Training Loss: 0.0823\n",
            "Epoch [19026/20000], Training Loss: 0.0790\n",
            "Epoch [19027/20000], Training Loss: 0.0801\n",
            "Epoch [19028/20000], Training Loss: 0.0821\n",
            "Epoch [19029/20000], Training Loss: 0.0761\n",
            "Epoch [19030/20000], Training Loss: 0.0778\n",
            "Epoch [19031/20000], Training Loss: 0.0732\n",
            "Epoch [19032/20000], Training Loss: 0.0752\n",
            "Epoch [19033/20000], Training Loss: 0.0777\n",
            "Epoch [19034/20000], Training Loss: 0.0877\n",
            "Epoch [19035/20000], Training Loss: 0.0807\n",
            "Epoch [19036/20000], Training Loss: 0.0793\n",
            "Epoch [19037/20000], Training Loss: 0.0795\n",
            "Epoch [19038/20000], Training Loss: 0.0792\n",
            "Epoch [19039/20000], Training Loss: 0.0782\n",
            "Epoch [19040/20000], Training Loss: 0.0748\n",
            "Epoch [19041/20000], Training Loss: 0.0802\n",
            "Epoch [19042/20000], Training Loss: 0.0782\n",
            "Epoch [19043/20000], Training Loss: 0.0829\n",
            "Epoch [19044/20000], Training Loss: 0.0815\n",
            "Epoch [19045/20000], Training Loss: 0.0817\n",
            "Epoch [19046/20000], Training Loss: 0.0868\n",
            "Epoch [19047/20000], Training Loss: 0.0850\n",
            "Epoch [19048/20000], Training Loss: 0.0748\n",
            "Epoch [19049/20000], Training Loss: 0.0788\n",
            "Epoch [19050/20000], Training Loss: 0.0808\n",
            "Epoch [19051/20000], Training Loss: 0.0849\n",
            "Epoch [19052/20000], Training Loss: 0.0784\n",
            "Epoch [19053/20000], Training Loss: 0.0761\n",
            "Epoch [19054/20000], Training Loss: 0.0822\n",
            "Epoch [19055/20000], Training Loss: 0.0822\n",
            "Epoch [19056/20000], Training Loss: 0.0757\n",
            "Epoch [19057/20000], Training Loss: 0.0810\n",
            "Epoch [19058/20000], Training Loss: 0.0783\n",
            "Epoch [19059/20000], Training Loss: 0.0817\n",
            "Epoch [19060/20000], Training Loss: 0.0806\n",
            "Epoch [19061/20000], Training Loss: 0.0780\n",
            "Epoch [19062/20000], Training Loss: 0.0772\n",
            "Epoch [19063/20000], Training Loss: 0.0760\n",
            "Epoch [19064/20000], Training Loss: 0.0746\n",
            "Epoch [19065/20000], Training Loss: 0.0768\n",
            "Epoch [19066/20000], Training Loss: 0.0772\n",
            "Epoch [19067/20000], Training Loss: 0.0826\n",
            "Epoch [19068/20000], Training Loss: 0.0793\n",
            "Epoch [19069/20000], Training Loss: 0.0816\n",
            "Epoch [19070/20000], Training Loss: 0.0791\n",
            "Epoch [19071/20000], Training Loss: 0.0829\n",
            "Epoch [19072/20000], Training Loss: 0.0816\n",
            "Epoch [19073/20000], Training Loss: 0.0807\n",
            "Epoch [19074/20000], Training Loss: 0.0825\n",
            "Epoch [19075/20000], Training Loss: 0.0844\n",
            "Epoch [19076/20000], Training Loss: 0.0745\n",
            "Epoch [19077/20000], Training Loss: 0.0754\n",
            "Epoch [19078/20000], Training Loss: 0.0869\n",
            "Epoch [19079/20000], Training Loss: 0.0802\n",
            "Epoch [19080/20000], Training Loss: 0.0826\n",
            "Epoch [19081/20000], Training Loss: 0.0747\n",
            "Epoch [19082/20000], Training Loss: 0.0814\n",
            "Epoch [19083/20000], Training Loss: 0.0851\n",
            "Epoch [19084/20000], Training Loss: 0.0836\n",
            "Epoch [19085/20000], Training Loss: 0.0863\n",
            "Epoch [19086/20000], Training Loss: 0.0797\n",
            "Epoch [19087/20000], Training Loss: 0.0762\n",
            "Epoch [19088/20000], Training Loss: 0.0797\n",
            "Epoch [19089/20000], Training Loss: 0.0799\n",
            "Epoch [19090/20000], Training Loss: 0.0759\n",
            "Epoch [19091/20000], Training Loss: 0.0853\n",
            "Epoch [19092/20000], Training Loss: 0.0747\n",
            "Epoch [19093/20000], Training Loss: 0.0756\n",
            "Epoch [19094/20000], Training Loss: 0.0808\n",
            "Epoch [19095/20000], Training Loss: 0.0862\n",
            "Epoch [19096/20000], Training Loss: 0.0810\n",
            "Epoch [19097/20000], Training Loss: 0.0723\n",
            "Epoch [19098/20000], Training Loss: 0.0865\n",
            "Epoch [19099/20000], Training Loss: 0.0808\n",
            "Epoch [19100/20000], Training Loss: 0.0803\n",
            "Epoch [19101/20000], Training Loss: 0.0768\n",
            "Epoch [19102/20000], Training Loss: 0.0862\n",
            "Epoch [19103/20000], Training Loss: 0.0751\n",
            "Epoch [19104/20000], Training Loss: 0.0802\n",
            "Epoch [19105/20000], Training Loss: 0.0734\n",
            "Epoch [19106/20000], Training Loss: 0.0782\n",
            "Epoch [19107/20000], Training Loss: 0.0782\n",
            "Epoch [19108/20000], Training Loss: 0.0741\n",
            "Epoch [19109/20000], Training Loss: 0.0811\n",
            "Epoch [19110/20000], Training Loss: 0.0840\n",
            "Epoch [19111/20000], Training Loss: 0.0777\n",
            "Epoch [19112/20000], Training Loss: 0.0810\n",
            "Epoch [19113/20000], Training Loss: 0.0796\n",
            "Epoch [19114/20000], Training Loss: 0.0793\n",
            "Epoch [19115/20000], Training Loss: 0.0800\n",
            "Epoch [19116/20000], Training Loss: 0.0766\n",
            "Epoch [19117/20000], Training Loss: 0.0880\n",
            "Epoch [19118/20000], Training Loss: 0.0780\n",
            "Epoch [19119/20000], Training Loss: 0.0769\n",
            "Epoch [19120/20000], Training Loss: 0.0824\n",
            "Epoch [19121/20000], Training Loss: 0.0763\n",
            "Epoch [19122/20000], Training Loss: 0.0742\n",
            "Epoch [19123/20000], Training Loss: 0.0754\n",
            "Epoch [19124/20000], Training Loss: 0.0857\n",
            "Epoch [19125/20000], Training Loss: 0.0746\n",
            "Epoch [19126/20000], Training Loss: 0.0820\n",
            "Epoch [19127/20000], Training Loss: 0.0756\n",
            "Epoch [19128/20000], Training Loss: 0.0848\n",
            "Epoch [19129/20000], Training Loss: 0.0746\n",
            "Epoch [19130/20000], Training Loss: 0.0772\n",
            "Epoch [19131/20000], Training Loss: 0.0796\n",
            "Epoch [19132/20000], Training Loss: 0.0851\n",
            "Epoch [19133/20000], Training Loss: 0.0785\n",
            "Epoch [19134/20000], Training Loss: 0.0810\n",
            "Epoch [19135/20000], Training Loss: 0.0746\n",
            "Epoch [19136/20000], Training Loss: 0.0785\n",
            "Epoch [19137/20000], Training Loss: 0.0778\n",
            "Epoch [19138/20000], Training Loss: 0.0804\n",
            "Epoch [19139/20000], Training Loss: 0.0744\n",
            "Epoch [19140/20000], Training Loss: 0.0762\n",
            "Epoch [19141/20000], Training Loss: 0.0789\n",
            "Epoch [19142/20000], Training Loss: 0.0791\n",
            "Epoch [19143/20000], Training Loss: 0.0783\n",
            "Epoch [19144/20000], Training Loss: 0.0746\n",
            "Epoch [19145/20000], Training Loss: 0.0835\n",
            "Epoch [19146/20000], Training Loss: 0.0844\n",
            "Epoch [19147/20000], Training Loss: 0.0791\n",
            "Epoch [19148/20000], Training Loss: 0.0837\n",
            "Epoch [19149/20000], Training Loss: 0.0841\n",
            "Epoch [19150/20000], Training Loss: 0.0741\n",
            "Epoch [19151/20000], Training Loss: 0.0832\n",
            "Epoch [19152/20000], Training Loss: 0.0796\n",
            "Epoch [19153/20000], Training Loss: 0.0842\n",
            "Epoch [19154/20000], Training Loss: 0.0863\n",
            "Epoch [19155/20000], Training Loss: 0.0829\n",
            "Epoch [19156/20000], Training Loss: 0.0783\n",
            "Epoch [19157/20000], Training Loss: 0.0763\n",
            "Epoch [19158/20000], Training Loss: 0.0744\n",
            "Epoch [19159/20000], Training Loss: 0.0806\n",
            "Epoch [19160/20000], Training Loss: 0.0846\n",
            "Epoch [19161/20000], Training Loss: 0.0752\n",
            "Epoch [19162/20000], Training Loss: 0.0802\n",
            "Epoch [19163/20000], Training Loss: 0.0797\n",
            "Epoch [19164/20000], Training Loss: 0.0791\n",
            "Epoch [19165/20000], Training Loss: 0.0755\n",
            "Epoch [19166/20000], Training Loss: 0.0794\n",
            "Epoch [19167/20000], Training Loss: 0.0789\n",
            "Epoch [19168/20000], Training Loss: 0.0779\n",
            "Epoch [19169/20000], Training Loss: 0.0784\n",
            "Epoch [19170/20000], Training Loss: 0.0768\n",
            "Epoch [19171/20000], Training Loss: 0.0782\n",
            "Epoch [19172/20000], Training Loss: 0.0743\n",
            "Epoch [19173/20000], Training Loss: 0.0763\n",
            "Epoch [19174/20000], Training Loss: 0.0797\n",
            "Epoch [19175/20000], Training Loss: 0.0816\n",
            "Epoch [19176/20000], Training Loss: 0.0760\n",
            "Epoch [19177/20000], Training Loss: 0.0773\n",
            "Epoch [19178/20000], Training Loss: 0.0774\n",
            "Epoch [19179/20000], Training Loss: 0.0832\n",
            "Epoch [19180/20000], Training Loss: 0.0772\n",
            "Epoch [19181/20000], Training Loss: 0.0789\n",
            "Epoch [19182/20000], Training Loss: 0.0857\n",
            "Epoch [19183/20000], Training Loss: 0.0793\n",
            "Epoch [19184/20000], Training Loss: 0.0834\n",
            "Epoch [19185/20000], Training Loss: 0.0798\n",
            "Epoch [19186/20000], Training Loss: 0.0819\n",
            "Epoch [19187/20000], Training Loss: 0.0781\n",
            "Epoch [19188/20000], Training Loss: 0.0758\n",
            "Epoch [19189/20000], Training Loss: 0.0764\n",
            "Epoch [19190/20000], Training Loss: 0.0794\n",
            "Epoch [19191/20000], Training Loss: 0.0794\n",
            "Epoch [19192/20000], Training Loss: 0.0821\n",
            "Epoch [19193/20000], Training Loss: 0.0854\n",
            "Epoch [19194/20000], Training Loss: 0.0793\n",
            "Epoch [19195/20000], Training Loss: 0.0751\n",
            "Epoch [19196/20000], Training Loss: 0.0845\n",
            "Epoch [19197/20000], Training Loss: 0.0756\n",
            "Epoch [19198/20000], Training Loss: 0.0827\n",
            "Epoch [19199/20000], Training Loss: 0.0796\n",
            "Epoch [19200/20000], Training Loss: 0.0783\n",
            "Epoch [19201/20000], Training Loss: 0.0782\n",
            "Epoch [19202/20000], Training Loss: 0.0737\n",
            "Epoch [19203/20000], Training Loss: 0.0810\n",
            "Epoch [19204/20000], Training Loss: 0.0802\n",
            "Epoch [19205/20000], Training Loss: 0.0760\n",
            "Epoch [19206/20000], Training Loss: 0.0825\n",
            "Epoch [19207/20000], Training Loss: 0.0804\n",
            "Epoch [19208/20000], Training Loss: 0.0797\n",
            "Epoch [19209/20000], Training Loss: 0.0828\n",
            "Epoch [19210/20000], Training Loss: 0.0804\n",
            "Epoch [19211/20000], Training Loss: 0.0816\n",
            "Epoch [19212/20000], Training Loss: 0.0810\n",
            "Epoch [19213/20000], Training Loss: 0.0791\n",
            "Epoch [19214/20000], Training Loss: 0.0840\n",
            "Epoch [19215/20000], Training Loss: 0.0810\n",
            "Epoch [19216/20000], Training Loss: 0.0731\n",
            "Epoch [19217/20000], Training Loss: 0.0796\n",
            "Epoch [19218/20000], Training Loss: 0.0772\n",
            "Epoch [19219/20000], Training Loss: 0.0787\n",
            "Epoch [19220/20000], Training Loss: 0.0721\n",
            "Epoch [19221/20000], Training Loss: 0.0858\n",
            "Epoch [19222/20000], Training Loss: 0.0854\n",
            "Epoch [19223/20000], Training Loss: 0.0812\n",
            "Epoch [19224/20000], Training Loss: 0.0802\n",
            "Epoch [19225/20000], Training Loss: 0.0815\n",
            "Epoch [19226/20000], Training Loss: 0.0794\n",
            "Epoch [19227/20000], Training Loss: 0.0768\n",
            "Epoch [19228/20000], Training Loss: 0.0836\n",
            "Epoch [19229/20000], Training Loss: 0.0831\n",
            "Epoch [19230/20000], Training Loss: 0.0835\n",
            "Epoch [19231/20000], Training Loss: 0.0794\n",
            "Epoch [19232/20000], Training Loss: 0.0779\n",
            "Epoch [19233/20000], Training Loss: 0.0836\n",
            "Epoch [19234/20000], Training Loss: 0.0780\n",
            "Epoch [19235/20000], Training Loss: 0.0807\n",
            "Epoch [19236/20000], Training Loss: 0.0789\n",
            "Epoch [19237/20000], Training Loss: 0.0834\n",
            "Epoch [19238/20000], Training Loss: 0.0845\n",
            "Epoch [19239/20000], Training Loss: 0.0762\n",
            "Epoch [19240/20000], Training Loss: 0.0788\n",
            "Epoch [19241/20000], Training Loss: 0.0794\n",
            "Epoch [19242/20000], Training Loss: 0.0796\n",
            "Epoch [19243/20000], Training Loss: 0.0843\n",
            "Epoch [19244/20000], Training Loss: 0.0764\n",
            "Epoch [19245/20000], Training Loss: 0.0795\n",
            "Epoch [19246/20000], Training Loss: 0.0791\n",
            "Epoch [19247/20000], Training Loss: 0.0776\n",
            "Epoch [19248/20000], Training Loss: 0.0779\n",
            "Epoch [19249/20000], Training Loss: 0.0775\n",
            "Epoch [19250/20000], Training Loss: 0.0808\n",
            "Epoch [19251/20000], Training Loss: 0.0784\n",
            "Epoch [19252/20000], Training Loss: 0.0744\n",
            "Epoch [19253/20000], Training Loss: 0.0760\n",
            "Epoch [19254/20000], Training Loss: 0.0805\n",
            "Epoch [19255/20000], Training Loss: 0.0756\n",
            "Epoch [19256/20000], Training Loss: 0.0741\n",
            "Epoch [19257/20000], Training Loss: 0.0828\n",
            "Epoch [19258/20000], Training Loss: 0.0782\n",
            "Epoch [19259/20000], Training Loss: 0.0764\n",
            "Epoch [19260/20000], Training Loss: 0.0820\n",
            "Epoch [19261/20000], Training Loss: 0.0757\n",
            "Epoch [19262/20000], Training Loss: 0.0765\n",
            "Epoch [19263/20000], Training Loss: 0.0865\n",
            "Epoch [19264/20000], Training Loss: 0.0802\n",
            "Epoch [19265/20000], Training Loss: 0.0799\n",
            "Epoch [19266/20000], Training Loss: 0.0725\n",
            "Epoch [19267/20000], Training Loss: 0.0829\n",
            "Epoch [19268/20000], Training Loss: 0.0819\n",
            "Epoch [19269/20000], Training Loss: 0.0836\n",
            "Epoch [19270/20000], Training Loss: 0.0816\n",
            "Epoch [19271/20000], Training Loss: 0.0743\n",
            "Epoch [19272/20000], Training Loss: 0.0802\n",
            "Epoch [19273/20000], Training Loss: 0.0874\n",
            "Epoch [19274/20000], Training Loss: 0.0814\n",
            "Epoch [19275/20000], Training Loss: 0.0837\n",
            "Epoch [19276/20000], Training Loss: 0.0815\n",
            "Epoch [19277/20000], Training Loss: 0.0764\n",
            "Epoch [19278/20000], Training Loss: 0.0814\n",
            "Epoch [19279/20000], Training Loss: 0.0734\n",
            "Epoch [19280/20000], Training Loss: 0.0752\n",
            "Epoch [19281/20000], Training Loss: 0.0805\n",
            "Epoch [19282/20000], Training Loss: 0.0802\n",
            "Epoch [19283/20000], Training Loss: 0.0811\n",
            "Epoch [19284/20000], Training Loss: 0.0747\n",
            "Epoch [19285/20000], Training Loss: 0.0834\n",
            "Epoch [19286/20000], Training Loss: 0.0746\n",
            "Epoch [19287/20000], Training Loss: 0.0817\n",
            "Epoch [19288/20000], Training Loss: 0.0799\n",
            "Epoch [19289/20000], Training Loss: 0.0822\n",
            "Epoch [19290/20000], Training Loss: 0.0788\n",
            "Epoch [19291/20000], Training Loss: 0.0838\n",
            "Epoch [19292/20000], Training Loss: 0.0769\n",
            "Epoch [19293/20000], Training Loss: 0.0767\n",
            "Epoch [19294/20000], Training Loss: 0.0784\n",
            "Epoch [19295/20000], Training Loss: 0.0785\n",
            "Epoch [19296/20000], Training Loss: 0.0785\n",
            "Epoch [19297/20000], Training Loss: 0.0738\n",
            "Epoch [19298/20000], Training Loss: 0.0874\n",
            "Epoch [19299/20000], Training Loss: 0.0766\n",
            "Epoch [19300/20000], Training Loss: 0.0720\n",
            "Epoch [19301/20000], Training Loss: 0.0794\n",
            "Epoch [19302/20000], Training Loss: 0.0773\n",
            "Epoch [19303/20000], Training Loss: 0.0831\n",
            "Epoch [19304/20000], Training Loss: 0.0823\n",
            "Epoch [19305/20000], Training Loss: 0.0786\n",
            "Epoch [19306/20000], Training Loss: 0.0839\n",
            "Epoch [19307/20000], Training Loss: 0.0823\n",
            "Epoch [19308/20000], Training Loss: 0.0773\n",
            "Epoch [19309/20000], Training Loss: 0.0857\n",
            "Epoch [19310/20000], Training Loss: 0.0761\n",
            "Epoch [19311/20000], Training Loss: 0.0835\n",
            "Epoch [19312/20000], Training Loss: 0.0827\n",
            "Epoch [19313/20000], Training Loss: 0.0762\n",
            "Epoch [19314/20000], Training Loss: 0.0753\n",
            "Epoch [19315/20000], Training Loss: 0.0795\n",
            "Epoch [19316/20000], Training Loss: 0.0842\n",
            "Epoch [19317/20000], Training Loss: 0.0799\n",
            "Epoch [19318/20000], Training Loss: 0.0808\n",
            "Epoch [19319/20000], Training Loss: 0.0754\n",
            "Epoch [19320/20000], Training Loss: 0.0759\n",
            "Epoch [19321/20000], Training Loss: 0.0807\n",
            "Epoch [19322/20000], Training Loss: 0.0830\n",
            "Epoch [19323/20000], Training Loss: 0.0722\n",
            "Epoch [19324/20000], Training Loss: 0.0822\n",
            "Epoch [19325/20000], Training Loss: 0.0806\n",
            "Epoch [19326/20000], Training Loss: 0.0781\n",
            "Epoch [19327/20000], Training Loss: 0.0795\n",
            "Epoch [19328/20000], Training Loss: 0.0793\n",
            "Epoch [19329/20000], Training Loss: 0.0870\n",
            "Epoch [19330/20000], Training Loss: 0.0801\n",
            "Epoch [19331/20000], Training Loss: 0.0794\n",
            "Epoch [19332/20000], Training Loss: 0.0781\n",
            "Epoch [19333/20000], Training Loss: 0.0815\n",
            "Epoch [19334/20000], Training Loss: 0.0801\n",
            "Epoch [19335/20000], Training Loss: 0.0762\n",
            "Epoch [19336/20000], Training Loss: 0.0733\n",
            "Epoch [19337/20000], Training Loss: 0.0755\n",
            "Epoch [19338/20000], Training Loss: 0.0865\n",
            "Epoch [19339/20000], Training Loss: 0.0807\n",
            "Epoch [19340/20000], Training Loss: 0.0744\n",
            "Epoch [19341/20000], Training Loss: 0.0820\n",
            "Epoch [19342/20000], Training Loss: 0.0805\n",
            "Epoch [19343/20000], Training Loss: 0.0852\n",
            "Epoch [19344/20000], Training Loss: 0.0767\n",
            "Epoch [19345/20000], Training Loss: 0.0788\n",
            "Epoch [19346/20000], Training Loss: 0.0831\n",
            "Epoch [19347/20000], Training Loss: 0.0748\n",
            "Epoch [19348/20000], Training Loss: 0.0840\n",
            "Epoch [19349/20000], Training Loss: 0.0799\n",
            "Epoch [19350/20000], Training Loss: 0.0804\n",
            "Epoch [19351/20000], Training Loss: 0.0864\n",
            "Epoch [19352/20000], Training Loss: 0.0871\n",
            "Epoch [19353/20000], Training Loss: 0.0800\n",
            "Epoch [19354/20000], Training Loss: 0.0830\n",
            "Epoch [19355/20000], Training Loss: 0.0822\n",
            "Epoch [19356/20000], Training Loss: 0.0779\n",
            "Epoch [19357/20000], Training Loss: 0.0806\n",
            "Epoch [19358/20000], Training Loss: 0.0798\n",
            "Epoch [19359/20000], Training Loss: 0.0808\n",
            "Epoch [19360/20000], Training Loss: 0.0758\n",
            "Epoch [19361/20000], Training Loss: 0.0813\n",
            "Epoch [19362/20000], Training Loss: 0.0752\n",
            "Epoch [19363/20000], Training Loss: 0.0808\n",
            "Epoch [19364/20000], Training Loss: 0.0789\n",
            "Epoch [19365/20000], Training Loss: 0.0813\n",
            "Epoch [19366/20000], Training Loss: 0.0795\n",
            "Epoch [19367/20000], Training Loss: 0.0749\n",
            "Epoch [19368/20000], Training Loss: 0.0781\n",
            "Epoch [19369/20000], Training Loss: 0.0804\n",
            "Epoch [19370/20000], Training Loss: 0.0784\n",
            "Epoch [19371/20000], Training Loss: 0.0782\n",
            "Epoch [19372/20000], Training Loss: 0.0797\n",
            "Epoch [19373/20000], Training Loss: 0.0800\n",
            "Epoch [19374/20000], Training Loss: 0.0771\n",
            "Epoch [19375/20000], Training Loss: 0.0733\n",
            "Epoch [19376/20000], Training Loss: 0.0807\n",
            "Epoch [19377/20000], Training Loss: 0.0813\n",
            "Epoch [19378/20000], Training Loss: 0.0744\n",
            "Epoch [19379/20000], Training Loss: 0.0842\n",
            "Epoch [19380/20000], Training Loss: 0.0817\n",
            "Epoch [19381/20000], Training Loss: 0.0794\n",
            "Epoch [19382/20000], Training Loss: 0.0780\n",
            "Epoch [19383/20000], Training Loss: 0.0822\n",
            "Epoch [19384/20000], Training Loss: 0.0747\n",
            "Epoch [19385/20000], Training Loss: 0.0879\n",
            "Epoch [19386/20000], Training Loss: 0.0810\n",
            "Epoch [19387/20000], Training Loss: 0.0749\n",
            "Epoch [19388/20000], Training Loss: 0.0786\n",
            "Epoch [19389/20000], Training Loss: 0.0750\n",
            "Epoch [19390/20000], Training Loss: 0.0864\n",
            "Epoch [19391/20000], Training Loss: 0.0781\n",
            "Epoch [19392/20000], Training Loss: 0.0806\n",
            "Epoch [19393/20000], Training Loss: 0.0792\n",
            "Epoch [19394/20000], Training Loss: 0.0792\n",
            "Epoch [19395/20000], Training Loss: 0.0734\n",
            "Epoch [19396/20000], Training Loss: 0.0814\n",
            "Epoch [19397/20000], Training Loss: 0.0752\n",
            "Epoch [19398/20000], Training Loss: 0.0841\n",
            "Epoch [19399/20000], Training Loss: 0.0757\n",
            "Epoch [19400/20000], Training Loss: 0.0799\n",
            "Epoch [19401/20000], Training Loss: 0.0854\n",
            "Epoch [19402/20000], Training Loss: 0.0807\n",
            "Epoch [19403/20000], Training Loss: 0.0788\n",
            "Epoch [19404/20000], Training Loss: 0.0832\n",
            "Epoch [19405/20000], Training Loss: 0.0838\n",
            "Epoch [19406/20000], Training Loss: 0.0787\n",
            "Epoch [19407/20000], Training Loss: 0.0776\n",
            "Epoch [19408/20000], Training Loss: 0.0856\n",
            "Epoch [19409/20000], Training Loss: 0.0813\n",
            "Epoch [19410/20000], Training Loss: 0.0844\n",
            "Epoch [19411/20000], Training Loss: 0.0867\n",
            "Epoch [19412/20000], Training Loss: 0.0786\n",
            "Epoch [19413/20000], Training Loss: 0.0801\n",
            "Epoch [19414/20000], Training Loss: 0.0767\n",
            "Epoch [19415/20000], Training Loss: 0.0849\n",
            "Epoch [19416/20000], Training Loss: 0.0784\n",
            "Epoch [19417/20000], Training Loss: 0.0748\n",
            "Epoch [19418/20000], Training Loss: 0.0815\n",
            "Epoch [19419/20000], Training Loss: 0.0823\n",
            "Epoch [19420/20000], Training Loss: 0.0793\n",
            "Epoch [19421/20000], Training Loss: 0.0824\n",
            "Epoch [19422/20000], Training Loss: 0.0854\n",
            "Epoch [19423/20000], Training Loss: 0.0800\n",
            "Epoch [19424/20000], Training Loss: 0.0843\n",
            "Epoch [19425/20000], Training Loss: 0.0734\n",
            "Epoch [19426/20000], Training Loss: 0.0809\n",
            "Epoch [19427/20000], Training Loss: 0.0787\n",
            "Epoch [19428/20000], Training Loss: 0.0783\n",
            "Epoch [19429/20000], Training Loss: 0.0783\n",
            "Epoch [19430/20000], Training Loss: 0.0744\n",
            "Epoch [19431/20000], Training Loss: 0.0771\n",
            "Epoch [19432/20000], Training Loss: 0.0747\n",
            "Epoch [19433/20000], Training Loss: 0.0786\n",
            "Epoch [19434/20000], Training Loss: 0.0823\n",
            "Epoch [19435/20000], Training Loss: 0.0773\n",
            "Epoch [19436/20000], Training Loss: 0.0782\n",
            "Epoch [19437/20000], Training Loss: 0.0890\n",
            "Epoch [19438/20000], Training Loss: 0.0836\n",
            "Epoch [19439/20000], Training Loss: 0.0780\n",
            "Epoch [19440/20000], Training Loss: 0.0810\n",
            "Epoch [19441/20000], Training Loss: 0.0772\n",
            "Epoch [19442/20000], Training Loss: 0.0736\n",
            "Epoch [19443/20000], Training Loss: 0.0749\n",
            "Epoch [19444/20000], Training Loss: 0.0830\n",
            "Epoch [19445/20000], Training Loss: 0.0851\n",
            "Epoch [19446/20000], Training Loss: 0.0801\n",
            "Epoch [19447/20000], Training Loss: 0.0857\n",
            "Epoch [19448/20000], Training Loss: 0.0858\n",
            "Epoch [19449/20000], Training Loss: 0.0786\n",
            "Epoch [19450/20000], Training Loss: 0.0819\n",
            "Epoch [19451/20000], Training Loss: 0.0787\n",
            "Epoch [19452/20000], Training Loss: 0.0841\n",
            "Epoch [19453/20000], Training Loss: 0.0821\n",
            "Epoch [19454/20000], Training Loss: 0.0788\n",
            "Epoch [19455/20000], Training Loss: 0.0818\n",
            "Epoch [19456/20000], Training Loss: 0.0767\n",
            "Epoch [19457/20000], Training Loss: 0.0809\n",
            "Epoch [19458/20000], Training Loss: 0.0747\n",
            "Epoch [19459/20000], Training Loss: 0.0868\n",
            "Epoch [19460/20000], Training Loss: 0.0796\n",
            "Epoch [19461/20000], Training Loss: 0.0794\n",
            "Epoch [19462/20000], Training Loss: 0.0776\n",
            "Epoch [19463/20000], Training Loss: 0.0765\n",
            "Epoch [19464/20000], Training Loss: 0.0754\n",
            "Epoch [19465/20000], Training Loss: 0.0811\n",
            "Epoch [19466/20000], Training Loss: 0.0805\n",
            "Epoch [19467/20000], Training Loss: 0.0757\n",
            "Epoch [19468/20000], Training Loss: 0.0758\n",
            "Epoch [19469/20000], Training Loss: 0.0739\n",
            "Epoch [19470/20000], Training Loss: 0.0773\n",
            "Epoch [19471/20000], Training Loss: 0.0800\n",
            "Epoch [19472/20000], Training Loss: 0.0836\n",
            "Epoch [19473/20000], Training Loss: 0.0764\n",
            "Epoch [19474/20000], Training Loss: 0.0818\n",
            "Epoch [19475/20000], Training Loss: 0.0798\n",
            "Epoch [19476/20000], Training Loss: 0.0795\n",
            "Epoch [19477/20000], Training Loss: 0.0799\n",
            "Epoch [19478/20000], Training Loss: 0.0852\n",
            "Epoch [19479/20000], Training Loss: 0.0802\n",
            "Epoch [19480/20000], Training Loss: 0.0790\n",
            "Epoch [19481/20000], Training Loss: 0.0789\n",
            "Epoch [19482/20000], Training Loss: 0.0809\n",
            "Epoch [19483/20000], Training Loss: 0.0789\n",
            "Epoch [19484/20000], Training Loss: 0.0748\n",
            "Epoch [19485/20000], Training Loss: 0.0859\n",
            "Epoch [19486/20000], Training Loss: 0.0828\n",
            "Epoch [19487/20000], Training Loss: 0.0796\n",
            "Epoch [19488/20000], Training Loss: 0.0804\n",
            "Epoch [19489/20000], Training Loss: 0.0813\n",
            "Epoch [19490/20000], Training Loss: 0.0808\n",
            "Epoch [19491/20000], Training Loss: 0.0846\n",
            "Epoch [19492/20000], Training Loss: 0.0854\n",
            "Epoch [19493/20000], Training Loss: 0.0786\n",
            "Epoch [19494/20000], Training Loss: 0.0797\n",
            "Epoch [19495/20000], Training Loss: 0.0792\n",
            "Epoch [19496/20000], Training Loss: 0.0841\n",
            "Epoch [19497/20000], Training Loss: 0.0864\n",
            "Epoch [19498/20000], Training Loss: 0.0793\n",
            "Epoch [19499/20000], Training Loss: 0.0794\n",
            "Epoch [19500/20000], Training Loss: 0.0804\n",
            "Epoch [19501/20000], Training Loss: 0.0808\n",
            "Epoch [19502/20000], Training Loss: 0.0764\n",
            "Epoch [19503/20000], Training Loss: 0.0852\n",
            "Epoch [19504/20000], Training Loss: 0.0804\n",
            "Epoch [19505/20000], Training Loss: 0.0785\n",
            "Epoch [19506/20000], Training Loss: 0.0775\n",
            "Epoch [19507/20000], Training Loss: 0.0740\n",
            "Epoch [19508/20000], Training Loss: 0.0850\n",
            "Epoch [19509/20000], Training Loss: 0.0834\n",
            "Epoch [19510/20000], Training Loss: 0.0796\n",
            "Epoch [19511/20000], Training Loss: 0.0852\n",
            "Epoch [19512/20000], Training Loss: 0.0774\n",
            "Epoch [19513/20000], Training Loss: 0.0846\n",
            "Epoch [19514/20000], Training Loss: 0.0821\n",
            "Epoch [19515/20000], Training Loss: 0.0802\n",
            "Epoch [19516/20000], Training Loss: 0.0764\n",
            "Epoch [19517/20000], Training Loss: 0.0781\n",
            "Epoch [19518/20000], Training Loss: 0.0789\n",
            "Epoch [19519/20000], Training Loss: 0.0742\n",
            "Epoch [19520/20000], Training Loss: 0.0791\n",
            "Epoch [19521/20000], Training Loss: 0.0819\n",
            "Epoch [19522/20000], Training Loss: 0.0728\n",
            "Epoch [19523/20000], Training Loss: 0.0777\n",
            "Epoch [19524/20000], Training Loss: 0.0744\n",
            "Epoch [19525/20000], Training Loss: 0.0795\n",
            "Epoch [19526/20000], Training Loss: 0.0731\n",
            "Epoch [19527/20000], Training Loss: 0.0791\n",
            "Epoch [19528/20000], Training Loss: 0.0746\n",
            "Epoch [19529/20000], Training Loss: 0.0795\n",
            "Epoch [19530/20000], Training Loss: 0.0766\n",
            "Epoch [19531/20000], Training Loss: 0.0800\n",
            "Epoch [19532/20000], Training Loss: 0.0776\n",
            "Epoch [19533/20000], Training Loss: 0.0786\n",
            "Epoch [19534/20000], Training Loss: 0.0820\n",
            "Epoch [19535/20000], Training Loss: 0.0805\n",
            "Epoch [19536/20000], Training Loss: 0.0833\n",
            "Epoch [19537/20000], Training Loss: 0.0735\n",
            "Epoch [19538/20000], Training Loss: 0.0763\n",
            "Epoch [19539/20000], Training Loss: 0.0799\n",
            "Epoch [19540/20000], Training Loss: 0.0756\n",
            "Epoch [19541/20000], Training Loss: 0.0812\n",
            "Epoch [19542/20000], Training Loss: 0.0779\n",
            "Epoch [19543/20000], Training Loss: 0.0753\n",
            "Epoch [19544/20000], Training Loss: 0.0760\n",
            "Epoch [19545/20000], Training Loss: 0.0740\n",
            "Epoch [19546/20000], Training Loss: 0.0793\n",
            "Epoch [19547/20000], Training Loss: 0.0782\n",
            "Epoch [19548/20000], Training Loss: 0.0841\n",
            "Epoch [19549/20000], Training Loss: 0.0776\n",
            "Epoch [19550/20000], Training Loss: 0.0813\n",
            "Epoch [19551/20000], Training Loss: 0.0818\n",
            "Epoch [19552/20000], Training Loss: 0.0866\n",
            "Epoch [19553/20000], Training Loss: 0.0777\n",
            "Epoch [19554/20000], Training Loss: 0.0776\n",
            "Epoch [19555/20000], Training Loss: 0.0774\n",
            "Epoch [19556/20000], Training Loss: 0.0781\n",
            "Epoch [19557/20000], Training Loss: 0.0788\n",
            "Epoch [19558/20000], Training Loss: 0.0752\n",
            "Epoch [19559/20000], Training Loss: 0.0778\n",
            "Epoch [19560/20000], Training Loss: 0.0823\n",
            "Epoch [19561/20000], Training Loss: 0.0818\n",
            "Epoch [19562/20000], Training Loss: 0.0799\n",
            "Epoch [19563/20000], Training Loss: 0.0775\n",
            "Epoch [19564/20000], Training Loss: 0.0774\n",
            "Epoch [19565/20000], Training Loss: 0.0819\n",
            "Epoch [19566/20000], Training Loss: 0.0801\n",
            "Epoch [19567/20000], Training Loss: 0.0780\n",
            "Epoch [19568/20000], Training Loss: 0.0785\n",
            "Epoch [19569/20000], Training Loss: 0.0796\n",
            "Epoch [19570/20000], Training Loss: 0.0851\n",
            "Epoch [19571/20000], Training Loss: 0.0779\n",
            "Epoch [19572/20000], Training Loss: 0.0742\n",
            "Epoch [19573/20000], Training Loss: 0.0747\n",
            "Epoch [19574/20000], Training Loss: 0.0796\n",
            "Epoch [19575/20000], Training Loss: 0.0818\n",
            "Epoch [19576/20000], Training Loss: 0.0827\n",
            "Epoch [19577/20000], Training Loss: 0.0776\n",
            "Epoch [19578/20000], Training Loss: 0.0782\n",
            "Epoch [19579/20000], Training Loss: 0.0816\n",
            "Epoch [19580/20000], Training Loss: 0.0743\n",
            "Epoch [19581/20000], Training Loss: 0.0806\n",
            "Epoch [19582/20000], Training Loss: 0.0843\n",
            "Epoch [19583/20000], Training Loss: 0.0743\n",
            "Epoch [19584/20000], Training Loss: 0.0782\n",
            "Epoch [19585/20000], Training Loss: 0.0817\n",
            "Epoch [19586/20000], Training Loss: 0.0733\n",
            "Epoch [19587/20000], Training Loss: 0.0833\n",
            "Epoch [19588/20000], Training Loss: 0.0778\n",
            "Epoch [19589/20000], Training Loss: 0.0787\n",
            "Epoch [19590/20000], Training Loss: 0.0759\n",
            "Epoch [19591/20000], Training Loss: 0.0790\n",
            "Epoch [19592/20000], Training Loss: 0.0729\n",
            "Epoch [19593/20000], Training Loss: 0.0803\n",
            "Epoch [19594/20000], Training Loss: 0.0777\n",
            "Epoch [19595/20000], Training Loss: 0.0807\n",
            "Epoch [19596/20000], Training Loss: 0.0798\n",
            "Epoch [19597/20000], Training Loss: 0.0742\n",
            "Epoch [19598/20000], Training Loss: 0.0827\n",
            "Epoch [19599/20000], Training Loss: 0.0758\n",
            "Epoch [19600/20000], Training Loss: 0.0794\n",
            "Epoch [19601/20000], Training Loss: 0.0801\n",
            "Epoch [19602/20000], Training Loss: 0.0809\n",
            "Epoch [19603/20000], Training Loss: 0.0813\n",
            "Epoch [19604/20000], Training Loss: 0.0818\n",
            "Epoch [19605/20000], Training Loss: 0.0779\n",
            "Epoch [19606/20000], Training Loss: 0.0824\n",
            "Epoch [19607/20000], Training Loss: 0.0795\n",
            "Epoch [19608/20000], Training Loss: 0.0803\n",
            "Epoch [19609/20000], Training Loss: 0.0778\n",
            "Epoch [19610/20000], Training Loss: 0.0822\n",
            "Epoch [19611/20000], Training Loss: 0.0809\n",
            "Epoch [19612/20000], Training Loss: 0.0795\n",
            "Epoch [19613/20000], Training Loss: 0.0804\n",
            "Epoch [19614/20000], Training Loss: 0.0776\n",
            "Epoch [19615/20000], Training Loss: 0.0790\n",
            "Epoch [19616/20000], Training Loss: 0.0778\n",
            "Epoch [19617/20000], Training Loss: 0.0795\n",
            "Epoch [19618/20000], Training Loss: 0.0807\n",
            "Epoch [19619/20000], Training Loss: 0.0850\n",
            "Epoch [19620/20000], Training Loss: 0.0820\n",
            "Epoch [19621/20000], Training Loss: 0.0815\n",
            "Epoch [19622/20000], Training Loss: 0.0761\n",
            "Epoch [19623/20000], Training Loss: 0.0831\n",
            "Epoch [19624/20000], Training Loss: 0.0768\n",
            "Epoch [19625/20000], Training Loss: 0.0883\n",
            "Epoch [19626/20000], Training Loss: 0.0779\n",
            "Epoch [19627/20000], Training Loss: 0.0795\n",
            "Epoch [19628/20000], Training Loss: 0.0827\n",
            "Epoch [19629/20000], Training Loss: 0.0826\n",
            "Epoch [19630/20000], Training Loss: 0.0814\n",
            "Epoch [19631/20000], Training Loss: 0.0849\n",
            "Epoch [19632/20000], Training Loss: 0.0859\n",
            "Epoch [19633/20000], Training Loss: 0.0862\n",
            "Epoch [19634/20000], Training Loss: 0.0817\n",
            "Epoch [19635/20000], Training Loss: 0.0774\n",
            "Epoch [19636/20000], Training Loss: 0.0785\n",
            "Epoch [19637/20000], Training Loss: 0.0833\n",
            "Epoch [19638/20000], Training Loss: 0.0838\n",
            "Epoch [19639/20000], Training Loss: 0.0754\n",
            "Epoch [19640/20000], Training Loss: 0.0866\n",
            "Epoch [19641/20000], Training Loss: 0.0815\n",
            "Epoch [19642/20000], Training Loss: 0.0795\n",
            "Epoch [19643/20000], Training Loss: 0.0792\n",
            "Epoch [19644/20000], Training Loss: 0.0733\n",
            "Epoch [19645/20000], Training Loss: 0.0819\n",
            "Epoch [19646/20000], Training Loss: 0.0740\n",
            "Epoch [19647/20000], Training Loss: 0.0757\n",
            "Epoch [19648/20000], Training Loss: 0.0828\n",
            "Epoch [19649/20000], Training Loss: 0.0805\n",
            "Epoch [19650/20000], Training Loss: 0.0816\n",
            "Epoch [19651/20000], Training Loss: 0.0749\n",
            "Epoch [19652/20000], Training Loss: 0.0828\n",
            "Epoch [19653/20000], Training Loss: 0.0797\n",
            "Epoch [19654/20000], Training Loss: 0.0801\n",
            "Epoch [19655/20000], Training Loss: 0.0758\n",
            "Epoch [19656/20000], Training Loss: 0.0837\n",
            "Epoch [19657/20000], Training Loss: 0.0825\n",
            "Epoch [19658/20000], Training Loss: 0.0846\n",
            "Epoch [19659/20000], Training Loss: 0.0848\n",
            "Epoch [19660/20000], Training Loss: 0.0759\n",
            "Epoch [19661/20000], Training Loss: 0.0801\n",
            "Epoch [19662/20000], Training Loss: 0.0789\n",
            "Epoch [19663/20000], Training Loss: 0.0804\n",
            "Epoch [19664/20000], Training Loss: 0.0753\n",
            "Epoch [19665/20000], Training Loss: 0.0793\n",
            "Epoch [19666/20000], Training Loss: 0.0738\n",
            "Epoch [19667/20000], Training Loss: 0.0824\n",
            "Epoch [19668/20000], Training Loss: 0.0759\n",
            "Epoch [19669/20000], Training Loss: 0.0779\n",
            "Epoch [19670/20000], Training Loss: 0.0730\n",
            "Epoch [19671/20000], Training Loss: 0.0812\n",
            "Epoch [19672/20000], Training Loss: 0.0853\n",
            "Epoch [19673/20000], Training Loss: 0.0813\n",
            "Epoch [19674/20000], Training Loss: 0.0720\n",
            "Epoch [19675/20000], Training Loss: 0.0852\n",
            "Epoch [19676/20000], Training Loss: 0.0828\n",
            "Epoch [19677/20000], Training Loss: 0.0766\n",
            "Epoch [19678/20000], Training Loss: 0.0800\n",
            "Epoch [19679/20000], Training Loss: 0.0778\n",
            "Epoch [19680/20000], Training Loss: 0.0780\n",
            "Epoch [19681/20000], Training Loss: 0.0792\n",
            "Epoch [19682/20000], Training Loss: 0.0757\n",
            "Epoch [19683/20000], Training Loss: 0.0750\n",
            "Epoch [19684/20000], Training Loss: 0.0850\n",
            "Epoch [19685/20000], Training Loss: 0.0758\n",
            "Epoch [19686/20000], Training Loss: 0.0805\n",
            "Epoch [19687/20000], Training Loss: 0.0753\n",
            "Epoch [19688/20000], Training Loss: 0.0854\n",
            "Epoch [19689/20000], Training Loss: 0.0784\n",
            "Epoch [19690/20000], Training Loss: 0.0748\n",
            "Epoch [19691/20000], Training Loss: 0.0823\n",
            "Epoch [19692/20000], Training Loss: 0.0848\n",
            "Epoch [19693/20000], Training Loss: 0.0800\n",
            "Epoch [19694/20000], Training Loss: 0.0794\n",
            "Epoch [19695/20000], Training Loss: 0.0762\n",
            "Epoch [19696/20000], Training Loss: 0.0739\n",
            "Epoch [19697/20000], Training Loss: 0.0752\n",
            "Epoch [19698/20000], Training Loss: 0.0849\n",
            "Epoch [19699/20000], Training Loss: 0.0789\n",
            "Epoch [19700/20000], Training Loss: 0.0814\n",
            "Epoch [19701/20000], Training Loss: 0.0878\n",
            "Epoch [19702/20000], Training Loss: 0.0818\n",
            "Epoch [19703/20000], Training Loss: 0.0828\n",
            "Epoch [19704/20000], Training Loss: 0.0779\n",
            "Epoch [19705/20000], Training Loss: 0.0735\n",
            "Epoch [19706/20000], Training Loss: 0.0828\n",
            "Epoch [19707/20000], Training Loss: 0.0824\n",
            "Epoch [19708/20000], Training Loss: 0.0794\n",
            "Epoch [19709/20000], Training Loss: 0.0783\n",
            "Epoch [19710/20000], Training Loss: 0.0852\n",
            "Epoch [19711/20000], Training Loss: 0.0802\n",
            "Epoch [19712/20000], Training Loss: 0.0771\n",
            "Epoch [19713/20000], Training Loss: 0.0775\n",
            "Epoch [19714/20000], Training Loss: 0.0747\n",
            "Epoch [19715/20000], Training Loss: 0.0808\n",
            "Epoch [19716/20000], Training Loss: 0.0813\n",
            "Epoch [19717/20000], Training Loss: 0.0857\n",
            "Epoch [19718/20000], Training Loss: 0.0798\n",
            "Epoch [19719/20000], Training Loss: 0.0775\n",
            "Epoch [19720/20000], Training Loss: 0.0764\n",
            "Epoch [19721/20000], Training Loss: 0.0796\n",
            "Epoch [19722/20000], Training Loss: 0.0806\n",
            "Epoch [19723/20000], Training Loss: 0.0812\n",
            "Epoch [19724/20000], Training Loss: 0.0793\n",
            "Epoch [19725/20000], Training Loss: 0.0826\n",
            "Epoch [19726/20000], Training Loss: 0.0783\n",
            "Epoch [19727/20000], Training Loss: 0.0744\n",
            "Epoch [19728/20000], Training Loss: 0.0802\n",
            "Epoch [19729/20000], Training Loss: 0.0754\n",
            "Epoch [19730/20000], Training Loss: 0.0772\n",
            "Epoch [19731/20000], Training Loss: 0.0814\n",
            "Epoch [19732/20000], Training Loss: 0.0768\n",
            "Epoch [19733/20000], Training Loss: 0.0809\n",
            "Epoch [19734/20000], Training Loss: 0.0751\n",
            "Epoch [19735/20000], Training Loss: 0.0738\n",
            "Epoch [19736/20000], Training Loss: 0.0799\n",
            "Epoch [19737/20000], Training Loss: 0.0798\n",
            "Epoch [19738/20000], Training Loss: 0.0818\n",
            "Epoch [19739/20000], Training Loss: 0.0775\n",
            "Epoch [19740/20000], Training Loss: 0.0790\n",
            "Epoch [19741/20000], Training Loss: 0.0841\n",
            "Epoch [19742/20000], Training Loss: 0.0811\n",
            "Epoch [19743/20000], Training Loss: 0.0827\n",
            "Epoch [19744/20000], Training Loss: 0.0821\n",
            "Epoch [19745/20000], Training Loss: 0.0850\n",
            "Epoch [19746/20000], Training Loss: 0.0770\n",
            "Epoch [19747/20000], Training Loss: 0.0762\n",
            "Epoch [19748/20000], Training Loss: 0.0839\n",
            "Epoch [19749/20000], Training Loss: 0.0744\n",
            "Epoch [19750/20000], Training Loss: 0.0824\n",
            "Epoch [19751/20000], Training Loss: 0.0790\n",
            "Epoch [19752/20000], Training Loss: 0.0776\n",
            "Epoch [19753/20000], Training Loss: 0.0728\n",
            "Epoch [19754/20000], Training Loss: 0.0763\n",
            "Epoch [19755/20000], Training Loss: 0.0795\n",
            "Epoch [19756/20000], Training Loss: 0.0819\n",
            "Epoch [19757/20000], Training Loss: 0.0781\n",
            "Epoch [19758/20000], Training Loss: 0.0849\n",
            "Epoch [19759/20000], Training Loss: 0.0768\n",
            "Epoch [19760/20000], Training Loss: 0.0773\n",
            "Epoch [19761/20000], Training Loss: 0.0769\n",
            "Epoch [19762/20000], Training Loss: 0.0807\n",
            "Epoch [19763/20000], Training Loss: 0.0866\n",
            "Epoch [19764/20000], Training Loss: 0.0859\n",
            "Epoch [19765/20000], Training Loss: 0.0750\n",
            "Epoch [19766/20000], Training Loss: 0.0815\n",
            "Epoch [19767/20000], Training Loss: 0.0813\n",
            "Epoch [19768/20000], Training Loss: 0.0817\n",
            "Epoch [19769/20000], Training Loss: 0.0801\n",
            "Epoch [19770/20000], Training Loss: 0.0738\n",
            "Epoch [19771/20000], Training Loss: 0.0824\n",
            "Epoch [19772/20000], Training Loss: 0.0882\n",
            "Epoch [19773/20000], Training Loss: 0.0787\n",
            "Epoch [19774/20000], Training Loss: 0.0809\n",
            "Epoch [19775/20000], Training Loss: 0.0786\n",
            "Epoch [19776/20000], Training Loss: 0.0813\n",
            "Epoch [19777/20000], Training Loss: 0.0858\n",
            "Epoch [19778/20000], Training Loss: 0.0845\n",
            "Epoch [19779/20000], Training Loss: 0.0866\n",
            "Epoch [19780/20000], Training Loss: 0.0790\n",
            "Epoch [19781/20000], Training Loss: 0.0817\n",
            "Epoch [19782/20000], Training Loss: 0.0816\n",
            "Epoch [19783/20000], Training Loss: 0.0811\n",
            "Epoch [19784/20000], Training Loss: 0.0832\n",
            "Epoch [19785/20000], Training Loss: 0.0874\n",
            "Epoch [19786/20000], Training Loss: 0.0746\n",
            "Epoch [19787/20000], Training Loss: 0.0751\n",
            "Epoch [19788/20000], Training Loss: 0.0748\n",
            "Epoch [19789/20000], Training Loss: 0.0762\n",
            "Epoch [19790/20000], Training Loss: 0.0747\n",
            "Epoch [19791/20000], Training Loss: 0.0748\n",
            "Epoch [19792/20000], Training Loss: 0.0838\n",
            "Epoch [19793/20000], Training Loss: 0.0859\n",
            "Epoch [19794/20000], Training Loss: 0.0759\n",
            "Epoch [19795/20000], Training Loss: 0.0733\n",
            "Epoch [19796/20000], Training Loss: 0.0819\n",
            "Epoch [19797/20000], Training Loss: 0.0830\n",
            "Epoch [19798/20000], Training Loss: 0.0788\n",
            "Epoch [19799/20000], Training Loss: 0.0848\n",
            "Epoch [19800/20000], Training Loss: 0.0846\n",
            "Epoch [19801/20000], Training Loss: 0.0800\n",
            "Epoch [19802/20000], Training Loss: 0.0775\n",
            "Epoch [19803/20000], Training Loss: 0.0785\n",
            "Epoch [19804/20000], Training Loss: 0.0853\n",
            "Epoch [19805/20000], Training Loss: 0.0804\n",
            "Epoch [19806/20000], Training Loss: 0.0768\n",
            "Epoch [19807/20000], Training Loss: 0.0836\n",
            "Epoch [19808/20000], Training Loss: 0.0763\n",
            "Epoch [19809/20000], Training Loss: 0.0770\n",
            "Epoch [19810/20000], Training Loss: 0.0786\n",
            "Epoch [19811/20000], Training Loss: 0.0764\n",
            "Epoch [19812/20000], Training Loss: 0.0748\n",
            "Epoch [19813/20000], Training Loss: 0.0810\n",
            "Epoch [19814/20000], Training Loss: 0.0774\n",
            "Epoch [19815/20000], Training Loss: 0.0760\n",
            "Epoch [19816/20000], Training Loss: 0.0761\n",
            "Epoch [19817/20000], Training Loss: 0.0757\n",
            "Epoch [19818/20000], Training Loss: 0.0845\n",
            "Epoch [19819/20000], Training Loss: 0.0789\n",
            "Epoch [19820/20000], Training Loss: 0.0835\n",
            "Epoch [19821/20000], Training Loss: 0.0819\n",
            "Epoch [19822/20000], Training Loss: 0.0845\n",
            "Epoch [19823/20000], Training Loss: 0.0795\n",
            "Epoch [19824/20000], Training Loss: 0.0794\n",
            "Epoch [19825/20000], Training Loss: 0.0751\n",
            "Epoch [19826/20000], Training Loss: 0.0815\n",
            "Epoch [19827/20000], Training Loss: 0.0815\n",
            "Epoch [19828/20000], Training Loss: 0.0757\n",
            "Epoch [19829/20000], Training Loss: 0.0778\n",
            "Epoch [19830/20000], Training Loss: 0.0824\n",
            "Epoch [19831/20000], Training Loss: 0.0801\n",
            "Epoch [19832/20000], Training Loss: 0.0767\n",
            "Epoch [19833/20000], Training Loss: 0.0803\n",
            "Epoch [19834/20000], Training Loss: 0.0802\n",
            "Epoch [19835/20000], Training Loss: 0.0779\n",
            "Epoch [19836/20000], Training Loss: 0.0748\n",
            "Epoch [19837/20000], Training Loss: 0.0806\n",
            "Epoch [19838/20000], Training Loss: 0.0816\n",
            "Epoch [19839/20000], Training Loss: 0.0737\n",
            "Epoch [19840/20000], Training Loss: 0.0826\n",
            "Epoch [19841/20000], Training Loss: 0.0818\n",
            "Epoch [19842/20000], Training Loss: 0.0786\n",
            "Epoch [19843/20000], Training Loss: 0.0880\n",
            "Epoch [19844/20000], Training Loss: 0.0785\n",
            "Epoch [19845/20000], Training Loss: 0.0849\n",
            "Epoch [19846/20000], Training Loss: 0.0781\n",
            "Epoch [19847/20000], Training Loss: 0.0738\n",
            "Epoch [19848/20000], Training Loss: 0.0793\n",
            "Epoch [19849/20000], Training Loss: 0.0773\n",
            "Epoch [19850/20000], Training Loss: 0.0749\n",
            "Epoch [19851/20000], Training Loss: 0.0761\n",
            "Epoch [19852/20000], Training Loss: 0.0775\n",
            "Epoch [19853/20000], Training Loss: 0.0790\n",
            "Epoch [19854/20000], Training Loss: 0.0751\n",
            "Epoch [19855/20000], Training Loss: 0.0856\n",
            "Epoch [19856/20000], Training Loss: 0.0819\n",
            "Epoch [19857/20000], Training Loss: 0.0783\n",
            "Epoch [19858/20000], Training Loss: 0.0766\n",
            "Epoch [19859/20000], Training Loss: 0.0758\n",
            "Epoch [19860/20000], Training Loss: 0.0841\n",
            "Epoch [19861/20000], Training Loss: 0.0747\n",
            "Epoch [19862/20000], Training Loss: 0.0811\n",
            "Epoch [19863/20000], Training Loss: 0.0790\n",
            "Epoch [19864/20000], Training Loss: 0.0802\n",
            "Epoch [19865/20000], Training Loss: 0.0798\n",
            "Epoch [19866/20000], Training Loss: 0.0745\n",
            "Epoch [19867/20000], Training Loss: 0.0807\n",
            "Epoch [19868/20000], Training Loss: 0.0811\n",
            "Epoch [19869/20000], Training Loss: 0.0789\n",
            "Epoch [19870/20000], Training Loss: 0.0760\n",
            "Epoch [19871/20000], Training Loss: 0.0782\n",
            "Epoch [19872/20000], Training Loss: 0.0808\n",
            "Epoch [19873/20000], Training Loss: 0.0858\n",
            "Epoch [19874/20000], Training Loss: 0.0806\n",
            "Epoch [19875/20000], Training Loss: 0.0721\n",
            "Epoch [19876/20000], Training Loss: 0.0808\n",
            "Epoch [19877/20000], Training Loss: 0.0869\n",
            "Epoch [19878/20000], Training Loss: 0.0796\n",
            "Epoch [19879/20000], Training Loss: 0.0784\n",
            "Epoch [19880/20000], Training Loss: 0.0785\n",
            "Epoch [19881/20000], Training Loss: 0.0832\n",
            "Epoch [19882/20000], Training Loss: 0.0817\n",
            "Epoch [19883/20000], Training Loss: 0.0776\n",
            "Epoch [19884/20000], Training Loss: 0.0777\n",
            "Epoch [19885/20000], Training Loss: 0.0786\n",
            "Epoch [19886/20000], Training Loss: 0.0768\n",
            "Epoch [19887/20000], Training Loss: 0.0798\n",
            "Epoch [19888/20000], Training Loss: 0.0745\n",
            "Epoch [19889/20000], Training Loss: 0.0900\n",
            "Epoch [19890/20000], Training Loss: 0.0824\n",
            "Epoch [19891/20000], Training Loss: 0.0809\n",
            "Epoch [19892/20000], Training Loss: 0.0779\n",
            "Epoch [19893/20000], Training Loss: 0.0851\n",
            "Epoch [19894/20000], Training Loss: 0.0787\n",
            "Epoch [19895/20000], Training Loss: 0.0775\n",
            "Epoch [19896/20000], Training Loss: 0.0796\n",
            "Epoch [19897/20000], Training Loss: 0.0845\n",
            "Epoch [19898/20000], Training Loss: 0.0799\n",
            "Epoch [19899/20000], Training Loss: 0.0812\n",
            "Epoch [19900/20000], Training Loss: 0.0814\n",
            "Epoch [19901/20000], Training Loss: 0.0739\n",
            "Epoch [19902/20000], Training Loss: 0.0761\n",
            "Epoch [19903/20000], Training Loss: 0.0750\n",
            "Epoch [19904/20000], Training Loss: 0.0788\n",
            "Epoch [19905/20000], Training Loss: 0.0811\n",
            "Epoch [19906/20000], Training Loss: 0.0799\n",
            "Epoch [19907/20000], Training Loss: 0.0764\n",
            "Epoch [19908/20000], Training Loss: 0.0820\n",
            "Epoch [19909/20000], Training Loss: 0.0803\n",
            "Epoch [19910/20000], Training Loss: 0.0762\n",
            "Epoch [19911/20000], Training Loss: 0.0761\n",
            "Epoch [19912/20000], Training Loss: 0.0766\n",
            "Epoch [19913/20000], Training Loss: 0.0785\n",
            "Epoch [19914/20000], Training Loss: 0.0781\n",
            "Epoch [19915/20000], Training Loss: 0.0810\n",
            "Epoch [19916/20000], Training Loss: 0.0804\n",
            "Epoch [19917/20000], Training Loss: 0.0784\n",
            "Epoch [19918/20000], Training Loss: 0.0778\n",
            "Epoch [19919/20000], Training Loss: 0.0786\n",
            "Epoch [19920/20000], Training Loss: 0.0768\n",
            "Epoch [19921/20000], Training Loss: 0.0789\n",
            "Epoch [19922/20000], Training Loss: 0.0763\n",
            "Epoch [19923/20000], Training Loss: 0.0756\n",
            "Epoch [19924/20000], Training Loss: 0.0842\n",
            "Epoch [19925/20000], Training Loss: 0.0821\n",
            "Epoch [19926/20000], Training Loss: 0.0816\n",
            "Epoch [19927/20000], Training Loss: 0.0800\n",
            "Epoch [19928/20000], Training Loss: 0.0782\n",
            "Epoch [19929/20000], Training Loss: 0.0775\n",
            "Epoch [19930/20000], Training Loss: 0.0746\n",
            "Epoch [19931/20000], Training Loss: 0.0769\n",
            "Epoch [19932/20000], Training Loss: 0.0767\n",
            "Epoch [19933/20000], Training Loss: 0.0848\n",
            "Epoch [19934/20000], Training Loss: 0.0747\n",
            "Epoch [19935/20000], Training Loss: 0.0761\n",
            "Epoch [19936/20000], Training Loss: 0.0837\n",
            "Epoch [19937/20000], Training Loss: 0.0792\n",
            "Epoch [19938/20000], Training Loss: 0.0757\n",
            "Epoch [19939/20000], Training Loss: 0.0839\n",
            "Epoch [19940/20000], Training Loss: 0.0788\n",
            "Epoch [19941/20000], Training Loss: 0.0794\n",
            "Epoch [19942/20000], Training Loss: 0.0801\n",
            "Epoch [19943/20000], Training Loss: 0.0746\n",
            "Epoch [19944/20000], Training Loss: 0.0791\n",
            "Epoch [19945/20000], Training Loss: 0.0817\n",
            "Epoch [19946/20000], Training Loss: 0.0802\n",
            "Epoch [19947/20000], Training Loss: 0.0820\n",
            "Epoch [19948/20000], Training Loss: 0.0786\n",
            "Epoch [19949/20000], Training Loss: 0.0749\n",
            "Epoch [19950/20000], Training Loss: 0.0799\n",
            "Epoch [19951/20000], Training Loss: 0.0813\n",
            "Epoch [19952/20000], Training Loss: 0.0746\n",
            "Epoch [19953/20000], Training Loss: 0.0817\n",
            "Epoch [19954/20000], Training Loss: 0.0749\n",
            "Epoch [19955/20000], Training Loss: 0.0772\n",
            "Epoch [19956/20000], Training Loss: 0.0852\n",
            "Epoch [19957/20000], Training Loss: 0.0739\n",
            "Epoch [19958/20000], Training Loss: 0.0756\n",
            "Epoch [19959/20000], Training Loss: 0.0805\n",
            "Epoch [19960/20000], Training Loss: 0.0796\n",
            "Epoch [19961/20000], Training Loss: 0.0786\n",
            "Epoch [19962/20000], Training Loss: 0.0842\n",
            "Epoch [19963/20000], Training Loss: 0.0800\n",
            "Epoch [19964/20000], Training Loss: 0.0760\n",
            "Epoch [19965/20000], Training Loss: 0.0818\n",
            "Epoch [19966/20000], Training Loss: 0.0811\n",
            "Epoch [19967/20000], Training Loss: 0.0809\n",
            "Epoch [19968/20000], Training Loss: 0.0739\n",
            "Epoch [19969/20000], Training Loss: 0.0761\n",
            "Epoch [19970/20000], Training Loss: 0.0753\n",
            "Epoch [19971/20000], Training Loss: 0.0855\n",
            "Epoch [19972/20000], Training Loss: 0.0836\n",
            "Epoch [19973/20000], Training Loss: 0.0819\n",
            "Epoch [19974/20000], Training Loss: 0.0749\n",
            "Epoch [19975/20000], Training Loss: 0.0800\n",
            "Epoch [19976/20000], Training Loss: 0.0765\n",
            "Epoch [19977/20000], Training Loss: 0.0859\n",
            "Epoch [19978/20000], Training Loss: 0.0790\n",
            "Epoch [19979/20000], Training Loss: 0.0811\n",
            "Epoch [19980/20000], Training Loss: 0.0807\n",
            "Epoch [19981/20000], Training Loss: 0.0778\n",
            "Epoch [19982/20000], Training Loss: 0.0858\n",
            "Epoch [19983/20000], Training Loss: 0.0749\n",
            "Epoch [19984/20000], Training Loss: 0.0793\n",
            "Epoch [19985/20000], Training Loss: 0.0785\n",
            "Epoch [19986/20000], Training Loss: 0.0746\n",
            "Epoch [19987/20000], Training Loss: 0.0831\n",
            "Epoch [19988/20000], Training Loss: 0.0765\n",
            "Epoch [19989/20000], Training Loss: 0.0756\n",
            "Epoch [19990/20000], Training Loss: 0.0848\n",
            "Epoch [19991/20000], Training Loss: 0.0787\n",
            "Epoch [19992/20000], Training Loss: 0.0785\n",
            "Epoch [19993/20000], Training Loss: 0.0743\n",
            "Epoch [19994/20000], Training Loss: 0.0851\n",
            "Epoch [19995/20000], Training Loss: 0.0797\n",
            "Epoch [19996/20000], Training Loss: 0.0793\n",
            "Epoch [19997/20000], Training Loss: 0.0828\n",
            "Epoch [19998/20000], Training Loss: 0.0760\n",
            "Epoch [19999/20000], Training Loss: 0.0784\n",
            "Epoch [20000/20000], Training Loss: 0.0850\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Normalize the data\n",
        "train_mean = train_data.mean()\n",
        "train_std = train_data.std()\n",
        "train_data = (train_data - train_mean) / train_std\n",
        "test_data = (test_data - train_mean) / train_std\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Define the hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20000\n",
        "batch_size = 16\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearModel(len(input_cols), len(output_cols))\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the training loop\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    batch_losses = []\n",
        "    # Shuffle the data\n",
        "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "    # Split the data into batches\n",
        "    for i in range(0, len(train_data), batch_size):\n",
        "        batch_data = train_data[i:i+batch_size]\n",
        "        # Extract the input and output data\n",
        "        x_batch = torch.tensor(batch_data[input_cols].values, dtype=torch.float32)\n",
        "        y_batch = torch.tensor(batch_data[output_cols].values, dtype=torch.float32)\n",
        "        # Forward pass\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_losses.append(loss.item())\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}')\n",
        "\n",
        "# Prepare the input data\n",
        "input_data = np.arange(2018, 2026).reshape(-1, 1)\n",
        "input_data = (input_data - train_mean['Year']) / train_std['Year']\n",
        "input_data = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "# Predict the output\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output_data = model(input_data)\n",
        "output_data = output_data.detach().numpy()\n",
        "\n",
        "# Convert the output data back to original scale\n",
        "output_data = (output_data * train_std[output_cols].values) + train_mean[output_cols].values\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Plot the predicted output\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(output_cols):\n",
        "    plt.plot(data['Year'].values, data[col].values, label=col)\n",
        "    plt.plot(np.arange(2018, 2026), output_data[:, i], label=col + ' (predicted)', linestyle='--')\n",
        "plt.legend()\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of cases')\n",
        "plt.title('Number of cases on Mental Health and substance use disorders Thailand(linear)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss over Time Linear')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.3707\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_inputs = torch.tensor(test_data[input_cols].values, dtype=torch.float32)\n",
        "    test_outputs = torch.tensor(test_data[output_cols].values, dtype=torch.float32)\n",
        "    test_predictions = model(test_inputs)\n",
        "    test_predictions = test_predictions.squeeze(1)  # remove the extra dimension\n",
        "    test_loss = criterion(test_predictions, test_outputs)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
