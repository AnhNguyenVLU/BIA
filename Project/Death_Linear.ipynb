{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z98MBnlEUsCU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load data and convert to PyTorch tensor\n",
        "data = pd.read_excel('Death.xlsx')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "fvG8dBGSWpC9",
        "outputId": "15a2ff2f-fd94-447b-ee2f-044713cf6fe5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Vietnam</th>\n",
              "      <th>Thailand</th>\n",
              "      <th>Laos</th>\n",
              "      <th>Cambodia</th>\n",
              "      <th>Myanmar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1990</td>\n",
              "      <td>53</td>\n",
              "      <td>58</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1991</td>\n",
              "      <td>56</td>\n",
              "      <td>63</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1992</td>\n",
              "      <td>59</td>\n",
              "      <td>69</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1993</td>\n",
              "      <td>63</td>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1994</td>\n",
              "      <td>66</td>\n",
              "      <td>86</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1995</td>\n",
              "      <td>71</td>\n",
              "      <td>99</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1996</td>\n",
              "      <td>74</td>\n",
              "      <td>112</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1997</td>\n",
              "      <td>78</td>\n",
              "      <td>116</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1998</td>\n",
              "      <td>82</td>\n",
              "      <td>125</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1999</td>\n",
              "      <td>86</td>\n",
              "      <td>138</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2000</td>\n",
              "      <td>89</td>\n",
              "      <td>146</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2001</td>\n",
              "      <td>93</td>\n",
              "      <td>150</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2002</td>\n",
              "      <td>97</td>\n",
              "      <td>158</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2003</td>\n",
              "      <td>102</td>\n",
              "      <td>160</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2004</td>\n",
              "      <td>108</td>\n",
              "      <td>163</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2005</td>\n",
              "      <td>113</td>\n",
              "      <td>164</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2006</td>\n",
              "      <td>118</td>\n",
              "      <td>168</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2007</td>\n",
              "      <td>122</td>\n",
              "      <td>175</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2008</td>\n",
              "      <td>127</td>\n",
              "      <td>181</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2009</td>\n",
              "      <td>133</td>\n",
              "      <td>186</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2010</td>\n",
              "      <td>139</td>\n",
              "      <td>195</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2011</td>\n",
              "      <td>146</td>\n",
              "      <td>204</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2012</td>\n",
              "      <td>153</td>\n",
              "      <td>215</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2013</td>\n",
              "      <td>160</td>\n",
              "      <td>224</td>\n",
              "      <td>8</td>\n",
              "      <td>11</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2014</td>\n",
              "      <td>168</td>\n",
              "      <td>238</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2015</td>\n",
              "      <td>177</td>\n",
              "      <td>251</td>\n",
              "      <td>9</td>\n",
              "      <td>13</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2016</td>\n",
              "      <td>186</td>\n",
              "      <td>272</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2017</td>\n",
              "      <td>195</td>\n",
              "      <td>287</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2018</td>\n",
              "      <td>204</td>\n",
              "      <td>302</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2019</td>\n",
              "      <td>212</td>\n",
              "      <td>317</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Year  Vietnam  Thailand  Laos  Cambodia  Myanmar\n",
              "0   1990       53        58     2         3       31\n",
              "1   1991       56        63     2         3       33\n",
              "2   1992       59        69     3         4       34\n",
              "3   1993       63        76     3         4       36\n",
              "4   1994       66        86     3         4       38\n",
              "5   1995       71        99     3         4       39\n",
              "6   1996       74       112     3         4       41\n",
              "7   1997       78       116     4         5       43\n",
              "8   1998       82       125     4         5       45\n",
              "9   1999       86       138     4         5       48\n",
              "10  2000       89       146     4         5       50\n",
              "11  2001       93       150     4         6       53\n",
              "12  2002       97       158     5         6       56\n",
              "13  2003      102       160     5         6       59\n",
              "14  2004      108       163     5         7       62\n",
              "15  2005      113       164     5         7       65\n",
              "16  2006      118       168     5         7       68\n",
              "17  2007      122       175     6         8       70\n",
              "18  2008      127       181     6         8       73\n",
              "19  2009      133       186     6         9       75\n",
              "20  2010      139       195     7         9       77\n",
              "21  2011      146       204     7        10       80\n",
              "22  2012      153       215     7        11       83\n",
              "23  2013      160       224     8        11       86\n",
              "24  2014      168       238     8        12       90\n",
              "25  2015      177       251     9        13       94\n",
              "26  2016      186       272     9        14       98\n",
              "27  2017      195       287     9        14      101\n",
              "28  2018      204       302    10        15      104\n",
              "29  2019      212       317    10        16      108"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sDa6UXB_5J3z"
      },
      "outputs": [],
      "source": [
        "columns = ['Year', 'Vietnam', 'Thailand', 'Laos', 'Cambodia', 'Myanmar']\n",
        "data = data[columns]\n",
        "\n",
        "# Define the input and output columns\n",
        "input_cols = ['Year']\n",
        "output_cols = ['Vietnam', 'Thailand', 'Laos', 'Cambodia', 'Myanmar']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = data[data['Year'] <= 2014]\n",
        "test_data = data[ data['Year'] >= 2015]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f3ZXO7axb6z1",
        "outputId": "e3b8d64b-82e0-4211-e81f-6f8129813a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20000], Training Loss: 1.2723\n",
            "Epoch [2/20000], Training Loss: 1.2458\n",
            "Epoch [3/20000], Training Loss: 1.3696\n",
            "Epoch [4/20000], Training Loss: 1.2414\n",
            "Epoch [5/20000], Training Loss: 1.2419\n",
            "Epoch [6/20000], Training Loss: 1.1917\n",
            "Epoch [7/20000], Training Loss: 1.1505\n",
            "Epoch [8/20000], Training Loss: 1.1620\n",
            "Epoch [9/20000], Training Loss: 1.0443\n",
            "Epoch [10/20000], Training Loss: 1.1152\n",
            "Epoch [11/20000], Training Loss: 1.0508\n",
            "Epoch [12/20000], Training Loss: 1.1402\n",
            "Epoch [13/20000], Training Loss: 1.0356\n",
            "Epoch [14/20000], Training Loss: 1.0502\n",
            "Epoch [15/20000], Training Loss: 1.0390\n",
            "Epoch [16/20000], Training Loss: 1.0203\n",
            "Epoch [17/20000], Training Loss: 1.0760\n",
            "Epoch [18/20000], Training Loss: 1.0905\n",
            "Epoch [19/20000], Training Loss: 0.9246\n",
            "Epoch [20/20000], Training Loss: 1.0005\n",
            "Epoch [21/20000], Training Loss: 0.9510\n",
            "Epoch [22/20000], Training Loss: 0.9166\n",
            "Epoch [23/20000], Training Loss: 0.9438\n",
            "Epoch [24/20000], Training Loss: 0.9115\n",
            "Epoch [25/20000], Training Loss: 0.8256\n",
            "Epoch [26/20000], Training Loss: 0.8610\n",
            "Epoch [27/20000], Training Loss: 0.8738\n",
            "Epoch [28/20000], Training Loss: 0.7906\n",
            "Epoch [29/20000], Training Loss: 0.7759\n",
            "Epoch [30/20000], Training Loss: 0.7640\n",
            "Epoch [31/20000], Training Loss: 0.8132\n",
            "Epoch [32/20000], Training Loss: 0.7586\n",
            "Epoch [33/20000], Training Loss: 0.7754\n",
            "Epoch [34/20000], Training Loss: 0.8337\n",
            "Epoch [35/20000], Training Loss: 0.7475\n",
            "Epoch [36/20000], Training Loss: 0.7774\n",
            "Epoch [37/20000], Training Loss: 0.7712\n",
            "Epoch [38/20000], Training Loss: 0.7522\n",
            "Epoch [39/20000], Training Loss: 0.6395\n",
            "Epoch [40/20000], Training Loss: 0.6933\n",
            "Epoch [41/20000], Training Loss: 0.7259\n",
            "Epoch [42/20000], Training Loss: 0.6777\n",
            "Epoch [43/20000], Training Loss: 0.6553\n",
            "Epoch [44/20000], Training Loss: 0.6492\n",
            "Epoch [45/20000], Training Loss: 0.6215\n",
            "Epoch [46/20000], Training Loss: 0.6578\n",
            "Epoch [47/20000], Training Loss: 0.6009\n",
            "Epoch [48/20000], Training Loss: 0.6280\n",
            "Epoch [49/20000], Training Loss: 0.5961\n",
            "Epoch [50/20000], Training Loss: 0.6631\n",
            "Epoch [51/20000], Training Loss: 0.5983\n",
            "Epoch [52/20000], Training Loss: 0.5904\n",
            "Epoch [53/20000], Training Loss: 0.5824\n",
            "Epoch [54/20000], Training Loss: 0.5484\n",
            "Epoch [55/20000], Training Loss: 0.5522\n",
            "Epoch [56/20000], Training Loss: 0.5836\n",
            "Epoch [57/20000], Training Loss: 0.5419\n",
            "Epoch [58/20000], Training Loss: 0.5391\n",
            "Epoch [59/20000], Training Loss: 0.5224\n",
            "Epoch [60/20000], Training Loss: 0.5621\n",
            "Epoch [61/20000], Training Loss: 0.4854\n",
            "Epoch [62/20000], Training Loss: 0.4869\n",
            "Epoch [63/20000], Training Loss: 0.5250\n",
            "Epoch [64/20000], Training Loss: 0.5017\n",
            "Epoch [65/20000], Training Loss: 0.5121\n",
            "Epoch [66/20000], Training Loss: 0.4827\n",
            "Epoch [67/20000], Training Loss: 0.4551\n",
            "Epoch [68/20000], Training Loss: 0.4822\n",
            "Epoch [69/20000], Training Loss: 0.4686\n",
            "Epoch [70/20000], Training Loss: 0.4673\n",
            "Epoch [71/20000], Training Loss: 0.4225\n",
            "Epoch [72/20000], Training Loss: 0.4805\n",
            "Epoch [73/20000], Training Loss: 0.4308\n",
            "Epoch [74/20000], Training Loss: 0.4121\n",
            "Epoch [75/20000], Training Loss: 0.3924\n",
            "Epoch [76/20000], Training Loss: 0.3947\n",
            "Epoch [77/20000], Training Loss: 0.4363\n",
            "Epoch [78/20000], Training Loss: 0.3916\n",
            "Epoch [79/20000], Training Loss: 0.4261\n",
            "Epoch [80/20000], Training Loss: 0.3701\n",
            "Epoch [81/20000], Training Loss: 0.3903\n",
            "Epoch [82/20000], Training Loss: 0.4160\n",
            "Epoch [83/20000], Training Loss: 0.3530\n",
            "Epoch [84/20000], Training Loss: 0.3875\n",
            "Epoch [85/20000], Training Loss: 0.3515\n",
            "Epoch [86/20000], Training Loss: 0.3550\n",
            "Epoch [87/20000], Training Loss: 0.3654\n",
            "Epoch [88/20000], Training Loss: 0.3373\n",
            "Epoch [89/20000], Training Loss: 0.3497\n",
            "Epoch [90/20000], Training Loss: 0.3294\n",
            "Epoch [91/20000], Training Loss: 0.3322\n",
            "Epoch [92/20000], Training Loss: 0.3120\n",
            "Epoch [93/20000], Training Loss: 0.3380\n",
            "Epoch [94/20000], Training Loss: 0.3210\n",
            "Epoch [95/20000], Training Loss: 0.3283\n",
            "Epoch [96/20000], Training Loss: 0.3266\n",
            "Epoch [97/20000], Training Loss: 0.3291\n",
            "Epoch [98/20000], Training Loss: 0.3167\n",
            "Epoch [99/20000], Training Loss: 0.3180\n",
            "Epoch [100/20000], Training Loss: 0.2816\n",
            "Epoch [101/20000], Training Loss: 0.2895\n",
            "Epoch [102/20000], Training Loss: 0.2830\n",
            "Epoch [103/20000], Training Loss: 0.2851\n",
            "Epoch [104/20000], Training Loss: 0.3006\n",
            "Epoch [105/20000], Training Loss: 0.2681\n",
            "Epoch [106/20000], Training Loss: 0.2721\n",
            "Epoch [107/20000], Training Loss: 0.2823\n",
            "Epoch [108/20000], Training Loss: 0.2639\n",
            "Epoch [109/20000], Training Loss: 0.2571\n",
            "Epoch [110/20000], Training Loss: 0.2654\n",
            "Epoch [111/20000], Training Loss: 0.2593\n",
            "Epoch [112/20000], Training Loss: 0.2494\n",
            "Epoch [113/20000], Training Loss: 0.2313\n",
            "Epoch [114/20000], Training Loss: 0.2471\n",
            "Epoch [115/20000], Training Loss: 0.2554\n",
            "Epoch [116/20000], Training Loss: 0.2405\n",
            "Epoch [117/20000], Training Loss: 0.2312\n",
            "Epoch [118/20000], Training Loss: 0.2241\n",
            "Epoch [119/20000], Training Loss: 0.2314\n",
            "Epoch [120/20000], Training Loss: 0.2305\n",
            "Epoch [121/20000], Training Loss: 0.2325\n",
            "Epoch [122/20000], Training Loss: 0.2142\n",
            "Epoch [123/20000], Training Loss: 0.2212\n",
            "Epoch [124/20000], Training Loss: 0.2066\n",
            "Epoch [125/20000], Training Loss: 0.2118\n",
            "Epoch [126/20000], Training Loss: 0.1925\n",
            "Epoch [127/20000], Training Loss: 0.2040\n",
            "Epoch [128/20000], Training Loss: 0.1847\n",
            "Epoch [129/20000], Training Loss: 0.1948\n",
            "Epoch [130/20000], Training Loss: 0.2069\n",
            "Epoch [131/20000], Training Loss: 0.1987\n",
            "Epoch [132/20000], Training Loss: 0.2025\n",
            "Epoch [133/20000], Training Loss: 0.1928\n",
            "Epoch [134/20000], Training Loss: 0.1835\n",
            "Epoch [135/20000], Training Loss: 0.1707\n",
            "Epoch [136/20000], Training Loss: 0.1896\n",
            "Epoch [137/20000], Training Loss: 0.1709\n",
            "Epoch [138/20000], Training Loss: 0.1778\n",
            "Epoch [139/20000], Training Loss: 0.1760\n",
            "Epoch [140/20000], Training Loss: 0.1703\n",
            "Epoch [141/20000], Training Loss: 0.1662\n",
            "Epoch [142/20000], Training Loss: 0.1701\n",
            "Epoch [143/20000], Training Loss: 0.1661\n",
            "Epoch [144/20000], Training Loss: 0.1622\n",
            "Epoch [145/20000], Training Loss: 0.1569\n",
            "Epoch [146/20000], Training Loss: 0.1590\n",
            "Epoch [147/20000], Training Loss: 0.1623\n",
            "Epoch [148/20000], Training Loss: 0.1604\n",
            "Epoch [149/20000], Training Loss: 0.1503\n",
            "Epoch [150/20000], Training Loss: 0.1505\n",
            "Epoch [151/20000], Training Loss: 0.1544\n",
            "Epoch [152/20000], Training Loss: 0.1408\n",
            "Epoch [153/20000], Training Loss: 0.1436\n",
            "Epoch [154/20000], Training Loss: 0.1447\n",
            "Epoch [155/20000], Training Loss: 0.1412\n",
            "Epoch [156/20000], Training Loss: 0.1340\n",
            "Epoch [157/20000], Training Loss: 0.1256\n",
            "Epoch [158/20000], Training Loss: 0.1447\n",
            "Epoch [159/20000], Training Loss: 0.1444\n",
            "Epoch [160/20000], Training Loss: 0.1342\n",
            "Epoch [161/20000], Training Loss: 0.1445\n",
            "Epoch [162/20000], Training Loss: 0.1292\n",
            "Epoch [163/20000], Training Loss: 0.1302\n",
            "Epoch [164/20000], Training Loss: 0.1241\n",
            "Epoch [165/20000], Training Loss: 0.1215\n",
            "Epoch [166/20000], Training Loss: 0.1154\n",
            "Epoch [167/20000], Training Loss: 0.1268\n",
            "Epoch [168/20000], Training Loss: 0.1195\n",
            "Epoch [169/20000], Training Loss: 0.1186\n",
            "Epoch [170/20000], Training Loss: 0.1172\n",
            "Epoch [171/20000], Training Loss: 0.1159\n",
            "Epoch [172/20000], Training Loss: 0.1218\n",
            "Epoch [173/20000], Training Loss: 0.1132\n",
            "Epoch [174/20000], Training Loss: 0.1073\n",
            "Epoch [175/20000], Training Loss: 0.1091\n",
            "Epoch [176/20000], Training Loss: 0.1043\n",
            "Epoch [177/20000], Training Loss: 0.1095\n",
            "Epoch [178/20000], Training Loss: 0.1058\n",
            "Epoch [179/20000], Training Loss: 0.0998\n",
            "Epoch [180/20000], Training Loss: 0.0955\n",
            "Epoch [181/20000], Training Loss: 0.0979\n",
            "Epoch [182/20000], Training Loss: 0.0980\n",
            "Epoch [183/20000], Training Loss: 0.1011\n",
            "Epoch [184/20000], Training Loss: 0.0971\n",
            "Epoch [185/20000], Training Loss: 0.0932\n",
            "Epoch [186/20000], Training Loss: 0.0970\n",
            "Epoch [187/20000], Training Loss: 0.1012\n",
            "Epoch [188/20000], Training Loss: 0.1054\n",
            "Epoch [189/20000], Training Loss: 0.0889\n",
            "Epoch [190/20000], Training Loss: 0.0925\n",
            "Epoch [191/20000], Training Loss: 0.0904\n",
            "Epoch [192/20000], Training Loss: 0.0931\n",
            "Epoch [193/20000], Training Loss: 0.0846\n",
            "Epoch [194/20000], Training Loss: 0.0844\n",
            "Epoch [195/20000], Training Loss: 0.0912\n",
            "Epoch [196/20000], Training Loss: 0.0821\n",
            "Epoch [197/20000], Training Loss: 0.0903\n",
            "Epoch [198/20000], Training Loss: 0.0839\n",
            "Epoch [199/20000], Training Loss: 0.0852\n",
            "Epoch [200/20000], Training Loss: 0.0886\n",
            "Epoch [201/20000], Training Loss: 0.0813\n",
            "Epoch [202/20000], Training Loss: 0.0880\n",
            "Epoch [203/20000], Training Loss: 0.0809\n",
            "Epoch [204/20000], Training Loss: 0.0778\n",
            "Epoch [205/20000], Training Loss: 0.0808\n",
            "Epoch [206/20000], Training Loss: 0.0776\n",
            "Epoch [207/20000], Training Loss: 0.0846\n",
            "Epoch [208/20000], Training Loss: 0.0728\n",
            "Epoch [209/20000], Training Loss: 0.0773\n",
            "Epoch [210/20000], Training Loss: 0.0765\n",
            "Epoch [211/20000], Training Loss: 0.0725\n",
            "Epoch [212/20000], Training Loss: 0.0732\n",
            "Epoch [213/20000], Training Loss: 0.0748\n",
            "Epoch [214/20000], Training Loss: 0.0644\n",
            "Epoch [215/20000], Training Loss: 0.0686\n",
            "Epoch [216/20000], Training Loss: 0.0752\n",
            "Epoch [217/20000], Training Loss: 0.0715\n",
            "Epoch [218/20000], Training Loss: 0.0748\n",
            "Epoch [219/20000], Training Loss: 0.0657\n",
            "Epoch [220/20000], Training Loss: 0.0705\n",
            "Epoch [221/20000], Training Loss: 0.0689\n",
            "Epoch [222/20000], Training Loss: 0.0700\n",
            "Epoch [223/20000], Training Loss: 0.0630\n",
            "Epoch [224/20000], Training Loss: 0.0675\n",
            "Epoch [225/20000], Training Loss: 0.0612\n",
            "Epoch [226/20000], Training Loss: 0.0680\n",
            "Epoch [227/20000], Training Loss: 0.0636\n",
            "Epoch [228/20000], Training Loss: 0.0600\n",
            "Epoch [229/20000], Training Loss: 0.0675\n",
            "Epoch [230/20000], Training Loss: 0.0596\n",
            "Epoch [231/20000], Training Loss: 0.0648\n",
            "Epoch [232/20000], Training Loss: 0.0633\n",
            "Epoch [233/20000], Training Loss: 0.0649\n",
            "Epoch [234/20000], Training Loss: 0.0604\n",
            "Epoch [235/20000], Training Loss: 0.0591\n",
            "Epoch [236/20000], Training Loss: 0.0613\n",
            "Epoch [237/20000], Training Loss: 0.0589\n",
            "Epoch [238/20000], Training Loss: 0.0542\n",
            "Epoch [239/20000], Training Loss: 0.0604\n",
            "Epoch [240/20000], Training Loss: 0.0543\n",
            "Epoch [241/20000], Training Loss: 0.0535\n",
            "Epoch [242/20000], Training Loss: 0.0542\n",
            "Epoch [243/20000], Training Loss: 0.0562\n",
            "Epoch [244/20000], Training Loss: 0.0558\n",
            "Epoch [245/20000], Training Loss: 0.0587\n",
            "Epoch [246/20000], Training Loss: 0.0491\n",
            "Epoch [247/20000], Training Loss: 0.0531\n",
            "Epoch [248/20000], Training Loss: 0.0544\n",
            "Epoch [249/20000], Training Loss: 0.0524\n",
            "Epoch [250/20000], Training Loss: 0.0517\n",
            "Epoch [251/20000], Training Loss: 0.0501\n",
            "Epoch [252/20000], Training Loss: 0.0477\n",
            "Epoch [253/20000], Training Loss: 0.0474\n",
            "Epoch [254/20000], Training Loss: 0.0453\n",
            "Epoch [255/20000], Training Loss: 0.0557\n",
            "Epoch [256/20000], Training Loss: 0.0542\n",
            "Epoch [257/20000], Training Loss: 0.0456\n",
            "Epoch [258/20000], Training Loss: 0.0491\n",
            "Epoch [259/20000], Training Loss: 0.0492\n",
            "Epoch [260/20000], Training Loss: 0.0457\n",
            "Epoch [261/20000], Training Loss: 0.0500\n",
            "Epoch [262/20000], Training Loss: 0.0436\n",
            "Epoch [263/20000], Training Loss: 0.0437\n",
            "Epoch [264/20000], Training Loss: 0.0447\n",
            "Epoch [265/20000], Training Loss: 0.0434\n",
            "Epoch [266/20000], Training Loss: 0.0452\n",
            "Epoch [267/20000], Training Loss: 0.0507\n",
            "Epoch [268/20000], Training Loss: 0.0477\n",
            "Epoch [269/20000], Training Loss: 0.0489\n",
            "Epoch [270/20000], Training Loss: 0.0431\n",
            "Epoch [271/20000], Training Loss: 0.0462\n",
            "Epoch [272/20000], Training Loss: 0.0435\n",
            "Epoch [273/20000], Training Loss: 0.0486\n",
            "Epoch [274/20000], Training Loss: 0.0442\n",
            "Epoch [275/20000], Training Loss: 0.0440\n",
            "Epoch [276/20000], Training Loss: 0.0404\n",
            "Epoch [277/20000], Training Loss: 0.0476\n",
            "Epoch [278/20000], Training Loss: 0.0467\n",
            "Epoch [279/20000], Training Loss: 0.0419\n",
            "Epoch [280/20000], Training Loss: 0.0460\n",
            "Epoch [281/20000], Training Loss: 0.0445\n",
            "Epoch [282/20000], Training Loss: 0.0427\n",
            "Epoch [283/20000], Training Loss: 0.0400\n",
            "Epoch [284/20000], Training Loss: 0.0416\n",
            "Epoch [285/20000], Training Loss: 0.0446\n",
            "Epoch [286/20000], Training Loss: 0.0376\n",
            "Epoch [287/20000], Training Loss: 0.0390\n",
            "Epoch [288/20000], Training Loss: 0.0430\n",
            "Epoch [289/20000], Training Loss: 0.0427\n",
            "Epoch [290/20000], Training Loss: 0.0383\n",
            "Epoch [291/20000], Training Loss: 0.0391\n",
            "Epoch [292/20000], Training Loss: 0.0422\n",
            "Epoch [293/20000], Training Loss: 0.0385\n",
            "Epoch [294/20000], Training Loss: 0.0376\n",
            "Epoch [295/20000], Training Loss: 0.0351\n",
            "Epoch [296/20000], Training Loss: 0.0359\n",
            "Epoch [297/20000], Training Loss: 0.0410\n",
            "Epoch [298/20000], Training Loss: 0.0388\n",
            "Epoch [299/20000], Training Loss: 0.0368\n",
            "Epoch [300/20000], Training Loss: 0.0371\n",
            "Epoch [301/20000], Training Loss: 0.0371\n",
            "Epoch [302/20000], Training Loss: 0.0382\n",
            "Epoch [303/20000], Training Loss: 0.0387\n",
            "Epoch [304/20000], Training Loss: 0.0356\n",
            "Epoch [305/20000], Training Loss: 0.0349\n",
            "Epoch [306/20000], Training Loss: 0.0337\n",
            "Epoch [307/20000], Training Loss: 0.0367\n",
            "Epoch [308/20000], Training Loss: 0.0357\n",
            "Epoch [309/20000], Training Loss: 0.0357\n",
            "Epoch [310/20000], Training Loss: 0.0356\n",
            "Epoch [311/20000], Training Loss: 0.0328\n",
            "Epoch [312/20000], Training Loss: 0.0346\n",
            "Epoch [313/20000], Training Loss: 0.0349\n",
            "Epoch [314/20000], Training Loss: 0.0366\n",
            "Epoch [315/20000], Training Loss: 0.0370\n",
            "Epoch [316/20000], Training Loss: 0.0347\n",
            "Epoch [317/20000], Training Loss: 0.0330\n",
            "Epoch [318/20000], Training Loss: 0.0346\n",
            "Epoch [319/20000], Training Loss: 0.0342\n",
            "Epoch [320/20000], Training Loss: 0.0341\n",
            "Epoch [321/20000], Training Loss: 0.0332\n",
            "Epoch [322/20000], Training Loss: 0.0331\n",
            "Epoch [323/20000], Training Loss: 0.0369\n",
            "Epoch [324/20000], Training Loss: 0.0382\n",
            "Epoch [325/20000], Training Loss: 0.0392\n",
            "Epoch [326/20000], Training Loss: 0.0326\n",
            "Epoch [327/20000], Training Loss: 0.0319\n",
            "Epoch [328/20000], Training Loss: 0.0326\n",
            "Epoch [329/20000], Training Loss: 0.0306\n",
            "Epoch [330/20000], Training Loss: 0.0342\n",
            "Epoch [331/20000], Training Loss: 0.0360\n",
            "Epoch [332/20000], Training Loss: 0.0351\n",
            "Epoch [333/20000], Training Loss: 0.0312\n",
            "Epoch [334/20000], Training Loss: 0.0357\n",
            "Epoch [335/20000], Training Loss: 0.0351\n",
            "Epoch [336/20000], Training Loss: 0.0313\n",
            "Epoch [337/20000], Training Loss: 0.0342\n",
            "Epoch [338/20000], Training Loss: 0.0345\n",
            "Epoch [339/20000], Training Loss: 0.0332\n",
            "Epoch [340/20000], Training Loss: 0.0341\n",
            "Epoch [341/20000], Training Loss: 0.0339\n",
            "Epoch [342/20000], Training Loss: 0.0353\n",
            "Epoch [343/20000], Training Loss: 0.0319\n",
            "Epoch [344/20000], Training Loss: 0.0341\n",
            "Epoch [345/20000], Training Loss: 0.0326\n",
            "Epoch [346/20000], Training Loss: 0.0356\n",
            "Epoch [347/20000], Training Loss: 0.0343\n",
            "Epoch [348/20000], Training Loss: 0.0330\n",
            "Epoch [349/20000], Training Loss: 0.0314\n",
            "Epoch [350/20000], Training Loss: 0.0329\n",
            "Epoch [351/20000], Training Loss: 0.0307\n",
            "Epoch [352/20000], Training Loss: 0.0292\n",
            "Epoch [353/20000], Training Loss: 0.0323\n",
            "Epoch [354/20000], Training Loss: 0.0323\n",
            "Epoch [355/20000], Training Loss: 0.0321\n",
            "Epoch [356/20000], Training Loss: 0.0293\n",
            "Epoch [357/20000], Training Loss: 0.0291\n",
            "Epoch [358/20000], Training Loss: 0.0323\n",
            "Epoch [359/20000], Training Loss: 0.0331\n",
            "Epoch [360/20000], Training Loss: 0.0309\n",
            "Epoch [361/20000], Training Loss: 0.0299\n",
            "Epoch [362/20000], Training Loss: 0.0313\n",
            "Epoch [363/20000], Training Loss: 0.0313\n",
            "Epoch [364/20000], Training Loss: 0.0297\n",
            "Epoch [365/20000], Training Loss: 0.0309\n",
            "Epoch [366/20000], Training Loss: 0.0311\n",
            "Epoch [367/20000], Training Loss: 0.0280\n",
            "Epoch [368/20000], Training Loss: 0.0289\n",
            "Epoch [369/20000], Training Loss: 0.0329\n",
            "Epoch [370/20000], Training Loss: 0.0284\n",
            "Epoch [371/20000], Training Loss: 0.0328\n",
            "Epoch [372/20000], Training Loss: 0.0303\n",
            "Epoch [373/20000], Training Loss: 0.0298\n",
            "Epoch [374/20000], Training Loss: 0.0279\n",
            "Epoch [375/20000], Training Loss: 0.0313\n",
            "Epoch [376/20000], Training Loss: 0.0298\n",
            "Epoch [377/20000], Training Loss: 0.0281\n",
            "Epoch [378/20000], Training Loss: 0.0280\n",
            "Epoch [379/20000], Training Loss: 0.0290\n",
            "Epoch [380/20000], Training Loss: 0.0274\n",
            "Epoch [381/20000], Training Loss: 0.0289\n",
            "Epoch [382/20000], Training Loss: 0.0293\n",
            "Epoch [383/20000], Training Loss: 0.0303\n",
            "Epoch [384/20000], Training Loss: 0.0276\n",
            "Epoch [385/20000], Training Loss: 0.0286\n",
            "Epoch [386/20000], Training Loss: 0.0284\n",
            "Epoch [387/20000], Training Loss: 0.0288\n",
            "Epoch [388/20000], Training Loss: 0.0293\n",
            "Epoch [389/20000], Training Loss: 0.0325\n",
            "Epoch [390/20000], Training Loss: 0.0279\n",
            "Epoch [391/20000], Training Loss: 0.0288\n",
            "Epoch [392/20000], Training Loss: 0.0292\n",
            "Epoch [393/20000], Training Loss: 0.0316\n",
            "Epoch [394/20000], Training Loss: 0.0278\n",
            "Epoch [395/20000], Training Loss: 0.0266\n",
            "Epoch [396/20000], Training Loss: 0.0291\n",
            "Epoch [397/20000], Training Loss: 0.0293\n",
            "Epoch [398/20000], Training Loss: 0.0297\n",
            "Epoch [399/20000], Training Loss: 0.0286\n",
            "Epoch [400/20000], Training Loss: 0.0272\n",
            "Epoch [401/20000], Training Loss: 0.0305\n",
            "Epoch [402/20000], Training Loss: 0.0281\n",
            "Epoch [403/20000], Training Loss: 0.0279\n",
            "Epoch [404/20000], Training Loss: 0.0315\n",
            "Epoch [405/20000], Training Loss: 0.0295\n",
            "Epoch [406/20000], Training Loss: 0.0311\n",
            "Epoch [407/20000], Training Loss: 0.0271\n",
            "Epoch [408/20000], Training Loss: 0.0301\n",
            "Epoch [409/20000], Training Loss: 0.0281\n",
            "Epoch [410/20000], Training Loss: 0.0277\n",
            "Epoch [411/20000], Training Loss: 0.0273\n",
            "Epoch [412/20000], Training Loss: 0.0318\n",
            "Epoch [413/20000], Training Loss: 0.0273\n",
            "Epoch [414/20000], Training Loss: 0.0285\n",
            "Epoch [415/20000], Training Loss: 0.0298\n",
            "Epoch [416/20000], Training Loss: 0.0304\n",
            "Epoch [417/20000], Training Loss: 0.0282\n",
            "Epoch [418/20000], Training Loss: 0.0285\n",
            "Epoch [419/20000], Training Loss: 0.0292\n",
            "Epoch [420/20000], Training Loss: 0.0299\n",
            "Epoch [421/20000], Training Loss: 0.0280\n",
            "Epoch [422/20000], Training Loss: 0.0309\n",
            "Epoch [423/20000], Training Loss: 0.0300\n",
            "Epoch [424/20000], Training Loss: 0.0266\n",
            "Epoch [425/20000], Training Loss: 0.0304\n",
            "Epoch [426/20000], Training Loss: 0.0303\n",
            "Epoch [427/20000], Training Loss: 0.0267\n",
            "Epoch [428/20000], Training Loss: 0.0279\n",
            "Epoch [429/20000], Training Loss: 0.0311\n",
            "Epoch [430/20000], Training Loss: 0.0264\n",
            "Epoch [431/20000], Training Loss: 0.0303\n",
            "Epoch [432/20000], Training Loss: 0.0266\n",
            "Epoch [433/20000], Training Loss: 0.0269\n",
            "Epoch [434/20000], Training Loss: 0.0293\n",
            "Epoch [435/20000], Training Loss: 0.0258\n",
            "Epoch [436/20000], Training Loss: 0.0261\n",
            "Epoch [437/20000], Training Loss: 0.0277\n",
            "Epoch [438/20000], Training Loss: 0.0264\n",
            "Epoch [439/20000], Training Loss: 0.0280\n",
            "Epoch [440/20000], Training Loss: 0.0301\n",
            "Epoch [441/20000], Training Loss: 0.0291\n",
            "Epoch [442/20000], Training Loss: 0.0277\n",
            "Epoch [443/20000], Training Loss: 0.0271\n",
            "Epoch [444/20000], Training Loss: 0.0292\n",
            "Epoch [445/20000], Training Loss: 0.0271\n",
            "Epoch [446/20000], Training Loss: 0.0295\n",
            "Epoch [447/20000], Training Loss: 0.0280\n",
            "Epoch [448/20000], Training Loss: 0.0268\n",
            "Epoch [449/20000], Training Loss: 0.0299\n",
            "Epoch [450/20000], Training Loss: 0.0267\n",
            "Epoch [451/20000], Training Loss: 0.0247\n",
            "Epoch [452/20000], Training Loss: 0.0284\n",
            "Epoch [453/20000], Training Loss: 0.0271\n",
            "Epoch [454/20000], Training Loss: 0.0250\n",
            "Epoch [455/20000], Training Loss: 0.0285\n",
            "Epoch [456/20000], Training Loss: 0.0255\n",
            "Epoch [457/20000], Training Loss: 0.0283\n",
            "Epoch [458/20000], Training Loss: 0.0285\n",
            "Epoch [459/20000], Training Loss: 0.0283\n",
            "Epoch [460/20000], Training Loss: 0.0272\n",
            "Epoch [461/20000], Training Loss: 0.0251\n",
            "Epoch [462/20000], Training Loss: 0.0256\n",
            "Epoch [463/20000], Training Loss: 0.0271\n",
            "Epoch [464/20000], Training Loss: 0.0268\n",
            "Epoch [465/20000], Training Loss: 0.0276\n",
            "Epoch [466/20000], Training Loss: 0.0280\n",
            "Epoch [467/20000], Training Loss: 0.0250\n",
            "Epoch [468/20000], Training Loss: 0.0269\n",
            "Epoch [469/20000], Training Loss: 0.0267\n",
            "Epoch [470/20000], Training Loss: 0.0266\n",
            "Epoch [471/20000], Training Loss: 0.0281\n",
            "Epoch [472/20000], Training Loss: 0.0277\n",
            "Epoch [473/20000], Training Loss: 0.0261\n",
            "Epoch [474/20000], Training Loss: 0.0283\n",
            "Epoch [475/20000], Training Loss: 0.0261\n",
            "Epoch [476/20000], Training Loss: 0.0262\n",
            "Epoch [477/20000], Training Loss: 0.0290\n",
            "Epoch [478/20000], Training Loss: 0.0263\n",
            "Epoch [479/20000], Training Loss: 0.0269\n",
            "Epoch [480/20000], Training Loss: 0.0254\n",
            "Epoch [481/20000], Training Loss: 0.0288\n",
            "Epoch [482/20000], Training Loss: 0.0256\n",
            "Epoch [483/20000], Training Loss: 0.0268\n",
            "Epoch [484/20000], Training Loss: 0.0280\n",
            "Epoch [485/20000], Training Loss: 0.0263\n",
            "Epoch [486/20000], Training Loss: 0.0275\n",
            "Epoch [487/20000], Training Loss: 0.0268\n",
            "Epoch [488/20000], Training Loss: 0.0260\n",
            "Epoch [489/20000], Training Loss: 0.0264\n",
            "Epoch [490/20000], Training Loss: 0.0268\n",
            "Epoch [491/20000], Training Loss: 0.0282\n",
            "Epoch [492/20000], Training Loss: 0.0271\n",
            "Epoch [493/20000], Training Loss: 0.0268\n",
            "Epoch [494/20000], Training Loss: 0.0249\n",
            "Epoch [495/20000], Training Loss: 0.0283\n",
            "Epoch [496/20000], Training Loss: 0.0247\n",
            "Epoch [497/20000], Training Loss: 0.0262\n",
            "Epoch [498/20000], Training Loss: 0.0284\n",
            "Epoch [499/20000], Training Loss: 0.0266\n",
            "Epoch [500/20000], Training Loss: 0.0270\n",
            "Epoch [501/20000], Training Loss: 0.0269\n",
            "Epoch [502/20000], Training Loss: 0.0281\n",
            "Epoch [503/20000], Training Loss: 0.0264\n",
            "Epoch [504/20000], Training Loss: 0.0259\n",
            "Epoch [505/20000], Training Loss: 0.0255\n",
            "Epoch [506/20000], Training Loss: 0.0263\n",
            "Epoch [507/20000], Training Loss: 0.0273\n",
            "Epoch [508/20000], Training Loss: 0.0265\n",
            "Epoch [509/20000], Training Loss: 0.0271\n",
            "Epoch [510/20000], Training Loss: 0.0279\n",
            "Epoch [511/20000], Training Loss: 0.0293\n",
            "Epoch [512/20000], Training Loss: 0.0268\n",
            "Epoch [513/20000], Training Loss: 0.0282\n",
            "Epoch [514/20000], Training Loss: 0.0282\n",
            "Epoch [515/20000], Training Loss: 0.0298\n",
            "Epoch [516/20000], Training Loss: 0.0248\n",
            "Epoch [517/20000], Training Loss: 0.0260\n",
            "Epoch [518/20000], Training Loss: 0.0265\n",
            "Epoch [519/20000], Training Loss: 0.0259\n",
            "Epoch [520/20000], Training Loss: 0.0252\n",
            "Epoch [521/20000], Training Loss: 0.0259\n",
            "Epoch [522/20000], Training Loss: 0.0253\n",
            "Epoch [523/20000], Training Loss: 0.0284\n",
            "Epoch [524/20000], Training Loss: 0.0279\n",
            "Epoch [525/20000], Training Loss: 0.0268\n",
            "Epoch [526/20000], Training Loss: 0.0287\n",
            "Epoch [527/20000], Training Loss: 0.0267\n",
            "Epoch [528/20000], Training Loss: 0.0287\n",
            "Epoch [529/20000], Training Loss: 0.0276\n",
            "Epoch [530/20000], Training Loss: 0.0266\n",
            "Epoch [531/20000], Training Loss: 0.0259\n",
            "Epoch [532/20000], Training Loss: 0.0265\n",
            "Epoch [533/20000], Training Loss: 0.0251\n",
            "Epoch [534/20000], Training Loss: 0.0281\n",
            "Epoch [535/20000], Training Loss: 0.0287\n",
            "Epoch [536/20000], Training Loss: 0.0294\n",
            "Epoch [537/20000], Training Loss: 0.0255\n",
            "Epoch [538/20000], Training Loss: 0.0247\n",
            "Epoch [539/20000], Training Loss: 0.0255\n",
            "Epoch [540/20000], Training Loss: 0.0277\n",
            "Epoch [541/20000], Training Loss: 0.0251\n",
            "Epoch [542/20000], Training Loss: 0.0277\n",
            "Epoch [543/20000], Training Loss: 0.0264\n",
            "Epoch [544/20000], Training Loss: 0.0276\n",
            "Epoch [545/20000], Training Loss: 0.0248\n",
            "Epoch [546/20000], Training Loss: 0.0261\n",
            "Epoch [547/20000], Training Loss: 0.0278\n",
            "Epoch [548/20000], Training Loss: 0.0243\n",
            "Epoch [549/20000], Training Loss: 0.0265\n",
            "Epoch [550/20000], Training Loss: 0.0262\n",
            "Epoch [551/20000], Training Loss: 0.0244\n",
            "Epoch [552/20000], Training Loss: 0.0289\n",
            "Epoch [553/20000], Training Loss: 0.0266\n",
            "Epoch [554/20000], Training Loss: 0.0246\n",
            "Epoch [555/20000], Training Loss: 0.0277\n",
            "Epoch [556/20000], Training Loss: 0.0247\n",
            "Epoch [557/20000], Training Loss: 0.0265\n",
            "Epoch [558/20000], Training Loss: 0.0276\n",
            "Epoch [559/20000], Training Loss: 0.0275\n",
            "Epoch [560/20000], Training Loss: 0.0257\n",
            "Epoch [561/20000], Training Loss: 0.0260\n",
            "Epoch [562/20000], Training Loss: 0.0253\n",
            "Epoch [563/20000], Training Loss: 0.0284\n",
            "Epoch [564/20000], Training Loss: 0.0258\n",
            "Epoch [565/20000], Training Loss: 0.0288\n",
            "Epoch [566/20000], Training Loss: 0.0283\n",
            "Epoch [567/20000], Training Loss: 0.0262\n",
            "Epoch [568/20000], Training Loss: 0.0271\n",
            "Epoch [569/20000], Training Loss: 0.0266\n",
            "Epoch [570/20000], Training Loss: 0.0265\n",
            "Epoch [571/20000], Training Loss: 0.0269\n",
            "Epoch [572/20000], Training Loss: 0.0284\n",
            "Epoch [573/20000], Training Loss: 0.0270\n",
            "Epoch [574/20000], Training Loss: 0.0263\n",
            "Epoch [575/20000], Training Loss: 0.0252\n",
            "Epoch [576/20000], Training Loss: 0.0291\n",
            "Epoch [577/20000], Training Loss: 0.0277\n",
            "Epoch [578/20000], Training Loss: 0.0275\n",
            "Epoch [579/20000], Training Loss: 0.0262\n",
            "Epoch [580/20000], Training Loss: 0.0264\n",
            "Epoch [581/20000], Training Loss: 0.0258\n",
            "Epoch [582/20000], Training Loss: 0.0275\n",
            "Epoch [583/20000], Training Loss: 0.0272\n",
            "Epoch [584/20000], Training Loss: 0.0250\n",
            "Epoch [585/20000], Training Loss: 0.0263\n",
            "Epoch [586/20000], Training Loss: 0.0280\n",
            "Epoch [587/20000], Training Loss: 0.0254\n",
            "Epoch [588/20000], Training Loss: 0.0259\n",
            "Epoch [589/20000], Training Loss: 0.0265\n",
            "Epoch [590/20000], Training Loss: 0.0257\n",
            "Epoch [591/20000], Training Loss: 0.0245\n",
            "Epoch [592/20000], Training Loss: 0.0276\n",
            "Epoch [593/20000], Training Loss: 0.0268\n",
            "Epoch [594/20000], Training Loss: 0.0272\n",
            "Epoch [595/20000], Training Loss: 0.0246\n",
            "Epoch [596/20000], Training Loss: 0.0275\n",
            "Epoch [597/20000], Training Loss: 0.0262\n",
            "Epoch [598/20000], Training Loss: 0.0281\n",
            "Epoch [599/20000], Training Loss: 0.0260\n",
            "Epoch [600/20000], Training Loss: 0.0268\n",
            "Epoch [601/20000], Training Loss: 0.0281\n",
            "Epoch [602/20000], Training Loss: 0.0263\n",
            "Epoch [603/20000], Training Loss: 0.0263\n",
            "Epoch [604/20000], Training Loss: 0.0257\n",
            "Epoch [605/20000], Training Loss: 0.0288\n",
            "Epoch [606/20000], Training Loss: 0.0240\n",
            "Epoch [607/20000], Training Loss: 0.0254\n",
            "Epoch [608/20000], Training Loss: 0.0259\n",
            "Epoch [609/20000], Training Loss: 0.0257\n",
            "Epoch [610/20000], Training Loss: 0.0265\n",
            "Epoch [611/20000], Training Loss: 0.0261\n",
            "Epoch [612/20000], Training Loss: 0.0263\n",
            "Epoch [613/20000], Training Loss: 0.0252\n",
            "Epoch [614/20000], Training Loss: 0.0267\n",
            "Epoch [615/20000], Training Loss: 0.0274\n",
            "Epoch [616/20000], Training Loss: 0.0267\n",
            "Epoch [617/20000], Training Loss: 0.0265\n",
            "Epoch [618/20000], Training Loss: 0.0261\n",
            "Epoch [619/20000], Training Loss: 0.0276\n",
            "Epoch [620/20000], Training Loss: 0.0256\n",
            "Epoch [621/20000], Training Loss: 0.0252\n",
            "Epoch [622/20000], Training Loss: 0.0269\n",
            "Epoch [623/20000], Training Loss: 0.0266\n",
            "Epoch [624/20000], Training Loss: 0.0244\n",
            "Epoch [625/20000], Training Loss: 0.0287\n",
            "Epoch [626/20000], Training Loss: 0.0267\n",
            "Epoch [627/20000], Training Loss: 0.0283\n",
            "Epoch [628/20000], Training Loss: 0.0255\n",
            "Epoch [629/20000], Training Loss: 0.0287\n",
            "Epoch [630/20000], Training Loss: 0.0262\n",
            "Epoch [631/20000], Training Loss: 0.0251\n",
            "Epoch [632/20000], Training Loss: 0.0262\n",
            "Epoch [633/20000], Training Loss: 0.0274\n",
            "Epoch [634/20000], Training Loss: 0.0282\n",
            "Epoch [635/20000], Training Loss: 0.0280\n",
            "Epoch [636/20000], Training Loss: 0.0266\n",
            "Epoch [637/20000], Training Loss: 0.0261\n",
            "Epoch [638/20000], Training Loss: 0.0279\n",
            "Epoch [639/20000], Training Loss: 0.0271\n",
            "Epoch [640/20000], Training Loss: 0.0244\n",
            "Epoch [641/20000], Training Loss: 0.0280\n",
            "Epoch [642/20000], Training Loss: 0.0257\n",
            "Epoch [643/20000], Training Loss: 0.0281\n",
            "Epoch [644/20000], Training Loss: 0.0271\n",
            "Epoch [645/20000], Training Loss: 0.0282\n",
            "Epoch [646/20000], Training Loss: 0.0266\n",
            "Epoch [647/20000], Training Loss: 0.0252\n",
            "Epoch [648/20000], Training Loss: 0.0271\n",
            "Epoch [649/20000], Training Loss: 0.0271\n",
            "Epoch [650/20000], Training Loss: 0.0259\n",
            "Epoch [651/20000], Training Loss: 0.0267\n",
            "Epoch [652/20000], Training Loss: 0.0274\n",
            "Epoch [653/20000], Training Loss: 0.0265\n",
            "Epoch [654/20000], Training Loss: 0.0284\n",
            "Epoch [655/20000], Training Loss: 0.0271\n",
            "Epoch [656/20000], Training Loss: 0.0260\n",
            "Epoch [657/20000], Training Loss: 0.0257\n",
            "Epoch [658/20000], Training Loss: 0.0271\n",
            "Epoch [659/20000], Training Loss: 0.0277\n",
            "Epoch [660/20000], Training Loss: 0.0287\n",
            "Epoch [661/20000], Training Loss: 0.0266\n",
            "Epoch [662/20000], Training Loss: 0.0288\n",
            "Epoch [663/20000], Training Loss: 0.0275\n",
            "Epoch [664/20000], Training Loss: 0.0267\n",
            "Epoch [665/20000], Training Loss: 0.0265\n",
            "Epoch [666/20000], Training Loss: 0.0275\n",
            "Epoch [667/20000], Training Loss: 0.0265\n",
            "Epoch [668/20000], Training Loss: 0.0294\n",
            "Epoch [669/20000], Training Loss: 0.0284\n",
            "Epoch [670/20000], Training Loss: 0.0260\n",
            "Epoch [671/20000], Training Loss: 0.0248\n",
            "Epoch [672/20000], Training Loss: 0.0260\n",
            "Epoch [673/20000], Training Loss: 0.0264\n",
            "Epoch [674/20000], Training Loss: 0.0276\n",
            "Epoch [675/20000], Training Loss: 0.0271\n",
            "Epoch [676/20000], Training Loss: 0.0267\n",
            "Epoch [677/20000], Training Loss: 0.0250\n",
            "Epoch [678/20000], Training Loss: 0.0282\n",
            "Epoch [679/20000], Training Loss: 0.0274\n",
            "Epoch [680/20000], Training Loss: 0.0275\n",
            "Epoch [681/20000], Training Loss: 0.0261\n",
            "Epoch [682/20000], Training Loss: 0.0282\n",
            "Epoch [683/20000], Training Loss: 0.0262\n",
            "Epoch [684/20000], Training Loss: 0.0292\n",
            "Epoch [685/20000], Training Loss: 0.0275\n",
            "Epoch [686/20000], Training Loss: 0.0277\n",
            "Epoch [687/20000], Training Loss: 0.0277\n",
            "Epoch [688/20000], Training Loss: 0.0270\n",
            "Epoch [689/20000], Training Loss: 0.0263\n",
            "Epoch [690/20000], Training Loss: 0.0242\n",
            "Epoch [691/20000], Training Loss: 0.0271\n",
            "Epoch [692/20000], Training Loss: 0.0256\n",
            "Epoch [693/20000], Training Loss: 0.0268\n",
            "Epoch [694/20000], Training Loss: 0.0263\n",
            "Epoch [695/20000], Training Loss: 0.0249\n",
            "Epoch [696/20000], Training Loss: 0.0255\n",
            "Epoch [697/20000], Training Loss: 0.0255\n",
            "Epoch [698/20000], Training Loss: 0.0258\n",
            "Epoch [699/20000], Training Loss: 0.0247\n",
            "Epoch [700/20000], Training Loss: 0.0257\n",
            "Epoch [701/20000], Training Loss: 0.0260\n",
            "Epoch [702/20000], Training Loss: 0.0258\n",
            "Epoch [703/20000], Training Loss: 0.0285\n",
            "Epoch [704/20000], Training Loss: 0.0249\n",
            "Epoch [705/20000], Training Loss: 0.0247\n",
            "Epoch [706/20000], Training Loss: 0.0263\n",
            "Epoch [707/20000], Training Loss: 0.0264\n",
            "Epoch [708/20000], Training Loss: 0.0267\n",
            "Epoch [709/20000], Training Loss: 0.0269\n",
            "Epoch [710/20000], Training Loss: 0.0264\n",
            "Epoch [711/20000], Training Loss: 0.0249\n",
            "Epoch [712/20000], Training Loss: 0.0274\n",
            "Epoch [713/20000], Training Loss: 0.0270\n",
            "Epoch [714/20000], Training Loss: 0.0249\n",
            "Epoch [715/20000], Training Loss: 0.0262\n",
            "Epoch [716/20000], Training Loss: 0.0267\n",
            "Epoch [717/20000], Training Loss: 0.0248\n",
            "Epoch [718/20000], Training Loss: 0.0264\n",
            "Epoch [719/20000], Training Loss: 0.0276\n",
            "Epoch [720/20000], Training Loss: 0.0281\n",
            "Epoch [721/20000], Training Loss: 0.0254\n",
            "Epoch [722/20000], Training Loss: 0.0268\n",
            "Epoch [723/20000], Training Loss: 0.0253\n",
            "Epoch [724/20000], Training Loss: 0.0295\n",
            "Epoch [725/20000], Training Loss: 0.0255\n",
            "Epoch [726/20000], Training Loss: 0.0287\n",
            "Epoch [727/20000], Training Loss: 0.0279\n",
            "Epoch [728/20000], Training Loss: 0.0280\n",
            "Epoch [729/20000], Training Loss: 0.0261\n",
            "Epoch [730/20000], Training Loss: 0.0259\n",
            "Epoch [731/20000], Training Loss: 0.0253\n",
            "Epoch [732/20000], Training Loss: 0.0272\n",
            "Epoch [733/20000], Training Loss: 0.0253\n",
            "Epoch [734/20000], Training Loss: 0.0255\n",
            "Epoch [735/20000], Training Loss: 0.0262\n",
            "Epoch [736/20000], Training Loss: 0.0278\n",
            "Epoch [737/20000], Training Loss: 0.0273\n",
            "Epoch [738/20000], Training Loss: 0.0250\n",
            "Epoch [739/20000], Training Loss: 0.0274\n",
            "Epoch [740/20000], Training Loss: 0.0252\n",
            "Epoch [741/20000], Training Loss: 0.0270\n",
            "Epoch [742/20000], Training Loss: 0.0262\n",
            "Epoch [743/20000], Training Loss: 0.0272\n",
            "Epoch [744/20000], Training Loss: 0.0266\n",
            "Epoch [745/20000], Training Loss: 0.0263\n",
            "Epoch [746/20000], Training Loss: 0.0260\n",
            "Epoch [747/20000], Training Loss: 0.0275\n",
            "Epoch [748/20000], Training Loss: 0.0280\n",
            "Epoch [749/20000], Training Loss: 0.0261\n",
            "Epoch [750/20000], Training Loss: 0.0273\n",
            "Epoch [751/20000], Training Loss: 0.0257\n",
            "Epoch [752/20000], Training Loss: 0.0275\n",
            "Epoch [753/20000], Training Loss: 0.0261\n",
            "Epoch [754/20000], Training Loss: 0.0252\n",
            "Epoch [755/20000], Training Loss: 0.0262\n",
            "Epoch [756/20000], Training Loss: 0.0239\n",
            "Epoch [757/20000], Training Loss: 0.0251\n",
            "Epoch [758/20000], Training Loss: 0.0251\n",
            "Epoch [759/20000], Training Loss: 0.0243\n",
            "Epoch [760/20000], Training Loss: 0.0270\n",
            "Epoch [761/20000], Training Loss: 0.0242\n",
            "Epoch [762/20000], Training Loss: 0.0286\n",
            "Epoch [763/20000], Training Loss: 0.0276\n",
            "Epoch [764/20000], Training Loss: 0.0256\n",
            "Epoch [765/20000], Training Loss: 0.0266\n",
            "Epoch [766/20000], Training Loss: 0.0272\n",
            "Epoch [767/20000], Training Loss: 0.0255\n",
            "Epoch [768/20000], Training Loss: 0.0282\n",
            "Epoch [769/20000], Training Loss: 0.0254\n",
            "Epoch [770/20000], Training Loss: 0.0241\n",
            "Epoch [771/20000], Training Loss: 0.0249\n",
            "Epoch [772/20000], Training Loss: 0.0274\n",
            "Epoch [773/20000], Training Loss: 0.0249\n",
            "Epoch [774/20000], Training Loss: 0.0259\n",
            "Epoch [775/20000], Training Loss: 0.0259\n",
            "Epoch [776/20000], Training Loss: 0.0269\n",
            "Epoch [777/20000], Training Loss: 0.0249\n",
            "Epoch [778/20000], Training Loss: 0.0263\n",
            "Epoch [779/20000], Training Loss: 0.0272\n",
            "Epoch [780/20000], Training Loss: 0.0247\n",
            "Epoch [781/20000], Training Loss: 0.0246\n",
            "Epoch [782/20000], Training Loss: 0.0290\n",
            "Epoch [783/20000], Training Loss: 0.0271\n",
            "Epoch [784/20000], Training Loss: 0.0267\n",
            "Epoch [785/20000], Training Loss: 0.0279\n",
            "Epoch [786/20000], Training Loss: 0.0264\n",
            "Epoch [787/20000], Training Loss: 0.0250\n",
            "Epoch [788/20000], Training Loss: 0.0288\n",
            "Epoch [789/20000], Training Loss: 0.0258\n",
            "Epoch [790/20000], Training Loss: 0.0274\n",
            "Epoch [791/20000], Training Loss: 0.0269\n",
            "Epoch [792/20000], Training Loss: 0.0270\n",
            "Epoch [793/20000], Training Loss: 0.0248\n",
            "Epoch [794/20000], Training Loss: 0.0275\n",
            "Epoch [795/20000], Training Loss: 0.0258\n",
            "Epoch [796/20000], Training Loss: 0.0280\n",
            "Epoch [797/20000], Training Loss: 0.0280\n",
            "Epoch [798/20000], Training Loss: 0.0270\n",
            "Epoch [799/20000], Training Loss: 0.0277\n",
            "Epoch [800/20000], Training Loss: 0.0276\n",
            "Epoch [801/20000], Training Loss: 0.0276\n",
            "Epoch [802/20000], Training Loss: 0.0257\n",
            "Epoch [803/20000], Training Loss: 0.0281\n",
            "Epoch [804/20000], Training Loss: 0.0272\n",
            "Epoch [805/20000], Training Loss: 0.0276\n",
            "Epoch [806/20000], Training Loss: 0.0264\n",
            "Epoch [807/20000], Training Loss: 0.0244\n",
            "Epoch [808/20000], Training Loss: 0.0257\n",
            "Epoch [809/20000], Training Loss: 0.0272\n",
            "Epoch [810/20000], Training Loss: 0.0297\n",
            "Epoch [811/20000], Training Loss: 0.0268\n",
            "Epoch [812/20000], Training Loss: 0.0274\n",
            "Epoch [813/20000], Training Loss: 0.0257\n",
            "Epoch [814/20000], Training Loss: 0.0264\n",
            "Epoch [815/20000], Training Loss: 0.0262\n",
            "Epoch [816/20000], Training Loss: 0.0281\n",
            "Epoch [817/20000], Training Loss: 0.0256\n",
            "Epoch [818/20000], Training Loss: 0.0260\n",
            "Epoch [819/20000], Training Loss: 0.0277\n",
            "Epoch [820/20000], Training Loss: 0.0266\n",
            "Epoch [821/20000], Training Loss: 0.0273\n",
            "Epoch [822/20000], Training Loss: 0.0255\n",
            "Epoch [823/20000], Training Loss: 0.0256\n",
            "Epoch [824/20000], Training Loss: 0.0273\n",
            "Epoch [825/20000], Training Loss: 0.0279\n",
            "Epoch [826/20000], Training Loss: 0.0248\n",
            "Epoch [827/20000], Training Loss: 0.0249\n",
            "Epoch [828/20000], Training Loss: 0.0256\n",
            "Epoch [829/20000], Training Loss: 0.0264\n",
            "Epoch [830/20000], Training Loss: 0.0269\n",
            "Epoch [831/20000], Training Loss: 0.0289\n",
            "Epoch [832/20000], Training Loss: 0.0253\n",
            "Epoch [833/20000], Training Loss: 0.0253\n",
            "Epoch [834/20000], Training Loss: 0.0288\n",
            "Epoch [835/20000], Training Loss: 0.0282\n",
            "Epoch [836/20000], Training Loss: 0.0253\n",
            "Epoch [837/20000], Training Loss: 0.0289\n",
            "Epoch [838/20000], Training Loss: 0.0259\n",
            "Epoch [839/20000], Training Loss: 0.0270\n",
            "Epoch [840/20000], Training Loss: 0.0288\n",
            "Epoch [841/20000], Training Loss: 0.0276\n",
            "Epoch [842/20000], Training Loss: 0.0256\n",
            "Epoch [843/20000], Training Loss: 0.0277\n",
            "Epoch [844/20000], Training Loss: 0.0250\n",
            "Epoch [845/20000], Training Loss: 0.0295\n",
            "Epoch [846/20000], Training Loss: 0.0274\n",
            "Epoch [847/20000], Training Loss: 0.0257\n",
            "Epoch [848/20000], Training Loss: 0.0260\n",
            "Epoch [849/20000], Training Loss: 0.0273\n",
            "Epoch [850/20000], Training Loss: 0.0272\n",
            "Epoch [851/20000], Training Loss: 0.0286\n",
            "Epoch [852/20000], Training Loss: 0.0281\n",
            "Epoch [853/20000], Training Loss: 0.0260\n",
            "Epoch [854/20000], Training Loss: 0.0253\n",
            "Epoch [855/20000], Training Loss: 0.0274\n",
            "Epoch [856/20000], Training Loss: 0.0286\n",
            "Epoch [857/20000], Training Loss: 0.0260\n",
            "Epoch [858/20000], Training Loss: 0.0262\n",
            "Epoch [859/20000], Training Loss: 0.0250\n",
            "Epoch [860/20000], Training Loss: 0.0293\n",
            "Epoch [861/20000], Training Loss: 0.0256\n",
            "Epoch [862/20000], Training Loss: 0.0269\n",
            "Epoch [863/20000], Training Loss: 0.0272\n",
            "Epoch [864/20000], Training Loss: 0.0256\n",
            "Epoch [865/20000], Training Loss: 0.0260\n",
            "Epoch [866/20000], Training Loss: 0.0278\n",
            "Epoch [867/20000], Training Loss: 0.0267\n",
            "Epoch [868/20000], Training Loss: 0.0257\n",
            "Epoch [869/20000], Training Loss: 0.0279\n",
            "Epoch [870/20000], Training Loss: 0.0262\n",
            "Epoch [871/20000], Training Loss: 0.0269\n",
            "Epoch [872/20000], Training Loss: 0.0285\n",
            "Epoch [873/20000], Training Loss: 0.0269\n",
            "Epoch [874/20000], Training Loss: 0.0266\n",
            "Epoch [875/20000], Training Loss: 0.0249\n",
            "Epoch [876/20000], Training Loss: 0.0256\n",
            "Epoch [877/20000], Training Loss: 0.0250\n",
            "Epoch [878/20000], Training Loss: 0.0249\n",
            "Epoch [879/20000], Training Loss: 0.0278\n",
            "Epoch [880/20000], Training Loss: 0.0268\n",
            "Epoch [881/20000], Training Loss: 0.0269\n",
            "Epoch [882/20000], Training Loss: 0.0252\n",
            "Epoch [883/20000], Training Loss: 0.0275\n",
            "Epoch [884/20000], Training Loss: 0.0247\n",
            "Epoch [885/20000], Training Loss: 0.0257\n",
            "Epoch [886/20000], Training Loss: 0.0271\n",
            "Epoch [887/20000], Training Loss: 0.0253\n",
            "Epoch [888/20000], Training Loss: 0.0278\n",
            "Epoch [889/20000], Training Loss: 0.0248\n",
            "Epoch [890/20000], Training Loss: 0.0289\n",
            "Epoch [891/20000], Training Loss: 0.0256\n",
            "Epoch [892/20000], Training Loss: 0.0250\n",
            "Epoch [893/20000], Training Loss: 0.0262\n",
            "Epoch [894/20000], Training Loss: 0.0253\n",
            "Epoch [895/20000], Training Loss: 0.0269\n",
            "Epoch [896/20000], Training Loss: 0.0277\n",
            "Epoch [897/20000], Training Loss: 0.0257\n",
            "Epoch [898/20000], Training Loss: 0.0297\n",
            "Epoch [899/20000], Training Loss: 0.0273\n",
            "Epoch [900/20000], Training Loss: 0.0274\n",
            "Epoch [901/20000], Training Loss: 0.0252\n",
            "Epoch [902/20000], Training Loss: 0.0269\n",
            "Epoch [903/20000], Training Loss: 0.0269\n",
            "Epoch [904/20000], Training Loss: 0.0248\n",
            "Epoch [905/20000], Training Loss: 0.0270\n",
            "Epoch [906/20000], Training Loss: 0.0268\n",
            "Epoch [907/20000], Training Loss: 0.0280\n",
            "Epoch [908/20000], Training Loss: 0.0240\n",
            "Epoch [909/20000], Training Loss: 0.0261\n",
            "Epoch [910/20000], Training Loss: 0.0266\n",
            "Epoch [911/20000], Training Loss: 0.0254\n",
            "Epoch [912/20000], Training Loss: 0.0272\n",
            "Epoch [913/20000], Training Loss: 0.0283\n",
            "Epoch [914/20000], Training Loss: 0.0268\n",
            "Epoch [915/20000], Training Loss: 0.0281\n",
            "Epoch [916/20000], Training Loss: 0.0236\n",
            "Epoch [917/20000], Training Loss: 0.0258\n",
            "Epoch [918/20000], Training Loss: 0.0287\n",
            "Epoch [919/20000], Training Loss: 0.0255\n",
            "Epoch [920/20000], Training Loss: 0.0262\n",
            "Epoch [921/20000], Training Loss: 0.0286\n",
            "Epoch [922/20000], Training Loss: 0.0274\n",
            "Epoch [923/20000], Training Loss: 0.0269\n",
            "Epoch [924/20000], Training Loss: 0.0273\n",
            "Epoch [925/20000], Training Loss: 0.0265\n",
            "Epoch [926/20000], Training Loss: 0.0262\n",
            "Epoch [927/20000], Training Loss: 0.0284\n",
            "Epoch [928/20000], Training Loss: 0.0281\n",
            "Epoch [929/20000], Training Loss: 0.0244\n",
            "Epoch [930/20000], Training Loss: 0.0278\n",
            "Epoch [931/20000], Training Loss: 0.0281\n",
            "Epoch [932/20000], Training Loss: 0.0263\n",
            "Epoch [933/20000], Training Loss: 0.0258\n",
            "Epoch [934/20000], Training Loss: 0.0250\n",
            "Epoch [935/20000], Training Loss: 0.0258\n",
            "Epoch [936/20000], Training Loss: 0.0266\n",
            "Epoch [937/20000], Training Loss: 0.0265\n",
            "Epoch [938/20000], Training Loss: 0.0274\n",
            "Epoch [939/20000], Training Loss: 0.0251\n",
            "Epoch [940/20000], Training Loss: 0.0256\n",
            "Epoch [941/20000], Training Loss: 0.0257\n",
            "Epoch [942/20000], Training Loss: 0.0246\n",
            "Epoch [943/20000], Training Loss: 0.0274\n",
            "Epoch [944/20000], Training Loss: 0.0262\n",
            "Epoch [945/20000], Training Loss: 0.0257\n",
            "Epoch [946/20000], Training Loss: 0.0264\n",
            "Epoch [947/20000], Training Loss: 0.0285\n",
            "Epoch [948/20000], Training Loss: 0.0258\n",
            "Epoch [949/20000], Training Loss: 0.0262\n",
            "Epoch [950/20000], Training Loss: 0.0283\n",
            "Epoch [951/20000], Training Loss: 0.0277\n",
            "Epoch [952/20000], Training Loss: 0.0245\n",
            "Epoch [953/20000], Training Loss: 0.0252\n",
            "Epoch [954/20000], Training Loss: 0.0285\n",
            "Epoch [955/20000], Training Loss: 0.0257\n",
            "Epoch [956/20000], Training Loss: 0.0253\n",
            "Epoch [957/20000], Training Loss: 0.0246\n",
            "Epoch [958/20000], Training Loss: 0.0266\n",
            "Epoch [959/20000], Training Loss: 0.0269\n",
            "Epoch [960/20000], Training Loss: 0.0267\n",
            "Epoch [961/20000], Training Loss: 0.0280\n",
            "Epoch [962/20000], Training Loss: 0.0264\n",
            "Epoch [963/20000], Training Loss: 0.0280\n",
            "Epoch [964/20000], Training Loss: 0.0257\n",
            "Epoch [965/20000], Training Loss: 0.0266\n",
            "Epoch [966/20000], Training Loss: 0.0275\n",
            "Epoch [967/20000], Training Loss: 0.0259\n",
            "Epoch [968/20000], Training Loss: 0.0266\n",
            "Epoch [969/20000], Training Loss: 0.0268\n",
            "Epoch [970/20000], Training Loss: 0.0264\n",
            "Epoch [971/20000], Training Loss: 0.0260\n",
            "Epoch [972/20000], Training Loss: 0.0257\n",
            "Epoch [973/20000], Training Loss: 0.0274\n",
            "Epoch [974/20000], Training Loss: 0.0254\n",
            "Epoch [975/20000], Training Loss: 0.0277\n",
            "Epoch [976/20000], Training Loss: 0.0268\n",
            "Epoch [977/20000], Training Loss: 0.0265\n",
            "Epoch [978/20000], Training Loss: 0.0284\n",
            "Epoch [979/20000], Training Loss: 0.0249\n",
            "Epoch [980/20000], Training Loss: 0.0275\n",
            "Epoch [981/20000], Training Loss: 0.0267\n",
            "Epoch [982/20000], Training Loss: 0.0274\n",
            "Epoch [983/20000], Training Loss: 0.0282\n",
            "Epoch [984/20000], Training Loss: 0.0271\n",
            "Epoch [985/20000], Training Loss: 0.0242\n",
            "Epoch [986/20000], Training Loss: 0.0256\n",
            "Epoch [987/20000], Training Loss: 0.0262\n",
            "Epoch [988/20000], Training Loss: 0.0270\n",
            "Epoch [989/20000], Training Loss: 0.0262\n",
            "Epoch [990/20000], Training Loss: 0.0273\n",
            "Epoch [991/20000], Training Loss: 0.0260\n",
            "Epoch [992/20000], Training Loss: 0.0274\n",
            "Epoch [993/20000], Training Loss: 0.0279\n",
            "Epoch [994/20000], Training Loss: 0.0268\n",
            "Epoch [995/20000], Training Loss: 0.0262\n",
            "Epoch [996/20000], Training Loss: 0.0275\n",
            "Epoch [997/20000], Training Loss: 0.0272\n",
            "Epoch [998/20000], Training Loss: 0.0279\n",
            "Epoch [999/20000], Training Loss: 0.0260\n",
            "Epoch [1000/20000], Training Loss: 0.0260\n",
            "Epoch [1001/20000], Training Loss: 0.0247\n",
            "Epoch [1002/20000], Training Loss: 0.0256\n",
            "Epoch [1003/20000], Training Loss: 0.0288\n",
            "Epoch [1004/20000], Training Loss: 0.0272\n",
            "Epoch [1005/20000], Training Loss: 0.0278\n",
            "Epoch [1006/20000], Training Loss: 0.0268\n",
            "Epoch [1007/20000], Training Loss: 0.0282\n",
            "Epoch [1008/20000], Training Loss: 0.0270\n",
            "Epoch [1009/20000], Training Loss: 0.0271\n",
            "Epoch [1010/20000], Training Loss: 0.0261\n",
            "Epoch [1011/20000], Training Loss: 0.0277\n",
            "Epoch [1012/20000], Training Loss: 0.0277\n",
            "Epoch [1013/20000], Training Loss: 0.0237\n",
            "Epoch [1014/20000], Training Loss: 0.0256\n",
            "Epoch [1015/20000], Training Loss: 0.0280\n",
            "Epoch [1016/20000], Training Loss: 0.0265\n",
            "Epoch [1017/20000], Training Loss: 0.0236\n",
            "Epoch [1018/20000], Training Loss: 0.0274\n",
            "Epoch [1019/20000], Training Loss: 0.0264\n",
            "Epoch [1020/20000], Training Loss: 0.0261\n",
            "Epoch [1021/20000], Training Loss: 0.0257\n",
            "Epoch [1022/20000], Training Loss: 0.0265\n",
            "Epoch [1023/20000], Training Loss: 0.0269\n",
            "Epoch [1024/20000], Training Loss: 0.0270\n",
            "Epoch [1025/20000], Training Loss: 0.0258\n",
            "Epoch [1026/20000], Training Loss: 0.0268\n",
            "Epoch [1027/20000], Training Loss: 0.0275\n",
            "Epoch [1028/20000], Training Loss: 0.0271\n",
            "Epoch [1029/20000], Training Loss: 0.0258\n",
            "Epoch [1030/20000], Training Loss: 0.0255\n",
            "Epoch [1031/20000], Training Loss: 0.0250\n",
            "Epoch [1032/20000], Training Loss: 0.0270\n",
            "Epoch [1033/20000], Training Loss: 0.0261\n",
            "Epoch [1034/20000], Training Loss: 0.0264\n",
            "Epoch [1035/20000], Training Loss: 0.0272\n",
            "Epoch [1036/20000], Training Loss: 0.0256\n",
            "Epoch [1037/20000], Training Loss: 0.0281\n",
            "Epoch [1038/20000], Training Loss: 0.0269\n",
            "Epoch [1039/20000], Training Loss: 0.0258\n",
            "Epoch [1040/20000], Training Loss: 0.0262\n",
            "Epoch [1041/20000], Training Loss: 0.0245\n",
            "Epoch [1042/20000], Training Loss: 0.0278\n",
            "Epoch [1043/20000], Training Loss: 0.0273\n",
            "Epoch [1044/20000], Training Loss: 0.0258\n",
            "Epoch [1045/20000], Training Loss: 0.0244\n",
            "Epoch [1046/20000], Training Loss: 0.0284\n",
            "Epoch [1047/20000], Training Loss: 0.0281\n",
            "Epoch [1048/20000], Training Loss: 0.0277\n",
            "Epoch [1049/20000], Training Loss: 0.0272\n",
            "Epoch [1050/20000], Training Loss: 0.0266\n",
            "Epoch [1051/20000], Training Loss: 0.0265\n",
            "Epoch [1052/20000], Training Loss: 0.0250\n",
            "Epoch [1053/20000], Training Loss: 0.0270\n",
            "Epoch [1054/20000], Training Loss: 0.0252\n",
            "Epoch [1055/20000], Training Loss: 0.0256\n",
            "Epoch [1056/20000], Training Loss: 0.0248\n",
            "Epoch [1057/20000], Training Loss: 0.0266\n",
            "Epoch [1058/20000], Training Loss: 0.0257\n",
            "Epoch [1059/20000], Training Loss: 0.0271\n",
            "Epoch [1060/20000], Training Loss: 0.0263\n",
            "Epoch [1061/20000], Training Loss: 0.0253\n",
            "Epoch [1062/20000], Training Loss: 0.0257\n",
            "Epoch [1063/20000], Training Loss: 0.0274\n",
            "Epoch [1064/20000], Training Loss: 0.0284\n",
            "Epoch [1065/20000], Training Loss: 0.0283\n",
            "Epoch [1066/20000], Training Loss: 0.0270\n",
            "Epoch [1067/20000], Training Loss: 0.0251\n",
            "Epoch [1068/20000], Training Loss: 0.0268\n",
            "Epoch [1069/20000], Training Loss: 0.0262\n",
            "Epoch [1070/20000], Training Loss: 0.0267\n",
            "Epoch [1071/20000], Training Loss: 0.0253\n",
            "Epoch [1072/20000], Training Loss: 0.0259\n",
            "Epoch [1073/20000], Training Loss: 0.0288\n",
            "Epoch [1074/20000], Training Loss: 0.0278\n",
            "Epoch [1075/20000], Training Loss: 0.0267\n",
            "Epoch [1076/20000], Training Loss: 0.0288\n",
            "Epoch [1077/20000], Training Loss: 0.0265\n",
            "Epoch [1078/20000], Training Loss: 0.0262\n",
            "Epoch [1079/20000], Training Loss: 0.0264\n",
            "Epoch [1080/20000], Training Loss: 0.0288\n",
            "Epoch [1081/20000], Training Loss: 0.0256\n",
            "Epoch [1082/20000], Training Loss: 0.0250\n",
            "Epoch [1083/20000], Training Loss: 0.0273\n",
            "Epoch [1084/20000], Training Loss: 0.0270\n",
            "Epoch [1085/20000], Training Loss: 0.0251\n",
            "Epoch [1086/20000], Training Loss: 0.0263\n",
            "Epoch [1087/20000], Training Loss: 0.0253\n",
            "Epoch [1088/20000], Training Loss: 0.0275\n",
            "Epoch [1089/20000], Training Loss: 0.0272\n",
            "Epoch [1090/20000], Training Loss: 0.0259\n",
            "Epoch [1091/20000], Training Loss: 0.0274\n",
            "Epoch [1092/20000], Training Loss: 0.0267\n",
            "Epoch [1093/20000], Training Loss: 0.0267\n",
            "Epoch [1094/20000], Training Loss: 0.0269\n",
            "Epoch [1095/20000], Training Loss: 0.0262\n",
            "Epoch [1096/20000], Training Loss: 0.0257\n",
            "Epoch [1097/20000], Training Loss: 0.0259\n",
            "Epoch [1098/20000], Training Loss: 0.0262\n",
            "Epoch [1099/20000], Training Loss: 0.0285\n",
            "Epoch [1100/20000], Training Loss: 0.0242\n",
            "Epoch [1101/20000], Training Loss: 0.0261\n",
            "Epoch [1102/20000], Training Loss: 0.0265\n",
            "Epoch [1103/20000], Training Loss: 0.0268\n",
            "Epoch [1104/20000], Training Loss: 0.0300\n",
            "Epoch [1105/20000], Training Loss: 0.0248\n",
            "Epoch [1106/20000], Training Loss: 0.0268\n",
            "Epoch [1107/20000], Training Loss: 0.0285\n",
            "Epoch [1108/20000], Training Loss: 0.0265\n",
            "Epoch [1109/20000], Training Loss: 0.0240\n",
            "Epoch [1110/20000], Training Loss: 0.0271\n",
            "Epoch [1111/20000], Training Loss: 0.0291\n",
            "Epoch [1112/20000], Training Loss: 0.0287\n",
            "Epoch [1113/20000], Training Loss: 0.0289\n",
            "Epoch [1114/20000], Training Loss: 0.0253\n",
            "Epoch [1115/20000], Training Loss: 0.0264\n",
            "Epoch [1116/20000], Training Loss: 0.0259\n",
            "Epoch [1117/20000], Training Loss: 0.0268\n",
            "Epoch [1118/20000], Training Loss: 0.0269\n",
            "Epoch [1119/20000], Training Loss: 0.0272\n",
            "Epoch [1120/20000], Training Loss: 0.0274\n",
            "Epoch [1121/20000], Training Loss: 0.0254\n",
            "Epoch [1122/20000], Training Loss: 0.0257\n",
            "Epoch [1123/20000], Training Loss: 0.0245\n",
            "Epoch [1124/20000], Training Loss: 0.0277\n",
            "Epoch [1125/20000], Training Loss: 0.0251\n",
            "Epoch [1126/20000], Training Loss: 0.0263\n",
            "Epoch [1127/20000], Training Loss: 0.0272\n",
            "Epoch [1128/20000], Training Loss: 0.0278\n",
            "Epoch [1129/20000], Training Loss: 0.0267\n",
            "Epoch [1130/20000], Training Loss: 0.0242\n",
            "Epoch [1131/20000], Training Loss: 0.0270\n",
            "Epoch [1132/20000], Training Loss: 0.0242\n",
            "Epoch [1133/20000], Training Loss: 0.0270\n",
            "Epoch [1134/20000], Training Loss: 0.0279\n",
            "Epoch [1135/20000], Training Loss: 0.0273\n",
            "Epoch [1136/20000], Training Loss: 0.0292\n",
            "Epoch [1137/20000], Training Loss: 0.0270\n",
            "Epoch [1138/20000], Training Loss: 0.0267\n",
            "Epoch [1139/20000], Training Loss: 0.0245\n",
            "Epoch [1140/20000], Training Loss: 0.0265\n",
            "Epoch [1141/20000], Training Loss: 0.0264\n",
            "Epoch [1142/20000], Training Loss: 0.0282\n",
            "Epoch [1143/20000], Training Loss: 0.0257\n",
            "Epoch [1144/20000], Training Loss: 0.0262\n",
            "Epoch [1145/20000], Training Loss: 0.0273\n",
            "Epoch [1146/20000], Training Loss: 0.0286\n",
            "Epoch [1147/20000], Training Loss: 0.0287\n",
            "Epoch [1148/20000], Training Loss: 0.0282\n",
            "Epoch [1149/20000], Training Loss: 0.0267\n",
            "Epoch [1150/20000], Training Loss: 0.0264\n",
            "Epoch [1151/20000], Training Loss: 0.0274\n",
            "Epoch [1152/20000], Training Loss: 0.0261\n",
            "Epoch [1153/20000], Training Loss: 0.0256\n",
            "Epoch [1154/20000], Training Loss: 0.0267\n",
            "Epoch [1155/20000], Training Loss: 0.0262\n",
            "Epoch [1156/20000], Training Loss: 0.0254\n",
            "Epoch [1157/20000], Training Loss: 0.0261\n",
            "Epoch [1158/20000], Training Loss: 0.0267\n",
            "Epoch [1159/20000], Training Loss: 0.0260\n",
            "Epoch [1160/20000], Training Loss: 0.0261\n",
            "Epoch [1161/20000], Training Loss: 0.0283\n",
            "Epoch [1162/20000], Training Loss: 0.0269\n",
            "Epoch [1163/20000], Training Loss: 0.0259\n",
            "Epoch [1164/20000], Training Loss: 0.0279\n",
            "Epoch [1165/20000], Training Loss: 0.0255\n",
            "Epoch [1166/20000], Training Loss: 0.0239\n",
            "Epoch [1167/20000], Training Loss: 0.0297\n",
            "Epoch [1168/20000], Training Loss: 0.0276\n",
            "Epoch [1169/20000], Training Loss: 0.0268\n",
            "Epoch [1170/20000], Training Loss: 0.0271\n",
            "Epoch [1171/20000], Training Loss: 0.0258\n",
            "Epoch [1172/20000], Training Loss: 0.0261\n",
            "Epoch [1173/20000], Training Loss: 0.0257\n",
            "Epoch [1174/20000], Training Loss: 0.0290\n",
            "Epoch [1175/20000], Training Loss: 0.0262\n",
            "Epoch [1176/20000], Training Loss: 0.0265\n",
            "Epoch [1177/20000], Training Loss: 0.0257\n",
            "Epoch [1178/20000], Training Loss: 0.0264\n",
            "Epoch [1179/20000], Training Loss: 0.0252\n",
            "Epoch [1180/20000], Training Loss: 0.0278\n",
            "Epoch [1181/20000], Training Loss: 0.0266\n",
            "Epoch [1182/20000], Training Loss: 0.0248\n",
            "Epoch [1183/20000], Training Loss: 0.0250\n",
            "Epoch [1184/20000], Training Loss: 0.0283\n",
            "Epoch [1185/20000], Training Loss: 0.0270\n",
            "Epoch [1186/20000], Training Loss: 0.0270\n",
            "Epoch [1187/20000], Training Loss: 0.0282\n",
            "Epoch [1188/20000], Training Loss: 0.0246\n",
            "Epoch [1189/20000], Training Loss: 0.0263\n",
            "Epoch [1190/20000], Training Loss: 0.0257\n",
            "Epoch [1191/20000], Training Loss: 0.0256\n",
            "Epoch [1192/20000], Training Loss: 0.0277\n",
            "Epoch [1193/20000], Training Loss: 0.0253\n",
            "Epoch [1194/20000], Training Loss: 0.0259\n",
            "Epoch [1195/20000], Training Loss: 0.0281\n",
            "Epoch [1196/20000], Training Loss: 0.0261\n",
            "Epoch [1197/20000], Training Loss: 0.0247\n",
            "Epoch [1198/20000], Training Loss: 0.0242\n",
            "Epoch [1199/20000], Training Loss: 0.0272\n",
            "Epoch [1200/20000], Training Loss: 0.0261\n",
            "Epoch [1201/20000], Training Loss: 0.0241\n",
            "Epoch [1202/20000], Training Loss: 0.0263\n",
            "Epoch [1203/20000], Training Loss: 0.0271\n",
            "Epoch [1204/20000], Training Loss: 0.0256\n",
            "Epoch [1205/20000], Training Loss: 0.0265\n",
            "Epoch [1206/20000], Training Loss: 0.0265\n",
            "Epoch [1207/20000], Training Loss: 0.0245\n",
            "Epoch [1208/20000], Training Loss: 0.0270\n",
            "Epoch [1209/20000], Training Loss: 0.0262\n",
            "Epoch [1210/20000], Training Loss: 0.0253\n",
            "Epoch [1211/20000], Training Loss: 0.0275\n",
            "Epoch [1212/20000], Training Loss: 0.0252\n",
            "Epoch [1213/20000], Training Loss: 0.0261\n",
            "Epoch [1214/20000], Training Loss: 0.0275\n",
            "Epoch [1215/20000], Training Loss: 0.0260\n",
            "Epoch [1216/20000], Training Loss: 0.0285\n",
            "Epoch [1217/20000], Training Loss: 0.0260\n",
            "Epoch [1218/20000], Training Loss: 0.0270\n",
            "Epoch [1219/20000], Training Loss: 0.0282\n",
            "Epoch [1220/20000], Training Loss: 0.0270\n",
            "Epoch [1221/20000], Training Loss: 0.0244\n",
            "Epoch [1222/20000], Training Loss: 0.0278\n",
            "Epoch [1223/20000], Training Loss: 0.0249\n",
            "Epoch [1224/20000], Training Loss: 0.0269\n",
            "Epoch [1225/20000], Training Loss: 0.0264\n",
            "Epoch [1226/20000], Training Loss: 0.0261\n",
            "Epoch [1227/20000], Training Loss: 0.0263\n",
            "Epoch [1228/20000], Training Loss: 0.0267\n",
            "Epoch [1229/20000], Training Loss: 0.0279\n",
            "Epoch [1230/20000], Training Loss: 0.0248\n",
            "Epoch [1231/20000], Training Loss: 0.0259\n",
            "Epoch [1232/20000], Training Loss: 0.0252\n",
            "Epoch [1233/20000], Training Loss: 0.0254\n",
            "Epoch [1234/20000], Training Loss: 0.0260\n",
            "Epoch [1235/20000], Training Loss: 0.0264\n",
            "Epoch [1236/20000], Training Loss: 0.0286\n",
            "Epoch [1237/20000], Training Loss: 0.0260\n",
            "Epoch [1238/20000], Training Loss: 0.0272\n",
            "Epoch [1239/20000], Training Loss: 0.0267\n",
            "Epoch [1240/20000], Training Loss: 0.0245\n",
            "Epoch [1241/20000], Training Loss: 0.0272\n",
            "Epoch [1242/20000], Training Loss: 0.0260\n",
            "Epoch [1243/20000], Training Loss: 0.0261\n",
            "Epoch [1244/20000], Training Loss: 0.0266\n",
            "Epoch [1245/20000], Training Loss: 0.0279\n",
            "Epoch [1246/20000], Training Loss: 0.0263\n",
            "Epoch [1247/20000], Training Loss: 0.0242\n",
            "Epoch [1248/20000], Training Loss: 0.0296\n",
            "Epoch [1249/20000], Training Loss: 0.0270\n",
            "Epoch [1250/20000], Training Loss: 0.0253\n",
            "Epoch [1251/20000], Training Loss: 0.0294\n",
            "Epoch [1252/20000], Training Loss: 0.0265\n",
            "Epoch [1253/20000], Training Loss: 0.0249\n",
            "Epoch [1254/20000], Training Loss: 0.0249\n",
            "Epoch [1255/20000], Training Loss: 0.0272\n",
            "Epoch [1256/20000], Training Loss: 0.0267\n",
            "Epoch [1257/20000], Training Loss: 0.0277\n",
            "Epoch [1258/20000], Training Loss: 0.0253\n",
            "Epoch [1259/20000], Training Loss: 0.0273\n",
            "Epoch [1260/20000], Training Loss: 0.0264\n",
            "Epoch [1261/20000], Training Loss: 0.0278\n",
            "Epoch [1262/20000], Training Loss: 0.0286\n",
            "Epoch [1263/20000], Training Loss: 0.0256\n",
            "Epoch [1264/20000], Training Loss: 0.0264\n",
            "Epoch [1265/20000], Training Loss: 0.0256\n",
            "Epoch [1266/20000], Training Loss: 0.0273\n",
            "Epoch [1267/20000], Training Loss: 0.0259\n",
            "Epoch [1268/20000], Training Loss: 0.0260\n",
            "Epoch [1269/20000], Training Loss: 0.0257\n",
            "Epoch [1270/20000], Training Loss: 0.0252\n",
            "Epoch [1271/20000], Training Loss: 0.0287\n",
            "Epoch [1272/20000], Training Loss: 0.0277\n",
            "Epoch [1273/20000], Training Loss: 0.0267\n",
            "Epoch [1274/20000], Training Loss: 0.0242\n",
            "Epoch [1275/20000], Training Loss: 0.0273\n",
            "Epoch [1276/20000], Training Loss: 0.0255\n",
            "Epoch [1277/20000], Training Loss: 0.0257\n",
            "Epoch [1278/20000], Training Loss: 0.0259\n",
            "Epoch [1279/20000], Training Loss: 0.0261\n",
            "Epoch [1280/20000], Training Loss: 0.0244\n",
            "Epoch [1281/20000], Training Loss: 0.0270\n",
            "Epoch [1282/20000], Training Loss: 0.0255\n",
            "Epoch [1283/20000], Training Loss: 0.0249\n",
            "Epoch [1284/20000], Training Loss: 0.0269\n",
            "Epoch [1285/20000], Training Loss: 0.0235\n",
            "Epoch [1286/20000], Training Loss: 0.0271\n",
            "Epoch [1287/20000], Training Loss: 0.0291\n",
            "Epoch [1288/20000], Training Loss: 0.0240\n",
            "Epoch [1289/20000], Training Loss: 0.0265\n",
            "Epoch [1290/20000], Training Loss: 0.0272\n",
            "Epoch [1291/20000], Training Loss: 0.0272\n",
            "Epoch [1292/20000], Training Loss: 0.0256\n",
            "Epoch [1293/20000], Training Loss: 0.0257\n",
            "Epoch [1294/20000], Training Loss: 0.0256\n",
            "Epoch [1295/20000], Training Loss: 0.0246\n",
            "Epoch [1296/20000], Training Loss: 0.0270\n",
            "Epoch [1297/20000], Training Loss: 0.0278\n",
            "Epoch [1298/20000], Training Loss: 0.0261\n",
            "Epoch [1299/20000], Training Loss: 0.0264\n",
            "Epoch [1300/20000], Training Loss: 0.0266\n",
            "Epoch [1301/20000], Training Loss: 0.0270\n",
            "Epoch [1302/20000], Training Loss: 0.0261\n",
            "Epoch [1303/20000], Training Loss: 0.0267\n",
            "Epoch [1304/20000], Training Loss: 0.0258\n",
            "Epoch [1305/20000], Training Loss: 0.0265\n",
            "Epoch [1306/20000], Training Loss: 0.0264\n",
            "Epoch [1307/20000], Training Loss: 0.0253\n",
            "Epoch [1308/20000], Training Loss: 0.0269\n",
            "Epoch [1309/20000], Training Loss: 0.0256\n",
            "Epoch [1310/20000], Training Loss: 0.0280\n",
            "Epoch [1311/20000], Training Loss: 0.0264\n",
            "Epoch [1312/20000], Training Loss: 0.0256\n",
            "Epoch [1313/20000], Training Loss: 0.0281\n",
            "Epoch [1314/20000], Training Loss: 0.0248\n",
            "Epoch [1315/20000], Training Loss: 0.0272\n",
            "Epoch [1316/20000], Training Loss: 0.0248\n",
            "Epoch [1317/20000], Training Loss: 0.0281\n",
            "Epoch [1318/20000], Training Loss: 0.0266\n",
            "Epoch [1319/20000], Training Loss: 0.0272\n",
            "Epoch [1320/20000], Training Loss: 0.0258\n",
            "Epoch [1321/20000], Training Loss: 0.0252\n",
            "Epoch [1322/20000], Training Loss: 0.0244\n",
            "Epoch [1323/20000], Training Loss: 0.0244\n",
            "Epoch [1324/20000], Training Loss: 0.0248\n",
            "Epoch [1325/20000], Training Loss: 0.0273\n",
            "Epoch [1326/20000], Training Loss: 0.0255\n",
            "Epoch [1327/20000], Training Loss: 0.0254\n",
            "Epoch [1328/20000], Training Loss: 0.0249\n",
            "Epoch [1329/20000], Training Loss: 0.0253\n",
            "Epoch [1330/20000], Training Loss: 0.0281\n",
            "Epoch [1331/20000], Training Loss: 0.0250\n",
            "Epoch [1332/20000], Training Loss: 0.0265\n",
            "Epoch [1333/20000], Training Loss: 0.0264\n",
            "Epoch [1334/20000], Training Loss: 0.0264\n",
            "Epoch [1335/20000], Training Loss: 0.0259\n",
            "Epoch [1336/20000], Training Loss: 0.0256\n",
            "Epoch [1337/20000], Training Loss: 0.0265\n",
            "Epoch [1338/20000], Training Loss: 0.0262\n",
            "Epoch [1339/20000], Training Loss: 0.0265\n",
            "Epoch [1340/20000], Training Loss: 0.0258\n",
            "Epoch [1341/20000], Training Loss: 0.0273\n",
            "Epoch [1342/20000], Training Loss: 0.0275\n",
            "Epoch [1343/20000], Training Loss: 0.0258\n",
            "Epoch [1344/20000], Training Loss: 0.0252\n",
            "Epoch [1345/20000], Training Loss: 0.0270\n",
            "Epoch [1346/20000], Training Loss: 0.0252\n",
            "Epoch [1347/20000], Training Loss: 0.0264\n",
            "Epoch [1348/20000], Training Loss: 0.0246\n",
            "Epoch [1349/20000], Training Loss: 0.0278\n",
            "Epoch [1350/20000], Training Loss: 0.0262\n",
            "Epoch [1351/20000], Training Loss: 0.0260\n",
            "Epoch [1352/20000], Training Loss: 0.0266\n",
            "Epoch [1353/20000], Training Loss: 0.0276\n",
            "Epoch [1354/20000], Training Loss: 0.0268\n",
            "Epoch [1355/20000], Training Loss: 0.0255\n",
            "Epoch [1356/20000], Training Loss: 0.0263\n",
            "Epoch [1357/20000], Training Loss: 0.0249\n",
            "Epoch [1358/20000], Training Loss: 0.0274\n",
            "Epoch [1359/20000], Training Loss: 0.0274\n",
            "Epoch [1360/20000], Training Loss: 0.0247\n",
            "Epoch [1361/20000], Training Loss: 0.0293\n",
            "Epoch [1362/20000], Training Loss: 0.0274\n",
            "Epoch [1363/20000], Training Loss: 0.0253\n",
            "Epoch [1364/20000], Training Loss: 0.0287\n",
            "Epoch [1365/20000], Training Loss: 0.0246\n",
            "Epoch [1366/20000], Training Loss: 0.0275\n",
            "Epoch [1367/20000], Training Loss: 0.0285\n",
            "Epoch [1368/20000], Training Loss: 0.0267\n",
            "Epoch [1369/20000], Training Loss: 0.0272\n",
            "Epoch [1370/20000], Training Loss: 0.0254\n",
            "Epoch [1371/20000], Training Loss: 0.0262\n",
            "Epoch [1372/20000], Training Loss: 0.0253\n",
            "Epoch [1373/20000], Training Loss: 0.0261\n",
            "Epoch [1374/20000], Training Loss: 0.0285\n",
            "Epoch [1375/20000], Training Loss: 0.0256\n",
            "Epoch [1376/20000], Training Loss: 0.0275\n",
            "Epoch [1377/20000], Training Loss: 0.0253\n",
            "Epoch [1378/20000], Training Loss: 0.0256\n",
            "Epoch [1379/20000], Training Loss: 0.0255\n",
            "Epoch [1380/20000], Training Loss: 0.0280\n",
            "Epoch [1381/20000], Training Loss: 0.0272\n",
            "Epoch [1382/20000], Training Loss: 0.0274\n",
            "Epoch [1383/20000], Training Loss: 0.0273\n",
            "Epoch [1384/20000], Training Loss: 0.0259\n",
            "Epoch [1385/20000], Training Loss: 0.0276\n",
            "Epoch [1386/20000], Training Loss: 0.0263\n",
            "Epoch [1387/20000], Training Loss: 0.0272\n",
            "Epoch [1388/20000], Training Loss: 0.0264\n",
            "Epoch [1389/20000], Training Loss: 0.0260\n",
            "Epoch [1390/20000], Training Loss: 0.0249\n",
            "Epoch [1391/20000], Training Loss: 0.0253\n",
            "Epoch [1392/20000], Training Loss: 0.0284\n",
            "Epoch [1393/20000], Training Loss: 0.0255\n",
            "Epoch [1394/20000], Training Loss: 0.0280\n",
            "Epoch [1395/20000], Training Loss: 0.0248\n",
            "Epoch [1396/20000], Training Loss: 0.0260\n",
            "Epoch [1397/20000], Training Loss: 0.0279\n",
            "Epoch [1398/20000], Training Loss: 0.0269\n",
            "Epoch [1399/20000], Training Loss: 0.0279\n",
            "Epoch [1400/20000], Training Loss: 0.0270\n",
            "Epoch [1401/20000], Training Loss: 0.0258\n",
            "Epoch [1402/20000], Training Loss: 0.0262\n",
            "Epoch [1403/20000], Training Loss: 0.0255\n",
            "Epoch [1404/20000], Training Loss: 0.0272\n",
            "Epoch [1405/20000], Training Loss: 0.0260\n",
            "Epoch [1406/20000], Training Loss: 0.0281\n",
            "Epoch [1407/20000], Training Loss: 0.0268\n",
            "Epoch [1408/20000], Training Loss: 0.0256\n",
            "Epoch [1409/20000], Training Loss: 0.0271\n",
            "Epoch [1410/20000], Training Loss: 0.0261\n",
            "Epoch [1411/20000], Training Loss: 0.0260\n",
            "Epoch [1412/20000], Training Loss: 0.0281\n",
            "Epoch [1413/20000], Training Loss: 0.0275\n",
            "Epoch [1414/20000], Training Loss: 0.0273\n",
            "Epoch [1415/20000], Training Loss: 0.0259\n",
            "Epoch [1416/20000], Training Loss: 0.0279\n",
            "Epoch [1417/20000], Training Loss: 0.0257\n",
            "Epoch [1418/20000], Training Loss: 0.0268\n",
            "Epoch [1419/20000], Training Loss: 0.0258\n",
            "Epoch [1420/20000], Training Loss: 0.0291\n",
            "Epoch [1421/20000], Training Loss: 0.0243\n",
            "Epoch [1422/20000], Training Loss: 0.0259\n",
            "Epoch [1423/20000], Training Loss: 0.0255\n",
            "Epoch [1424/20000], Training Loss: 0.0278\n",
            "Epoch [1425/20000], Training Loss: 0.0254\n",
            "Epoch [1426/20000], Training Loss: 0.0254\n",
            "Epoch [1427/20000], Training Loss: 0.0278\n",
            "Epoch [1428/20000], Training Loss: 0.0256\n",
            "Epoch [1429/20000], Training Loss: 0.0265\n",
            "Epoch [1430/20000], Training Loss: 0.0276\n",
            "Epoch [1431/20000], Training Loss: 0.0256\n",
            "Epoch [1432/20000], Training Loss: 0.0264\n",
            "Epoch [1433/20000], Training Loss: 0.0264\n",
            "Epoch [1434/20000], Training Loss: 0.0270\n",
            "Epoch [1435/20000], Training Loss: 0.0266\n",
            "Epoch [1436/20000], Training Loss: 0.0282\n",
            "Epoch [1437/20000], Training Loss: 0.0268\n",
            "Epoch [1438/20000], Training Loss: 0.0260\n",
            "Epoch [1439/20000], Training Loss: 0.0298\n",
            "Epoch [1440/20000], Training Loss: 0.0263\n",
            "Epoch [1441/20000], Training Loss: 0.0282\n",
            "Epoch [1442/20000], Training Loss: 0.0255\n",
            "Epoch [1443/20000], Training Loss: 0.0275\n",
            "Epoch [1444/20000], Training Loss: 0.0257\n",
            "Epoch [1445/20000], Training Loss: 0.0268\n",
            "Epoch [1446/20000], Training Loss: 0.0249\n",
            "Epoch [1447/20000], Training Loss: 0.0260\n",
            "Epoch [1448/20000], Training Loss: 0.0248\n",
            "Epoch [1449/20000], Training Loss: 0.0263\n",
            "Epoch [1450/20000], Training Loss: 0.0269\n",
            "Epoch [1451/20000], Training Loss: 0.0278\n",
            "Epoch [1452/20000], Training Loss: 0.0248\n",
            "Epoch [1453/20000], Training Loss: 0.0285\n",
            "Epoch [1454/20000], Training Loss: 0.0266\n",
            "Epoch [1455/20000], Training Loss: 0.0244\n",
            "Epoch [1456/20000], Training Loss: 0.0282\n",
            "Epoch [1457/20000], Training Loss: 0.0263\n",
            "Epoch [1458/20000], Training Loss: 0.0253\n",
            "Epoch [1459/20000], Training Loss: 0.0260\n",
            "Epoch [1460/20000], Training Loss: 0.0262\n",
            "Epoch [1461/20000], Training Loss: 0.0279\n",
            "Epoch [1462/20000], Training Loss: 0.0262\n",
            "Epoch [1463/20000], Training Loss: 0.0259\n",
            "Epoch [1464/20000], Training Loss: 0.0275\n",
            "Epoch [1465/20000], Training Loss: 0.0254\n",
            "Epoch [1466/20000], Training Loss: 0.0269\n",
            "Epoch [1467/20000], Training Loss: 0.0268\n",
            "Epoch [1468/20000], Training Loss: 0.0252\n",
            "Epoch [1469/20000], Training Loss: 0.0262\n",
            "Epoch [1470/20000], Training Loss: 0.0256\n",
            "Epoch [1471/20000], Training Loss: 0.0265\n",
            "Epoch [1472/20000], Training Loss: 0.0242\n",
            "Epoch [1473/20000], Training Loss: 0.0271\n",
            "Epoch [1474/20000], Training Loss: 0.0253\n",
            "Epoch [1475/20000], Training Loss: 0.0249\n",
            "Epoch [1476/20000], Training Loss: 0.0255\n",
            "Epoch [1477/20000], Training Loss: 0.0275\n",
            "Epoch [1478/20000], Training Loss: 0.0283\n",
            "Epoch [1479/20000], Training Loss: 0.0274\n",
            "Epoch [1480/20000], Training Loss: 0.0254\n",
            "Epoch [1481/20000], Training Loss: 0.0254\n",
            "Epoch [1482/20000], Training Loss: 0.0278\n",
            "Epoch [1483/20000], Training Loss: 0.0257\n",
            "Epoch [1484/20000], Training Loss: 0.0254\n",
            "Epoch [1485/20000], Training Loss: 0.0267\n",
            "Epoch [1486/20000], Training Loss: 0.0255\n",
            "Epoch [1487/20000], Training Loss: 0.0282\n",
            "Epoch [1488/20000], Training Loss: 0.0265\n",
            "Epoch [1489/20000], Training Loss: 0.0246\n",
            "Epoch [1490/20000], Training Loss: 0.0251\n",
            "Epoch [1491/20000], Training Loss: 0.0259\n",
            "Epoch [1492/20000], Training Loss: 0.0252\n",
            "Epoch [1493/20000], Training Loss: 0.0275\n",
            "Epoch [1494/20000], Training Loss: 0.0250\n",
            "Epoch [1495/20000], Training Loss: 0.0268\n",
            "Epoch [1496/20000], Training Loss: 0.0273\n",
            "Epoch [1497/20000], Training Loss: 0.0270\n",
            "Epoch [1498/20000], Training Loss: 0.0271\n",
            "Epoch [1499/20000], Training Loss: 0.0270\n",
            "Epoch [1500/20000], Training Loss: 0.0266\n",
            "Epoch [1501/20000], Training Loss: 0.0261\n",
            "Epoch [1502/20000], Training Loss: 0.0273\n",
            "Epoch [1503/20000], Training Loss: 0.0270\n",
            "Epoch [1504/20000], Training Loss: 0.0259\n",
            "Epoch [1505/20000], Training Loss: 0.0250\n",
            "Epoch [1506/20000], Training Loss: 0.0265\n",
            "Epoch [1507/20000], Training Loss: 0.0252\n",
            "Epoch [1508/20000], Training Loss: 0.0268\n",
            "Epoch [1509/20000], Training Loss: 0.0259\n",
            "Epoch [1510/20000], Training Loss: 0.0259\n",
            "Epoch [1511/20000], Training Loss: 0.0277\n",
            "Epoch [1512/20000], Training Loss: 0.0262\n",
            "Epoch [1513/20000], Training Loss: 0.0271\n",
            "Epoch [1514/20000], Training Loss: 0.0273\n",
            "Epoch [1515/20000], Training Loss: 0.0265\n",
            "Epoch [1516/20000], Training Loss: 0.0266\n",
            "Epoch [1517/20000], Training Loss: 0.0288\n",
            "Epoch [1518/20000], Training Loss: 0.0293\n",
            "Epoch [1519/20000], Training Loss: 0.0249\n",
            "Epoch [1520/20000], Training Loss: 0.0255\n",
            "Epoch [1521/20000], Training Loss: 0.0255\n",
            "Epoch [1522/20000], Training Loss: 0.0276\n",
            "Epoch [1523/20000], Training Loss: 0.0243\n",
            "Epoch [1524/20000], Training Loss: 0.0254\n",
            "Epoch [1525/20000], Training Loss: 0.0258\n",
            "Epoch [1526/20000], Training Loss: 0.0265\n",
            "Epoch [1527/20000], Training Loss: 0.0279\n",
            "Epoch [1528/20000], Training Loss: 0.0269\n",
            "Epoch [1529/20000], Training Loss: 0.0253\n",
            "Epoch [1530/20000], Training Loss: 0.0274\n",
            "Epoch [1531/20000], Training Loss: 0.0263\n",
            "Epoch [1532/20000], Training Loss: 0.0266\n",
            "Epoch [1533/20000], Training Loss: 0.0267\n",
            "Epoch [1534/20000], Training Loss: 0.0269\n",
            "Epoch [1535/20000], Training Loss: 0.0267\n",
            "Epoch [1536/20000], Training Loss: 0.0283\n",
            "Epoch [1537/20000], Training Loss: 0.0273\n",
            "Epoch [1538/20000], Training Loss: 0.0257\n",
            "Epoch [1539/20000], Training Loss: 0.0270\n",
            "Epoch [1540/20000], Training Loss: 0.0280\n",
            "Epoch [1541/20000], Training Loss: 0.0302\n",
            "Epoch [1542/20000], Training Loss: 0.0285\n",
            "Epoch [1543/20000], Training Loss: 0.0270\n",
            "Epoch [1544/20000], Training Loss: 0.0259\n",
            "Epoch [1545/20000], Training Loss: 0.0268\n",
            "Epoch [1546/20000], Training Loss: 0.0269\n",
            "Epoch [1547/20000], Training Loss: 0.0253\n",
            "Epoch [1548/20000], Training Loss: 0.0277\n",
            "Epoch [1549/20000], Training Loss: 0.0274\n",
            "Epoch [1550/20000], Training Loss: 0.0259\n",
            "Epoch [1551/20000], Training Loss: 0.0259\n",
            "Epoch [1552/20000], Training Loss: 0.0289\n",
            "Epoch [1553/20000], Training Loss: 0.0268\n",
            "Epoch [1554/20000], Training Loss: 0.0264\n",
            "Epoch [1555/20000], Training Loss: 0.0263\n",
            "Epoch [1556/20000], Training Loss: 0.0251\n",
            "Epoch [1557/20000], Training Loss: 0.0260\n",
            "Epoch [1558/20000], Training Loss: 0.0261\n",
            "Epoch [1559/20000], Training Loss: 0.0236\n",
            "Epoch [1560/20000], Training Loss: 0.0264\n",
            "Epoch [1561/20000], Training Loss: 0.0267\n",
            "Epoch [1562/20000], Training Loss: 0.0273\n",
            "Epoch [1563/20000], Training Loss: 0.0280\n",
            "Epoch [1564/20000], Training Loss: 0.0255\n",
            "Epoch [1565/20000], Training Loss: 0.0264\n",
            "Epoch [1566/20000], Training Loss: 0.0258\n",
            "Epoch [1567/20000], Training Loss: 0.0244\n",
            "Epoch [1568/20000], Training Loss: 0.0261\n",
            "Epoch [1569/20000], Training Loss: 0.0263\n",
            "Epoch [1570/20000], Training Loss: 0.0263\n",
            "Epoch [1571/20000], Training Loss: 0.0275\n",
            "Epoch [1572/20000], Training Loss: 0.0262\n",
            "Epoch [1573/20000], Training Loss: 0.0251\n",
            "Epoch [1574/20000], Training Loss: 0.0275\n",
            "Epoch [1575/20000], Training Loss: 0.0255\n",
            "Epoch [1576/20000], Training Loss: 0.0260\n",
            "Epoch [1577/20000], Training Loss: 0.0279\n",
            "Epoch [1578/20000], Training Loss: 0.0259\n",
            "Epoch [1579/20000], Training Loss: 0.0261\n",
            "Epoch [1580/20000], Training Loss: 0.0276\n",
            "Epoch [1581/20000], Training Loss: 0.0273\n",
            "Epoch [1582/20000], Training Loss: 0.0261\n",
            "Epoch [1583/20000], Training Loss: 0.0261\n",
            "Epoch [1584/20000], Training Loss: 0.0247\n",
            "Epoch [1585/20000], Training Loss: 0.0254\n",
            "Epoch [1586/20000], Training Loss: 0.0276\n",
            "Epoch [1587/20000], Training Loss: 0.0285\n",
            "Epoch [1588/20000], Training Loss: 0.0281\n",
            "Epoch [1589/20000], Training Loss: 0.0282\n",
            "Epoch [1590/20000], Training Loss: 0.0262\n",
            "Epoch [1591/20000], Training Loss: 0.0264\n",
            "Epoch [1592/20000], Training Loss: 0.0280\n",
            "Epoch [1593/20000], Training Loss: 0.0272\n",
            "Epoch [1594/20000], Training Loss: 0.0252\n",
            "Epoch [1595/20000], Training Loss: 0.0279\n",
            "Epoch [1596/20000], Training Loss: 0.0263\n",
            "Epoch [1597/20000], Training Loss: 0.0256\n",
            "Epoch [1598/20000], Training Loss: 0.0260\n",
            "Epoch [1599/20000], Training Loss: 0.0276\n",
            "Epoch [1600/20000], Training Loss: 0.0272\n",
            "Epoch [1601/20000], Training Loss: 0.0256\n",
            "Epoch [1602/20000], Training Loss: 0.0282\n",
            "Epoch [1603/20000], Training Loss: 0.0248\n",
            "Epoch [1604/20000], Training Loss: 0.0277\n",
            "Epoch [1605/20000], Training Loss: 0.0265\n",
            "Epoch [1606/20000], Training Loss: 0.0269\n",
            "Epoch [1607/20000], Training Loss: 0.0282\n",
            "Epoch [1608/20000], Training Loss: 0.0249\n",
            "Epoch [1609/20000], Training Loss: 0.0252\n",
            "Epoch [1610/20000], Training Loss: 0.0242\n",
            "Epoch [1611/20000], Training Loss: 0.0282\n",
            "Epoch [1612/20000], Training Loss: 0.0272\n",
            "Epoch [1613/20000], Training Loss: 0.0261\n",
            "Epoch [1614/20000], Training Loss: 0.0255\n",
            "Epoch [1615/20000], Training Loss: 0.0263\n",
            "Epoch [1616/20000], Training Loss: 0.0254\n",
            "Epoch [1617/20000], Training Loss: 0.0256\n",
            "Epoch [1618/20000], Training Loss: 0.0249\n",
            "Epoch [1619/20000], Training Loss: 0.0246\n",
            "Epoch [1620/20000], Training Loss: 0.0243\n",
            "Epoch [1621/20000], Training Loss: 0.0255\n",
            "Epoch [1622/20000], Training Loss: 0.0276\n",
            "Epoch [1623/20000], Training Loss: 0.0269\n",
            "Epoch [1624/20000], Training Loss: 0.0295\n",
            "Epoch [1625/20000], Training Loss: 0.0264\n",
            "Epoch [1626/20000], Training Loss: 0.0243\n",
            "Epoch [1627/20000], Training Loss: 0.0247\n",
            "Epoch [1628/20000], Training Loss: 0.0252\n",
            "Epoch [1629/20000], Training Loss: 0.0270\n",
            "Epoch [1630/20000], Training Loss: 0.0269\n",
            "Epoch [1631/20000], Training Loss: 0.0256\n",
            "Epoch [1632/20000], Training Loss: 0.0267\n",
            "Epoch [1633/20000], Training Loss: 0.0251\n",
            "Epoch [1634/20000], Training Loss: 0.0261\n",
            "Epoch [1635/20000], Training Loss: 0.0257\n",
            "Epoch [1636/20000], Training Loss: 0.0262\n",
            "Epoch [1637/20000], Training Loss: 0.0252\n",
            "Epoch [1638/20000], Training Loss: 0.0253\n",
            "Epoch [1639/20000], Training Loss: 0.0274\n",
            "Epoch [1640/20000], Training Loss: 0.0256\n",
            "Epoch [1641/20000], Training Loss: 0.0252\n",
            "Epoch [1642/20000], Training Loss: 0.0243\n",
            "Epoch [1643/20000], Training Loss: 0.0262\n",
            "Epoch [1644/20000], Training Loss: 0.0256\n",
            "Epoch [1645/20000], Training Loss: 0.0259\n",
            "Epoch [1646/20000], Training Loss: 0.0274\n",
            "Epoch [1647/20000], Training Loss: 0.0269\n",
            "Epoch [1648/20000], Training Loss: 0.0261\n",
            "Epoch [1649/20000], Training Loss: 0.0243\n",
            "Epoch [1650/20000], Training Loss: 0.0276\n",
            "Epoch [1651/20000], Training Loss: 0.0275\n",
            "Epoch [1652/20000], Training Loss: 0.0284\n",
            "Epoch [1653/20000], Training Loss: 0.0270\n",
            "Epoch [1654/20000], Training Loss: 0.0244\n",
            "Epoch [1655/20000], Training Loss: 0.0282\n",
            "Epoch [1656/20000], Training Loss: 0.0256\n",
            "Epoch [1657/20000], Training Loss: 0.0274\n",
            "Epoch [1658/20000], Training Loss: 0.0273\n",
            "Epoch [1659/20000], Training Loss: 0.0243\n",
            "Epoch [1660/20000], Training Loss: 0.0260\n",
            "Epoch [1661/20000], Training Loss: 0.0243\n",
            "Epoch [1662/20000], Training Loss: 0.0264\n",
            "Epoch [1663/20000], Training Loss: 0.0254\n",
            "Epoch [1664/20000], Training Loss: 0.0278\n",
            "Epoch [1665/20000], Training Loss: 0.0253\n",
            "Epoch [1666/20000], Training Loss: 0.0254\n",
            "Epoch [1667/20000], Training Loss: 0.0254\n",
            "Epoch [1668/20000], Training Loss: 0.0267\n",
            "Epoch [1669/20000], Training Loss: 0.0253\n",
            "Epoch [1670/20000], Training Loss: 0.0262\n",
            "Epoch [1671/20000], Training Loss: 0.0241\n",
            "Epoch [1672/20000], Training Loss: 0.0271\n",
            "Epoch [1673/20000], Training Loss: 0.0282\n",
            "Epoch [1674/20000], Training Loss: 0.0276\n",
            "Epoch [1675/20000], Training Loss: 0.0264\n",
            "Epoch [1676/20000], Training Loss: 0.0262\n",
            "Epoch [1677/20000], Training Loss: 0.0265\n",
            "Epoch [1678/20000], Training Loss: 0.0288\n",
            "Epoch [1679/20000], Training Loss: 0.0273\n",
            "Epoch [1680/20000], Training Loss: 0.0251\n",
            "Epoch [1681/20000], Training Loss: 0.0271\n",
            "Epoch [1682/20000], Training Loss: 0.0274\n",
            "Epoch [1683/20000], Training Loss: 0.0246\n",
            "Epoch [1684/20000], Training Loss: 0.0252\n",
            "Epoch [1685/20000], Training Loss: 0.0261\n",
            "Epoch [1686/20000], Training Loss: 0.0289\n",
            "Epoch [1687/20000], Training Loss: 0.0287\n",
            "Epoch [1688/20000], Training Loss: 0.0279\n",
            "Epoch [1689/20000], Training Loss: 0.0260\n",
            "Epoch [1690/20000], Training Loss: 0.0247\n",
            "Epoch [1691/20000], Training Loss: 0.0248\n",
            "Epoch [1692/20000], Training Loss: 0.0268\n",
            "Epoch [1693/20000], Training Loss: 0.0281\n",
            "Epoch [1694/20000], Training Loss: 0.0259\n",
            "Epoch [1695/20000], Training Loss: 0.0267\n",
            "Epoch [1696/20000], Training Loss: 0.0275\n",
            "Epoch [1697/20000], Training Loss: 0.0250\n",
            "Epoch [1698/20000], Training Loss: 0.0269\n",
            "Epoch [1699/20000], Training Loss: 0.0280\n",
            "Epoch [1700/20000], Training Loss: 0.0259\n",
            "Epoch [1701/20000], Training Loss: 0.0239\n",
            "Epoch [1702/20000], Training Loss: 0.0263\n",
            "Epoch [1703/20000], Training Loss: 0.0272\n",
            "Epoch [1704/20000], Training Loss: 0.0248\n",
            "Epoch [1705/20000], Training Loss: 0.0251\n",
            "Epoch [1706/20000], Training Loss: 0.0248\n",
            "Epoch [1707/20000], Training Loss: 0.0246\n",
            "Epoch [1708/20000], Training Loss: 0.0253\n",
            "Epoch [1709/20000], Training Loss: 0.0277\n",
            "Epoch [1710/20000], Training Loss: 0.0257\n",
            "Epoch [1711/20000], Training Loss: 0.0270\n",
            "Epoch [1712/20000], Training Loss: 0.0274\n",
            "Epoch [1713/20000], Training Loss: 0.0260\n",
            "Epoch [1714/20000], Training Loss: 0.0269\n",
            "Epoch [1715/20000], Training Loss: 0.0254\n",
            "Epoch [1716/20000], Training Loss: 0.0274\n",
            "Epoch [1717/20000], Training Loss: 0.0274\n",
            "Epoch [1718/20000], Training Loss: 0.0283\n",
            "Epoch [1719/20000], Training Loss: 0.0266\n",
            "Epoch [1720/20000], Training Loss: 0.0272\n",
            "Epoch [1721/20000], Training Loss: 0.0281\n",
            "Epoch [1722/20000], Training Loss: 0.0250\n",
            "Epoch [1723/20000], Training Loss: 0.0275\n",
            "Epoch [1724/20000], Training Loss: 0.0270\n",
            "Epoch [1725/20000], Training Loss: 0.0265\n",
            "Epoch [1726/20000], Training Loss: 0.0265\n",
            "Epoch [1727/20000], Training Loss: 0.0268\n",
            "Epoch [1728/20000], Training Loss: 0.0278\n",
            "Epoch [1729/20000], Training Loss: 0.0276\n",
            "Epoch [1730/20000], Training Loss: 0.0287\n",
            "Epoch [1731/20000], Training Loss: 0.0249\n",
            "Epoch [1732/20000], Training Loss: 0.0250\n",
            "Epoch [1733/20000], Training Loss: 0.0263\n",
            "Epoch [1734/20000], Training Loss: 0.0285\n",
            "Epoch [1735/20000], Training Loss: 0.0254\n",
            "Epoch [1736/20000], Training Loss: 0.0249\n",
            "Epoch [1737/20000], Training Loss: 0.0286\n",
            "Epoch [1738/20000], Training Loss: 0.0276\n",
            "Epoch [1739/20000], Training Loss: 0.0245\n",
            "Epoch [1740/20000], Training Loss: 0.0277\n",
            "Epoch [1741/20000], Training Loss: 0.0255\n",
            "Epoch [1742/20000], Training Loss: 0.0287\n",
            "Epoch [1743/20000], Training Loss: 0.0268\n",
            "Epoch [1744/20000], Training Loss: 0.0256\n",
            "Epoch [1745/20000], Training Loss: 0.0257\n",
            "Epoch [1746/20000], Training Loss: 0.0267\n",
            "Epoch [1747/20000], Training Loss: 0.0272\n",
            "Epoch [1748/20000], Training Loss: 0.0264\n",
            "Epoch [1749/20000], Training Loss: 0.0269\n",
            "Epoch [1750/20000], Training Loss: 0.0255\n",
            "Epoch [1751/20000], Training Loss: 0.0261\n",
            "Epoch [1752/20000], Training Loss: 0.0242\n",
            "Epoch [1753/20000], Training Loss: 0.0256\n",
            "Epoch [1754/20000], Training Loss: 0.0269\n",
            "Epoch [1755/20000], Training Loss: 0.0253\n",
            "Epoch [1756/20000], Training Loss: 0.0261\n",
            "Epoch [1757/20000], Training Loss: 0.0289\n",
            "Epoch [1758/20000], Training Loss: 0.0257\n",
            "Epoch [1759/20000], Training Loss: 0.0247\n",
            "Epoch [1760/20000], Training Loss: 0.0257\n",
            "Epoch [1761/20000], Training Loss: 0.0259\n",
            "Epoch [1762/20000], Training Loss: 0.0249\n",
            "Epoch [1763/20000], Training Loss: 0.0260\n",
            "Epoch [1764/20000], Training Loss: 0.0292\n",
            "Epoch [1765/20000], Training Loss: 0.0279\n",
            "Epoch [1766/20000], Training Loss: 0.0252\n",
            "Epoch [1767/20000], Training Loss: 0.0248\n",
            "Epoch [1768/20000], Training Loss: 0.0263\n",
            "Epoch [1769/20000], Training Loss: 0.0266\n",
            "Epoch [1770/20000], Training Loss: 0.0244\n",
            "Epoch [1771/20000], Training Loss: 0.0269\n",
            "Epoch [1772/20000], Training Loss: 0.0273\n",
            "Epoch [1773/20000], Training Loss: 0.0253\n",
            "Epoch [1774/20000], Training Loss: 0.0273\n",
            "Epoch [1775/20000], Training Loss: 0.0267\n",
            "Epoch [1776/20000], Training Loss: 0.0244\n",
            "Epoch [1777/20000], Training Loss: 0.0278\n",
            "Epoch [1778/20000], Training Loss: 0.0263\n",
            "Epoch [1779/20000], Training Loss: 0.0249\n",
            "Epoch [1780/20000], Training Loss: 0.0281\n",
            "Epoch [1781/20000], Training Loss: 0.0275\n",
            "Epoch [1782/20000], Training Loss: 0.0271\n",
            "Epoch [1783/20000], Training Loss: 0.0265\n",
            "Epoch [1784/20000], Training Loss: 0.0259\n",
            "Epoch [1785/20000], Training Loss: 0.0272\n",
            "Epoch [1786/20000], Training Loss: 0.0269\n",
            "Epoch [1787/20000], Training Loss: 0.0268\n",
            "Epoch [1788/20000], Training Loss: 0.0277\n",
            "Epoch [1789/20000], Training Loss: 0.0271\n",
            "Epoch [1790/20000], Training Loss: 0.0264\n",
            "Epoch [1791/20000], Training Loss: 0.0261\n",
            "Epoch [1792/20000], Training Loss: 0.0262\n",
            "Epoch [1793/20000], Training Loss: 0.0277\n",
            "Epoch [1794/20000], Training Loss: 0.0272\n",
            "Epoch [1795/20000], Training Loss: 0.0270\n",
            "Epoch [1796/20000], Training Loss: 0.0273\n",
            "Epoch [1797/20000], Training Loss: 0.0266\n",
            "Epoch [1798/20000], Training Loss: 0.0272\n",
            "Epoch [1799/20000], Training Loss: 0.0250\n",
            "Epoch [1800/20000], Training Loss: 0.0271\n",
            "Epoch [1801/20000], Training Loss: 0.0278\n",
            "Epoch [1802/20000], Training Loss: 0.0272\n",
            "Epoch [1803/20000], Training Loss: 0.0265\n",
            "Epoch [1804/20000], Training Loss: 0.0249\n",
            "Epoch [1805/20000], Training Loss: 0.0248\n",
            "Epoch [1806/20000], Training Loss: 0.0268\n",
            "Epoch [1807/20000], Training Loss: 0.0294\n",
            "Epoch [1808/20000], Training Loss: 0.0268\n",
            "Epoch [1809/20000], Training Loss: 0.0275\n",
            "Epoch [1810/20000], Training Loss: 0.0256\n",
            "Epoch [1811/20000], Training Loss: 0.0268\n",
            "Epoch [1812/20000], Training Loss: 0.0258\n",
            "Epoch [1813/20000], Training Loss: 0.0266\n",
            "Epoch [1814/20000], Training Loss: 0.0270\n",
            "Epoch [1815/20000], Training Loss: 0.0250\n",
            "Epoch [1816/20000], Training Loss: 0.0264\n",
            "Epoch [1817/20000], Training Loss: 0.0270\n",
            "Epoch [1818/20000], Training Loss: 0.0264\n",
            "Epoch [1819/20000], Training Loss: 0.0245\n",
            "Epoch [1820/20000], Training Loss: 0.0267\n",
            "Epoch [1821/20000], Training Loss: 0.0271\n",
            "Epoch [1822/20000], Training Loss: 0.0263\n",
            "Epoch [1823/20000], Training Loss: 0.0253\n",
            "Epoch [1824/20000], Training Loss: 0.0253\n",
            "Epoch [1825/20000], Training Loss: 0.0262\n",
            "Epoch [1826/20000], Training Loss: 0.0271\n",
            "Epoch [1827/20000], Training Loss: 0.0262\n",
            "Epoch [1828/20000], Training Loss: 0.0266\n",
            "Epoch [1829/20000], Training Loss: 0.0252\n",
            "Epoch [1830/20000], Training Loss: 0.0288\n",
            "Epoch [1831/20000], Training Loss: 0.0264\n",
            "Epoch [1832/20000], Training Loss: 0.0268\n",
            "Epoch [1833/20000], Training Loss: 0.0277\n",
            "Epoch [1834/20000], Training Loss: 0.0252\n",
            "Epoch [1835/20000], Training Loss: 0.0285\n",
            "Epoch [1836/20000], Training Loss: 0.0281\n",
            "Epoch [1837/20000], Training Loss: 0.0270\n",
            "Epoch [1838/20000], Training Loss: 0.0272\n",
            "Epoch [1839/20000], Training Loss: 0.0266\n",
            "Epoch [1840/20000], Training Loss: 0.0269\n",
            "Epoch [1841/20000], Training Loss: 0.0262\n",
            "Epoch [1842/20000], Training Loss: 0.0270\n",
            "Epoch [1843/20000], Training Loss: 0.0263\n",
            "Epoch [1844/20000], Training Loss: 0.0267\n",
            "Epoch [1845/20000], Training Loss: 0.0273\n",
            "Epoch [1846/20000], Training Loss: 0.0256\n",
            "Epoch [1847/20000], Training Loss: 0.0246\n",
            "Epoch [1848/20000], Training Loss: 0.0266\n",
            "Epoch [1849/20000], Training Loss: 0.0260\n",
            "Epoch [1850/20000], Training Loss: 0.0254\n",
            "Epoch [1851/20000], Training Loss: 0.0253\n",
            "Epoch [1852/20000], Training Loss: 0.0261\n",
            "Epoch [1853/20000], Training Loss: 0.0268\n",
            "Epoch [1854/20000], Training Loss: 0.0280\n",
            "Epoch [1855/20000], Training Loss: 0.0245\n",
            "Epoch [1856/20000], Training Loss: 0.0263\n",
            "Epoch [1857/20000], Training Loss: 0.0269\n",
            "Epoch [1858/20000], Training Loss: 0.0253\n",
            "Epoch [1859/20000], Training Loss: 0.0240\n",
            "Epoch [1860/20000], Training Loss: 0.0274\n",
            "Epoch [1861/20000], Training Loss: 0.0275\n",
            "Epoch [1862/20000], Training Loss: 0.0242\n",
            "Epoch [1863/20000], Training Loss: 0.0261\n",
            "Epoch [1864/20000], Training Loss: 0.0257\n",
            "Epoch [1865/20000], Training Loss: 0.0254\n",
            "Epoch [1866/20000], Training Loss: 0.0260\n",
            "Epoch [1867/20000], Training Loss: 0.0250\n",
            "Epoch [1868/20000], Training Loss: 0.0254\n",
            "Epoch [1869/20000], Training Loss: 0.0259\n",
            "Epoch [1870/20000], Training Loss: 0.0263\n",
            "Epoch [1871/20000], Training Loss: 0.0272\n",
            "Epoch [1872/20000], Training Loss: 0.0263\n",
            "Epoch [1873/20000], Training Loss: 0.0262\n",
            "Epoch [1874/20000], Training Loss: 0.0275\n",
            "Epoch [1875/20000], Training Loss: 0.0263\n",
            "Epoch [1876/20000], Training Loss: 0.0250\n",
            "Epoch [1877/20000], Training Loss: 0.0284\n",
            "Epoch [1878/20000], Training Loss: 0.0263\n",
            "Epoch [1879/20000], Training Loss: 0.0270\n",
            "Epoch [1880/20000], Training Loss: 0.0250\n",
            "Epoch [1881/20000], Training Loss: 0.0270\n",
            "Epoch [1882/20000], Training Loss: 0.0266\n",
            "Epoch [1883/20000], Training Loss: 0.0279\n",
            "Epoch [1884/20000], Training Loss: 0.0286\n",
            "Epoch [1885/20000], Training Loss: 0.0239\n",
            "Epoch [1886/20000], Training Loss: 0.0272\n",
            "Epoch [1887/20000], Training Loss: 0.0277\n",
            "Epoch [1888/20000], Training Loss: 0.0252\n",
            "Epoch [1889/20000], Training Loss: 0.0283\n",
            "Epoch [1890/20000], Training Loss: 0.0256\n",
            "Epoch [1891/20000], Training Loss: 0.0269\n",
            "Epoch [1892/20000], Training Loss: 0.0266\n",
            "Epoch [1893/20000], Training Loss: 0.0273\n",
            "Epoch [1894/20000], Training Loss: 0.0270\n",
            "Epoch [1895/20000], Training Loss: 0.0278\n",
            "Epoch [1896/20000], Training Loss: 0.0270\n",
            "Epoch [1897/20000], Training Loss: 0.0273\n",
            "Epoch [1898/20000], Training Loss: 0.0255\n",
            "Epoch [1899/20000], Training Loss: 0.0279\n",
            "Epoch [1900/20000], Training Loss: 0.0259\n",
            "Epoch [1901/20000], Training Loss: 0.0276\n",
            "Epoch [1902/20000], Training Loss: 0.0264\n",
            "Epoch [1903/20000], Training Loss: 0.0276\n",
            "Epoch [1904/20000], Training Loss: 0.0276\n",
            "Epoch [1905/20000], Training Loss: 0.0245\n",
            "Epoch [1906/20000], Training Loss: 0.0273\n",
            "Epoch [1907/20000], Training Loss: 0.0262\n",
            "Epoch [1908/20000], Training Loss: 0.0257\n",
            "Epoch [1909/20000], Training Loss: 0.0276\n",
            "Epoch [1910/20000], Training Loss: 0.0252\n",
            "Epoch [1911/20000], Training Loss: 0.0246\n",
            "Epoch [1912/20000], Training Loss: 0.0253\n",
            "Epoch [1913/20000], Training Loss: 0.0255\n",
            "Epoch [1914/20000], Training Loss: 0.0259\n",
            "Epoch [1915/20000], Training Loss: 0.0255\n",
            "Epoch [1916/20000], Training Loss: 0.0245\n",
            "Epoch [1917/20000], Training Loss: 0.0293\n",
            "Epoch [1918/20000], Training Loss: 0.0252\n",
            "Epoch [1919/20000], Training Loss: 0.0292\n",
            "Epoch [1920/20000], Training Loss: 0.0268\n",
            "Epoch [1921/20000], Training Loss: 0.0256\n",
            "Epoch [1922/20000], Training Loss: 0.0257\n",
            "Epoch [1923/20000], Training Loss: 0.0257\n",
            "Epoch [1924/20000], Training Loss: 0.0269\n",
            "Epoch [1925/20000], Training Loss: 0.0256\n",
            "Epoch [1926/20000], Training Loss: 0.0258\n",
            "Epoch [1927/20000], Training Loss: 0.0266\n",
            "Epoch [1928/20000], Training Loss: 0.0256\n",
            "Epoch [1929/20000], Training Loss: 0.0273\n",
            "Epoch [1930/20000], Training Loss: 0.0275\n",
            "Epoch [1931/20000], Training Loss: 0.0253\n",
            "Epoch [1932/20000], Training Loss: 0.0261\n",
            "Epoch [1933/20000], Training Loss: 0.0251\n",
            "Epoch [1934/20000], Training Loss: 0.0273\n",
            "Epoch [1935/20000], Training Loss: 0.0244\n",
            "Epoch [1936/20000], Training Loss: 0.0248\n",
            "Epoch [1937/20000], Training Loss: 0.0289\n",
            "Epoch [1938/20000], Training Loss: 0.0261\n",
            "Epoch [1939/20000], Training Loss: 0.0277\n",
            "Epoch [1940/20000], Training Loss: 0.0270\n",
            "Epoch [1941/20000], Training Loss: 0.0265\n",
            "Epoch [1942/20000], Training Loss: 0.0273\n",
            "Epoch [1943/20000], Training Loss: 0.0268\n",
            "Epoch [1944/20000], Training Loss: 0.0248\n",
            "Epoch [1945/20000], Training Loss: 0.0266\n",
            "Epoch [1946/20000], Training Loss: 0.0251\n",
            "Epoch [1947/20000], Training Loss: 0.0267\n",
            "Epoch [1948/20000], Training Loss: 0.0275\n",
            "Epoch [1949/20000], Training Loss: 0.0240\n",
            "Epoch [1950/20000], Training Loss: 0.0267\n",
            "Epoch [1951/20000], Training Loss: 0.0268\n",
            "Epoch [1952/20000], Training Loss: 0.0258\n",
            "Epoch [1953/20000], Training Loss: 0.0262\n",
            "Epoch [1954/20000], Training Loss: 0.0238\n",
            "Epoch [1955/20000], Training Loss: 0.0275\n",
            "Epoch [1956/20000], Training Loss: 0.0277\n",
            "Epoch [1957/20000], Training Loss: 0.0278\n",
            "Epoch [1958/20000], Training Loss: 0.0267\n",
            "Epoch [1959/20000], Training Loss: 0.0266\n",
            "Epoch [1960/20000], Training Loss: 0.0287\n",
            "Epoch [1961/20000], Training Loss: 0.0275\n",
            "Epoch [1962/20000], Training Loss: 0.0276\n",
            "Epoch [1963/20000], Training Loss: 0.0257\n",
            "Epoch [1964/20000], Training Loss: 0.0257\n",
            "Epoch [1965/20000], Training Loss: 0.0264\n",
            "Epoch [1966/20000], Training Loss: 0.0268\n",
            "Epoch [1967/20000], Training Loss: 0.0256\n",
            "Epoch [1968/20000], Training Loss: 0.0282\n",
            "Epoch [1969/20000], Training Loss: 0.0270\n",
            "Epoch [1970/20000], Training Loss: 0.0272\n",
            "Epoch [1971/20000], Training Loss: 0.0279\n",
            "Epoch [1972/20000], Training Loss: 0.0256\n",
            "Epoch [1973/20000], Training Loss: 0.0265\n",
            "Epoch [1974/20000], Training Loss: 0.0264\n",
            "Epoch [1975/20000], Training Loss: 0.0266\n",
            "Epoch [1976/20000], Training Loss: 0.0255\n",
            "Epoch [1977/20000], Training Loss: 0.0262\n",
            "Epoch [1978/20000], Training Loss: 0.0273\n",
            "Epoch [1979/20000], Training Loss: 0.0273\n",
            "Epoch [1980/20000], Training Loss: 0.0244\n",
            "Epoch [1981/20000], Training Loss: 0.0254\n",
            "Epoch [1982/20000], Training Loss: 0.0278\n",
            "Epoch [1983/20000], Training Loss: 0.0258\n",
            "Epoch [1984/20000], Training Loss: 0.0293\n",
            "Epoch [1985/20000], Training Loss: 0.0259\n",
            "Epoch [1986/20000], Training Loss: 0.0264\n",
            "Epoch [1987/20000], Training Loss: 0.0270\n",
            "Epoch [1988/20000], Training Loss: 0.0259\n",
            "Epoch [1989/20000], Training Loss: 0.0283\n",
            "Epoch [1990/20000], Training Loss: 0.0261\n",
            "Epoch [1991/20000], Training Loss: 0.0271\n",
            "Epoch [1992/20000], Training Loss: 0.0248\n",
            "Epoch [1993/20000], Training Loss: 0.0263\n",
            "Epoch [1994/20000], Training Loss: 0.0277\n",
            "Epoch [1995/20000], Training Loss: 0.0271\n",
            "Epoch [1996/20000], Training Loss: 0.0273\n",
            "Epoch [1997/20000], Training Loss: 0.0277\n",
            "Epoch [1998/20000], Training Loss: 0.0282\n",
            "Epoch [1999/20000], Training Loss: 0.0258\n",
            "Epoch [2000/20000], Training Loss: 0.0282\n",
            "Epoch [2001/20000], Training Loss: 0.0251\n",
            "Epoch [2002/20000], Training Loss: 0.0248\n",
            "Epoch [2003/20000], Training Loss: 0.0257\n",
            "Epoch [2004/20000], Training Loss: 0.0247\n",
            "Epoch [2005/20000], Training Loss: 0.0261\n",
            "Epoch [2006/20000], Training Loss: 0.0272\n",
            "Epoch [2007/20000], Training Loss: 0.0252\n",
            "Epoch [2008/20000], Training Loss: 0.0262\n",
            "Epoch [2009/20000], Training Loss: 0.0277\n",
            "Epoch [2010/20000], Training Loss: 0.0274\n",
            "Epoch [2011/20000], Training Loss: 0.0280\n",
            "Epoch [2012/20000], Training Loss: 0.0263\n",
            "Epoch [2013/20000], Training Loss: 0.0257\n",
            "Epoch [2014/20000], Training Loss: 0.0257\n",
            "Epoch [2015/20000], Training Loss: 0.0246\n",
            "Epoch [2016/20000], Training Loss: 0.0249\n",
            "Epoch [2017/20000], Training Loss: 0.0255\n",
            "Epoch [2018/20000], Training Loss: 0.0275\n",
            "Epoch [2019/20000], Training Loss: 0.0279\n",
            "Epoch [2020/20000], Training Loss: 0.0247\n",
            "Epoch [2021/20000], Training Loss: 0.0254\n",
            "Epoch [2022/20000], Training Loss: 0.0285\n",
            "Epoch [2023/20000], Training Loss: 0.0252\n",
            "Epoch [2024/20000], Training Loss: 0.0296\n",
            "Epoch [2025/20000], Training Loss: 0.0272\n",
            "Epoch [2026/20000], Training Loss: 0.0268\n",
            "Epoch [2027/20000], Training Loss: 0.0254\n",
            "Epoch [2028/20000], Training Loss: 0.0247\n",
            "Epoch [2029/20000], Training Loss: 0.0258\n",
            "Epoch [2030/20000], Training Loss: 0.0275\n",
            "Epoch [2031/20000], Training Loss: 0.0255\n",
            "Epoch [2032/20000], Training Loss: 0.0249\n",
            "Epoch [2033/20000], Training Loss: 0.0252\n",
            "Epoch [2034/20000], Training Loss: 0.0275\n",
            "Epoch [2035/20000], Training Loss: 0.0261\n",
            "Epoch [2036/20000], Training Loss: 0.0266\n",
            "Epoch [2037/20000], Training Loss: 0.0282\n",
            "Epoch [2038/20000], Training Loss: 0.0291\n",
            "Epoch [2039/20000], Training Loss: 0.0258\n",
            "Epoch [2040/20000], Training Loss: 0.0273\n",
            "Epoch [2041/20000], Training Loss: 0.0262\n",
            "Epoch [2042/20000], Training Loss: 0.0255\n",
            "Epoch [2043/20000], Training Loss: 0.0269\n",
            "Epoch [2044/20000], Training Loss: 0.0254\n",
            "Epoch [2045/20000], Training Loss: 0.0270\n",
            "Epoch [2046/20000], Training Loss: 0.0241\n",
            "Epoch [2047/20000], Training Loss: 0.0271\n",
            "Epoch [2048/20000], Training Loss: 0.0275\n",
            "Epoch [2049/20000], Training Loss: 0.0262\n",
            "Epoch [2050/20000], Training Loss: 0.0281\n",
            "Epoch [2051/20000], Training Loss: 0.0262\n",
            "Epoch [2052/20000], Training Loss: 0.0276\n",
            "Epoch [2053/20000], Training Loss: 0.0284\n",
            "Epoch [2054/20000], Training Loss: 0.0285\n",
            "Epoch [2055/20000], Training Loss: 0.0281\n",
            "Epoch [2056/20000], Training Loss: 0.0280\n",
            "Epoch [2057/20000], Training Loss: 0.0251\n",
            "Epoch [2058/20000], Training Loss: 0.0282\n",
            "Epoch [2059/20000], Training Loss: 0.0263\n",
            "Epoch [2060/20000], Training Loss: 0.0263\n",
            "Epoch [2061/20000], Training Loss: 0.0264\n",
            "Epoch [2062/20000], Training Loss: 0.0263\n",
            "Epoch [2063/20000], Training Loss: 0.0252\n",
            "Epoch [2064/20000], Training Loss: 0.0263\n",
            "Epoch [2065/20000], Training Loss: 0.0261\n",
            "Epoch [2066/20000], Training Loss: 0.0280\n",
            "Epoch [2067/20000], Training Loss: 0.0271\n",
            "Epoch [2068/20000], Training Loss: 0.0269\n",
            "Epoch [2069/20000], Training Loss: 0.0259\n",
            "Epoch [2070/20000], Training Loss: 0.0261\n",
            "Epoch [2071/20000], Training Loss: 0.0278\n",
            "Epoch [2072/20000], Training Loss: 0.0264\n",
            "Epoch [2073/20000], Training Loss: 0.0269\n",
            "Epoch [2074/20000], Training Loss: 0.0253\n",
            "Epoch [2075/20000], Training Loss: 0.0247\n",
            "Epoch [2076/20000], Training Loss: 0.0273\n",
            "Epoch [2077/20000], Training Loss: 0.0251\n",
            "Epoch [2078/20000], Training Loss: 0.0246\n",
            "Epoch [2079/20000], Training Loss: 0.0247\n",
            "Epoch [2080/20000], Training Loss: 0.0268\n",
            "Epoch [2081/20000], Training Loss: 0.0286\n",
            "Epoch [2082/20000], Training Loss: 0.0287\n",
            "Epoch [2083/20000], Training Loss: 0.0262\n",
            "Epoch [2084/20000], Training Loss: 0.0264\n",
            "Epoch [2085/20000], Training Loss: 0.0261\n",
            "Epoch [2086/20000], Training Loss: 0.0250\n",
            "Epoch [2087/20000], Training Loss: 0.0288\n",
            "Epoch [2088/20000], Training Loss: 0.0245\n",
            "Epoch [2089/20000], Training Loss: 0.0254\n",
            "Epoch [2090/20000], Training Loss: 0.0257\n",
            "Epoch [2091/20000], Training Loss: 0.0245\n",
            "Epoch [2092/20000], Training Loss: 0.0260\n",
            "Epoch [2093/20000], Training Loss: 0.0243\n",
            "Epoch [2094/20000], Training Loss: 0.0253\n",
            "Epoch [2095/20000], Training Loss: 0.0269\n",
            "Epoch [2096/20000], Training Loss: 0.0245\n",
            "Epoch [2097/20000], Training Loss: 0.0265\n",
            "Epoch [2098/20000], Training Loss: 0.0264\n",
            "Epoch [2099/20000], Training Loss: 0.0291\n",
            "Epoch [2100/20000], Training Loss: 0.0268\n",
            "Epoch [2101/20000], Training Loss: 0.0264\n",
            "Epoch [2102/20000], Training Loss: 0.0272\n",
            "Epoch [2103/20000], Training Loss: 0.0250\n",
            "Epoch [2104/20000], Training Loss: 0.0259\n",
            "Epoch [2105/20000], Training Loss: 0.0272\n",
            "Epoch [2106/20000], Training Loss: 0.0278\n",
            "Epoch [2107/20000], Training Loss: 0.0273\n",
            "Epoch [2108/20000], Training Loss: 0.0264\n",
            "Epoch [2109/20000], Training Loss: 0.0278\n",
            "Epoch [2110/20000], Training Loss: 0.0287\n",
            "Epoch [2111/20000], Training Loss: 0.0259\n",
            "Epoch [2112/20000], Training Loss: 0.0280\n",
            "Epoch [2113/20000], Training Loss: 0.0278\n",
            "Epoch [2114/20000], Training Loss: 0.0255\n",
            "Epoch [2115/20000], Training Loss: 0.0252\n",
            "Epoch [2116/20000], Training Loss: 0.0269\n",
            "Epoch [2117/20000], Training Loss: 0.0243\n",
            "Epoch [2118/20000], Training Loss: 0.0248\n",
            "Epoch [2119/20000], Training Loss: 0.0270\n",
            "Epoch [2120/20000], Training Loss: 0.0273\n",
            "Epoch [2121/20000], Training Loss: 0.0260\n",
            "Epoch [2122/20000], Training Loss: 0.0266\n",
            "Epoch [2123/20000], Training Loss: 0.0278\n",
            "Epoch [2124/20000], Training Loss: 0.0260\n",
            "Epoch [2125/20000], Training Loss: 0.0250\n",
            "Epoch [2126/20000], Training Loss: 0.0252\n",
            "Epoch [2127/20000], Training Loss: 0.0255\n",
            "Epoch [2128/20000], Training Loss: 0.0270\n",
            "Epoch [2129/20000], Training Loss: 0.0239\n",
            "Epoch [2130/20000], Training Loss: 0.0272\n",
            "Epoch [2131/20000], Training Loss: 0.0261\n",
            "Epoch [2132/20000], Training Loss: 0.0268\n",
            "Epoch [2133/20000], Training Loss: 0.0273\n",
            "Epoch [2134/20000], Training Loss: 0.0241\n",
            "Epoch [2135/20000], Training Loss: 0.0285\n",
            "Epoch [2136/20000], Training Loss: 0.0245\n",
            "Epoch [2137/20000], Training Loss: 0.0264\n",
            "Epoch [2138/20000], Training Loss: 0.0256\n",
            "Epoch [2139/20000], Training Loss: 0.0287\n",
            "Epoch [2140/20000], Training Loss: 0.0252\n",
            "Epoch [2141/20000], Training Loss: 0.0256\n",
            "Epoch [2142/20000], Training Loss: 0.0254\n",
            "Epoch [2143/20000], Training Loss: 0.0260\n",
            "Epoch [2144/20000], Training Loss: 0.0264\n",
            "Epoch [2145/20000], Training Loss: 0.0251\n",
            "Epoch [2146/20000], Training Loss: 0.0248\n",
            "Epoch [2147/20000], Training Loss: 0.0259\n",
            "Epoch [2148/20000], Training Loss: 0.0243\n",
            "Epoch [2149/20000], Training Loss: 0.0275\n",
            "Epoch [2150/20000], Training Loss: 0.0259\n",
            "Epoch [2151/20000], Training Loss: 0.0263\n",
            "Epoch [2152/20000], Training Loss: 0.0251\n",
            "Epoch [2153/20000], Training Loss: 0.0287\n",
            "Epoch [2154/20000], Training Loss: 0.0276\n",
            "Epoch [2155/20000], Training Loss: 0.0269\n",
            "Epoch [2156/20000], Training Loss: 0.0270\n",
            "Epoch [2157/20000], Training Loss: 0.0281\n",
            "Epoch [2158/20000], Training Loss: 0.0261\n",
            "Epoch [2159/20000], Training Loss: 0.0266\n",
            "Epoch [2160/20000], Training Loss: 0.0276\n",
            "Epoch [2161/20000], Training Loss: 0.0247\n",
            "Epoch [2162/20000], Training Loss: 0.0263\n",
            "Epoch [2163/20000], Training Loss: 0.0298\n",
            "Epoch [2164/20000], Training Loss: 0.0269\n",
            "Epoch [2165/20000], Training Loss: 0.0264\n",
            "Epoch [2166/20000], Training Loss: 0.0259\n",
            "Epoch [2167/20000], Training Loss: 0.0284\n",
            "Epoch [2168/20000], Training Loss: 0.0255\n",
            "Epoch [2169/20000], Training Loss: 0.0259\n",
            "Epoch [2170/20000], Training Loss: 0.0284\n",
            "Epoch [2171/20000], Training Loss: 0.0298\n",
            "Epoch [2172/20000], Training Loss: 0.0267\n",
            "Epoch [2173/20000], Training Loss: 0.0270\n",
            "Epoch [2174/20000], Training Loss: 0.0272\n",
            "Epoch [2175/20000], Training Loss: 0.0258\n",
            "Epoch [2176/20000], Training Loss: 0.0275\n",
            "Epoch [2177/20000], Training Loss: 0.0242\n",
            "Epoch [2178/20000], Training Loss: 0.0281\n",
            "Epoch [2179/20000], Training Loss: 0.0258\n",
            "Epoch [2180/20000], Training Loss: 0.0246\n",
            "Epoch [2181/20000], Training Loss: 0.0247\n",
            "Epoch [2182/20000], Training Loss: 0.0259\n",
            "Epoch [2183/20000], Training Loss: 0.0258\n",
            "Epoch [2184/20000], Training Loss: 0.0254\n",
            "Epoch [2185/20000], Training Loss: 0.0277\n",
            "Epoch [2186/20000], Training Loss: 0.0273\n",
            "Epoch [2187/20000], Training Loss: 0.0280\n",
            "Epoch [2188/20000], Training Loss: 0.0258\n",
            "Epoch [2189/20000], Training Loss: 0.0245\n",
            "Epoch [2190/20000], Training Loss: 0.0268\n",
            "Epoch [2191/20000], Training Loss: 0.0273\n",
            "Epoch [2192/20000], Training Loss: 0.0255\n",
            "Epoch [2193/20000], Training Loss: 0.0269\n",
            "Epoch [2194/20000], Training Loss: 0.0282\n",
            "Epoch [2195/20000], Training Loss: 0.0265\n",
            "Epoch [2196/20000], Training Loss: 0.0265\n",
            "Epoch [2197/20000], Training Loss: 0.0275\n",
            "Epoch [2198/20000], Training Loss: 0.0267\n",
            "Epoch [2199/20000], Training Loss: 0.0255\n",
            "Epoch [2200/20000], Training Loss: 0.0270\n",
            "Epoch [2201/20000], Training Loss: 0.0276\n",
            "Epoch [2202/20000], Training Loss: 0.0277\n",
            "Epoch [2203/20000], Training Loss: 0.0287\n",
            "Epoch [2204/20000], Training Loss: 0.0271\n",
            "Epoch [2205/20000], Training Loss: 0.0269\n",
            "Epoch [2206/20000], Training Loss: 0.0268\n",
            "Epoch [2207/20000], Training Loss: 0.0265\n",
            "Epoch [2208/20000], Training Loss: 0.0246\n",
            "Epoch [2209/20000], Training Loss: 0.0266\n",
            "Epoch [2210/20000], Training Loss: 0.0248\n",
            "Epoch [2211/20000], Training Loss: 0.0266\n",
            "Epoch [2212/20000], Training Loss: 0.0262\n",
            "Epoch [2213/20000], Training Loss: 0.0269\n",
            "Epoch [2214/20000], Training Loss: 0.0266\n",
            "Epoch [2215/20000], Training Loss: 0.0250\n",
            "Epoch [2216/20000], Training Loss: 0.0259\n",
            "Epoch [2217/20000], Training Loss: 0.0289\n",
            "Epoch [2218/20000], Training Loss: 0.0262\n",
            "Epoch [2219/20000], Training Loss: 0.0273\n",
            "Epoch [2220/20000], Training Loss: 0.0269\n",
            "Epoch [2221/20000], Training Loss: 0.0272\n",
            "Epoch [2222/20000], Training Loss: 0.0278\n",
            "Epoch [2223/20000], Training Loss: 0.0260\n",
            "Epoch [2224/20000], Training Loss: 0.0252\n",
            "Epoch [2225/20000], Training Loss: 0.0245\n",
            "Epoch [2226/20000], Training Loss: 0.0262\n",
            "Epoch [2227/20000], Training Loss: 0.0248\n",
            "Epoch [2228/20000], Training Loss: 0.0277\n",
            "Epoch [2229/20000], Training Loss: 0.0278\n",
            "Epoch [2230/20000], Training Loss: 0.0257\n",
            "Epoch [2231/20000], Training Loss: 0.0268\n",
            "Epoch [2232/20000], Training Loss: 0.0261\n",
            "Epoch [2233/20000], Training Loss: 0.0261\n",
            "Epoch [2234/20000], Training Loss: 0.0245\n",
            "Epoch [2235/20000], Training Loss: 0.0260\n",
            "Epoch [2236/20000], Training Loss: 0.0246\n",
            "Epoch [2237/20000], Training Loss: 0.0260\n",
            "Epoch [2238/20000], Training Loss: 0.0276\n",
            "Epoch [2239/20000], Training Loss: 0.0254\n",
            "Epoch [2240/20000], Training Loss: 0.0274\n",
            "Epoch [2241/20000], Training Loss: 0.0272\n",
            "Epoch [2242/20000], Training Loss: 0.0265\n",
            "Epoch [2243/20000], Training Loss: 0.0274\n",
            "Epoch [2244/20000], Training Loss: 0.0237\n",
            "Epoch [2245/20000], Training Loss: 0.0253\n",
            "Epoch [2246/20000], Training Loss: 0.0266\n",
            "Epoch [2247/20000], Training Loss: 0.0282\n",
            "Epoch [2248/20000], Training Loss: 0.0276\n",
            "Epoch [2249/20000], Training Loss: 0.0254\n",
            "Epoch [2250/20000], Training Loss: 0.0273\n",
            "Epoch [2251/20000], Training Loss: 0.0265\n",
            "Epoch [2252/20000], Training Loss: 0.0273\n",
            "Epoch [2253/20000], Training Loss: 0.0277\n",
            "Epoch [2254/20000], Training Loss: 0.0259\n",
            "Epoch [2255/20000], Training Loss: 0.0260\n",
            "Epoch [2256/20000], Training Loss: 0.0262\n",
            "Epoch [2257/20000], Training Loss: 0.0266\n",
            "Epoch [2258/20000], Training Loss: 0.0255\n",
            "Epoch [2259/20000], Training Loss: 0.0262\n",
            "Epoch [2260/20000], Training Loss: 0.0254\n",
            "Epoch [2261/20000], Training Loss: 0.0260\n",
            "Epoch [2262/20000], Training Loss: 0.0260\n",
            "Epoch [2263/20000], Training Loss: 0.0253\n",
            "Epoch [2264/20000], Training Loss: 0.0267\n",
            "Epoch [2265/20000], Training Loss: 0.0267\n",
            "Epoch [2266/20000], Training Loss: 0.0249\n",
            "Epoch [2267/20000], Training Loss: 0.0263\n",
            "Epoch [2268/20000], Training Loss: 0.0287\n",
            "Epoch [2269/20000], Training Loss: 0.0271\n",
            "Epoch [2270/20000], Training Loss: 0.0281\n",
            "Epoch [2271/20000], Training Loss: 0.0270\n",
            "Epoch [2272/20000], Training Loss: 0.0287\n",
            "Epoch [2273/20000], Training Loss: 0.0267\n",
            "Epoch [2274/20000], Training Loss: 0.0274\n",
            "Epoch [2275/20000], Training Loss: 0.0257\n",
            "Epoch [2276/20000], Training Loss: 0.0285\n",
            "Epoch [2277/20000], Training Loss: 0.0285\n",
            "Epoch [2278/20000], Training Loss: 0.0269\n",
            "Epoch [2279/20000], Training Loss: 0.0286\n",
            "Epoch [2280/20000], Training Loss: 0.0263\n",
            "Epoch [2281/20000], Training Loss: 0.0271\n",
            "Epoch [2282/20000], Training Loss: 0.0244\n",
            "Epoch [2283/20000], Training Loss: 0.0240\n",
            "Epoch [2284/20000], Training Loss: 0.0259\n",
            "Epoch [2285/20000], Training Loss: 0.0258\n",
            "Epoch [2286/20000], Training Loss: 0.0270\n",
            "Epoch [2287/20000], Training Loss: 0.0259\n",
            "Epoch [2288/20000], Training Loss: 0.0244\n",
            "Epoch [2289/20000], Training Loss: 0.0282\n",
            "Epoch [2290/20000], Training Loss: 0.0286\n",
            "Epoch [2291/20000], Training Loss: 0.0268\n",
            "Epoch [2292/20000], Training Loss: 0.0253\n",
            "Epoch [2293/20000], Training Loss: 0.0271\n",
            "Epoch [2294/20000], Training Loss: 0.0258\n",
            "Epoch [2295/20000], Training Loss: 0.0296\n",
            "Epoch [2296/20000], Training Loss: 0.0266\n",
            "Epoch [2297/20000], Training Loss: 0.0274\n",
            "Epoch [2298/20000], Training Loss: 0.0261\n",
            "Epoch [2299/20000], Training Loss: 0.0286\n",
            "Epoch [2300/20000], Training Loss: 0.0273\n",
            "Epoch [2301/20000], Training Loss: 0.0250\n",
            "Epoch [2302/20000], Training Loss: 0.0261\n",
            "Epoch [2303/20000], Training Loss: 0.0267\n",
            "Epoch [2304/20000], Training Loss: 0.0273\n",
            "Epoch [2305/20000], Training Loss: 0.0261\n",
            "Epoch [2306/20000], Training Loss: 0.0254\n",
            "Epoch [2307/20000], Training Loss: 0.0266\n",
            "Epoch [2308/20000], Training Loss: 0.0258\n",
            "Epoch [2309/20000], Training Loss: 0.0245\n",
            "Epoch [2310/20000], Training Loss: 0.0256\n",
            "Epoch [2311/20000], Training Loss: 0.0261\n",
            "Epoch [2312/20000], Training Loss: 0.0267\n",
            "Epoch [2313/20000], Training Loss: 0.0270\n",
            "Epoch [2314/20000], Training Loss: 0.0257\n",
            "Epoch [2315/20000], Training Loss: 0.0264\n",
            "Epoch [2316/20000], Training Loss: 0.0282\n",
            "Epoch [2317/20000], Training Loss: 0.0256\n",
            "Epoch [2318/20000], Training Loss: 0.0284\n",
            "Epoch [2319/20000], Training Loss: 0.0266\n",
            "Epoch [2320/20000], Training Loss: 0.0261\n",
            "Epoch [2321/20000], Training Loss: 0.0248\n",
            "Epoch [2322/20000], Training Loss: 0.0272\n",
            "Epoch [2323/20000], Training Loss: 0.0277\n",
            "Epoch [2324/20000], Training Loss: 0.0254\n",
            "Epoch [2325/20000], Training Loss: 0.0240\n",
            "Epoch [2326/20000], Training Loss: 0.0270\n",
            "Epoch [2327/20000], Training Loss: 0.0268\n",
            "Epoch [2328/20000], Training Loss: 0.0250\n",
            "Epoch [2329/20000], Training Loss: 0.0265\n",
            "Epoch [2330/20000], Training Loss: 0.0269\n",
            "Epoch [2331/20000], Training Loss: 0.0283\n",
            "Epoch [2332/20000], Training Loss: 0.0281\n",
            "Epoch [2333/20000], Training Loss: 0.0276\n",
            "Epoch [2334/20000], Training Loss: 0.0258\n",
            "Epoch [2335/20000], Training Loss: 0.0265\n",
            "Epoch [2336/20000], Training Loss: 0.0259\n",
            "Epoch [2337/20000], Training Loss: 0.0279\n",
            "Epoch [2338/20000], Training Loss: 0.0271\n",
            "Epoch [2339/20000], Training Loss: 0.0245\n",
            "Epoch [2340/20000], Training Loss: 0.0259\n",
            "Epoch [2341/20000], Training Loss: 0.0265\n",
            "Epoch [2342/20000], Training Loss: 0.0270\n",
            "Epoch [2343/20000], Training Loss: 0.0265\n",
            "Epoch [2344/20000], Training Loss: 0.0279\n",
            "Epoch [2345/20000], Training Loss: 0.0283\n",
            "Epoch [2346/20000], Training Loss: 0.0260\n",
            "Epoch [2347/20000], Training Loss: 0.0286\n",
            "Epoch [2348/20000], Training Loss: 0.0255\n",
            "Epoch [2349/20000], Training Loss: 0.0284\n",
            "Epoch [2350/20000], Training Loss: 0.0258\n",
            "Epoch [2351/20000], Training Loss: 0.0275\n",
            "Epoch [2352/20000], Training Loss: 0.0263\n",
            "Epoch [2353/20000], Training Loss: 0.0257\n",
            "Epoch [2354/20000], Training Loss: 0.0271\n",
            "Epoch [2355/20000], Training Loss: 0.0277\n",
            "Epoch [2356/20000], Training Loss: 0.0278\n",
            "Epoch [2357/20000], Training Loss: 0.0275\n",
            "Epoch [2358/20000], Training Loss: 0.0251\n",
            "Epoch [2359/20000], Training Loss: 0.0254\n",
            "Epoch [2360/20000], Training Loss: 0.0263\n",
            "Epoch [2361/20000], Training Loss: 0.0252\n",
            "Epoch [2362/20000], Training Loss: 0.0255\n",
            "Epoch [2363/20000], Training Loss: 0.0281\n",
            "Epoch [2364/20000], Training Loss: 0.0267\n",
            "Epoch [2365/20000], Training Loss: 0.0243\n",
            "Epoch [2366/20000], Training Loss: 0.0281\n",
            "Epoch [2367/20000], Training Loss: 0.0276\n",
            "Epoch [2368/20000], Training Loss: 0.0266\n",
            "Epoch [2369/20000], Training Loss: 0.0268\n",
            "Epoch [2370/20000], Training Loss: 0.0267\n",
            "Epoch [2371/20000], Training Loss: 0.0268\n",
            "Epoch [2372/20000], Training Loss: 0.0276\n",
            "Epoch [2373/20000], Training Loss: 0.0270\n",
            "Epoch [2374/20000], Training Loss: 0.0281\n",
            "Epoch [2375/20000], Training Loss: 0.0273\n",
            "Epoch [2376/20000], Training Loss: 0.0256\n",
            "Epoch [2377/20000], Training Loss: 0.0257\n",
            "Epoch [2378/20000], Training Loss: 0.0261\n",
            "Epoch [2379/20000], Training Loss: 0.0264\n",
            "Epoch [2380/20000], Training Loss: 0.0273\n",
            "Epoch [2381/20000], Training Loss: 0.0269\n",
            "Epoch [2382/20000], Training Loss: 0.0269\n",
            "Epoch [2383/20000], Training Loss: 0.0260\n",
            "Epoch [2384/20000], Training Loss: 0.0265\n",
            "Epoch [2385/20000], Training Loss: 0.0262\n",
            "Epoch [2386/20000], Training Loss: 0.0254\n",
            "Epoch [2387/20000], Training Loss: 0.0262\n",
            "Epoch [2388/20000], Training Loss: 0.0274\n",
            "Epoch [2389/20000], Training Loss: 0.0257\n",
            "Epoch [2390/20000], Training Loss: 0.0263\n",
            "Epoch [2391/20000], Training Loss: 0.0273\n",
            "Epoch [2392/20000], Training Loss: 0.0272\n",
            "Epoch [2393/20000], Training Loss: 0.0256\n",
            "Epoch [2394/20000], Training Loss: 0.0250\n",
            "Epoch [2395/20000], Training Loss: 0.0273\n",
            "Epoch [2396/20000], Training Loss: 0.0280\n",
            "Epoch [2397/20000], Training Loss: 0.0256\n",
            "Epoch [2398/20000], Training Loss: 0.0275\n",
            "Epoch [2399/20000], Training Loss: 0.0284\n",
            "Epoch [2400/20000], Training Loss: 0.0269\n",
            "Epoch [2401/20000], Training Loss: 0.0268\n",
            "Epoch [2402/20000], Training Loss: 0.0249\n",
            "Epoch [2403/20000], Training Loss: 0.0282\n",
            "Epoch [2404/20000], Training Loss: 0.0279\n",
            "Epoch [2405/20000], Training Loss: 0.0260\n",
            "Epoch [2406/20000], Training Loss: 0.0278\n",
            "Epoch [2407/20000], Training Loss: 0.0289\n",
            "Epoch [2408/20000], Training Loss: 0.0266\n",
            "Epoch [2409/20000], Training Loss: 0.0241\n",
            "Epoch [2410/20000], Training Loss: 0.0266\n",
            "Epoch [2411/20000], Training Loss: 0.0269\n",
            "Epoch [2412/20000], Training Loss: 0.0268\n",
            "Epoch [2413/20000], Training Loss: 0.0249\n",
            "Epoch [2414/20000], Training Loss: 0.0278\n",
            "Epoch [2415/20000], Training Loss: 0.0292\n",
            "Epoch [2416/20000], Training Loss: 0.0271\n",
            "Epoch [2417/20000], Training Loss: 0.0276\n",
            "Epoch [2418/20000], Training Loss: 0.0247\n",
            "Epoch [2419/20000], Training Loss: 0.0254\n",
            "Epoch [2420/20000], Training Loss: 0.0255\n",
            "Epoch [2421/20000], Training Loss: 0.0264\n",
            "Epoch [2422/20000], Training Loss: 0.0255\n",
            "Epoch [2423/20000], Training Loss: 0.0270\n",
            "Epoch [2424/20000], Training Loss: 0.0253\n",
            "Epoch [2425/20000], Training Loss: 0.0270\n",
            "Epoch [2426/20000], Training Loss: 0.0276\n",
            "Epoch [2427/20000], Training Loss: 0.0254\n",
            "Epoch [2428/20000], Training Loss: 0.0268\n",
            "Epoch [2429/20000], Training Loss: 0.0272\n",
            "Epoch [2430/20000], Training Loss: 0.0277\n",
            "Epoch [2431/20000], Training Loss: 0.0286\n",
            "Epoch [2432/20000], Training Loss: 0.0273\n",
            "Epoch [2433/20000], Training Loss: 0.0283\n",
            "Epoch [2434/20000], Training Loss: 0.0276\n",
            "Epoch [2435/20000], Training Loss: 0.0276\n",
            "Epoch [2436/20000], Training Loss: 0.0258\n",
            "Epoch [2437/20000], Training Loss: 0.0278\n",
            "Epoch [2438/20000], Training Loss: 0.0264\n",
            "Epoch [2439/20000], Training Loss: 0.0263\n",
            "Epoch [2440/20000], Training Loss: 0.0258\n",
            "Epoch [2441/20000], Training Loss: 0.0270\n",
            "Epoch [2442/20000], Training Loss: 0.0279\n",
            "Epoch [2443/20000], Training Loss: 0.0260\n",
            "Epoch [2444/20000], Training Loss: 0.0266\n",
            "Epoch [2445/20000], Training Loss: 0.0274\n",
            "Epoch [2446/20000], Training Loss: 0.0254\n",
            "Epoch [2447/20000], Training Loss: 0.0249\n",
            "Epoch [2448/20000], Training Loss: 0.0266\n",
            "Epoch [2449/20000], Training Loss: 0.0285\n",
            "Epoch [2450/20000], Training Loss: 0.0272\n",
            "Epoch [2451/20000], Training Loss: 0.0257\n",
            "Epoch [2452/20000], Training Loss: 0.0248\n",
            "Epoch [2453/20000], Training Loss: 0.0272\n",
            "Epoch [2454/20000], Training Loss: 0.0262\n",
            "Epoch [2455/20000], Training Loss: 0.0260\n",
            "Epoch [2456/20000], Training Loss: 0.0274\n",
            "Epoch [2457/20000], Training Loss: 0.0269\n",
            "Epoch [2458/20000], Training Loss: 0.0266\n",
            "Epoch [2459/20000], Training Loss: 0.0253\n",
            "Epoch [2460/20000], Training Loss: 0.0281\n",
            "Epoch [2461/20000], Training Loss: 0.0256\n",
            "Epoch [2462/20000], Training Loss: 0.0294\n",
            "Epoch [2463/20000], Training Loss: 0.0283\n",
            "Epoch [2464/20000], Training Loss: 0.0248\n",
            "Epoch [2465/20000], Training Loss: 0.0260\n",
            "Epoch [2466/20000], Training Loss: 0.0274\n",
            "Epoch [2467/20000], Training Loss: 0.0246\n",
            "Epoch [2468/20000], Training Loss: 0.0246\n",
            "Epoch [2469/20000], Training Loss: 0.0265\n",
            "Epoch [2470/20000], Training Loss: 0.0278\n",
            "Epoch [2471/20000], Training Loss: 0.0253\n",
            "Epoch [2472/20000], Training Loss: 0.0259\n",
            "Epoch [2473/20000], Training Loss: 0.0274\n",
            "Epoch [2474/20000], Training Loss: 0.0274\n",
            "Epoch [2475/20000], Training Loss: 0.0262\n",
            "Epoch [2476/20000], Training Loss: 0.0252\n",
            "Epoch [2477/20000], Training Loss: 0.0256\n",
            "Epoch [2478/20000], Training Loss: 0.0280\n",
            "Epoch [2479/20000], Training Loss: 0.0250\n",
            "Epoch [2480/20000], Training Loss: 0.0270\n",
            "Epoch [2481/20000], Training Loss: 0.0275\n",
            "Epoch [2482/20000], Training Loss: 0.0275\n",
            "Epoch [2483/20000], Training Loss: 0.0263\n",
            "Epoch [2484/20000], Training Loss: 0.0251\n",
            "Epoch [2485/20000], Training Loss: 0.0255\n",
            "Epoch [2486/20000], Training Loss: 0.0257\n",
            "Epoch [2487/20000], Training Loss: 0.0245\n",
            "Epoch [2488/20000], Training Loss: 0.0278\n",
            "Epoch [2489/20000], Training Loss: 0.0245\n",
            "Epoch [2490/20000], Training Loss: 0.0254\n",
            "Epoch [2491/20000], Training Loss: 0.0243\n",
            "Epoch [2492/20000], Training Loss: 0.0270\n",
            "Epoch [2493/20000], Training Loss: 0.0247\n",
            "Epoch [2494/20000], Training Loss: 0.0276\n",
            "Epoch [2495/20000], Training Loss: 0.0249\n",
            "Epoch [2496/20000], Training Loss: 0.0257\n",
            "Epoch [2497/20000], Training Loss: 0.0251\n",
            "Epoch [2498/20000], Training Loss: 0.0263\n",
            "Epoch [2499/20000], Training Loss: 0.0263\n",
            "Epoch [2500/20000], Training Loss: 0.0249\n",
            "Epoch [2501/20000], Training Loss: 0.0277\n",
            "Epoch [2502/20000], Training Loss: 0.0259\n",
            "Epoch [2503/20000], Training Loss: 0.0246\n",
            "Epoch [2504/20000], Training Loss: 0.0259\n",
            "Epoch [2505/20000], Training Loss: 0.0270\n",
            "Epoch [2506/20000], Training Loss: 0.0262\n",
            "Epoch [2507/20000], Training Loss: 0.0263\n",
            "Epoch [2508/20000], Training Loss: 0.0265\n",
            "Epoch [2509/20000], Training Loss: 0.0242\n",
            "Epoch [2510/20000], Training Loss: 0.0257\n",
            "Epoch [2511/20000], Training Loss: 0.0255\n",
            "Epoch [2512/20000], Training Loss: 0.0281\n",
            "Epoch [2513/20000], Training Loss: 0.0251\n",
            "Epoch [2514/20000], Training Loss: 0.0245\n",
            "Epoch [2515/20000], Training Loss: 0.0266\n",
            "Epoch [2516/20000], Training Loss: 0.0267\n",
            "Epoch [2517/20000], Training Loss: 0.0270\n",
            "Epoch [2518/20000], Training Loss: 0.0274\n",
            "Epoch [2519/20000], Training Loss: 0.0269\n",
            "Epoch [2520/20000], Training Loss: 0.0268\n",
            "Epoch [2521/20000], Training Loss: 0.0277\n",
            "Epoch [2522/20000], Training Loss: 0.0261\n",
            "Epoch [2523/20000], Training Loss: 0.0268\n",
            "Epoch [2524/20000], Training Loss: 0.0266\n",
            "Epoch [2525/20000], Training Loss: 0.0250\n",
            "Epoch [2526/20000], Training Loss: 0.0242\n",
            "Epoch [2527/20000], Training Loss: 0.0270\n",
            "Epoch [2528/20000], Training Loss: 0.0267\n",
            "Epoch [2529/20000], Training Loss: 0.0249\n",
            "Epoch [2530/20000], Training Loss: 0.0266\n",
            "Epoch [2531/20000], Training Loss: 0.0260\n",
            "Epoch [2532/20000], Training Loss: 0.0260\n",
            "Epoch [2533/20000], Training Loss: 0.0276\n",
            "Epoch [2534/20000], Training Loss: 0.0269\n",
            "Epoch [2535/20000], Training Loss: 0.0275\n",
            "Epoch [2536/20000], Training Loss: 0.0250\n",
            "Epoch [2537/20000], Training Loss: 0.0248\n",
            "Epoch [2538/20000], Training Loss: 0.0252\n",
            "Epoch [2539/20000], Training Loss: 0.0258\n",
            "Epoch [2540/20000], Training Loss: 0.0278\n",
            "Epoch [2541/20000], Training Loss: 0.0247\n",
            "Epoch [2542/20000], Training Loss: 0.0280\n",
            "Epoch [2543/20000], Training Loss: 0.0273\n",
            "Epoch [2544/20000], Training Loss: 0.0281\n",
            "Epoch [2545/20000], Training Loss: 0.0271\n",
            "Epoch [2546/20000], Training Loss: 0.0282\n",
            "Epoch [2547/20000], Training Loss: 0.0260\n",
            "Epoch [2548/20000], Training Loss: 0.0272\n",
            "Epoch [2549/20000], Training Loss: 0.0265\n",
            "Epoch [2550/20000], Training Loss: 0.0274\n",
            "Epoch [2551/20000], Training Loss: 0.0281\n",
            "Epoch [2552/20000], Training Loss: 0.0253\n",
            "Epoch [2553/20000], Training Loss: 0.0281\n",
            "Epoch [2554/20000], Training Loss: 0.0253\n",
            "Epoch [2555/20000], Training Loss: 0.0254\n",
            "Epoch [2556/20000], Training Loss: 0.0260\n",
            "Epoch [2557/20000], Training Loss: 0.0275\n",
            "Epoch [2558/20000], Training Loss: 0.0277\n",
            "Epoch [2559/20000], Training Loss: 0.0263\n",
            "Epoch [2560/20000], Training Loss: 0.0264\n",
            "Epoch [2561/20000], Training Loss: 0.0259\n",
            "Epoch [2562/20000], Training Loss: 0.0259\n",
            "Epoch [2563/20000], Training Loss: 0.0251\n",
            "Epoch [2564/20000], Training Loss: 0.0262\n",
            "Epoch [2565/20000], Training Loss: 0.0273\n",
            "Epoch [2566/20000], Training Loss: 0.0280\n",
            "Epoch [2567/20000], Training Loss: 0.0260\n",
            "Epoch [2568/20000], Training Loss: 0.0280\n",
            "Epoch [2569/20000], Training Loss: 0.0245\n",
            "Epoch [2570/20000], Training Loss: 0.0258\n",
            "Epoch [2571/20000], Training Loss: 0.0253\n",
            "Epoch [2572/20000], Training Loss: 0.0281\n",
            "Epoch [2573/20000], Training Loss: 0.0253\n",
            "Epoch [2574/20000], Training Loss: 0.0270\n",
            "Epoch [2575/20000], Training Loss: 0.0280\n",
            "Epoch [2576/20000], Training Loss: 0.0262\n",
            "Epoch [2577/20000], Training Loss: 0.0260\n",
            "Epoch [2578/20000], Training Loss: 0.0247\n",
            "Epoch [2579/20000], Training Loss: 0.0258\n",
            "Epoch [2580/20000], Training Loss: 0.0254\n",
            "Epoch [2581/20000], Training Loss: 0.0244\n",
            "Epoch [2582/20000], Training Loss: 0.0253\n",
            "Epoch [2583/20000], Training Loss: 0.0280\n",
            "Epoch [2584/20000], Training Loss: 0.0261\n",
            "Epoch [2585/20000], Training Loss: 0.0257\n",
            "Epoch [2586/20000], Training Loss: 0.0269\n",
            "Epoch [2587/20000], Training Loss: 0.0283\n",
            "Epoch [2588/20000], Training Loss: 0.0250\n",
            "Epoch [2589/20000], Training Loss: 0.0253\n",
            "Epoch [2590/20000], Training Loss: 0.0257\n",
            "Epoch [2591/20000], Training Loss: 0.0243\n",
            "Epoch [2592/20000], Training Loss: 0.0262\n",
            "Epoch [2593/20000], Training Loss: 0.0251\n",
            "Epoch [2594/20000], Training Loss: 0.0252\n",
            "Epoch [2595/20000], Training Loss: 0.0262\n",
            "Epoch [2596/20000], Training Loss: 0.0261\n",
            "Epoch [2597/20000], Training Loss: 0.0257\n",
            "Epoch [2598/20000], Training Loss: 0.0274\n",
            "Epoch [2599/20000], Training Loss: 0.0259\n",
            "Epoch [2600/20000], Training Loss: 0.0250\n",
            "Epoch [2601/20000], Training Loss: 0.0259\n",
            "Epoch [2602/20000], Training Loss: 0.0254\n",
            "Epoch [2603/20000], Training Loss: 0.0257\n",
            "Epoch [2604/20000], Training Loss: 0.0278\n",
            "Epoch [2605/20000], Training Loss: 0.0273\n",
            "Epoch [2606/20000], Training Loss: 0.0273\n",
            "Epoch [2607/20000], Training Loss: 0.0262\n",
            "Epoch [2608/20000], Training Loss: 0.0261\n",
            "Epoch [2609/20000], Training Loss: 0.0256\n",
            "Epoch [2610/20000], Training Loss: 0.0260\n",
            "Epoch [2611/20000], Training Loss: 0.0252\n",
            "Epoch [2612/20000], Training Loss: 0.0239\n",
            "Epoch [2613/20000], Training Loss: 0.0269\n",
            "Epoch [2614/20000], Training Loss: 0.0261\n",
            "Epoch [2615/20000], Training Loss: 0.0261\n",
            "Epoch [2616/20000], Training Loss: 0.0267\n",
            "Epoch [2617/20000], Training Loss: 0.0261\n",
            "Epoch [2618/20000], Training Loss: 0.0277\n",
            "Epoch [2619/20000], Training Loss: 0.0279\n",
            "Epoch [2620/20000], Training Loss: 0.0265\n",
            "Epoch [2621/20000], Training Loss: 0.0275\n",
            "Epoch [2622/20000], Training Loss: 0.0251\n",
            "Epoch [2623/20000], Training Loss: 0.0278\n",
            "Epoch [2624/20000], Training Loss: 0.0277\n",
            "Epoch [2625/20000], Training Loss: 0.0254\n",
            "Epoch [2626/20000], Training Loss: 0.0271\n",
            "Epoch [2627/20000], Training Loss: 0.0276\n",
            "Epoch [2628/20000], Training Loss: 0.0244\n",
            "Epoch [2629/20000], Training Loss: 0.0280\n",
            "Epoch [2630/20000], Training Loss: 0.0247\n",
            "Epoch [2631/20000], Training Loss: 0.0280\n",
            "Epoch [2632/20000], Training Loss: 0.0253\n",
            "Epoch [2633/20000], Training Loss: 0.0269\n",
            "Epoch [2634/20000], Training Loss: 0.0265\n",
            "Epoch [2635/20000], Training Loss: 0.0288\n",
            "Epoch [2636/20000], Training Loss: 0.0257\n",
            "Epoch [2637/20000], Training Loss: 0.0284\n",
            "Epoch [2638/20000], Training Loss: 0.0276\n",
            "Epoch [2639/20000], Training Loss: 0.0272\n",
            "Epoch [2640/20000], Training Loss: 0.0264\n",
            "Epoch [2641/20000], Training Loss: 0.0272\n",
            "Epoch [2642/20000], Training Loss: 0.0249\n",
            "Epoch [2643/20000], Training Loss: 0.0272\n",
            "Epoch [2644/20000], Training Loss: 0.0270\n",
            "Epoch [2645/20000], Training Loss: 0.0274\n",
            "Epoch [2646/20000], Training Loss: 0.0261\n",
            "Epoch [2647/20000], Training Loss: 0.0262\n",
            "Epoch [2648/20000], Training Loss: 0.0274\n",
            "Epoch [2649/20000], Training Loss: 0.0249\n",
            "Epoch [2650/20000], Training Loss: 0.0254\n",
            "Epoch [2651/20000], Training Loss: 0.0249\n",
            "Epoch [2652/20000], Training Loss: 0.0260\n",
            "Epoch [2653/20000], Training Loss: 0.0253\n",
            "Epoch [2654/20000], Training Loss: 0.0284\n",
            "Epoch [2655/20000], Training Loss: 0.0245\n",
            "Epoch [2656/20000], Training Loss: 0.0257\n",
            "Epoch [2657/20000], Training Loss: 0.0273\n",
            "Epoch [2658/20000], Training Loss: 0.0261\n",
            "Epoch [2659/20000], Training Loss: 0.0264\n",
            "Epoch [2660/20000], Training Loss: 0.0266\n",
            "Epoch [2661/20000], Training Loss: 0.0275\n",
            "Epoch [2662/20000], Training Loss: 0.0269\n",
            "Epoch [2663/20000], Training Loss: 0.0276\n",
            "Epoch [2664/20000], Training Loss: 0.0260\n",
            "Epoch [2665/20000], Training Loss: 0.0286\n",
            "Epoch [2666/20000], Training Loss: 0.0272\n",
            "Epoch [2667/20000], Training Loss: 0.0249\n",
            "Epoch [2668/20000], Training Loss: 0.0253\n",
            "Epoch [2669/20000], Training Loss: 0.0259\n",
            "Epoch [2670/20000], Training Loss: 0.0264\n",
            "Epoch [2671/20000], Training Loss: 0.0272\n",
            "Epoch [2672/20000], Training Loss: 0.0264\n",
            "Epoch [2673/20000], Training Loss: 0.0270\n",
            "Epoch [2674/20000], Training Loss: 0.0273\n",
            "Epoch [2675/20000], Training Loss: 0.0264\n",
            "Epoch [2676/20000], Training Loss: 0.0264\n",
            "Epoch [2677/20000], Training Loss: 0.0259\n",
            "Epoch [2678/20000], Training Loss: 0.0274\n",
            "Epoch [2679/20000], Training Loss: 0.0283\n",
            "Epoch [2680/20000], Training Loss: 0.0262\n",
            "Epoch [2681/20000], Training Loss: 0.0264\n",
            "Epoch [2682/20000], Training Loss: 0.0267\n",
            "Epoch [2683/20000], Training Loss: 0.0268\n",
            "Epoch [2684/20000], Training Loss: 0.0255\n",
            "Epoch [2685/20000], Training Loss: 0.0248\n",
            "Epoch [2686/20000], Training Loss: 0.0261\n",
            "Epoch [2687/20000], Training Loss: 0.0254\n",
            "Epoch [2688/20000], Training Loss: 0.0287\n",
            "Epoch [2689/20000], Training Loss: 0.0267\n",
            "Epoch [2690/20000], Training Loss: 0.0265\n",
            "Epoch [2691/20000], Training Loss: 0.0247\n",
            "Epoch [2692/20000], Training Loss: 0.0269\n",
            "Epoch [2693/20000], Training Loss: 0.0273\n",
            "Epoch [2694/20000], Training Loss: 0.0271\n",
            "Epoch [2695/20000], Training Loss: 0.0271\n",
            "Epoch [2696/20000], Training Loss: 0.0274\n",
            "Epoch [2697/20000], Training Loss: 0.0274\n",
            "Epoch [2698/20000], Training Loss: 0.0244\n",
            "Epoch [2699/20000], Training Loss: 0.0275\n",
            "Epoch [2700/20000], Training Loss: 0.0270\n",
            "Epoch [2701/20000], Training Loss: 0.0261\n",
            "Epoch [2702/20000], Training Loss: 0.0243\n",
            "Epoch [2703/20000], Training Loss: 0.0269\n",
            "Epoch [2704/20000], Training Loss: 0.0268\n",
            "Epoch [2705/20000], Training Loss: 0.0274\n",
            "Epoch [2706/20000], Training Loss: 0.0267\n",
            "Epoch [2707/20000], Training Loss: 0.0277\n",
            "Epoch [2708/20000], Training Loss: 0.0283\n",
            "Epoch [2709/20000], Training Loss: 0.0254\n",
            "Epoch [2710/20000], Training Loss: 0.0257\n",
            "Epoch [2711/20000], Training Loss: 0.0261\n",
            "Epoch [2712/20000], Training Loss: 0.0271\n",
            "Epoch [2713/20000], Training Loss: 0.0260\n",
            "Epoch [2714/20000], Training Loss: 0.0264\n",
            "Epoch [2715/20000], Training Loss: 0.0265\n",
            "Epoch [2716/20000], Training Loss: 0.0271\n",
            "Epoch [2717/20000], Training Loss: 0.0273\n",
            "Epoch [2718/20000], Training Loss: 0.0275\n",
            "Epoch [2719/20000], Training Loss: 0.0268\n",
            "Epoch [2720/20000], Training Loss: 0.0259\n",
            "Epoch [2721/20000], Training Loss: 0.0259\n",
            "Epoch [2722/20000], Training Loss: 0.0256\n",
            "Epoch [2723/20000], Training Loss: 0.0283\n",
            "Epoch [2724/20000], Training Loss: 0.0269\n",
            "Epoch [2725/20000], Training Loss: 0.0289\n",
            "Epoch [2726/20000], Training Loss: 0.0256\n",
            "Epoch [2727/20000], Training Loss: 0.0266\n",
            "Epoch [2728/20000], Training Loss: 0.0277\n",
            "Epoch [2729/20000], Training Loss: 0.0280\n",
            "Epoch [2730/20000], Training Loss: 0.0265\n",
            "Epoch [2731/20000], Training Loss: 0.0250\n",
            "Epoch [2732/20000], Training Loss: 0.0258\n",
            "Epoch [2733/20000], Training Loss: 0.0263\n",
            "Epoch [2734/20000], Training Loss: 0.0276\n",
            "Epoch [2735/20000], Training Loss: 0.0283\n",
            "Epoch [2736/20000], Training Loss: 0.0263\n",
            "Epoch [2737/20000], Training Loss: 0.0256\n",
            "Epoch [2738/20000], Training Loss: 0.0256\n",
            "Epoch [2739/20000], Training Loss: 0.0262\n",
            "Epoch [2740/20000], Training Loss: 0.0256\n",
            "Epoch [2741/20000], Training Loss: 0.0278\n",
            "Epoch [2742/20000], Training Loss: 0.0253\n",
            "Epoch [2743/20000], Training Loss: 0.0283\n",
            "Epoch [2744/20000], Training Loss: 0.0256\n",
            "Epoch [2745/20000], Training Loss: 0.0267\n",
            "Epoch [2746/20000], Training Loss: 0.0255\n",
            "Epoch [2747/20000], Training Loss: 0.0274\n",
            "Epoch [2748/20000], Training Loss: 0.0248\n",
            "Epoch [2749/20000], Training Loss: 0.0274\n",
            "Epoch [2750/20000], Training Loss: 0.0284\n",
            "Epoch [2751/20000], Training Loss: 0.0270\n",
            "Epoch [2752/20000], Training Loss: 0.0255\n",
            "Epoch [2753/20000], Training Loss: 0.0267\n",
            "Epoch [2754/20000], Training Loss: 0.0265\n",
            "Epoch [2755/20000], Training Loss: 0.0254\n",
            "Epoch [2756/20000], Training Loss: 0.0253\n",
            "Epoch [2757/20000], Training Loss: 0.0272\n",
            "Epoch [2758/20000], Training Loss: 0.0262\n",
            "Epoch [2759/20000], Training Loss: 0.0266\n",
            "Epoch [2760/20000], Training Loss: 0.0285\n",
            "Epoch [2761/20000], Training Loss: 0.0263\n",
            "Epoch [2762/20000], Training Loss: 0.0281\n",
            "Epoch [2763/20000], Training Loss: 0.0254\n",
            "Epoch [2764/20000], Training Loss: 0.0255\n",
            "Epoch [2765/20000], Training Loss: 0.0258\n",
            "Epoch [2766/20000], Training Loss: 0.0269\n",
            "Epoch [2767/20000], Training Loss: 0.0264\n",
            "Epoch [2768/20000], Training Loss: 0.0298\n",
            "Epoch [2769/20000], Training Loss: 0.0251\n",
            "Epoch [2770/20000], Training Loss: 0.0249\n",
            "Epoch [2771/20000], Training Loss: 0.0238\n",
            "Epoch [2772/20000], Training Loss: 0.0262\n",
            "Epoch [2773/20000], Training Loss: 0.0249\n",
            "Epoch [2774/20000], Training Loss: 0.0264\n",
            "Epoch [2775/20000], Training Loss: 0.0284\n",
            "Epoch [2776/20000], Training Loss: 0.0259\n",
            "Epoch [2777/20000], Training Loss: 0.0251\n",
            "Epoch [2778/20000], Training Loss: 0.0254\n",
            "Epoch [2779/20000], Training Loss: 0.0264\n",
            "Epoch [2780/20000], Training Loss: 0.0278\n",
            "Epoch [2781/20000], Training Loss: 0.0265\n",
            "Epoch [2782/20000], Training Loss: 0.0262\n",
            "Epoch [2783/20000], Training Loss: 0.0270\n",
            "Epoch [2784/20000], Training Loss: 0.0300\n",
            "Epoch [2785/20000], Training Loss: 0.0249\n",
            "Epoch [2786/20000], Training Loss: 0.0260\n",
            "Epoch [2787/20000], Training Loss: 0.0269\n",
            "Epoch [2788/20000], Training Loss: 0.0287\n",
            "Epoch [2789/20000], Training Loss: 0.0255\n",
            "Epoch [2790/20000], Training Loss: 0.0275\n",
            "Epoch [2791/20000], Training Loss: 0.0242\n",
            "Epoch [2792/20000], Training Loss: 0.0245\n",
            "Epoch [2793/20000], Training Loss: 0.0252\n",
            "Epoch [2794/20000], Training Loss: 0.0267\n",
            "Epoch [2795/20000], Training Loss: 0.0240\n",
            "Epoch [2796/20000], Training Loss: 0.0265\n",
            "Epoch [2797/20000], Training Loss: 0.0279\n",
            "Epoch [2798/20000], Training Loss: 0.0266\n",
            "Epoch [2799/20000], Training Loss: 0.0254\n",
            "Epoch [2800/20000], Training Loss: 0.0263\n",
            "Epoch [2801/20000], Training Loss: 0.0286\n",
            "Epoch [2802/20000], Training Loss: 0.0263\n",
            "Epoch [2803/20000], Training Loss: 0.0267\n",
            "Epoch [2804/20000], Training Loss: 0.0285\n",
            "Epoch [2805/20000], Training Loss: 0.0266\n",
            "Epoch [2806/20000], Training Loss: 0.0275\n",
            "Epoch [2807/20000], Training Loss: 0.0252\n",
            "Epoch [2808/20000], Training Loss: 0.0291\n",
            "Epoch [2809/20000], Training Loss: 0.0278\n",
            "Epoch [2810/20000], Training Loss: 0.0264\n",
            "Epoch [2811/20000], Training Loss: 0.0262\n",
            "Epoch [2812/20000], Training Loss: 0.0264\n",
            "Epoch [2813/20000], Training Loss: 0.0249\n",
            "Epoch [2814/20000], Training Loss: 0.0273\n",
            "Epoch [2815/20000], Training Loss: 0.0248\n",
            "Epoch [2816/20000], Training Loss: 0.0261\n",
            "Epoch [2817/20000], Training Loss: 0.0247\n",
            "Epoch [2818/20000], Training Loss: 0.0276\n",
            "Epoch [2819/20000], Training Loss: 0.0259\n",
            "Epoch [2820/20000], Training Loss: 0.0250\n",
            "Epoch [2821/20000], Training Loss: 0.0266\n",
            "Epoch [2822/20000], Training Loss: 0.0270\n",
            "Epoch [2823/20000], Training Loss: 0.0267\n",
            "Epoch [2824/20000], Training Loss: 0.0270\n",
            "Epoch [2825/20000], Training Loss: 0.0262\n",
            "Epoch [2826/20000], Training Loss: 0.0266\n",
            "Epoch [2827/20000], Training Loss: 0.0294\n",
            "Epoch [2828/20000], Training Loss: 0.0260\n",
            "Epoch [2829/20000], Training Loss: 0.0267\n",
            "Epoch [2830/20000], Training Loss: 0.0265\n",
            "Epoch [2831/20000], Training Loss: 0.0260\n",
            "Epoch [2832/20000], Training Loss: 0.0271\n",
            "Epoch [2833/20000], Training Loss: 0.0263\n",
            "Epoch [2834/20000], Training Loss: 0.0266\n",
            "Epoch [2835/20000], Training Loss: 0.0266\n",
            "Epoch [2836/20000], Training Loss: 0.0251\n",
            "Epoch [2837/20000], Training Loss: 0.0246\n",
            "Epoch [2838/20000], Training Loss: 0.0278\n",
            "Epoch [2839/20000], Training Loss: 0.0272\n",
            "Epoch [2840/20000], Training Loss: 0.0267\n",
            "Epoch [2841/20000], Training Loss: 0.0265\n",
            "Epoch [2842/20000], Training Loss: 0.0251\n",
            "Epoch [2843/20000], Training Loss: 0.0264\n",
            "Epoch [2844/20000], Training Loss: 0.0277\n",
            "Epoch [2845/20000], Training Loss: 0.0254\n",
            "Epoch [2846/20000], Training Loss: 0.0270\n",
            "Epoch [2847/20000], Training Loss: 0.0264\n",
            "Epoch [2848/20000], Training Loss: 0.0263\n",
            "Epoch [2849/20000], Training Loss: 0.0277\n",
            "Epoch [2850/20000], Training Loss: 0.0283\n",
            "Epoch [2851/20000], Training Loss: 0.0279\n",
            "Epoch [2852/20000], Training Loss: 0.0263\n",
            "Epoch [2853/20000], Training Loss: 0.0275\n",
            "Epoch [2854/20000], Training Loss: 0.0265\n",
            "Epoch [2855/20000], Training Loss: 0.0283\n",
            "Epoch [2856/20000], Training Loss: 0.0270\n",
            "Epoch [2857/20000], Training Loss: 0.0258\n",
            "Epoch [2858/20000], Training Loss: 0.0281\n",
            "Epoch [2859/20000], Training Loss: 0.0298\n",
            "Epoch [2860/20000], Training Loss: 0.0254\n",
            "Epoch [2861/20000], Training Loss: 0.0267\n",
            "Epoch [2862/20000], Training Loss: 0.0268\n",
            "Epoch [2863/20000], Training Loss: 0.0264\n",
            "Epoch [2864/20000], Training Loss: 0.0253\n",
            "Epoch [2865/20000], Training Loss: 0.0264\n",
            "Epoch [2866/20000], Training Loss: 0.0273\n",
            "Epoch [2867/20000], Training Loss: 0.0269\n",
            "Epoch [2868/20000], Training Loss: 0.0274\n",
            "Epoch [2869/20000], Training Loss: 0.0261\n",
            "Epoch [2870/20000], Training Loss: 0.0283\n",
            "Epoch [2871/20000], Training Loss: 0.0254\n",
            "Epoch [2872/20000], Training Loss: 0.0241\n",
            "Epoch [2873/20000], Training Loss: 0.0259\n",
            "Epoch [2874/20000], Training Loss: 0.0276\n",
            "Epoch [2875/20000], Training Loss: 0.0265\n",
            "Epoch [2876/20000], Training Loss: 0.0269\n",
            "Epoch [2877/20000], Training Loss: 0.0255\n",
            "Epoch [2878/20000], Training Loss: 0.0265\n",
            "Epoch [2879/20000], Training Loss: 0.0251\n",
            "Epoch [2880/20000], Training Loss: 0.0256\n",
            "Epoch [2881/20000], Training Loss: 0.0282\n",
            "Epoch [2882/20000], Training Loss: 0.0262\n",
            "Epoch [2883/20000], Training Loss: 0.0254\n",
            "Epoch [2884/20000], Training Loss: 0.0266\n",
            "Epoch [2885/20000], Training Loss: 0.0265\n",
            "Epoch [2886/20000], Training Loss: 0.0261\n",
            "Epoch [2887/20000], Training Loss: 0.0281\n",
            "Epoch [2888/20000], Training Loss: 0.0280\n",
            "Epoch [2889/20000], Training Loss: 0.0265\n",
            "Epoch [2890/20000], Training Loss: 0.0253\n",
            "Epoch [2891/20000], Training Loss: 0.0254\n",
            "Epoch [2892/20000], Training Loss: 0.0255\n",
            "Epoch [2893/20000], Training Loss: 0.0266\n",
            "Epoch [2894/20000], Training Loss: 0.0252\n",
            "Epoch [2895/20000], Training Loss: 0.0290\n",
            "Epoch [2896/20000], Training Loss: 0.0255\n",
            "Epoch [2897/20000], Training Loss: 0.0271\n",
            "Epoch [2898/20000], Training Loss: 0.0274\n",
            "Epoch [2899/20000], Training Loss: 0.0271\n",
            "Epoch [2900/20000], Training Loss: 0.0267\n",
            "Epoch [2901/20000], Training Loss: 0.0247\n",
            "Epoch [2902/20000], Training Loss: 0.0279\n",
            "Epoch [2903/20000], Training Loss: 0.0264\n",
            "Epoch [2904/20000], Training Loss: 0.0251\n",
            "Epoch [2905/20000], Training Loss: 0.0265\n",
            "Epoch [2906/20000], Training Loss: 0.0260\n",
            "Epoch [2907/20000], Training Loss: 0.0266\n",
            "Epoch [2908/20000], Training Loss: 0.0278\n",
            "Epoch [2909/20000], Training Loss: 0.0259\n",
            "Epoch [2910/20000], Training Loss: 0.0268\n",
            "Epoch [2911/20000], Training Loss: 0.0259\n",
            "Epoch [2912/20000], Training Loss: 0.0265\n",
            "Epoch [2913/20000], Training Loss: 0.0276\n",
            "Epoch [2914/20000], Training Loss: 0.0282\n",
            "Epoch [2915/20000], Training Loss: 0.0260\n",
            "Epoch [2916/20000], Training Loss: 0.0268\n",
            "Epoch [2917/20000], Training Loss: 0.0273\n",
            "Epoch [2918/20000], Training Loss: 0.0257\n",
            "Epoch [2919/20000], Training Loss: 0.0245\n",
            "Epoch [2920/20000], Training Loss: 0.0237\n",
            "Epoch [2921/20000], Training Loss: 0.0249\n",
            "Epoch [2922/20000], Training Loss: 0.0248\n",
            "Epoch [2923/20000], Training Loss: 0.0267\n",
            "Epoch [2924/20000], Training Loss: 0.0273\n",
            "Epoch [2925/20000], Training Loss: 0.0263\n",
            "Epoch [2926/20000], Training Loss: 0.0261\n",
            "Epoch [2927/20000], Training Loss: 0.0287\n",
            "Epoch [2928/20000], Training Loss: 0.0286\n",
            "Epoch [2929/20000], Training Loss: 0.0264\n",
            "Epoch [2930/20000], Training Loss: 0.0267\n",
            "Epoch [2931/20000], Training Loss: 0.0261\n",
            "Epoch [2932/20000], Training Loss: 0.0288\n",
            "Epoch [2933/20000], Training Loss: 0.0266\n",
            "Epoch [2934/20000], Training Loss: 0.0256\n",
            "Epoch [2935/20000], Training Loss: 0.0297\n",
            "Epoch [2936/20000], Training Loss: 0.0266\n",
            "Epoch [2937/20000], Training Loss: 0.0250\n",
            "Epoch [2938/20000], Training Loss: 0.0284\n",
            "Epoch [2939/20000], Training Loss: 0.0262\n",
            "Epoch [2940/20000], Training Loss: 0.0277\n",
            "Epoch [2941/20000], Training Loss: 0.0288\n",
            "Epoch [2942/20000], Training Loss: 0.0261\n",
            "Epoch [2943/20000], Training Loss: 0.0255\n",
            "Epoch [2944/20000], Training Loss: 0.0248\n",
            "Epoch [2945/20000], Training Loss: 0.0266\n",
            "Epoch [2946/20000], Training Loss: 0.0263\n",
            "Epoch [2947/20000], Training Loss: 0.0278\n",
            "Epoch [2948/20000], Training Loss: 0.0241\n",
            "Epoch [2949/20000], Training Loss: 0.0273\n",
            "Epoch [2950/20000], Training Loss: 0.0279\n",
            "Epoch [2951/20000], Training Loss: 0.0249\n",
            "Epoch [2952/20000], Training Loss: 0.0261\n",
            "Epoch [2953/20000], Training Loss: 0.0272\n",
            "Epoch [2954/20000], Training Loss: 0.0263\n",
            "Epoch [2955/20000], Training Loss: 0.0255\n",
            "Epoch [2956/20000], Training Loss: 0.0253\n",
            "Epoch [2957/20000], Training Loss: 0.0258\n",
            "Epoch [2958/20000], Training Loss: 0.0274\n",
            "Epoch [2959/20000], Training Loss: 0.0280\n",
            "Epoch [2960/20000], Training Loss: 0.0279\n",
            "Epoch [2961/20000], Training Loss: 0.0268\n",
            "Epoch [2962/20000], Training Loss: 0.0268\n",
            "Epoch [2963/20000], Training Loss: 0.0247\n",
            "Epoch [2964/20000], Training Loss: 0.0257\n",
            "Epoch [2965/20000], Training Loss: 0.0248\n",
            "Epoch [2966/20000], Training Loss: 0.0273\n",
            "Epoch [2967/20000], Training Loss: 0.0254\n",
            "Epoch [2968/20000], Training Loss: 0.0262\n",
            "Epoch [2969/20000], Training Loss: 0.0265\n",
            "Epoch [2970/20000], Training Loss: 0.0253\n",
            "Epoch [2971/20000], Training Loss: 0.0253\n",
            "Epoch [2972/20000], Training Loss: 0.0257\n",
            "Epoch [2973/20000], Training Loss: 0.0265\n",
            "Epoch [2974/20000], Training Loss: 0.0264\n",
            "Epoch [2975/20000], Training Loss: 0.0258\n",
            "Epoch [2976/20000], Training Loss: 0.0244\n",
            "Epoch [2977/20000], Training Loss: 0.0244\n",
            "Epoch [2978/20000], Training Loss: 0.0256\n",
            "Epoch [2979/20000], Training Loss: 0.0249\n",
            "Epoch [2980/20000], Training Loss: 0.0272\n",
            "Epoch [2981/20000], Training Loss: 0.0257\n",
            "Epoch [2982/20000], Training Loss: 0.0276\n",
            "Epoch [2983/20000], Training Loss: 0.0282\n",
            "Epoch [2984/20000], Training Loss: 0.0254\n",
            "Epoch [2985/20000], Training Loss: 0.0288\n",
            "Epoch [2986/20000], Training Loss: 0.0279\n",
            "Epoch [2987/20000], Training Loss: 0.0282\n",
            "Epoch [2988/20000], Training Loss: 0.0280\n",
            "Epoch [2989/20000], Training Loss: 0.0268\n",
            "Epoch [2990/20000], Training Loss: 0.0258\n",
            "Epoch [2991/20000], Training Loss: 0.0291\n",
            "Epoch [2992/20000], Training Loss: 0.0272\n",
            "Epoch [2993/20000], Training Loss: 0.0262\n",
            "Epoch [2994/20000], Training Loss: 0.0269\n",
            "Epoch [2995/20000], Training Loss: 0.0280\n",
            "Epoch [2996/20000], Training Loss: 0.0253\n",
            "Epoch [2997/20000], Training Loss: 0.0242\n",
            "Epoch [2998/20000], Training Loss: 0.0278\n",
            "Epoch [2999/20000], Training Loss: 0.0258\n",
            "Epoch [3000/20000], Training Loss: 0.0270\n",
            "Epoch [3001/20000], Training Loss: 0.0259\n",
            "Epoch [3002/20000], Training Loss: 0.0278\n",
            "Epoch [3003/20000], Training Loss: 0.0261\n",
            "Epoch [3004/20000], Training Loss: 0.0271\n",
            "Epoch [3005/20000], Training Loss: 0.0282\n",
            "Epoch [3006/20000], Training Loss: 0.0275\n",
            "Epoch [3007/20000], Training Loss: 0.0267\n",
            "Epoch [3008/20000], Training Loss: 0.0262\n",
            "Epoch [3009/20000], Training Loss: 0.0275\n",
            "Epoch [3010/20000], Training Loss: 0.0261\n",
            "Epoch [3011/20000], Training Loss: 0.0266\n",
            "Epoch [3012/20000], Training Loss: 0.0261\n",
            "Epoch [3013/20000], Training Loss: 0.0262\n",
            "Epoch [3014/20000], Training Loss: 0.0267\n",
            "Epoch [3015/20000], Training Loss: 0.0256\n",
            "Epoch [3016/20000], Training Loss: 0.0256\n",
            "Epoch [3017/20000], Training Loss: 0.0268\n",
            "Epoch [3018/20000], Training Loss: 0.0251\n",
            "Epoch [3019/20000], Training Loss: 0.0253\n",
            "Epoch [3020/20000], Training Loss: 0.0264\n",
            "Epoch [3021/20000], Training Loss: 0.0262\n",
            "Epoch [3022/20000], Training Loss: 0.0237\n",
            "Epoch [3023/20000], Training Loss: 0.0244\n",
            "Epoch [3024/20000], Training Loss: 0.0244\n",
            "Epoch [3025/20000], Training Loss: 0.0266\n",
            "Epoch [3026/20000], Training Loss: 0.0274\n",
            "Epoch [3027/20000], Training Loss: 0.0258\n",
            "Epoch [3028/20000], Training Loss: 0.0269\n",
            "Epoch [3029/20000], Training Loss: 0.0256\n",
            "Epoch [3030/20000], Training Loss: 0.0282\n",
            "Epoch [3031/20000], Training Loss: 0.0269\n",
            "Epoch [3032/20000], Training Loss: 0.0248\n",
            "Epoch [3033/20000], Training Loss: 0.0284\n",
            "Epoch [3034/20000], Training Loss: 0.0265\n",
            "Epoch [3035/20000], Training Loss: 0.0258\n",
            "Epoch [3036/20000], Training Loss: 0.0258\n",
            "Epoch [3037/20000], Training Loss: 0.0271\n",
            "Epoch [3038/20000], Training Loss: 0.0259\n",
            "Epoch [3039/20000], Training Loss: 0.0275\n",
            "Epoch [3040/20000], Training Loss: 0.0263\n",
            "Epoch [3041/20000], Training Loss: 0.0281\n",
            "Epoch [3042/20000], Training Loss: 0.0262\n",
            "Epoch [3043/20000], Training Loss: 0.0247\n",
            "Epoch [3044/20000], Training Loss: 0.0266\n",
            "Epoch [3045/20000], Training Loss: 0.0258\n",
            "Epoch [3046/20000], Training Loss: 0.0273\n",
            "Epoch [3047/20000], Training Loss: 0.0269\n",
            "Epoch [3048/20000], Training Loss: 0.0293\n",
            "Epoch [3049/20000], Training Loss: 0.0256\n",
            "Epoch [3050/20000], Training Loss: 0.0282\n",
            "Epoch [3051/20000], Training Loss: 0.0257\n",
            "Epoch [3052/20000], Training Loss: 0.0269\n",
            "Epoch [3053/20000], Training Loss: 0.0258\n",
            "Epoch [3054/20000], Training Loss: 0.0249\n",
            "Epoch [3055/20000], Training Loss: 0.0270\n",
            "Epoch [3056/20000], Training Loss: 0.0256\n",
            "Epoch [3057/20000], Training Loss: 0.0260\n",
            "Epoch [3058/20000], Training Loss: 0.0249\n",
            "Epoch [3059/20000], Training Loss: 0.0263\n",
            "Epoch [3060/20000], Training Loss: 0.0249\n",
            "Epoch [3061/20000], Training Loss: 0.0264\n",
            "Epoch [3062/20000], Training Loss: 0.0261\n",
            "Epoch [3063/20000], Training Loss: 0.0266\n",
            "Epoch [3064/20000], Training Loss: 0.0261\n",
            "Epoch [3065/20000], Training Loss: 0.0265\n",
            "Epoch [3066/20000], Training Loss: 0.0251\n",
            "Epoch [3067/20000], Training Loss: 0.0253\n",
            "Epoch [3068/20000], Training Loss: 0.0260\n",
            "Epoch [3069/20000], Training Loss: 0.0264\n",
            "Epoch [3070/20000], Training Loss: 0.0273\n",
            "Epoch [3071/20000], Training Loss: 0.0271\n",
            "Epoch [3072/20000], Training Loss: 0.0261\n",
            "Epoch [3073/20000], Training Loss: 0.0271\n",
            "Epoch [3074/20000], Training Loss: 0.0274\n",
            "Epoch [3075/20000], Training Loss: 0.0262\n",
            "Epoch [3076/20000], Training Loss: 0.0262\n",
            "Epoch [3077/20000], Training Loss: 0.0283\n",
            "Epoch [3078/20000], Training Loss: 0.0262\n",
            "Epoch [3079/20000], Training Loss: 0.0263\n",
            "Epoch [3080/20000], Training Loss: 0.0259\n",
            "Epoch [3081/20000], Training Loss: 0.0247\n",
            "Epoch [3082/20000], Training Loss: 0.0243\n",
            "Epoch [3083/20000], Training Loss: 0.0273\n",
            "Epoch [3084/20000], Training Loss: 0.0274\n",
            "Epoch [3085/20000], Training Loss: 0.0262\n",
            "Epoch [3086/20000], Training Loss: 0.0279\n",
            "Epoch [3087/20000], Training Loss: 0.0247\n",
            "Epoch [3088/20000], Training Loss: 0.0273\n",
            "Epoch [3089/20000], Training Loss: 0.0264\n",
            "Epoch [3090/20000], Training Loss: 0.0273\n",
            "Epoch [3091/20000], Training Loss: 0.0270\n",
            "Epoch [3092/20000], Training Loss: 0.0285\n",
            "Epoch [3093/20000], Training Loss: 0.0262\n",
            "Epoch [3094/20000], Training Loss: 0.0272\n",
            "Epoch [3095/20000], Training Loss: 0.0259\n",
            "Epoch [3096/20000], Training Loss: 0.0271\n",
            "Epoch [3097/20000], Training Loss: 0.0278\n",
            "Epoch [3098/20000], Training Loss: 0.0274\n",
            "Epoch [3099/20000], Training Loss: 0.0263\n",
            "Epoch [3100/20000], Training Loss: 0.0253\n",
            "Epoch [3101/20000], Training Loss: 0.0285\n",
            "Epoch [3102/20000], Training Loss: 0.0261\n",
            "Epoch [3103/20000], Training Loss: 0.0270\n",
            "Epoch [3104/20000], Training Loss: 0.0253\n",
            "Epoch [3105/20000], Training Loss: 0.0260\n",
            "Epoch [3106/20000], Training Loss: 0.0266\n",
            "Epoch [3107/20000], Training Loss: 0.0256\n",
            "Epoch [3108/20000], Training Loss: 0.0293\n",
            "Epoch [3109/20000], Training Loss: 0.0272\n",
            "Epoch [3110/20000], Training Loss: 0.0266\n",
            "Epoch [3111/20000], Training Loss: 0.0268\n",
            "Epoch [3112/20000], Training Loss: 0.0258\n",
            "Epoch [3113/20000], Training Loss: 0.0268\n",
            "Epoch [3114/20000], Training Loss: 0.0286\n",
            "Epoch [3115/20000], Training Loss: 0.0266\n",
            "Epoch [3116/20000], Training Loss: 0.0268\n",
            "Epoch [3117/20000], Training Loss: 0.0288\n",
            "Epoch [3118/20000], Training Loss: 0.0280\n",
            "Epoch [3119/20000], Training Loss: 0.0268\n",
            "Epoch [3120/20000], Training Loss: 0.0280\n",
            "Epoch [3121/20000], Training Loss: 0.0271\n",
            "Epoch [3122/20000], Training Loss: 0.0262\n",
            "Epoch [3123/20000], Training Loss: 0.0257\n",
            "Epoch [3124/20000], Training Loss: 0.0268\n",
            "Epoch [3125/20000], Training Loss: 0.0242\n",
            "Epoch [3126/20000], Training Loss: 0.0248\n",
            "Epoch [3127/20000], Training Loss: 0.0267\n",
            "Epoch [3128/20000], Training Loss: 0.0259\n",
            "Epoch [3129/20000], Training Loss: 0.0267\n",
            "Epoch [3130/20000], Training Loss: 0.0283\n",
            "Epoch [3131/20000], Training Loss: 0.0259\n",
            "Epoch [3132/20000], Training Loss: 0.0257\n",
            "Epoch [3133/20000], Training Loss: 0.0256\n",
            "Epoch [3134/20000], Training Loss: 0.0260\n",
            "Epoch [3135/20000], Training Loss: 0.0285\n",
            "Epoch [3136/20000], Training Loss: 0.0277\n",
            "Epoch [3137/20000], Training Loss: 0.0263\n",
            "Epoch [3138/20000], Training Loss: 0.0267\n",
            "Epoch [3139/20000], Training Loss: 0.0273\n",
            "Epoch [3140/20000], Training Loss: 0.0268\n",
            "Epoch [3141/20000], Training Loss: 0.0264\n",
            "Epoch [3142/20000], Training Loss: 0.0261\n",
            "Epoch [3143/20000], Training Loss: 0.0259\n",
            "Epoch [3144/20000], Training Loss: 0.0280\n",
            "Epoch [3145/20000], Training Loss: 0.0253\n",
            "Epoch [3146/20000], Training Loss: 0.0278\n",
            "Epoch [3147/20000], Training Loss: 0.0263\n",
            "Epoch [3148/20000], Training Loss: 0.0265\n",
            "Epoch [3149/20000], Training Loss: 0.0277\n",
            "Epoch [3150/20000], Training Loss: 0.0279\n",
            "Epoch [3151/20000], Training Loss: 0.0251\n",
            "Epoch [3152/20000], Training Loss: 0.0272\n",
            "Epoch [3153/20000], Training Loss: 0.0276\n",
            "Epoch [3154/20000], Training Loss: 0.0258\n",
            "Epoch [3155/20000], Training Loss: 0.0264\n",
            "Epoch [3156/20000], Training Loss: 0.0270\n",
            "Epoch [3157/20000], Training Loss: 0.0248\n",
            "Epoch [3158/20000], Training Loss: 0.0261\n",
            "Epoch [3159/20000], Training Loss: 0.0265\n",
            "Epoch [3160/20000], Training Loss: 0.0260\n",
            "Epoch [3161/20000], Training Loss: 0.0283\n",
            "Epoch [3162/20000], Training Loss: 0.0276\n",
            "Epoch [3163/20000], Training Loss: 0.0269\n",
            "Epoch [3164/20000], Training Loss: 0.0268\n",
            "Epoch [3165/20000], Training Loss: 0.0271\n",
            "Epoch [3166/20000], Training Loss: 0.0278\n",
            "Epoch [3167/20000], Training Loss: 0.0241\n",
            "Epoch [3168/20000], Training Loss: 0.0263\n",
            "Epoch [3169/20000], Training Loss: 0.0264\n",
            "Epoch [3170/20000], Training Loss: 0.0266\n",
            "Epoch [3171/20000], Training Loss: 0.0261\n",
            "Epoch [3172/20000], Training Loss: 0.0277\n",
            "Epoch [3173/20000], Training Loss: 0.0257\n",
            "Epoch [3174/20000], Training Loss: 0.0268\n",
            "Epoch [3175/20000], Training Loss: 0.0272\n",
            "Epoch [3176/20000], Training Loss: 0.0260\n",
            "Epoch [3177/20000], Training Loss: 0.0238\n",
            "Epoch [3178/20000], Training Loss: 0.0253\n",
            "Epoch [3179/20000], Training Loss: 0.0259\n",
            "Epoch [3180/20000], Training Loss: 0.0265\n",
            "Epoch [3181/20000], Training Loss: 0.0270\n",
            "Epoch [3182/20000], Training Loss: 0.0249\n",
            "Epoch [3183/20000], Training Loss: 0.0261\n",
            "Epoch [3184/20000], Training Loss: 0.0275\n",
            "Epoch [3185/20000], Training Loss: 0.0266\n",
            "Epoch [3186/20000], Training Loss: 0.0279\n",
            "Epoch [3187/20000], Training Loss: 0.0267\n",
            "Epoch [3188/20000], Training Loss: 0.0257\n",
            "Epoch [3189/20000], Training Loss: 0.0271\n",
            "Epoch [3190/20000], Training Loss: 0.0255\n",
            "Epoch [3191/20000], Training Loss: 0.0248\n",
            "Epoch [3192/20000], Training Loss: 0.0267\n",
            "Epoch [3193/20000], Training Loss: 0.0256\n",
            "Epoch [3194/20000], Training Loss: 0.0254\n",
            "Epoch [3195/20000], Training Loss: 0.0262\n",
            "Epoch [3196/20000], Training Loss: 0.0278\n",
            "Epoch [3197/20000], Training Loss: 0.0267\n",
            "Epoch [3198/20000], Training Loss: 0.0265\n",
            "Epoch [3199/20000], Training Loss: 0.0277\n",
            "Epoch [3200/20000], Training Loss: 0.0261\n",
            "Epoch [3201/20000], Training Loss: 0.0255\n",
            "Epoch [3202/20000], Training Loss: 0.0294\n",
            "Epoch [3203/20000], Training Loss: 0.0262\n",
            "Epoch [3204/20000], Training Loss: 0.0269\n",
            "Epoch [3205/20000], Training Loss: 0.0263\n",
            "Epoch [3206/20000], Training Loss: 0.0264\n",
            "Epoch [3207/20000], Training Loss: 0.0284\n",
            "Epoch [3208/20000], Training Loss: 0.0259\n",
            "Epoch [3209/20000], Training Loss: 0.0291\n",
            "Epoch [3210/20000], Training Loss: 0.0270\n",
            "Epoch [3211/20000], Training Loss: 0.0251\n",
            "Epoch [3212/20000], Training Loss: 0.0258\n",
            "Epoch [3213/20000], Training Loss: 0.0257\n",
            "Epoch [3214/20000], Training Loss: 0.0261\n",
            "Epoch [3215/20000], Training Loss: 0.0264\n",
            "Epoch [3216/20000], Training Loss: 0.0257\n",
            "Epoch [3217/20000], Training Loss: 0.0286\n",
            "Epoch [3218/20000], Training Loss: 0.0266\n",
            "Epoch [3219/20000], Training Loss: 0.0282\n",
            "Epoch [3220/20000], Training Loss: 0.0277\n",
            "Epoch [3221/20000], Training Loss: 0.0265\n",
            "Epoch [3222/20000], Training Loss: 0.0249\n",
            "Epoch [3223/20000], Training Loss: 0.0269\n",
            "Epoch [3224/20000], Training Loss: 0.0255\n",
            "Epoch [3225/20000], Training Loss: 0.0286\n",
            "Epoch [3226/20000], Training Loss: 0.0274\n",
            "Epoch [3227/20000], Training Loss: 0.0274\n",
            "Epoch [3228/20000], Training Loss: 0.0248\n",
            "Epoch [3229/20000], Training Loss: 0.0275\n",
            "Epoch [3230/20000], Training Loss: 0.0260\n",
            "Epoch [3231/20000], Training Loss: 0.0282\n",
            "Epoch [3232/20000], Training Loss: 0.0280\n",
            "Epoch [3233/20000], Training Loss: 0.0275\n",
            "Epoch [3234/20000], Training Loss: 0.0280\n",
            "Epoch [3235/20000], Training Loss: 0.0256\n",
            "Epoch [3236/20000], Training Loss: 0.0259\n",
            "Epoch [3237/20000], Training Loss: 0.0246\n",
            "Epoch [3238/20000], Training Loss: 0.0269\n",
            "Epoch [3239/20000], Training Loss: 0.0248\n",
            "Epoch [3240/20000], Training Loss: 0.0274\n",
            "Epoch [3241/20000], Training Loss: 0.0266\n",
            "Epoch [3242/20000], Training Loss: 0.0271\n",
            "Epoch [3243/20000], Training Loss: 0.0254\n",
            "Epoch [3244/20000], Training Loss: 0.0269\n",
            "Epoch [3245/20000], Training Loss: 0.0269\n",
            "Epoch [3246/20000], Training Loss: 0.0243\n",
            "Epoch [3247/20000], Training Loss: 0.0271\n",
            "Epoch [3248/20000], Training Loss: 0.0264\n",
            "Epoch [3249/20000], Training Loss: 0.0275\n",
            "Epoch [3250/20000], Training Loss: 0.0279\n",
            "Epoch [3251/20000], Training Loss: 0.0258\n",
            "Epoch [3252/20000], Training Loss: 0.0268\n",
            "Epoch [3253/20000], Training Loss: 0.0246\n",
            "Epoch [3254/20000], Training Loss: 0.0254\n",
            "Epoch [3255/20000], Training Loss: 0.0278\n",
            "Epoch [3256/20000], Training Loss: 0.0253\n",
            "Epoch [3257/20000], Training Loss: 0.0258\n",
            "Epoch [3258/20000], Training Loss: 0.0284\n",
            "Epoch [3259/20000], Training Loss: 0.0269\n",
            "Epoch [3260/20000], Training Loss: 0.0261\n",
            "Epoch [3261/20000], Training Loss: 0.0274\n",
            "Epoch [3262/20000], Training Loss: 0.0273\n",
            "Epoch [3263/20000], Training Loss: 0.0286\n",
            "Epoch [3264/20000], Training Loss: 0.0273\n",
            "Epoch [3265/20000], Training Loss: 0.0256\n",
            "Epoch [3266/20000], Training Loss: 0.0271\n",
            "Epoch [3267/20000], Training Loss: 0.0271\n",
            "Epoch [3268/20000], Training Loss: 0.0246\n",
            "Epoch [3269/20000], Training Loss: 0.0252\n",
            "Epoch [3270/20000], Training Loss: 0.0278\n",
            "Epoch [3271/20000], Training Loss: 0.0244\n",
            "Epoch [3272/20000], Training Loss: 0.0289\n",
            "Epoch [3273/20000], Training Loss: 0.0265\n",
            "Epoch [3274/20000], Training Loss: 0.0272\n",
            "Epoch [3275/20000], Training Loss: 0.0247\n",
            "Epoch [3276/20000], Training Loss: 0.0272\n",
            "Epoch [3277/20000], Training Loss: 0.0256\n",
            "Epoch [3278/20000], Training Loss: 0.0270\n",
            "Epoch [3279/20000], Training Loss: 0.0261\n",
            "Epoch [3280/20000], Training Loss: 0.0285\n",
            "Epoch [3281/20000], Training Loss: 0.0281\n",
            "Epoch [3282/20000], Training Loss: 0.0261\n",
            "Epoch [3283/20000], Training Loss: 0.0254\n",
            "Epoch [3284/20000], Training Loss: 0.0288\n",
            "Epoch [3285/20000], Training Loss: 0.0284\n",
            "Epoch [3286/20000], Training Loss: 0.0251\n",
            "Epoch [3287/20000], Training Loss: 0.0258\n",
            "Epoch [3288/20000], Training Loss: 0.0264\n",
            "Epoch [3289/20000], Training Loss: 0.0275\n",
            "Epoch [3290/20000], Training Loss: 0.0274\n",
            "Epoch [3291/20000], Training Loss: 0.0275\n",
            "Epoch [3292/20000], Training Loss: 0.0263\n",
            "Epoch [3293/20000], Training Loss: 0.0267\n",
            "Epoch [3294/20000], Training Loss: 0.0242\n",
            "Epoch [3295/20000], Training Loss: 0.0252\n",
            "Epoch [3296/20000], Training Loss: 0.0279\n",
            "Epoch [3297/20000], Training Loss: 0.0264\n",
            "Epoch [3298/20000], Training Loss: 0.0254\n",
            "Epoch [3299/20000], Training Loss: 0.0247\n",
            "Epoch [3300/20000], Training Loss: 0.0256\n",
            "Epoch [3301/20000], Training Loss: 0.0271\n",
            "Epoch [3302/20000], Training Loss: 0.0278\n",
            "Epoch [3303/20000], Training Loss: 0.0271\n",
            "Epoch [3304/20000], Training Loss: 0.0274\n",
            "Epoch [3305/20000], Training Loss: 0.0251\n",
            "Epoch [3306/20000], Training Loss: 0.0295\n",
            "Epoch [3307/20000], Training Loss: 0.0276\n",
            "Epoch [3308/20000], Training Loss: 0.0277\n",
            "Epoch [3309/20000], Training Loss: 0.0265\n",
            "Epoch [3310/20000], Training Loss: 0.0278\n",
            "Epoch [3311/20000], Training Loss: 0.0278\n",
            "Epoch [3312/20000], Training Loss: 0.0279\n",
            "Epoch [3313/20000], Training Loss: 0.0271\n",
            "Epoch [3314/20000], Training Loss: 0.0257\n",
            "Epoch [3315/20000], Training Loss: 0.0247\n",
            "Epoch [3316/20000], Training Loss: 0.0256\n",
            "Epoch [3317/20000], Training Loss: 0.0236\n",
            "Epoch [3318/20000], Training Loss: 0.0268\n",
            "Epoch [3319/20000], Training Loss: 0.0264\n",
            "Epoch [3320/20000], Training Loss: 0.0261\n",
            "Epoch [3321/20000], Training Loss: 0.0252\n",
            "Epoch [3322/20000], Training Loss: 0.0262\n",
            "Epoch [3323/20000], Training Loss: 0.0287\n",
            "Epoch [3324/20000], Training Loss: 0.0275\n",
            "Epoch [3325/20000], Training Loss: 0.0239\n",
            "Epoch [3326/20000], Training Loss: 0.0272\n",
            "Epoch [3327/20000], Training Loss: 0.0255\n",
            "Epoch [3328/20000], Training Loss: 0.0275\n",
            "Epoch [3329/20000], Training Loss: 0.0303\n",
            "Epoch [3330/20000], Training Loss: 0.0248\n",
            "Epoch [3331/20000], Training Loss: 0.0271\n",
            "Epoch [3332/20000], Training Loss: 0.0269\n",
            "Epoch [3333/20000], Training Loss: 0.0255\n",
            "Epoch [3334/20000], Training Loss: 0.0284\n",
            "Epoch [3335/20000], Training Loss: 0.0248\n",
            "Epoch [3336/20000], Training Loss: 0.0263\n",
            "Epoch [3337/20000], Training Loss: 0.0263\n",
            "Epoch [3338/20000], Training Loss: 0.0271\n",
            "Epoch [3339/20000], Training Loss: 0.0269\n",
            "Epoch [3340/20000], Training Loss: 0.0253\n",
            "Epoch [3341/20000], Training Loss: 0.0247\n",
            "Epoch [3342/20000], Training Loss: 0.0240\n",
            "Epoch [3343/20000], Training Loss: 0.0247\n",
            "Epoch [3344/20000], Training Loss: 0.0265\n",
            "Epoch [3345/20000], Training Loss: 0.0264\n",
            "Epoch [3346/20000], Training Loss: 0.0270\n",
            "Epoch [3347/20000], Training Loss: 0.0252\n",
            "Epoch [3348/20000], Training Loss: 0.0267\n",
            "Epoch [3349/20000], Training Loss: 0.0272\n",
            "Epoch [3350/20000], Training Loss: 0.0247\n",
            "Epoch [3351/20000], Training Loss: 0.0279\n",
            "Epoch [3352/20000], Training Loss: 0.0271\n",
            "Epoch [3353/20000], Training Loss: 0.0263\n",
            "Epoch [3354/20000], Training Loss: 0.0251\n",
            "Epoch [3355/20000], Training Loss: 0.0270\n",
            "Epoch [3356/20000], Training Loss: 0.0269\n",
            "Epoch [3357/20000], Training Loss: 0.0267\n",
            "Epoch [3358/20000], Training Loss: 0.0279\n",
            "Epoch [3359/20000], Training Loss: 0.0284\n",
            "Epoch [3360/20000], Training Loss: 0.0271\n",
            "Epoch [3361/20000], Training Loss: 0.0249\n",
            "Epoch [3362/20000], Training Loss: 0.0258\n",
            "Epoch [3363/20000], Training Loss: 0.0257\n",
            "Epoch [3364/20000], Training Loss: 0.0252\n",
            "Epoch [3365/20000], Training Loss: 0.0264\n",
            "Epoch [3366/20000], Training Loss: 0.0298\n",
            "Epoch [3367/20000], Training Loss: 0.0253\n",
            "Epoch [3368/20000], Training Loss: 0.0265\n",
            "Epoch [3369/20000], Training Loss: 0.0272\n",
            "Epoch [3370/20000], Training Loss: 0.0258\n",
            "Epoch [3371/20000], Training Loss: 0.0283\n",
            "Epoch [3372/20000], Training Loss: 0.0250\n",
            "Epoch [3373/20000], Training Loss: 0.0254\n",
            "Epoch [3374/20000], Training Loss: 0.0258\n",
            "Epoch [3375/20000], Training Loss: 0.0257\n",
            "Epoch [3376/20000], Training Loss: 0.0256\n",
            "Epoch [3377/20000], Training Loss: 0.0261\n",
            "Epoch [3378/20000], Training Loss: 0.0282\n",
            "Epoch [3379/20000], Training Loss: 0.0279\n",
            "Epoch [3380/20000], Training Loss: 0.0272\n",
            "Epoch [3381/20000], Training Loss: 0.0260\n",
            "Epoch [3382/20000], Training Loss: 0.0259\n",
            "Epoch [3383/20000], Training Loss: 0.0266\n",
            "Epoch [3384/20000], Training Loss: 0.0276\n",
            "Epoch [3385/20000], Training Loss: 0.0269\n",
            "Epoch [3386/20000], Training Loss: 0.0258\n",
            "Epoch [3387/20000], Training Loss: 0.0253\n",
            "Epoch [3388/20000], Training Loss: 0.0274\n",
            "Epoch [3389/20000], Training Loss: 0.0271\n",
            "Epoch [3390/20000], Training Loss: 0.0266\n",
            "Epoch [3391/20000], Training Loss: 0.0272\n",
            "Epoch [3392/20000], Training Loss: 0.0264\n",
            "Epoch [3393/20000], Training Loss: 0.0242\n",
            "Epoch [3394/20000], Training Loss: 0.0271\n",
            "Epoch [3395/20000], Training Loss: 0.0268\n",
            "Epoch [3396/20000], Training Loss: 0.0265\n",
            "Epoch [3397/20000], Training Loss: 0.0262\n",
            "Epoch [3398/20000], Training Loss: 0.0257\n",
            "Epoch [3399/20000], Training Loss: 0.0257\n",
            "Epoch [3400/20000], Training Loss: 0.0247\n",
            "Epoch [3401/20000], Training Loss: 0.0265\n",
            "Epoch [3402/20000], Training Loss: 0.0269\n",
            "Epoch [3403/20000], Training Loss: 0.0257\n",
            "Epoch [3404/20000], Training Loss: 0.0247\n",
            "Epoch [3405/20000], Training Loss: 0.0256\n",
            "Epoch [3406/20000], Training Loss: 0.0257\n",
            "Epoch [3407/20000], Training Loss: 0.0268\n",
            "Epoch [3408/20000], Training Loss: 0.0264\n",
            "Epoch [3409/20000], Training Loss: 0.0264\n",
            "Epoch [3410/20000], Training Loss: 0.0284\n",
            "Epoch [3411/20000], Training Loss: 0.0268\n",
            "Epoch [3412/20000], Training Loss: 0.0275\n",
            "Epoch [3413/20000], Training Loss: 0.0270\n",
            "Epoch [3414/20000], Training Loss: 0.0264\n",
            "Epoch [3415/20000], Training Loss: 0.0289\n",
            "Epoch [3416/20000], Training Loss: 0.0277\n",
            "Epoch [3417/20000], Training Loss: 0.0260\n",
            "Epoch [3418/20000], Training Loss: 0.0281\n",
            "Epoch [3419/20000], Training Loss: 0.0288\n",
            "Epoch [3420/20000], Training Loss: 0.0266\n",
            "Epoch [3421/20000], Training Loss: 0.0277\n",
            "Epoch [3422/20000], Training Loss: 0.0280\n",
            "Epoch [3423/20000], Training Loss: 0.0248\n",
            "Epoch [3424/20000], Training Loss: 0.0264\n",
            "Epoch [3425/20000], Training Loss: 0.0251\n",
            "Epoch [3426/20000], Training Loss: 0.0274\n",
            "Epoch [3427/20000], Training Loss: 0.0280\n",
            "Epoch [3428/20000], Training Loss: 0.0247\n",
            "Epoch [3429/20000], Training Loss: 0.0248\n",
            "Epoch [3430/20000], Training Loss: 0.0257\n",
            "Epoch [3431/20000], Training Loss: 0.0265\n",
            "Epoch [3432/20000], Training Loss: 0.0265\n",
            "Epoch [3433/20000], Training Loss: 0.0268\n",
            "Epoch [3434/20000], Training Loss: 0.0267\n",
            "Epoch [3435/20000], Training Loss: 0.0262\n",
            "Epoch [3436/20000], Training Loss: 0.0251\n",
            "Epoch [3437/20000], Training Loss: 0.0246\n",
            "Epoch [3438/20000], Training Loss: 0.0261\n",
            "Epoch [3439/20000], Training Loss: 0.0255\n",
            "Epoch [3440/20000], Training Loss: 0.0270\n",
            "Epoch [3441/20000], Training Loss: 0.0276\n",
            "Epoch [3442/20000], Training Loss: 0.0259\n",
            "Epoch [3443/20000], Training Loss: 0.0277\n",
            "Epoch [3444/20000], Training Loss: 0.0274\n",
            "Epoch [3445/20000], Training Loss: 0.0280\n",
            "Epoch [3446/20000], Training Loss: 0.0292\n",
            "Epoch [3447/20000], Training Loss: 0.0272\n",
            "Epoch [3448/20000], Training Loss: 0.0258\n",
            "Epoch [3449/20000], Training Loss: 0.0248\n",
            "Epoch [3450/20000], Training Loss: 0.0262\n",
            "Epoch [3451/20000], Training Loss: 0.0270\n",
            "Epoch [3452/20000], Training Loss: 0.0246\n",
            "Epoch [3453/20000], Training Loss: 0.0275\n",
            "Epoch [3454/20000], Training Loss: 0.0274\n",
            "Epoch [3455/20000], Training Loss: 0.0270\n",
            "Epoch [3456/20000], Training Loss: 0.0270\n",
            "Epoch [3457/20000], Training Loss: 0.0267\n",
            "Epoch [3458/20000], Training Loss: 0.0262\n",
            "Epoch [3459/20000], Training Loss: 0.0266\n",
            "Epoch [3460/20000], Training Loss: 0.0267\n",
            "Epoch [3461/20000], Training Loss: 0.0243\n",
            "Epoch [3462/20000], Training Loss: 0.0278\n",
            "Epoch [3463/20000], Training Loss: 0.0262\n",
            "Epoch [3464/20000], Training Loss: 0.0271\n",
            "Epoch [3465/20000], Training Loss: 0.0266\n",
            "Epoch [3466/20000], Training Loss: 0.0244\n",
            "Epoch [3467/20000], Training Loss: 0.0266\n",
            "Epoch [3468/20000], Training Loss: 0.0252\n",
            "Epoch [3469/20000], Training Loss: 0.0271\n",
            "Epoch [3470/20000], Training Loss: 0.0267\n",
            "Epoch [3471/20000], Training Loss: 0.0269\n",
            "Epoch [3472/20000], Training Loss: 0.0265\n",
            "Epoch [3473/20000], Training Loss: 0.0258\n",
            "Epoch [3474/20000], Training Loss: 0.0238\n",
            "Epoch [3475/20000], Training Loss: 0.0272\n",
            "Epoch [3476/20000], Training Loss: 0.0243\n",
            "Epoch [3477/20000], Training Loss: 0.0275\n",
            "Epoch [3478/20000], Training Loss: 0.0266\n",
            "Epoch [3479/20000], Training Loss: 0.0250\n",
            "Epoch [3480/20000], Training Loss: 0.0271\n",
            "Epoch [3481/20000], Training Loss: 0.0247\n",
            "Epoch [3482/20000], Training Loss: 0.0286\n",
            "Epoch [3483/20000], Training Loss: 0.0267\n",
            "Epoch [3484/20000], Training Loss: 0.0261\n",
            "Epoch [3485/20000], Training Loss: 0.0273\n",
            "Epoch [3486/20000], Training Loss: 0.0269\n",
            "Epoch [3487/20000], Training Loss: 0.0249\n",
            "Epoch [3488/20000], Training Loss: 0.0260\n",
            "Epoch [3489/20000], Training Loss: 0.0252\n",
            "Epoch [3490/20000], Training Loss: 0.0263\n",
            "Epoch [3491/20000], Training Loss: 0.0260\n",
            "Epoch [3492/20000], Training Loss: 0.0260\n",
            "Epoch [3493/20000], Training Loss: 0.0270\n",
            "Epoch [3494/20000], Training Loss: 0.0268\n",
            "Epoch [3495/20000], Training Loss: 0.0288\n",
            "Epoch [3496/20000], Training Loss: 0.0268\n",
            "Epoch [3497/20000], Training Loss: 0.0260\n",
            "Epoch [3498/20000], Training Loss: 0.0281\n",
            "Epoch [3499/20000], Training Loss: 0.0277\n",
            "Epoch [3500/20000], Training Loss: 0.0268\n",
            "Epoch [3501/20000], Training Loss: 0.0259\n",
            "Epoch [3502/20000], Training Loss: 0.0275\n",
            "Epoch [3503/20000], Training Loss: 0.0252\n",
            "Epoch [3504/20000], Training Loss: 0.0265\n",
            "Epoch [3505/20000], Training Loss: 0.0256\n",
            "Epoch [3506/20000], Training Loss: 0.0289\n",
            "Epoch [3507/20000], Training Loss: 0.0249\n",
            "Epoch [3508/20000], Training Loss: 0.0257\n",
            "Epoch [3509/20000], Training Loss: 0.0271\n",
            "Epoch [3510/20000], Training Loss: 0.0270\n",
            "Epoch [3511/20000], Training Loss: 0.0264\n",
            "Epoch [3512/20000], Training Loss: 0.0258\n",
            "Epoch [3513/20000], Training Loss: 0.0258\n",
            "Epoch [3514/20000], Training Loss: 0.0250\n",
            "Epoch [3515/20000], Training Loss: 0.0268\n",
            "Epoch [3516/20000], Training Loss: 0.0279\n",
            "Epoch [3517/20000], Training Loss: 0.0256\n",
            "Epoch [3518/20000], Training Loss: 0.0275\n",
            "Epoch [3519/20000], Training Loss: 0.0258\n",
            "Epoch [3520/20000], Training Loss: 0.0270\n",
            "Epoch [3521/20000], Training Loss: 0.0268\n",
            "Epoch [3522/20000], Training Loss: 0.0252\n",
            "Epoch [3523/20000], Training Loss: 0.0274\n",
            "Epoch [3524/20000], Training Loss: 0.0261\n",
            "Epoch [3525/20000], Training Loss: 0.0264\n",
            "Epoch [3526/20000], Training Loss: 0.0274\n",
            "Epoch [3527/20000], Training Loss: 0.0265\n",
            "Epoch [3528/20000], Training Loss: 0.0284\n",
            "Epoch [3529/20000], Training Loss: 0.0256\n",
            "Epoch [3530/20000], Training Loss: 0.0289\n",
            "Epoch [3531/20000], Training Loss: 0.0256\n",
            "Epoch [3532/20000], Training Loss: 0.0251\n",
            "Epoch [3533/20000], Training Loss: 0.0255\n",
            "Epoch [3534/20000], Training Loss: 0.0293\n",
            "Epoch [3535/20000], Training Loss: 0.0265\n",
            "Epoch [3536/20000], Training Loss: 0.0282\n",
            "Epoch [3537/20000], Training Loss: 0.0282\n",
            "Epoch [3538/20000], Training Loss: 0.0258\n",
            "Epoch [3539/20000], Training Loss: 0.0244\n",
            "Epoch [3540/20000], Training Loss: 0.0268\n",
            "Epoch [3541/20000], Training Loss: 0.0258\n",
            "Epoch [3542/20000], Training Loss: 0.0265\n",
            "Epoch [3543/20000], Training Loss: 0.0269\n",
            "Epoch [3544/20000], Training Loss: 0.0284\n",
            "Epoch [3545/20000], Training Loss: 0.0264\n",
            "Epoch [3546/20000], Training Loss: 0.0268\n",
            "Epoch [3547/20000], Training Loss: 0.0281\n",
            "Epoch [3548/20000], Training Loss: 0.0263\n",
            "Epoch [3549/20000], Training Loss: 0.0292\n",
            "Epoch [3550/20000], Training Loss: 0.0255\n",
            "Epoch [3551/20000], Training Loss: 0.0246\n",
            "Epoch [3552/20000], Training Loss: 0.0252\n",
            "Epoch [3553/20000], Training Loss: 0.0267\n",
            "Epoch [3554/20000], Training Loss: 0.0278\n",
            "Epoch [3555/20000], Training Loss: 0.0275\n",
            "Epoch [3556/20000], Training Loss: 0.0253\n",
            "Epoch [3557/20000], Training Loss: 0.0257\n",
            "Epoch [3558/20000], Training Loss: 0.0258\n",
            "Epoch [3559/20000], Training Loss: 0.0271\n",
            "Epoch [3560/20000], Training Loss: 0.0266\n",
            "Epoch [3561/20000], Training Loss: 0.0257\n",
            "Epoch [3562/20000], Training Loss: 0.0271\n",
            "Epoch [3563/20000], Training Loss: 0.0256\n",
            "Epoch [3564/20000], Training Loss: 0.0260\n",
            "Epoch [3565/20000], Training Loss: 0.0263\n",
            "Epoch [3566/20000], Training Loss: 0.0251\n",
            "Epoch [3567/20000], Training Loss: 0.0265\n",
            "Epoch [3568/20000], Training Loss: 0.0263\n",
            "Epoch [3569/20000], Training Loss: 0.0267\n",
            "Epoch [3570/20000], Training Loss: 0.0280\n",
            "Epoch [3571/20000], Training Loss: 0.0261\n",
            "Epoch [3572/20000], Training Loss: 0.0286\n",
            "Epoch [3573/20000], Training Loss: 0.0285\n",
            "Epoch [3574/20000], Training Loss: 0.0256\n",
            "Epoch [3575/20000], Training Loss: 0.0262\n",
            "Epoch [3576/20000], Training Loss: 0.0257\n",
            "Epoch [3577/20000], Training Loss: 0.0269\n",
            "Epoch [3578/20000], Training Loss: 0.0286\n",
            "Epoch [3579/20000], Training Loss: 0.0250\n",
            "Epoch [3580/20000], Training Loss: 0.0277\n",
            "Epoch [3581/20000], Training Loss: 0.0269\n",
            "Epoch [3582/20000], Training Loss: 0.0271\n",
            "Epoch [3583/20000], Training Loss: 0.0256\n",
            "Epoch [3584/20000], Training Loss: 0.0285\n",
            "Epoch [3585/20000], Training Loss: 0.0270\n",
            "Epoch [3586/20000], Training Loss: 0.0270\n",
            "Epoch [3587/20000], Training Loss: 0.0272\n",
            "Epoch [3588/20000], Training Loss: 0.0279\n",
            "Epoch [3589/20000], Training Loss: 0.0259\n",
            "Epoch [3590/20000], Training Loss: 0.0280\n",
            "Epoch [3591/20000], Training Loss: 0.0272\n",
            "Epoch [3592/20000], Training Loss: 0.0262\n",
            "Epoch [3593/20000], Training Loss: 0.0268\n",
            "Epoch [3594/20000], Training Loss: 0.0276\n",
            "Epoch [3595/20000], Training Loss: 0.0255\n",
            "Epoch [3596/20000], Training Loss: 0.0260\n",
            "Epoch [3597/20000], Training Loss: 0.0255\n",
            "Epoch [3598/20000], Training Loss: 0.0276\n",
            "Epoch [3599/20000], Training Loss: 0.0253\n",
            "Epoch [3600/20000], Training Loss: 0.0241\n",
            "Epoch [3601/20000], Training Loss: 0.0269\n",
            "Epoch [3602/20000], Training Loss: 0.0266\n",
            "Epoch [3603/20000], Training Loss: 0.0259\n",
            "Epoch [3604/20000], Training Loss: 0.0244\n",
            "Epoch [3605/20000], Training Loss: 0.0266\n",
            "Epoch [3606/20000], Training Loss: 0.0290\n",
            "Epoch [3607/20000], Training Loss: 0.0258\n",
            "Epoch [3608/20000], Training Loss: 0.0284\n",
            "Epoch [3609/20000], Training Loss: 0.0273\n",
            "Epoch [3610/20000], Training Loss: 0.0281\n",
            "Epoch [3611/20000], Training Loss: 0.0274\n",
            "Epoch [3612/20000], Training Loss: 0.0286\n",
            "Epoch [3613/20000], Training Loss: 0.0290\n",
            "Epoch [3614/20000], Training Loss: 0.0264\n",
            "Epoch [3615/20000], Training Loss: 0.0247\n",
            "Epoch [3616/20000], Training Loss: 0.0264\n",
            "Epoch [3617/20000], Training Loss: 0.0261\n",
            "Epoch [3618/20000], Training Loss: 0.0280\n",
            "Epoch [3619/20000], Training Loss: 0.0264\n",
            "Epoch [3620/20000], Training Loss: 0.0275\n",
            "Epoch [3621/20000], Training Loss: 0.0261\n",
            "Epoch [3622/20000], Training Loss: 0.0254\n",
            "Epoch [3623/20000], Training Loss: 0.0246\n",
            "Epoch [3624/20000], Training Loss: 0.0257\n",
            "Epoch [3625/20000], Training Loss: 0.0261\n",
            "Epoch [3626/20000], Training Loss: 0.0281\n",
            "Epoch [3627/20000], Training Loss: 0.0275\n",
            "Epoch [3628/20000], Training Loss: 0.0281\n",
            "Epoch [3629/20000], Training Loss: 0.0259\n",
            "Epoch [3630/20000], Training Loss: 0.0256\n",
            "Epoch [3631/20000], Training Loss: 0.0243\n",
            "Epoch [3632/20000], Training Loss: 0.0273\n",
            "Epoch [3633/20000], Training Loss: 0.0262\n",
            "Epoch [3634/20000], Training Loss: 0.0280\n",
            "Epoch [3635/20000], Training Loss: 0.0277\n",
            "Epoch [3636/20000], Training Loss: 0.0271\n",
            "Epoch [3637/20000], Training Loss: 0.0256\n",
            "Epoch [3638/20000], Training Loss: 0.0267\n",
            "Epoch [3639/20000], Training Loss: 0.0269\n",
            "Epoch [3640/20000], Training Loss: 0.0243\n",
            "Epoch [3641/20000], Training Loss: 0.0286\n",
            "Epoch [3642/20000], Training Loss: 0.0284\n",
            "Epoch [3643/20000], Training Loss: 0.0252\n",
            "Epoch [3644/20000], Training Loss: 0.0274\n",
            "Epoch [3645/20000], Training Loss: 0.0246\n",
            "Epoch [3646/20000], Training Loss: 0.0260\n",
            "Epoch [3647/20000], Training Loss: 0.0262\n",
            "Epoch [3648/20000], Training Loss: 0.0260\n",
            "Epoch [3649/20000], Training Loss: 0.0250\n",
            "Epoch [3650/20000], Training Loss: 0.0272\n",
            "Epoch [3651/20000], Training Loss: 0.0247\n",
            "Epoch [3652/20000], Training Loss: 0.0274\n",
            "Epoch [3653/20000], Training Loss: 0.0242\n",
            "Epoch [3654/20000], Training Loss: 0.0282\n",
            "Epoch [3655/20000], Training Loss: 0.0270\n",
            "Epoch [3656/20000], Training Loss: 0.0277\n",
            "Epoch [3657/20000], Training Loss: 0.0280\n",
            "Epoch [3658/20000], Training Loss: 0.0263\n",
            "Epoch [3659/20000], Training Loss: 0.0271\n",
            "Epoch [3660/20000], Training Loss: 0.0265\n",
            "Epoch [3661/20000], Training Loss: 0.0278\n",
            "Epoch [3662/20000], Training Loss: 0.0282\n",
            "Epoch [3663/20000], Training Loss: 0.0264\n",
            "Epoch [3664/20000], Training Loss: 0.0285\n",
            "Epoch [3665/20000], Training Loss: 0.0248\n",
            "Epoch [3666/20000], Training Loss: 0.0251\n",
            "Epoch [3667/20000], Training Loss: 0.0270\n",
            "Epoch [3668/20000], Training Loss: 0.0274\n",
            "Epoch [3669/20000], Training Loss: 0.0257\n",
            "Epoch [3670/20000], Training Loss: 0.0268\n",
            "Epoch [3671/20000], Training Loss: 0.0288\n",
            "Epoch [3672/20000], Training Loss: 0.0274\n",
            "Epoch [3673/20000], Training Loss: 0.0261\n",
            "Epoch [3674/20000], Training Loss: 0.0278\n",
            "Epoch [3675/20000], Training Loss: 0.0252\n",
            "Epoch [3676/20000], Training Loss: 0.0246\n",
            "Epoch [3677/20000], Training Loss: 0.0253\n",
            "Epoch [3678/20000], Training Loss: 0.0257\n",
            "Epoch [3679/20000], Training Loss: 0.0254\n",
            "Epoch [3680/20000], Training Loss: 0.0255\n",
            "Epoch [3681/20000], Training Loss: 0.0262\n",
            "Epoch [3682/20000], Training Loss: 0.0264\n",
            "Epoch [3683/20000], Training Loss: 0.0268\n",
            "Epoch [3684/20000], Training Loss: 0.0263\n",
            "Epoch [3685/20000], Training Loss: 0.0268\n",
            "Epoch [3686/20000], Training Loss: 0.0259\n",
            "Epoch [3687/20000], Training Loss: 0.0256\n",
            "Epoch [3688/20000], Training Loss: 0.0275\n",
            "Epoch [3689/20000], Training Loss: 0.0268\n",
            "Epoch [3690/20000], Training Loss: 0.0254\n",
            "Epoch [3691/20000], Training Loss: 0.0247\n",
            "Epoch [3692/20000], Training Loss: 0.0278\n",
            "Epoch [3693/20000], Training Loss: 0.0269\n",
            "Epoch [3694/20000], Training Loss: 0.0266\n",
            "Epoch [3695/20000], Training Loss: 0.0291\n",
            "Epoch [3696/20000], Training Loss: 0.0267\n",
            "Epoch [3697/20000], Training Loss: 0.0284\n",
            "Epoch [3698/20000], Training Loss: 0.0280\n",
            "Epoch [3699/20000], Training Loss: 0.0256\n",
            "Epoch [3700/20000], Training Loss: 0.0285\n",
            "Epoch [3701/20000], Training Loss: 0.0273\n",
            "Epoch [3702/20000], Training Loss: 0.0253\n",
            "Epoch [3703/20000], Training Loss: 0.0279\n",
            "Epoch [3704/20000], Training Loss: 0.0257\n",
            "Epoch [3705/20000], Training Loss: 0.0253\n",
            "Epoch [3706/20000], Training Loss: 0.0265\n",
            "Epoch [3707/20000], Training Loss: 0.0249\n",
            "Epoch [3708/20000], Training Loss: 0.0260\n",
            "Epoch [3709/20000], Training Loss: 0.0259\n",
            "Epoch [3710/20000], Training Loss: 0.0279\n",
            "Epoch [3711/20000], Training Loss: 0.0291\n",
            "Epoch [3712/20000], Training Loss: 0.0244\n",
            "Epoch [3713/20000], Training Loss: 0.0240\n",
            "Epoch [3714/20000], Training Loss: 0.0275\n",
            "Epoch [3715/20000], Training Loss: 0.0270\n",
            "Epoch [3716/20000], Training Loss: 0.0273\n",
            "Epoch [3717/20000], Training Loss: 0.0292\n",
            "Epoch [3718/20000], Training Loss: 0.0253\n",
            "Epoch [3719/20000], Training Loss: 0.0272\n",
            "Epoch [3720/20000], Training Loss: 0.0251\n",
            "Epoch [3721/20000], Training Loss: 0.0258\n",
            "Epoch [3722/20000], Training Loss: 0.0280\n",
            "Epoch [3723/20000], Training Loss: 0.0265\n",
            "Epoch [3724/20000], Training Loss: 0.0258\n",
            "Epoch [3725/20000], Training Loss: 0.0264\n",
            "Epoch [3726/20000], Training Loss: 0.0257\n",
            "Epoch [3727/20000], Training Loss: 0.0254\n",
            "Epoch [3728/20000], Training Loss: 0.0244\n",
            "Epoch [3729/20000], Training Loss: 0.0254\n",
            "Epoch [3730/20000], Training Loss: 0.0265\n",
            "Epoch [3731/20000], Training Loss: 0.0260\n",
            "Epoch [3732/20000], Training Loss: 0.0255\n",
            "Epoch [3733/20000], Training Loss: 0.0269\n",
            "Epoch [3734/20000], Training Loss: 0.0272\n",
            "Epoch [3735/20000], Training Loss: 0.0257\n",
            "Epoch [3736/20000], Training Loss: 0.0242\n",
            "Epoch [3737/20000], Training Loss: 0.0279\n",
            "Epoch [3738/20000], Training Loss: 0.0264\n",
            "Epoch [3739/20000], Training Loss: 0.0248\n",
            "Epoch [3740/20000], Training Loss: 0.0287\n",
            "Epoch [3741/20000], Training Loss: 0.0256\n",
            "Epoch [3742/20000], Training Loss: 0.0252\n",
            "Epoch [3743/20000], Training Loss: 0.0275\n",
            "Epoch [3744/20000], Training Loss: 0.0255\n",
            "Epoch [3745/20000], Training Loss: 0.0252\n",
            "Epoch [3746/20000], Training Loss: 0.0270\n",
            "Epoch [3747/20000], Training Loss: 0.0297\n",
            "Epoch [3748/20000], Training Loss: 0.0258\n",
            "Epoch [3749/20000], Training Loss: 0.0262\n",
            "Epoch [3750/20000], Training Loss: 0.0281\n",
            "Epoch [3751/20000], Training Loss: 0.0245\n",
            "Epoch [3752/20000], Training Loss: 0.0275\n",
            "Epoch [3753/20000], Training Loss: 0.0265\n",
            "Epoch [3754/20000], Training Loss: 0.0258\n",
            "Epoch [3755/20000], Training Loss: 0.0278\n",
            "Epoch [3756/20000], Training Loss: 0.0278\n",
            "Epoch [3757/20000], Training Loss: 0.0264\n",
            "Epoch [3758/20000], Training Loss: 0.0272\n",
            "Epoch [3759/20000], Training Loss: 0.0285\n",
            "Epoch [3760/20000], Training Loss: 0.0249\n",
            "Epoch [3761/20000], Training Loss: 0.0275\n",
            "Epoch [3762/20000], Training Loss: 0.0267\n",
            "Epoch [3763/20000], Training Loss: 0.0271\n",
            "Epoch [3764/20000], Training Loss: 0.0268\n",
            "Epoch [3765/20000], Training Loss: 0.0279\n",
            "Epoch [3766/20000], Training Loss: 0.0250\n",
            "Epoch [3767/20000], Training Loss: 0.0256\n",
            "Epoch [3768/20000], Training Loss: 0.0275\n",
            "Epoch [3769/20000], Training Loss: 0.0262\n",
            "Epoch [3770/20000], Training Loss: 0.0264\n",
            "Epoch [3771/20000], Training Loss: 0.0274\n",
            "Epoch [3772/20000], Training Loss: 0.0261\n",
            "Epoch [3773/20000], Training Loss: 0.0270\n",
            "Epoch [3774/20000], Training Loss: 0.0279\n",
            "Epoch [3775/20000], Training Loss: 0.0262\n",
            "Epoch [3776/20000], Training Loss: 0.0254\n",
            "Epoch [3777/20000], Training Loss: 0.0248\n",
            "Epoch [3778/20000], Training Loss: 0.0275\n",
            "Epoch [3779/20000], Training Loss: 0.0266\n",
            "Epoch [3780/20000], Training Loss: 0.0254\n",
            "Epoch [3781/20000], Training Loss: 0.0267\n",
            "Epoch [3782/20000], Training Loss: 0.0248\n",
            "Epoch [3783/20000], Training Loss: 0.0272\n",
            "Epoch [3784/20000], Training Loss: 0.0262\n",
            "Epoch [3785/20000], Training Loss: 0.0254\n",
            "Epoch [3786/20000], Training Loss: 0.0280\n",
            "Epoch [3787/20000], Training Loss: 0.0256\n",
            "Epoch [3788/20000], Training Loss: 0.0276\n",
            "Epoch [3789/20000], Training Loss: 0.0254\n",
            "Epoch [3790/20000], Training Loss: 0.0276\n",
            "Epoch [3791/20000], Training Loss: 0.0265\n",
            "Epoch [3792/20000], Training Loss: 0.0265\n",
            "Epoch [3793/20000], Training Loss: 0.0270\n",
            "Epoch [3794/20000], Training Loss: 0.0266\n",
            "Epoch [3795/20000], Training Loss: 0.0251\n",
            "Epoch [3796/20000], Training Loss: 0.0263\n",
            "Epoch [3797/20000], Training Loss: 0.0254\n",
            "Epoch [3798/20000], Training Loss: 0.0271\n",
            "Epoch [3799/20000], Training Loss: 0.0250\n",
            "Epoch [3800/20000], Training Loss: 0.0255\n",
            "Epoch [3801/20000], Training Loss: 0.0263\n",
            "Epoch [3802/20000], Training Loss: 0.0255\n",
            "Epoch [3803/20000], Training Loss: 0.0276\n",
            "Epoch [3804/20000], Training Loss: 0.0274\n",
            "Epoch [3805/20000], Training Loss: 0.0272\n",
            "Epoch [3806/20000], Training Loss: 0.0264\n",
            "Epoch [3807/20000], Training Loss: 0.0273\n",
            "Epoch [3808/20000], Training Loss: 0.0246\n",
            "Epoch [3809/20000], Training Loss: 0.0265\n",
            "Epoch [3810/20000], Training Loss: 0.0268\n",
            "Epoch [3811/20000], Training Loss: 0.0273\n",
            "Epoch [3812/20000], Training Loss: 0.0272\n",
            "Epoch [3813/20000], Training Loss: 0.0280\n",
            "Epoch [3814/20000], Training Loss: 0.0262\n",
            "Epoch [3815/20000], Training Loss: 0.0251\n",
            "Epoch [3816/20000], Training Loss: 0.0266\n",
            "Epoch [3817/20000], Training Loss: 0.0278\n",
            "Epoch [3818/20000], Training Loss: 0.0256\n",
            "Epoch [3819/20000], Training Loss: 0.0278\n",
            "Epoch [3820/20000], Training Loss: 0.0273\n",
            "Epoch [3821/20000], Training Loss: 0.0271\n",
            "Epoch [3822/20000], Training Loss: 0.0255\n",
            "Epoch [3823/20000], Training Loss: 0.0278\n",
            "Epoch [3824/20000], Training Loss: 0.0252\n",
            "Epoch [3825/20000], Training Loss: 0.0262\n",
            "Epoch [3826/20000], Training Loss: 0.0241\n",
            "Epoch [3827/20000], Training Loss: 0.0253\n",
            "Epoch [3828/20000], Training Loss: 0.0274\n",
            "Epoch [3829/20000], Training Loss: 0.0259\n",
            "Epoch [3830/20000], Training Loss: 0.0258\n",
            "Epoch [3831/20000], Training Loss: 0.0262\n",
            "Epoch [3832/20000], Training Loss: 0.0251\n",
            "Epoch [3833/20000], Training Loss: 0.0266\n",
            "Epoch [3834/20000], Training Loss: 0.0274\n",
            "Epoch [3835/20000], Training Loss: 0.0265\n",
            "Epoch [3836/20000], Training Loss: 0.0263\n",
            "Epoch [3837/20000], Training Loss: 0.0270\n",
            "Epoch [3838/20000], Training Loss: 0.0263\n",
            "Epoch [3839/20000], Training Loss: 0.0249\n",
            "Epoch [3840/20000], Training Loss: 0.0252\n",
            "Epoch [3841/20000], Training Loss: 0.0260\n",
            "Epoch [3842/20000], Training Loss: 0.0254\n",
            "Epoch [3843/20000], Training Loss: 0.0261\n",
            "Epoch [3844/20000], Training Loss: 0.0272\n",
            "Epoch [3845/20000], Training Loss: 0.0268\n",
            "Epoch [3846/20000], Training Loss: 0.0269\n",
            "Epoch [3847/20000], Training Loss: 0.0261\n",
            "Epoch [3848/20000], Training Loss: 0.0286\n",
            "Epoch [3849/20000], Training Loss: 0.0274\n",
            "Epoch [3850/20000], Training Loss: 0.0249\n",
            "Epoch [3851/20000], Training Loss: 0.0276\n",
            "Epoch [3852/20000], Training Loss: 0.0288\n",
            "Epoch [3853/20000], Training Loss: 0.0268\n",
            "Epoch [3854/20000], Training Loss: 0.0273\n",
            "Epoch [3855/20000], Training Loss: 0.0255\n",
            "Epoch [3856/20000], Training Loss: 0.0255\n",
            "Epoch [3857/20000], Training Loss: 0.0268\n",
            "Epoch [3858/20000], Training Loss: 0.0275\n",
            "Epoch [3859/20000], Training Loss: 0.0261\n",
            "Epoch [3860/20000], Training Loss: 0.0275\n",
            "Epoch [3861/20000], Training Loss: 0.0277\n",
            "Epoch [3862/20000], Training Loss: 0.0278\n",
            "Epoch [3863/20000], Training Loss: 0.0266\n",
            "Epoch [3864/20000], Training Loss: 0.0249\n",
            "Epoch [3865/20000], Training Loss: 0.0269\n",
            "Epoch [3866/20000], Training Loss: 0.0265\n",
            "Epoch [3867/20000], Training Loss: 0.0275\n",
            "Epoch [3868/20000], Training Loss: 0.0283\n",
            "Epoch [3869/20000], Training Loss: 0.0291\n",
            "Epoch [3870/20000], Training Loss: 0.0262\n",
            "Epoch [3871/20000], Training Loss: 0.0280\n",
            "Epoch [3872/20000], Training Loss: 0.0275\n",
            "Epoch [3873/20000], Training Loss: 0.0285\n",
            "Epoch [3874/20000], Training Loss: 0.0281\n",
            "Epoch [3875/20000], Training Loss: 0.0282\n",
            "Epoch [3876/20000], Training Loss: 0.0272\n",
            "Epoch [3877/20000], Training Loss: 0.0252\n",
            "Epoch [3878/20000], Training Loss: 0.0276\n",
            "Epoch [3879/20000], Training Loss: 0.0258\n",
            "Epoch [3880/20000], Training Loss: 0.0248\n",
            "Epoch [3881/20000], Training Loss: 0.0255\n",
            "Epoch [3882/20000], Training Loss: 0.0247\n",
            "Epoch [3883/20000], Training Loss: 0.0255\n",
            "Epoch [3884/20000], Training Loss: 0.0285\n",
            "Epoch [3885/20000], Training Loss: 0.0244\n",
            "Epoch [3886/20000], Training Loss: 0.0277\n",
            "Epoch [3887/20000], Training Loss: 0.0269\n",
            "Epoch [3888/20000], Training Loss: 0.0273\n",
            "Epoch [3889/20000], Training Loss: 0.0259\n",
            "Epoch [3890/20000], Training Loss: 0.0269\n",
            "Epoch [3891/20000], Training Loss: 0.0264\n",
            "Epoch [3892/20000], Training Loss: 0.0262\n",
            "Epoch [3893/20000], Training Loss: 0.0250\n",
            "Epoch [3894/20000], Training Loss: 0.0249\n",
            "Epoch [3895/20000], Training Loss: 0.0273\n",
            "Epoch [3896/20000], Training Loss: 0.0248\n",
            "Epoch [3897/20000], Training Loss: 0.0245\n",
            "Epoch [3898/20000], Training Loss: 0.0277\n",
            "Epoch [3899/20000], Training Loss: 0.0261\n",
            "Epoch [3900/20000], Training Loss: 0.0279\n",
            "Epoch [3901/20000], Training Loss: 0.0274\n",
            "Epoch [3902/20000], Training Loss: 0.0258\n",
            "Epoch [3903/20000], Training Loss: 0.0269\n",
            "Epoch [3904/20000], Training Loss: 0.0266\n",
            "Epoch [3905/20000], Training Loss: 0.0281\n",
            "Epoch [3906/20000], Training Loss: 0.0261\n",
            "Epoch [3907/20000], Training Loss: 0.0284\n",
            "Epoch [3908/20000], Training Loss: 0.0246\n",
            "Epoch [3909/20000], Training Loss: 0.0265\n",
            "Epoch [3910/20000], Training Loss: 0.0259\n",
            "Epoch [3911/20000], Training Loss: 0.0258\n",
            "Epoch [3912/20000], Training Loss: 0.0281\n",
            "Epoch [3913/20000], Training Loss: 0.0251\n",
            "Epoch [3914/20000], Training Loss: 0.0274\n",
            "Epoch [3915/20000], Training Loss: 0.0264\n",
            "Epoch [3916/20000], Training Loss: 0.0271\n",
            "Epoch [3917/20000], Training Loss: 0.0255\n",
            "Epoch [3918/20000], Training Loss: 0.0245\n",
            "Epoch [3919/20000], Training Loss: 0.0283\n",
            "Epoch [3920/20000], Training Loss: 0.0282\n",
            "Epoch [3921/20000], Training Loss: 0.0250\n",
            "Epoch [3922/20000], Training Loss: 0.0272\n",
            "Epoch [3923/20000], Training Loss: 0.0249\n",
            "Epoch [3924/20000], Training Loss: 0.0252\n",
            "Epoch [3925/20000], Training Loss: 0.0274\n",
            "Epoch [3926/20000], Training Loss: 0.0269\n",
            "Epoch [3927/20000], Training Loss: 0.0264\n",
            "Epoch [3928/20000], Training Loss: 0.0256\n",
            "Epoch [3929/20000], Training Loss: 0.0264\n",
            "Epoch [3930/20000], Training Loss: 0.0276\n",
            "Epoch [3931/20000], Training Loss: 0.0255\n",
            "Epoch [3932/20000], Training Loss: 0.0253\n",
            "Epoch [3933/20000], Training Loss: 0.0251\n",
            "Epoch [3934/20000], Training Loss: 0.0252\n",
            "Epoch [3935/20000], Training Loss: 0.0261\n",
            "Epoch [3936/20000], Training Loss: 0.0251\n",
            "Epoch [3937/20000], Training Loss: 0.0277\n",
            "Epoch [3938/20000], Training Loss: 0.0252\n",
            "Epoch [3939/20000], Training Loss: 0.0267\n",
            "Epoch [3940/20000], Training Loss: 0.0274\n",
            "Epoch [3941/20000], Training Loss: 0.0283\n",
            "Epoch [3942/20000], Training Loss: 0.0282\n",
            "Epoch [3943/20000], Training Loss: 0.0266\n",
            "Epoch [3944/20000], Training Loss: 0.0270\n",
            "Epoch [3945/20000], Training Loss: 0.0284\n",
            "Epoch [3946/20000], Training Loss: 0.0275\n",
            "Epoch [3947/20000], Training Loss: 0.0254\n",
            "Epoch [3948/20000], Training Loss: 0.0274\n",
            "Epoch [3949/20000], Training Loss: 0.0279\n",
            "Epoch [3950/20000], Training Loss: 0.0257\n",
            "Epoch [3951/20000], Training Loss: 0.0285\n",
            "Epoch [3952/20000], Training Loss: 0.0262\n",
            "Epoch [3953/20000], Training Loss: 0.0266\n",
            "Epoch [3954/20000], Training Loss: 0.0251\n",
            "Epoch [3955/20000], Training Loss: 0.0263\n",
            "Epoch [3956/20000], Training Loss: 0.0262\n",
            "Epoch [3957/20000], Training Loss: 0.0249\n",
            "Epoch [3958/20000], Training Loss: 0.0271\n",
            "Epoch [3959/20000], Training Loss: 0.0276\n",
            "Epoch [3960/20000], Training Loss: 0.0281\n",
            "Epoch [3961/20000], Training Loss: 0.0251\n",
            "Epoch [3962/20000], Training Loss: 0.0260\n",
            "Epoch [3963/20000], Training Loss: 0.0265\n",
            "Epoch [3964/20000], Training Loss: 0.0259\n",
            "Epoch [3965/20000], Training Loss: 0.0250\n",
            "Epoch [3966/20000], Training Loss: 0.0266\n",
            "Epoch [3967/20000], Training Loss: 0.0280\n",
            "Epoch [3968/20000], Training Loss: 0.0251\n",
            "Epoch [3969/20000], Training Loss: 0.0274\n",
            "Epoch [3970/20000], Training Loss: 0.0272\n",
            "Epoch [3971/20000], Training Loss: 0.0263\n",
            "Epoch [3972/20000], Training Loss: 0.0274\n",
            "Epoch [3973/20000], Training Loss: 0.0257\n",
            "Epoch [3974/20000], Training Loss: 0.0281\n",
            "Epoch [3975/20000], Training Loss: 0.0280\n",
            "Epoch [3976/20000], Training Loss: 0.0247\n",
            "Epoch [3977/20000], Training Loss: 0.0277\n",
            "Epoch [3978/20000], Training Loss: 0.0267\n",
            "Epoch [3979/20000], Training Loss: 0.0265\n",
            "Epoch [3980/20000], Training Loss: 0.0261\n",
            "Epoch [3981/20000], Training Loss: 0.0244\n",
            "Epoch [3982/20000], Training Loss: 0.0296\n",
            "Epoch [3983/20000], Training Loss: 0.0260\n",
            "Epoch [3984/20000], Training Loss: 0.0241\n",
            "Epoch [3985/20000], Training Loss: 0.0258\n",
            "Epoch [3986/20000], Training Loss: 0.0264\n",
            "Epoch [3987/20000], Training Loss: 0.0251\n",
            "Epoch [3988/20000], Training Loss: 0.0271\n",
            "Epoch [3989/20000], Training Loss: 0.0250\n",
            "Epoch [3990/20000], Training Loss: 0.0253\n",
            "Epoch [3991/20000], Training Loss: 0.0256\n",
            "Epoch [3992/20000], Training Loss: 0.0281\n",
            "Epoch [3993/20000], Training Loss: 0.0251\n",
            "Epoch [3994/20000], Training Loss: 0.0288\n",
            "Epoch [3995/20000], Training Loss: 0.0276\n",
            "Epoch [3996/20000], Training Loss: 0.0276\n",
            "Epoch [3997/20000], Training Loss: 0.0260\n",
            "Epoch [3998/20000], Training Loss: 0.0273\n",
            "Epoch [3999/20000], Training Loss: 0.0244\n",
            "Epoch [4000/20000], Training Loss: 0.0278\n",
            "Epoch [4001/20000], Training Loss: 0.0260\n",
            "Epoch [4002/20000], Training Loss: 0.0255\n",
            "Epoch [4003/20000], Training Loss: 0.0271\n",
            "Epoch [4004/20000], Training Loss: 0.0263\n",
            "Epoch [4005/20000], Training Loss: 0.0264\n",
            "Epoch [4006/20000], Training Loss: 0.0253\n",
            "Epoch [4007/20000], Training Loss: 0.0269\n",
            "Epoch [4008/20000], Training Loss: 0.0255\n",
            "Epoch [4009/20000], Training Loss: 0.0282\n",
            "Epoch [4010/20000], Training Loss: 0.0268\n",
            "Epoch [4011/20000], Training Loss: 0.0299\n",
            "Epoch [4012/20000], Training Loss: 0.0241\n",
            "Epoch [4013/20000], Training Loss: 0.0256\n",
            "Epoch [4014/20000], Training Loss: 0.0300\n",
            "Epoch [4015/20000], Training Loss: 0.0265\n",
            "Epoch [4016/20000], Training Loss: 0.0277\n",
            "Epoch [4017/20000], Training Loss: 0.0261\n",
            "Epoch [4018/20000], Training Loss: 0.0280\n",
            "Epoch [4019/20000], Training Loss: 0.0259\n",
            "Epoch [4020/20000], Training Loss: 0.0261\n",
            "Epoch [4021/20000], Training Loss: 0.0249\n",
            "Epoch [4022/20000], Training Loss: 0.0254\n",
            "Epoch [4023/20000], Training Loss: 0.0275\n",
            "Epoch [4024/20000], Training Loss: 0.0263\n",
            "Epoch [4025/20000], Training Loss: 0.0256\n",
            "Epoch [4026/20000], Training Loss: 0.0273\n",
            "Epoch [4027/20000], Training Loss: 0.0264\n",
            "Epoch [4028/20000], Training Loss: 0.0264\n",
            "Epoch [4029/20000], Training Loss: 0.0254\n",
            "Epoch [4030/20000], Training Loss: 0.0266\n",
            "Epoch [4031/20000], Training Loss: 0.0279\n",
            "Epoch [4032/20000], Training Loss: 0.0253\n",
            "Epoch [4033/20000], Training Loss: 0.0271\n",
            "Epoch [4034/20000], Training Loss: 0.0261\n",
            "Epoch [4035/20000], Training Loss: 0.0271\n",
            "Epoch [4036/20000], Training Loss: 0.0271\n",
            "Epoch [4037/20000], Training Loss: 0.0276\n",
            "Epoch [4038/20000], Training Loss: 0.0281\n",
            "Epoch [4039/20000], Training Loss: 0.0254\n",
            "Epoch [4040/20000], Training Loss: 0.0259\n",
            "Epoch [4041/20000], Training Loss: 0.0268\n",
            "Epoch [4042/20000], Training Loss: 0.0251\n",
            "Epoch [4043/20000], Training Loss: 0.0248\n",
            "Epoch [4044/20000], Training Loss: 0.0298\n",
            "Epoch [4045/20000], Training Loss: 0.0265\n",
            "Epoch [4046/20000], Training Loss: 0.0253\n",
            "Epoch [4047/20000], Training Loss: 0.0269\n",
            "Epoch [4048/20000], Training Loss: 0.0250\n",
            "Epoch [4049/20000], Training Loss: 0.0269\n",
            "Epoch [4050/20000], Training Loss: 0.0266\n",
            "Epoch [4051/20000], Training Loss: 0.0273\n",
            "Epoch [4052/20000], Training Loss: 0.0289\n",
            "Epoch [4053/20000], Training Loss: 0.0280\n",
            "Epoch [4054/20000], Training Loss: 0.0262\n",
            "Epoch [4055/20000], Training Loss: 0.0246\n",
            "Epoch [4056/20000], Training Loss: 0.0255\n",
            "Epoch [4057/20000], Training Loss: 0.0273\n",
            "Epoch [4058/20000], Training Loss: 0.0268\n",
            "Epoch [4059/20000], Training Loss: 0.0271\n",
            "Epoch [4060/20000], Training Loss: 0.0262\n",
            "Epoch [4061/20000], Training Loss: 0.0276\n",
            "Epoch [4062/20000], Training Loss: 0.0269\n",
            "Epoch [4063/20000], Training Loss: 0.0262\n",
            "Epoch [4064/20000], Training Loss: 0.0269\n",
            "Epoch [4065/20000], Training Loss: 0.0263\n",
            "Epoch [4066/20000], Training Loss: 0.0256\n",
            "Epoch [4067/20000], Training Loss: 0.0266\n",
            "Epoch [4068/20000], Training Loss: 0.0261\n",
            "Epoch [4069/20000], Training Loss: 0.0248\n",
            "Epoch [4070/20000], Training Loss: 0.0289\n",
            "Epoch [4071/20000], Training Loss: 0.0258\n",
            "Epoch [4072/20000], Training Loss: 0.0279\n",
            "Epoch [4073/20000], Training Loss: 0.0256\n",
            "Epoch [4074/20000], Training Loss: 0.0268\n",
            "Epoch [4075/20000], Training Loss: 0.0282\n",
            "Epoch [4076/20000], Training Loss: 0.0275\n",
            "Epoch [4077/20000], Training Loss: 0.0256\n",
            "Epoch [4078/20000], Training Loss: 0.0268\n",
            "Epoch [4079/20000], Training Loss: 0.0253\n",
            "Epoch [4080/20000], Training Loss: 0.0254\n",
            "Epoch [4081/20000], Training Loss: 0.0267\n",
            "Epoch [4082/20000], Training Loss: 0.0256\n",
            "Epoch [4083/20000], Training Loss: 0.0273\n",
            "Epoch [4084/20000], Training Loss: 0.0270\n",
            "Epoch [4085/20000], Training Loss: 0.0275\n",
            "Epoch [4086/20000], Training Loss: 0.0276\n",
            "Epoch [4087/20000], Training Loss: 0.0255\n",
            "Epoch [4088/20000], Training Loss: 0.0268\n",
            "Epoch [4089/20000], Training Loss: 0.0258\n",
            "Epoch [4090/20000], Training Loss: 0.0264\n",
            "Epoch [4091/20000], Training Loss: 0.0257\n",
            "Epoch [4092/20000], Training Loss: 0.0275\n",
            "Epoch [4093/20000], Training Loss: 0.0269\n",
            "Epoch [4094/20000], Training Loss: 0.0262\n",
            "Epoch [4095/20000], Training Loss: 0.0250\n",
            "Epoch [4096/20000], Training Loss: 0.0270\n",
            "Epoch [4097/20000], Training Loss: 0.0266\n",
            "Epoch [4098/20000], Training Loss: 0.0271\n",
            "Epoch [4099/20000], Training Loss: 0.0250\n",
            "Epoch [4100/20000], Training Loss: 0.0286\n",
            "Epoch [4101/20000], Training Loss: 0.0257\n",
            "Epoch [4102/20000], Training Loss: 0.0270\n",
            "Epoch [4103/20000], Training Loss: 0.0266\n",
            "Epoch [4104/20000], Training Loss: 0.0263\n",
            "Epoch [4105/20000], Training Loss: 0.0263\n",
            "Epoch [4106/20000], Training Loss: 0.0262\n",
            "Epoch [4107/20000], Training Loss: 0.0269\n",
            "Epoch [4108/20000], Training Loss: 0.0265\n",
            "Epoch [4109/20000], Training Loss: 0.0262\n",
            "Epoch [4110/20000], Training Loss: 0.0269\n",
            "Epoch [4111/20000], Training Loss: 0.0275\n",
            "Epoch [4112/20000], Training Loss: 0.0272\n",
            "Epoch [4113/20000], Training Loss: 0.0282\n",
            "Epoch [4114/20000], Training Loss: 0.0260\n",
            "Epoch [4115/20000], Training Loss: 0.0272\n",
            "Epoch [4116/20000], Training Loss: 0.0247\n",
            "Epoch [4117/20000], Training Loss: 0.0257\n",
            "Epoch [4118/20000], Training Loss: 0.0252\n",
            "Epoch [4119/20000], Training Loss: 0.0268\n",
            "Epoch [4120/20000], Training Loss: 0.0280\n",
            "Epoch [4121/20000], Training Loss: 0.0278\n",
            "Epoch [4122/20000], Training Loss: 0.0278\n",
            "Epoch [4123/20000], Training Loss: 0.0261\n",
            "Epoch [4124/20000], Training Loss: 0.0276\n",
            "Epoch [4125/20000], Training Loss: 0.0266\n",
            "Epoch [4126/20000], Training Loss: 0.0271\n",
            "Epoch [4127/20000], Training Loss: 0.0259\n",
            "Epoch [4128/20000], Training Loss: 0.0254\n",
            "Epoch [4129/20000], Training Loss: 0.0257\n",
            "Epoch [4130/20000], Training Loss: 0.0256\n",
            "Epoch [4131/20000], Training Loss: 0.0262\n",
            "Epoch [4132/20000], Training Loss: 0.0262\n",
            "Epoch [4133/20000], Training Loss: 0.0288\n",
            "Epoch [4134/20000], Training Loss: 0.0302\n",
            "Epoch [4135/20000], Training Loss: 0.0273\n",
            "Epoch [4136/20000], Training Loss: 0.0254\n",
            "Epoch [4137/20000], Training Loss: 0.0265\n",
            "Epoch [4138/20000], Training Loss: 0.0256\n",
            "Epoch [4139/20000], Training Loss: 0.0269\n",
            "Epoch [4140/20000], Training Loss: 0.0265\n",
            "Epoch [4141/20000], Training Loss: 0.0259\n",
            "Epoch [4142/20000], Training Loss: 0.0265\n",
            "Epoch [4143/20000], Training Loss: 0.0260\n",
            "Epoch [4144/20000], Training Loss: 0.0249\n",
            "Epoch [4145/20000], Training Loss: 0.0257\n",
            "Epoch [4146/20000], Training Loss: 0.0242\n",
            "Epoch [4147/20000], Training Loss: 0.0266\n",
            "Epoch [4148/20000], Training Loss: 0.0278\n",
            "Epoch [4149/20000], Training Loss: 0.0277\n",
            "Epoch [4150/20000], Training Loss: 0.0250\n",
            "Epoch [4151/20000], Training Loss: 0.0279\n",
            "Epoch [4152/20000], Training Loss: 0.0280\n",
            "Epoch [4153/20000], Training Loss: 0.0262\n",
            "Epoch [4154/20000], Training Loss: 0.0253\n",
            "Epoch [4155/20000], Training Loss: 0.0246\n",
            "Epoch [4156/20000], Training Loss: 0.0282\n",
            "Epoch [4157/20000], Training Loss: 0.0275\n",
            "Epoch [4158/20000], Training Loss: 0.0275\n",
            "Epoch [4159/20000], Training Loss: 0.0254\n",
            "Epoch [4160/20000], Training Loss: 0.0284\n",
            "Epoch [4161/20000], Training Loss: 0.0284\n",
            "Epoch [4162/20000], Training Loss: 0.0285\n",
            "Epoch [4163/20000], Training Loss: 0.0266\n",
            "Epoch [4164/20000], Training Loss: 0.0269\n",
            "Epoch [4165/20000], Training Loss: 0.0274\n",
            "Epoch [4166/20000], Training Loss: 0.0239\n",
            "Epoch [4167/20000], Training Loss: 0.0280\n",
            "Epoch [4168/20000], Training Loss: 0.0247\n",
            "Epoch [4169/20000], Training Loss: 0.0255\n",
            "Epoch [4170/20000], Training Loss: 0.0266\n",
            "Epoch [4171/20000], Training Loss: 0.0256\n",
            "Epoch [4172/20000], Training Loss: 0.0249\n",
            "Epoch [4173/20000], Training Loss: 0.0272\n",
            "Epoch [4174/20000], Training Loss: 0.0259\n",
            "Epoch [4175/20000], Training Loss: 0.0299\n",
            "Epoch [4176/20000], Training Loss: 0.0260\n",
            "Epoch [4177/20000], Training Loss: 0.0281\n",
            "Epoch [4178/20000], Training Loss: 0.0286\n",
            "Epoch [4179/20000], Training Loss: 0.0276\n",
            "Epoch [4180/20000], Training Loss: 0.0259\n",
            "Epoch [4181/20000], Training Loss: 0.0269\n",
            "Epoch [4182/20000], Training Loss: 0.0259\n",
            "Epoch [4183/20000], Training Loss: 0.0263\n",
            "Epoch [4184/20000], Training Loss: 0.0266\n",
            "Epoch [4185/20000], Training Loss: 0.0268\n",
            "Epoch [4186/20000], Training Loss: 0.0256\n",
            "Epoch [4187/20000], Training Loss: 0.0257\n",
            "Epoch [4188/20000], Training Loss: 0.0270\n",
            "Epoch [4189/20000], Training Loss: 0.0259\n",
            "Epoch [4190/20000], Training Loss: 0.0274\n",
            "Epoch [4191/20000], Training Loss: 0.0263\n",
            "Epoch [4192/20000], Training Loss: 0.0255\n",
            "Epoch [4193/20000], Training Loss: 0.0261\n",
            "Epoch [4194/20000], Training Loss: 0.0264\n",
            "Epoch [4195/20000], Training Loss: 0.0276\n",
            "Epoch [4196/20000], Training Loss: 0.0267\n",
            "Epoch [4197/20000], Training Loss: 0.0255\n",
            "Epoch [4198/20000], Training Loss: 0.0266\n",
            "Epoch [4199/20000], Training Loss: 0.0278\n",
            "Epoch [4200/20000], Training Loss: 0.0267\n",
            "Epoch [4201/20000], Training Loss: 0.0253\n",
            "Epoch [4202/20000], Training Loss: 0.0261\n",
            "Epoch [4203/20000], Training Loss: 0.0252\n",
            "Epoch [4204/20000], Training Loss: 0.0255\n",
            "Epoch [4205/20000], Training Loss: 0.0278\n",
            "Epoch [4206/20000], Training Loss: 0.0247\n",
            "Epoch [4207/20000], Training Loss: 0.0269\n",
            "Epoch [4208/20000], Training Loss: 0.0269\n",
            "Epoch [4209/20000], Training Loss: 0.0280\n",
            "Epoch [4210/20000], Training Loss: 0.0256\n",
            "Epoch [4211/20000], Training Loss: 0.0271\n",
            "Epoch [4212/20000], Training Loss: 0.0285\n",
            "Epoch [4213/20000], Training Loss: 0.0278\n",
            "Epoch [4214/20000], Training Loss: 0.0250\n",
            "Epoch [4215/20000], Training Loss: 0.0249\n",
            "Epoch [4216/20000], Training Loss: 0.0302\n",
            "Epoch [4217/20000], Training Loss: 0.0249\n",
            "Epoch [4218/20000], Training Loss: 0.0263\n",
            "Epoch [4219/20000], Training Loss: 0.0274\n",
            "Epoch [4220/20000], Training Loss: 0.0250\n",
            "Epoch [4221/20000], Training Loss: 0.0264\n",
            "Epoch [4222/20000], Training Loss: 0.0252\n",
            "Epoch [4223/20000], Training Loss: 0.0273\n",
            "Epoch [4224/20000], Training Loss: 0.0273\n",
            "Epoch [4225/20000], Training Loss: 0.0264\n",
            "Epoch [4226/20000], Training Loss: 0.0273\n",
            "Epoch [4227/20000], Training Loss: 0.0272\n",
            "Epoch [4228/20000], Training Loss: 0.0276\n",
            "Epoch [4229/20000], Training Loss: 0.0254\n",
            "Epoch [4230/20000], Training Loss: 0.0252\n",
            "Epoch [4231/20000], Training Loss: 0.0262\n",
            "Epoch [4232/20000], Training Loss: 0.0266\n",
            "Epoch [4233/20000], Training Loss: 0.0266\n",
            "Epoch [4234/20000], Training Loss: 0.0272\n",
            "Epoch [4235/20000], Training Loss: 0.0253\n",
            "Epoch [4236/20000], Training Loss: 0.0261\n",
            "Epoch [4237/20000], Training Loss: 0.0263\n",
            "Epoch [4238/20000], Training Loss: 0.0263\n",
            "Epoch [4239/20000], Training Loss: 0.0266\n",
            "Epoch [4240/20000], Training Loss: 0.0247\n",
            "Epoch [4241/20000], Training Loss: 0.0271\n",
            "Epoch [4242/20000], Training Loss: 0.0266\n",
            "Epoch [4243/20000], Training Loss: 0.0261\n",
            "Epoch [4244/20000], Training Loss: 0.0273\n",
            "Epoch [4245/20000], Training Loss: 0.0265\n",
            "Epoch [4246/20000], Training Loss: 0.0251\n",
            "Epoch [4247/20000], Training Loss: 0.0279\n",
            "Epoch [4248/20000], Training Loss: 0.0273\n",
            "Epoch [4249/20000], Training Loss: 0.0271\n",
            "Epoch [4250/20000], Training Loss: 0.0276\n",
            "Epoch [4251/20000], Training Loss: 0.0252\n",
            "Epoch [4252/20000], Training Loss: 0.0254\n",
            "Epoch [4253/20000], Training Loss: 0.0260\n",
            "Epoch [4254/20000], Training Loss: 0.0280\n",
            "Epoch [4255/20000], Training Loss: 0.0274\n",
            "Epoch [4256/20000], Training Loss: 0.0265\n",
            "Epoch [4257/20000], Training Loss: 0.0267\n",
            "Epoch [4258/20000], Training Loss: 0.0258\n",
            "Epoch [4259/20000], Training Loss: 0.0266\n",
            "Epoch [4260/20000], Training Loss: 0.0269\n",
            "Epoch [4261/20000], Training Loss: 0.0291\n",
            "Epoch [4262/20000], Training Loss: 0.0261\n",
            "Epoch [4263/20000], Training Loss: 0.0269\n",
            "Epoch [4264/20000], Training Loss: 0.0257\n",
            "Epoch [4265/20000], Training Loss: 0.0265\n",
            "Epoch [4266/20000], Training Loss: 0.0254\n",
            "Epoch [4267/20000], Training Loss: 0.0279\n",
            "Epoch [4268/20000], Training Loss: 0.0269\n",
            "Epoch [4269/20000], Training Loss: 0.0256\n",
            "Epoch [4270/20000], Training Loss: 0.0283\n",
            "Epoch [4271/20000], Training Loss: 0.0283\n",
            "Epoch [4272/20000], Training Loss: 0.0289\n",
            "Epoch [4273/20000], Training Loss: 0.0266\n",
            "Epoch [4274/20000], Training Loss: 0.0250\n",
            "Epoch [4275/20000], Training Loss: 0.0253\n",
            "Epoch [4276/20000], Training Loss: 0.0277\n",
            "Epoch [4277/20000], Training Loss: 0.0251\n",
            "Epoch [4278/20000], Training Loss: 0.0256\n",
            "Epoch [4279/20000], Training Loss: 0.0253\n",
            "Epoch [4280/20000], Training Loss: 0.0244\n",
            "Epoch [4281/20000], Training Loss: 0.0265\n",
            "Epoch [4282/20000], Training Loss: 0.0259\n",
            "Epoch [4283/20000], Training Loss: 0.0269\n",
            "Epoch [4284/20000], Training Loss: 0.0243\n",
            "Epoch [4285/20000], Training Loss: 0.0265\n",
            "Epoch [4286/20000], Training Loss: 0.0266\n",
            "Epoch [4287/20000], Training Loss: 0.0258\n",
            "Epoch [4288/20000], Training Loss: 0.0273\n",
            "Epoch [4289/20000], Training Loss: 0.0260\n",
            "Epoch [4290/20000], Training Loss: 0.0270\n",
            "Epoch [4291/20000], Training Loss: 0.0273\n",
            "Epoch [4292/20000], Training Loss: 0.0280\n",
            "Epoch [4293/20000], Training Loss: 0.0262\n",
            "Epoch [4294/20000], Training Loss: 0.0256\n",
            "Epoch [4295/20000], Training Loss: 0.0280\n",
            "Epoch [4296/20000], Training Loss: 0.0292\n",
            "Epoch [4297/20000], Training Loss: 0.0260\n",
            "Epoch [4298/20000], Training Loss: 0.0273\n",
            "Epoch [4299/20000], Training Loss: 0.0254\n",
            "Epoch [4300/20000], Training Loss: 0.0264\n",
            "Epoch [4301/20000], Training Loss: 0.0250\n",
            "Epoch [4302/20000], Training Loss: 0.0267\n",
            "Epoch [4303/20000], Training Loss: 0.0264\n",
            "Epoch [4304/20000], Training Loss: 0.0261\n",
            "Epoch [4305/20000], Training Loss: 0.0267\n",
            "Epoch [4306/20000], Training Loss: 0.0265\n",
            "Epoch [4307/20000], Training Loss: 0.0251\n",
            "Epoch [4308/20000], Training Loss: 0.0255\n",
            "Epoch [4309/20000], Training Loss: 0.0276\n",
            "Epoch [4310/20000], Training Loss: 0.0258\n",
            "Epoch [4311/20000], Training Loss: 0.0273\n",
            "Epoch [4312/20000], Training Loss: 0.0262\n",
            "Epoch [4313/20000], Training Loss: 0.0270\n",
            "Epoch [4314/20000], Training Loss: 0.0243\n",
            "Epoch [4315/20000], Training Loss: 0.0274\n",
            "Epoch [4316/20000], Training Loss: 0.0263\n",
            "Epoch [4317/20000], Training Loss: 0.0269\n",
            "Epoch [4318/20000], Training Loss: 0.0273\n",
            "Epoch [4319/20000], Training Loss: 0.0260\n",
            "Epoch [4320/20000], Training Loss: 0.0261\n",
            "Epoch [4321/20000], Training Loss: 0.0264\n",
            "Epoch [4322/20000], Training Loss: 0.0268\n",
            "Epoch [4323/20000], Training Loss: 0.0259\n",
            "Epoch [4324/20000], Training Loss: 0.0274\n",
            "Epoch [4325/20000], Training Loss: 0.0272\n",
            "Epoch [4326/20000], Training Loss: 0.0257\n",
            "Epoch [4327/20000], Training Loss: 0.0255\n",
            "Epoch [4328/20000], Training Loss: 0.0255\n",
            "Epoch [4329/20000], Training Loss: 0.0245\n",
            "Epoch [4330/20000], Training Loss: 0.0265\n",
            "Epoch [4331/20000], Training Loss: 0.0252\n",
            "Epoch [4332/20000], Training Loss: 0.0264\n",
            "Epoch [4333/20000], Training Loss: 0.0274\n",
            "Epoch [4334/20000], Training Loss: 0.0270\n",
            "Epoch [4335/20000], Training Loss: 0.0267\n",
            "Epoch [4336/20000], Training Loss: 0.0273\n",
            "Epoch [4337/20000], Training Loss: 0.0258\n",
            "Epoch [4338/20000], Training Loss: 0.0264\n",
            "Epoch [4339/20000], Training Loss: 0.0260\n",
            "Epoch [4340/20000], Training Loss: 0.0248\n",
            "Epoch [4341/20000], Training Loss: 0.0251\n",
            "Epoch [4342/20000], Training Loss: 0.0275\n",
            "Epoch [4343/20000], Training Loss: 0.0267\n",
            "Epoch [4344/20000], Training Loss: 0.0273\n",
            "Epoch [4345/20000], Training Loss: 0.0249\n",
            "Epoch [4346/20000], Training Loss: 0.0239\n",
            "Epoch [4347/20000], Training Loss: 0.0259\n",
            "Epoch [4348/20000], Training Loss: 0.0261\n",
            "Epoch [4349/20000], Training Loss: 0.0245\n",
            "Epoch [4350/20000], Training Loss: 0.0261\n",
            "Epoch [4351/20000], Training Loss: 0.0284\n",
            "Epoch [4352/20000], Training Loss: 0.0268\n",
            "Epoch [4353/20000], Training Loss: 0.0256\n",
            "Epoch [4354/20000], Training Loss: 0.0261\n",
            "Epoch [4355/20000], Training Loss: 0.0273\n",
            "Epoch [4356/20000], Training Loss: 0.0273\n",
            "Epoch [4357/20000], Training Loss: 0.0260\n",
            "Epoch [4358/20000], Training Loss: 0.0264\n",
            "Epoch [4359/20000], Training Loss: 0.0245\n",
            "Epoch [4360/20000], Training Loss: 0.0265\n",
            "Epoch [4361/20000], Training Loss: 0.0244\n",
            "Epoch [4362/20000], Training Loss: 0.0259\n",
            "Epoch [4363/20000], Training Loss: 0.0259\n",
            "Epoch [4364/20000], Training Loss: 0.0258\n",
            "Epoch [4365/20000], Training Loss: 0.0288\n",
            "Epoch [4366/20000], Training Loss: 0.0267\n",
            "Epoch [4367/20000], Training Loss: 0.0276\n",
            "Epoch [4368/20000], Training Loss: 0.0251\n",
            "Epoch [4369/20000], Training Loss: 0.0270\n",
            "Epoch [4370/20000], Training Loss: 0.0247\n",
            "Epoch [4371/20000], Training Loss: 0.0262\n",
            "Epoch [4372/20000], Training Loss: 0.0265\n",
            "Epoch [4373/20000], Training Loss: 0.0267\n",
            "Epoch [4374/20000], Training Loss: 0.0256\n",
            "Epoch [4375/20000], Training Loss: 0.0270\n",
            "Epoch [4376/20000], Training Loss: 0.0254\n",
            "Epoch [4377/20000], Training Loss: 0.0267\n",
            "Epoch [4378/20000], Training Loss: 0.0275\n",
            "Epoch [4379/20000], Training Loss: 0.0265\n",
            "Epoch [4380/20000], Training Loss: 0.0280\n",
            "Epoch [4381/20000], Training Loss: 0.0265\n",
            "Epoch [4382/20000], Training Loss: 0.0252\n",
            "Epoch [4383/20000], Training Loss: 0.0260\n",
            "Epoch [4384/20000], Training Loss: 0.0262\n",
            "Epoch [4385/20000], Training Loss: 0.0263\n",
            "Epoch [4386/20000], Training Loss: 0.0263\n",
            "Epoch [4387/20000], Training Loss: 0.0249\n",
            "Epoch [4388/20000], Training Loss: 0.0267\n",
            "Epoch [4389/20000], Training Loss: 0.0260\n",
            "Epoch [4390/20000], Training Loss: 0.0246\n",
            "Epoch [4391/20000], Training Loss: 0.0262\n",
            "Epoch [4392/20000], Training Loss: 0.0250\n",
            "Epoch [4393/20000], Training Loss: 0.0262\n",
            "Epoch [4394/20000], Training Loss: 0.0274\n",
            "Epoch [4395/20000], Training Loss: 0.0266\n",
            "Epoch [4396/20000], Training Loss: 0.0271\n",
            "Epoch [4397/20000], Training Loss: 0.0261\n",
            "Epoch [4398/20000], Training Loss: 0.0272\n",
            "Epoch [4399/20000], Training Loss: 0.0263\n",
            "Epoch [4400/20000], Training Loss: 0.0260\n",
            "Epoch [4401/20000], Training Loss: 0.0264\n",
            "Epoch [4402/20000], Training Loss: 0.0259\n",
            "Epoch [4403/20000], Training Loss: 0.0270\n",
            "Epoch [4404/20000], Training Loss: 0.0272\n",
            "Epoch [4405/20000], Training Loss: 0.0255\n",
            "Epoch [4406/20000], Training Loss: 0.0261\n",
            "Epoch [4407/20000], Training Loss: 0.0245\n",
            "Epoch [4408/20000], Training Loss: 0.0273\n",
            "Epoch [4409/20000], Training Loss: 0.0266\n",
            "Epoch [4410/20000], Training Loss: 0.0295\n",
            "Epoch [4411/20000], Training Loss: 0.0258\n",
            "Epoch [4412/20000], Training Loss: 0.0270\n",
            "Epoch [4413/20000], Training Loss: 0.0272\n",
            "Epoch [4414/20000], Training Loss: 0.0273\n",
            "Epoch [4415/20000], Training Loss: 0.0258\n",
            "Epoch [4416/20000], Training Loss: 0.0252\n",
            "Epoch [4417/20000], Training Loss: 0.0252\n",
            "Epoch [4418/20000], Training Loss: 0.0271\n",
            "Epoch [4419/20000], Training Loss: 0.0274\n",
            "Epoch [4420/20000], Training Loss: 0.0251\n",
            "Epoch [4421/20000], Training Loss: 0.0265\n",
            "Epoch [4422/20000], Training Loss: 0.0249\n",
            "Epoch [4423/20000], Training Loss: 0.0260\n",
            "Epoch [4424/20000], Training Loss: 0.0255\n",
            "Epoch [4425/20000], Training Loss: 0.0293\n",
            "Epoch [4426/20000], Training Loss: 0.0262\n",
            "Epoch [4427/20000], Training Loss: 0.0268\n",
            "Epoch [4428/20000], Training Loss: 0.0268\n",
            "Epoch [4429/20000], Training Loss: 0.0243\n",
            "Epoch [4430/20000], Training Loss: 0.0262\n",
            "Epoch [4431/20000], Training Loss: 0.0261\n",
            "Epoch [4432/20000], Training Loss: 0.0280\n",
            "Epoch [4433/20000], Training Loss: 0.0254\n",
            "Epoch [4434/20000], Training Loss: 0.0267\n",
            "Epoch [4435/20000], Training Loss: 0.0266\n",
            "Epoch [4436/20000], Training Loss: 0.0265\n",
            "Epoch [4437/20000], Training Loss: 0.0270\n",
            "Epoch [4438/20000], Training Loss: 0.0276\n",
            "Epoch [4439/20000], Training Loss: 0.0272\n",
            "Epoch [4440/20000], Training Loss: 0.0261\n",
            "Epoch [4441/20000], Training Loss: 0.0262\n",
            "Epoch [4442/20000], Training Loss: 0.0270\n",
            "Epoch [4443/20000], Training Loss: 0.0268\n",
            "Epoch [4444/20000], Training Loss: 0.0260\n",
            "Epoch [4445/20000], Training Loss: 0.0259\n",
            "Epoch [4446/20000], Training Loss: 0.0256\n",
            "Epoch [4447/20000], Training Loss: 0.0269\n",
            "Epoch [4448/20000], Training Loss: 0.0266\n",
            "Epoch [4449/20000], Training Loss: 0.0266\n",
            "Epoch [4450/20000], Training Loss: 0.0249\n",
            "Epoch [4451/20000], Training Loss: 0.0279\n",
            "Epoch [4452/20000], Training Loss: 0.0243\n",
            "Epoch [4453/20000], Training Loss: 0.0273\n",
            "Epoch [4454/20000], Training Loss: 0.0261\n",
            "Epoch [4455/20000], Training Loss: 0.0268\n",
            "Epoch [4456/20000], Training Loss: 0.0261\n",
            "Epoch [4457/20000], Training Loss: 0.0259\n",
            "Epoch [4458/20000], Training Loss: 0.0265\n",
            "Epoch [4459/20000], Training Loss: 0.0237\n",
            "Epoch [4460/20000], Training Loss: 0.0270\n",
            "Epoch [4461/20000], Training Loss: 0.0280\n",
            "Epoch [4462/20000], Training Loss: 0.0260\n",
            "Epoch [4463/20000], Training Loss: 0.0262\n",
            "Epoch [4464/20000], Training Loss: 0.0274\n",
            "Epoch [4465/20000], Training Loss: 0.0265\n",
            "Epoch [4466/20000], Training Loss: 0.0257\n",
            "Epoch [4467/20000], Training Loss: 0.0251\n",
            "Epoch [4468/20000], Training Loss: 0.0258\n",
            "Epoch [4469/20000], Training Loss: 0.0265\n",
            "Epoch [4470/20000], Training Loss: 0.0250\n",
            "Epoch [4471/20000], Training Loss: 0.0262\n",
            "Epoch [4472/20000], Training Loss: 0.0297\n",
            "Epoch [4473/20000], Training Loss: 0.0284\n",
            "Epoch [4474/20000], Training Loss: 0.0256\n",
            "Epoch [4475/20000], Training Loss: 0.0252\n",
            "Epoch [4476/20000], Training Loss: 0.0278\n",
            "Epoch [4477/20000], Training Loss: 0.0262\n",
            "Epoch [4478/20000], Training Loss: 0.0246\n",
            "Epoch [4479/20000], Training Loss: 0.0248\n",
            "Epoch [4480/20000], Training Loss: 0.0253\n",
            "Epoch [4481/20000], Training Loss: 0.0255\n",
            "Epoch [4482/20000], Training Loss: 0.0254\n",
            "Epoch [4483/20000], Training Loss: 0.0265\n",
            "Epoch [4484/20000], Training Loss: 0.0272\n",
            "Epoch [4485/20000], Training Loss: 0.0243\n",
            "Epoch [4486/20000], Training Loss: 0.0260\n",
            "Epoch [4487/20000], Training Loss: 0.0279\n",
            "Epoch [4488/20000], Training Loss: 0.0255\n",
            "Epoch [4489/20000], Training Loss: 0.0263\n",
            "Epoch [4490/20000], Training Loss: 0.0255\n",
            "Epoch [4491/20000], Training Loss: 0.0256\n",
            "Epoch [4492/20000], Training Loss: 0.0265\n",
            "Epoch [4493/20000], Training Loss: 0.0262\n",
            "Epoch [4494/20000], Training Loss: 0.0276\n",
            "Epoch [4495/20000], Training Loss: 0.0266\n",
            "Epoch [4496/20000], Training Loss: 0.0264\n",
            "Epoch [4497/20000], Training Loss: 0.0255\n",
            "Epoch [4498/20000], Training Loss: 0.0256\n",
            "Epoch [4499/20000], Training Loss: 0.0278\n",
            "Epoch [4500/20000], Training Loss: 0.0259\n",
            "Epoch [4501/20000], Training Loss: 0.0287\n",
            "Epoch [4502/20000], Training Loss: 0.0281\n",
            "Epoch [4503/20000], Training Loss: 0.0253\n",
            "Epoch [4504/20000], Training Loss: 0.0276\n",
            "Epoch [4505/20000], Training Loss: 0.0268\n",
            "Epoch [4506/20000], Training Loss: 0.0265\n",
            "Epoch [4507/20000], Training Loss: 0.0250\n",
            "Epoch [4508/20000], Training Loss: 0.0251\n",
            "Epoch [4509/20000], Training Loss: 0.0266\n",
            "Epoch [4510/20000], Training Loss: 0.0273\n",
            "Epoch [4511/20000], Training Loss: 0.0249\n",
            "Epoch [4512/20000], Training Loss: 0.0275\n",
            "Epoch [4513/20000], Training Loss: 0.0269\n",
            "Epoch [4514/20000], Training Loss: 0.0273\n",
            "Epoch [4515/20000], Training Loss: 0.0264\n",
            "Epoch [4516/20000], Training Loss: 0.0248\n",
            "Epoch [4517/20000], Training Loss: 0.0250\n",
            "Epoch [4518/20000], Training Loss: 0.0257\n",
            "Epoch [4519/20000], Training Loss: 0.0263\n",
            "Epoch [4520/20000], Training Loss: 0.0275\n",
            "Epoch [4521/20000], Training Loss: 0.0279\n",
            "Epoch [4522/20000], Training Loss: 0.0285\n",
            "Epoch [4523/20000], Training Loss: 0.0232\n",
            "Epoch [4524/20000], Training Loss: 0.0279\n",
            "Epoch [4525/20000], Training Loss: 0.0251\n",
            "Epoch [4526/20000], Training Loss: 0.0263\n",
            "Epoch [4527/20000], Training Loss: 0.0240\n",
            "Epoch [4528/20000], Training Loss: 0.0250\n",
            "Epoch [4529/20000], Training Loss: 0.0270\n",
            "Epoch [4530/20000], Training Loss: 0.0256\n",
            "Epoch [4531/20000], Training Loss: 0.0275\n",
            "Epoch [4532/20000], Training Loss: 0.0257\n",
            "Epoch [4533/20000], Training Loss: 0.0281\n",
            "Epoch [4534/20000], Training Loss: 0.0258\n",
            "Epoch [4535/20000], Training Loss: 0.0246\n",
            "Epoch [4536/20000], Training Loss: 0.0255\n",
            "Epoch [4537/20000], Training Loss: 0.0266\n",
            "Epoch [4538/20000], Training Loss: 0.0262\n",
            "Epoch [4539/20000], Training Loss: 0.0251\n",
            "Epoch [4540/20000], Training Loss: 0.0258\n",
            "Epoch [4541/20000], Training Loss: 0.0268\n",
            "Epoch [4542/20000], Training Loss: 0.0278\n",
            "Epoch [4543/20000], Training Loss: 0.0260\n",
            "Epoch [4544/20000], Training Loss: 0.0262\n",
            "Epoch [4545/20000], Training Loss: 0.0248\n",
            "Epoch [4546/20000], Training Loss: 0.0270\n",
            "Epoch [4547/20000], Training Loss: 0.0261\n",
            "Epoch [4548/20000], Training Loss: 0.0259\n",
            "Epoch [4549/20000], Training Loss: 0.0271\n",
            "Epoch [4550/20000], Training Loss: 0.0274\n",
            "Epoch [4551/20000], Training Loss: 0.0258\n",
            "Epoch [4552/20000], Training Loss: 0.0259\n",
            "Epoch [4553/20000], Training Loss: 0.0263\n",
            "Epoch [4554/20000], Training Loss: 0.0246\n",
            "Epoch [4555/20000], Training Loss: 0.0255\n",
            "Epoch [4556/20000], Training Loss: 0.0270\n",
            "Epoch [4557/20000], Training Loss: 0.0272\n",
            "Epoch [4558/20000], Training Loss: 0.0271\n",
            "Epoch [4559/20000], Training Loss: 0.0258\n",
            "Epoch [4560/20000], Training Loss: 0.0249\n",
            "Epoch [4561/20000], Training Loss: 0.0262\n",
            "Epoch [4562/20000], Training Loss: 0.0256\n",
            "Epoch [4563/20000], Training Loss: 0.0268\n",
            "Epoch [4564/20000], Training Loss: 0.0250\n",
            "Epoch [4565/20000], Training Loss: 0.0264\n",
            "Epoch [4566/20000], Training Loss: 0.0240\n",
            "Epoch [4567/20000], Training Loss: 0.0255\n",
            "Epoch [4568/20000], Training Loss: 0.0266\n",
            "Epoch [4569/20000], Training Loss: 0.0245\n",
            "Epoch [4570/20000], Training Loss: 0.0271\n",
            "Epoch [4571/20000], Training Loss: 0.0271\n",
            "Epoch [4572/20000], Training Loss: 0.0257\n",
            "Epoch [4573/20000], Training Loss: 0.0252\n",
            "Epoch [4574/20000], Training Loss: 0.0257\n",
            "Epoch [4575/20000], Training Loss: 0.0267\n",
            "Epoch [4576/20000], Training Loss: 0.0263\n",
            "Epoch [4577/20000], Training Loss: 0.0282\n",
            "Epoch [4578/20000], Training Loss: 0.0259\n",
            "Epoch [4579/20000], Training Loss: 0.0260\n",
            "Epoch [4580/20000], Training Loss: 0.0260\n",
            "Epoch [4581/20000], Training Loss: 0.0266\n",
            "Epoch [4582/20000], Training Loss: 0.0279\n",
            "Epoch [4583/20000], Training Loss: 0.0266\n",
            "Epoch [4584/20000], Training Loss: 0.0266\n",
            "Epoch [4585/20000], Training Loss: 0.0280\n",
            "Epoch [4586/20000], Training Loss: 0.0267\n",
            "Epoch [4587/20000], Training Loss: 0.0268\n",
            "Epoch [4588/20000], Training Loss: 0.0269\n",
            "Epoch [4589/20000], Training Loss: 0.0278\n",
            "Epoch [4590/20000], Training Loss: 0.0262\n",
            "Epoch [4591/20000], Training Loss: 0.0241\n",
            "Epoch [4592/20000], Training Loss: 0.0276\n",
            "Epoch [4593/20000], Training Loss: 0.0254\n",
            "Epoch [4594/20000], Training Loss: 0.0264\n",
            "Epoch [4595/20000], Training Loss: 0.0256\n",
            "Epoch [4596/20000], Training Loss: 0.0269\n",
            "Epoch [4597/20000], Training Loss: 0.0266\n",
            "Epoch [4598/20000], Training Loss: 0.0271\n",
            "Epoch [4599/20000], Training Loss: 0.0277\n",
            "Epoch [4600/20000], Training Loss: 0.0258\n",
            "Epoch [4601/20000], Training Loss: 0.0268\n",
            "Epoch [4602/20000], Training Loss: 0.0257\n",
            "Epoch [4603/20000], Training Loss: 0.0262\n",
            "Epoch [4604/20000], Training Loss: 0.0268\n",
            "Epoch [4605/20000], Training Loss: 0.0276\n",
            "Epoch [4606/20000], Training Loss: 0.0273\n",
            "Epoch [4607/20000], Training Loss: 0.0257\n",
            "Epoch [4608/20000], Training Loss: 0.0284\n",
            "Epoch [4609/20000], Training Loss: 0.0252\n",
            "Epoch [4610/20000], Training Loss: 0.0261\n",
            "Epoch [4611/20000], Training Loss: 0.0256\n",
            "Epoch [4612/20000], Training Loss: 0.0244\n",
            "Epoch [4613/20000], Training Loss: 0.0247\n",
            "Epoch [4614/20000], Training Loss: 0.0279\n",
            "Epoch [4615/20000], Training Loss: 0.0259\n",
            "Epoch [4616/20000], Training Loss: 0.0257\n",
            "Epoch [4617/20000], Training Loss: 0.0247\n",
            "Epoch [4618/20000], Training Loss: 0.0247\n",
            "Epoch [4619/20000], Training Loss: 0.0291\n",
            "Epoch [4620/20000], Training Loss: 0.0252\n",
            "Epoch [4621/20000], Training Loss: 0.0269\n",
            "Epoch [4622/20000], Training Loss: 0.0250\n",
            "Epoch [4623/20000], Training Loss: 0.0267\n",
            "Epoch [4624/20000], Training Loss: 0.0263\n",
            "Epoch [4625/20000], Training Loss: 0.0246\n",
            "Epoch [4626/20000], Training Loss: 0.0263\n",
            "Epoch [4627/20000], Training Loss: 0.0255\n",
            "Epoch [4628/20000], Training Loss: 0.0245\n",
            "Epoch [4629/20000], Training Loss: 0.0252\n",
            "Epoch [4630/20000], Training Loss: 0.0252\n",
            "Epoch [4631/20000], Training Loss: 0.0255\n",
            "Epoch [4632/20000], Training Loss: 0.0259\n",
            "Epoch [4633/20000], Training Loss: 0.0265\n",
            "Epoch [4634/20000], Training Loss: 0.0253\n",
            "Epoch [4635/20000], Training Loss: 0.0260\n",
            "Epoch [4636/20000], Training Loss: 0.0257\n",
            "Epoch [4637/20000], Training Loss: 0.0249\n",
            "Epoch [4638/20000], Training Loss: 0.0274\n",
            "Epoch [4639/20000], Training Loss: 0.0283\n",
            "Epoch [4640/20000], Training Loss: 0.0266\n",
            "Epoch [4641/20000], Training Loss: 0.0263\n",
            "Epoch [4642/20000], Training Loss: 0.0276\n",
            "Epoch [4643/20000], Training Loss: 0.0261\n",
            "Epoch [4644/20000], Training Loss: 0.0258\n",
            "Epoch [4645/20000], Training Loss: 0.0248\n",
            "Epoch [4646/20000], Training Loss: 0.0271\n",
            "Epoch [4647/20000], Training Loss: 0.0270\n",
            "Epoch [4648/20000], Training Loss: 0.0287\n",
            "Epoch [4649/20000], Training Loss: 0.0280\n",
            "Epoch [4650/20000], Training Loss: 0.0264\n",
            "Epoch [4651/20000], Training Loss: 0.0284\n",
            "Epoch [4652/20000], Training Loss: 0.0262\n",
            "Epoch [4653/20000], Training Loss: 0.0244\n",
            "Epoch [4654/20000], Training Loss: 0.0269\n",
            "Epoch [4655/20000], Training Loss: 0.0282\n",
            "Epoch [4656/20000], Training Loss: 0.0252\n",
            "Epoch [4657/20000], Training Loss: 0.0254\n",
            "Epoch [4658/20000], Training Loss: 0.0249\n",
            "Epoch [4659/20000], Training Loss: 0.0280\n",
            "Epoch [4660/20000], Training Loss: 0.0273\n",
            "Epoch [4661/20000], Training Loss: 0.0273\n",
            "Epoch [4662/20000], Training Loss: 0.0262\n",
            "Epoch [4663/20000], Training Loss: 0.0244\n",
            "Epoch [4664/20000], Training Loss: 0.0258\n",
            "Epoch [4665/20000], Training Loss: 0.0265\n",
            "Epoch [4666/20000], Training Loss: 0.0263\n",
            "Epoch [4667/20000], Training Loss: 0.0265\n",
            "Epoch [4668/20000], Training Loss: 0.0264\n",
            "Epoch [4669/20000], Training Loss: 0.0267\n",
            "Epoch [4670/20000], Training Loss: 0.0262\n",
            "Epoch [4671/20000], Training Loss: 0.0265\n",
            "Epoch [4672/20000], Training Loss: 0.0257\n",
            "Epoch [4673/20000], Training Loss: 0.0251\n",
            "Epoch [4674/20000], Training Loss: 0.0268\n",
            "Epoch [4675/20000], Training Loss: 0.0267\n",
            "Epoch [4676/20000], Training Loss: 0.0280\n",
            "Epoch [4677/20000], Training Loss: 0.0265\n",
            "Epoch [4678/20000], Training Loss: 0.0252\n",
            "Epoch [4679/20000], Training Loss: 0.0248\n",
            "Epoch [4680/20000], Training Loss: 0.0276\n",
            "Epoch [4681/20000], Training Loss: 0.0286\n",
            "Epoch [4682/20000], Training Loss: 0.0263\n",
            "Epoch [4683/20000], Training Loss: 0.0282\n",
            "Epoch [4684/20000], Training Loss: 0.0257\n",
            "Epoch [4685/20000], Training Loss: 0.0269\n",
            "Epoch [4686/20000], Training Loss: 0.0266\n",
            "Epoch [4687/20000], Training Loss: 0.0269\n",
            "Epoch [4688/20000], Training Loss: 0.0271\n",
            "Epoch [4689/20000], Training Loss: 0.0259\n",
            "Epoch [4690/20000], Training Loss: 0.0256\n",
            "Epoch [4691/20000], Training Loss: 0.0252\n",
            "Epoch [4692/20000], Training Loss: 0.0267\n",
            "Epoch [4693/20000], Training Loss: 0.0244\n",
            "Epoch [4694/20000], Training Loss: 0.0295\n",
            "Epoch [4695/20000], Training Loss: 0.0282\n",
            "Epoch [4696/20000], Training Loss: 0.0240\n",
            "Epoch [4697/20000], Training Loss: 0.0267\n",
            "Epoch [4698/20000], Training Loss: 0.0272\n",
            "Epoch [4699/20000], Training Loss: 0.0261\n",
            "Epoch [4700/20000], Training Loss: 0.0261\n",
            "Epoch [4701/20000], Training Loss: 0.0267\n",
            "Epoch [4702/20000], Training Loss: 0.0257\n",
            "Epoch [4703/20000], Training Loss: 0.0278\n",
            "Epoch [4704/20000], Training Loss: 0.0272\n",
            "Epoch [4705/20000], Training Loss: 0.0268\n",
            "Epoch [4706/20000], Training Loss: 0.0270\n",
            "Epoch [4707/20000], Training Loss: 0.0270\n",
            "Epoch [4708/20000], Training Loss: 0.0280\n",
            "Epoch [4709/20000], Training Loss: 0.0285\n",
            "Epoch [4710/20000], Training Loss: 0.0268\n",
            "Epoch [4711/20000], Training Loss: 0.0273\n",
            "Epoch [4712/20000], Training Loss: 0.0255\n",
            "Epoch [4713/20000], Training Loss: 0.0240\n",
            "Epoch [4714/20000], Training Loss: 0.0290\n",
            "Epoch [4715/20000], Training Loss: 0.0263\n",
            "Epoch [4716/20000], Training Loss: 0.0255\n",
            "Epoch [4717/20000], Training Loss: 0.0264\n",
            "Epoch [4718/20000], Training Loss: 0.0266\n",
            "Epoch [4719/20000], Training Loss: 0.0278\n",
            "Epoch [4720/20000], Training Loss: 0.0245\n",
            "Epoch [4721/20000], Training Loss: 0.0275\n",
            "Epoch [4722/20000], Training Loss: 0.0287\n",
            "Epoch [4723/20000], Training Loss: 0.0258\n",
            "Epoch [4724/20000], Training Loss: 0.0252\n",
            "Epoch [4725/20000], Training Loss: 0.0275\n",
            "Epoch [4726/20000], Training Loss: 0.0260\n",
            "Epoch [4727/20000], Training Loss: 0.0276\n",
            "Epoch [4728/20000], Training Loss: 0.0249\n",
            "Epoch [4729/20000], Training Loss: 0.0244\n",
            "Epoch [4730/20000], Training Loss: 0.0252\n",
            "Epoch [4731/20000], Training Loss: 0.0240\n",
            "Epoch [4732/20000], Training Loss: 0.0260\n",
            "Epoch [4733/20000], Training Loss: 0.0247\n",
            "Epoch [4734/20000], Training Loss: 0.0262\n",
            "Epoch [4735/20000], Training Loss: 0.0285\n",
            "Epoch [4736/20000], Training Loss: 0.0272\n",
            "Epoch [4737/20000], Training Loss: 0.0267\n",
            "Epoch [4738/20000], Training Loss: 0.0274\n",
            "Epoch [4739/20000], Training Loss: 0.0252\n",
            "Epoch [4740/20000], Training Loss: 0.0270\n",
            "Epoch [4741/20000], Training Loss: 0.0263\n",
            "Epoch [4742/20000], Training Loss: 0.0254\n",
            "Epoch [4743/20000], Training Loss: 0.0285\n",
            "Epoch [4744/20000], Training Loss: 0.0256\n",
            "Epoch [4745/20000], Training Loss: 0.0264\n",
            "Epoch [4746/20000], Training Loss: 0.0278\n",
            "Epoch [4747/20000], Training Loss: 0.0264\n",
            "Epoch [4748/20000], Training Loss: 0.0278\n",
            "Epoch [4749/20000], Training Loss: 0.0267\n",
            "Epoch [4750/20000], Training Loss: 0.0287\n",
            "Epoch [4751/20000], Training Loss: 0.0272\n",
            "Epoch [4752/20000], Training Loss: 0.0270\n",
            "Epoch [4753/20000], Training Loss: 0.0259\n",
            "Epoch [4754/20000], Training Loss: 0.0248\n",
            "Epoch [4755/20000], Training Loss: 0.0278\n",
            "Epoch [4756/20000], Training Loss: 0.0251\n",
            "Epoch [4757/20000], Training Loss: 0.0283\n",
            "Epoch [4758/20000], Training Loss: 0.0266\n",
            "Epoch [4759/20000], Training Loss: 0.0260\n",
            "Epoch [4760/20000], Training Loss: 0.0259\n",
            "Epoch [4761/20000], Training Loss: 0.0255\n",
            "Epoch [4762/20000], Training Loss: 0.0262\n",
            "Epoch [4763/20000], Training Loss: 0.0257\n",
            "Epoch [4764/20000], Training Loss: 0.0264\n",
            "Epoch [4765/20000], Training Loss: 0.0264\n",
            "Epoch [4766/20000], Training Loss: 0.0263\n",
            "Epoch [4767/20000], Training Loss: 0.0273\n",
            "Epoch [4768/20000], Training Loss: 0.0257\n",
            "Epoch [4769/20000], Training Loss: 0.0259\n",
            "Epoch [4770/20000], Training Loss: 0.0250\n",
            "Epoch [4771/20000], Training Loss: 0.0262\n",
            "Epoch [4772/20000], Training Loss: 0.0265\n",
            "Epoch [4773/20000], Training Loss: 0.0266\n",
            "Epoch [4774/20000], Training Loss: 0.0257\n",
            "Epoch [4775/20000], Training Loss: 0.0296\n",
            "Epoch [4776/20000], Training Loss: 0.0268\n",
            "Epoch [4777/20000], Training Loss: 0.0254\n",
            "Epoch [4778/20000], Training Loss: 0.0247\n",
            "Epoch [4779/20000], Training Loss: 0.0252\n",
            "Epoch [4780/20000], Training Loss: 0.0264\n",
            "Epoch [4781/20000], Training Loss: 0.0252\n",
            "Epoch [4782/20000], Training Loss: 0.0254\n",
            "Epoch [4783/20000], Training Loss: 0.0248\n",
            "Epoch [4784/20000], Training Loss: 0.0240\n",
            "Epoch [4785/20000], Training Loss: 0.0260\n",
            "Epoch [4786/20000], Training Loss: 0.0250\n",
            "Epoch [4787/20000], Training Loss: 0.0270\n",
            "Epoch [4788/20000], Training Loss: 0.0281\n",
            "Epoch [4789/20000], Training Loss: 0.0254\n",
            "Epoch [4790/20000], Training Loss: 0.0248\n",
            "Epoch [4791/20000], Training Loss: 0.0278\n",
            "Epoch [4792/20000], Training Loss: 0.0257\n",
            "Epoch [4793/20000], Training Loss: 0.0261\n",
            "Epoch [4794/20000], Training Loss: 0.0243\n",
            "Epoch [4795/20000], Training Loss: 0.0263\n",
            "Epoch [4796/20000], Training Loss: 0.0240\n",
            "Epoch [4797/20000], Training Loss: 0.0275\n",
            "Epoch [4798/20000], Training Loss: 0.0263\n",
            "Epoch [4799/20000], Training Loss: 0.0262\n",
            "Epoch [4800/20000], Training Loss: 0.0262\n",
            "Epoch [4801/20000], Training Loss: 0.0267\n",
            "Epoch [4802/20000], Training Loss: 0.0257\n",
            "Epoch [4803/20000], Training Loss: 0.0260\n",
            "Epoch [4804/20000], Training Loss: 0.0250\n",
            "Epoch [4805/20000], Training Loss: 0.0250\n",
            "Epoch [4806/20000], Training Loss: 0.0264\n",
            "Epoch [4807/20000], Training Loss: 0.0286\n",
            "Epoch [4808/20000], Training Loss: 0.0267\n",
            "Epoch [4809/20000], Training Loss: 0.0279\n",
            "Epoch [4810/20000], Training Loss: 0.0247\n",
            "Epoch [4811/20000], Training Loss: 0.0249\n",
            "Epoch [4812/20000], Training Loss: 0.0258\n",
            "Epoch [4813/20000], Training Loss: 0.0263\n",
            "Epoch [4814/20000], Training Loss: 0.0263\n",
            "Epoch [4815/20000], Training Loss: 0.0270\n",
            "Epoch [4816/20000], Training Loss: 0.0260\n",
            "Epoch [4817/20000], Training Loss: 0.0255\n",
            "Epoch [4818/20000], Training Loss: 0.0282\n",
            "Epoch [4819/20000], Training Loss: 0.0253\n",
            "Epoch [4820/20000], Training Loss: 0.0248\n",
            "Epoch [4821/20000], Training Loss: 0.0295\n",
            "Epoch [4822/20000], Training Loss: 0.0272\n",
            "Epoch [4823/20000], Training Loss: 0.0270\n",
            "Epoch [4824/20000], Training Loss: 0.0272\n",
            "Epoch [4825/20000], Training Loss: 0.0268\n",
            "Epoch [4826/20000], Training Loss: 0.0255\n",
            "Epoch [4827/20000], Training Loss: 0.0270\n",
            "Epoch [4828/20000], Training Loss: 0.0238\n",
            "Epoch [4829/20000], Training Loss: 0.0263\n",
            "Epoch [4830/20000], Training Loss: 0.0274\n",
            "Epoch [4831/20000], Training Loss: 0.0260\n",
            "Epoch [4832/20000], Training Loss: 0.0259\n",
            "Epoch [4833/20000], Training Loss: 0.0259\n",
            "Epoch [4834/20000], Training Loss: 0.0279\n",
            "Epoch [4835/20000], Training Loss: 0.0285\n",
            "Epoch [4836/20000], Training Loss: 0.0239\n",
            "Epoch [4837/20000], Training Loss: 0.0255\n",
            "Epoch [4838/20000], Training Loss: 0.0275\n",
            "Epoch [4839/20000], Training Loss: 0.0274\n",
            "Epoch [4840/20000], Training Loss: 0.0266\n",
            "Epoch [4841/20000], Training Loss: 0.0295\n",
            "Epoch [4842/20000], Training Loss: 0.0257\n",
            "Epoch [4843/20000], Training Loss: 0.0282\n",
            "Epoch [4844/20000], Training Loss: 0.0256\n",
            "Epoch [4845/20000], Training Loss: 0.0279\n",
            "Epoch [4846/20000], Training Loss: 0.0289\n",
            "Epoch [4847/20000], Training Loss: 0.0272\n",
            "Epoch [4848/20000], Training Loss: 0.0280\n",
            "Epoch [4849/20000], Training Loss: 0.0268\n",
            "Epoch [4850/20000], Training Loss: 0.0249\n",
            "Epoch [4851/20000], Training Loss: 0.0269\n",
            "Epoch [4852/20000], Training Loss: 0.0277\n",
            "Epoch [4853/20000], Training Loss: 0.0274\n",
            "Epoch [4854/20000], Training Loss: 0.0260\n",
            "Epoch [4855/20000], Training Loss: 0.0290\n",
            "Epoch [4856/20000], Training Loss: 0.0284\n",
            "Epoch [4857/20000], Training Loss: 0.0284\n",
            "Epoch [4858/20000], Training Loss: 0.0264\n",
            "Epoch [4859/20000], Training Loss: 0.0294\n",
            "Epoch [4860/20000], Training Loss: 0.0265\n",
            "Epoch [4861/20000], Training Loss: 0.0247\n",
            "Epoch [4862/20000], Training Loss: 0.0260\n",
            "Epoch [4863/20000], Training Loss: 0.0268\n",
            "Epoch [4864/20000], Training Loss: 0.0268\n",
            "Epoch [4865/20000], Training Loss: 0.0290\n",
            "Epoch [4866/20000], Training Loss: 0.0274\n",
            "Epoch [4867/20000], Training Loss: 0.0290\n",
            "Epoch [4868/20000], Training Loss: 0.0276\n",
            "Epoch [4869/20000], Training Loss: 0.0244\n",
            "Epoch [4870/20000], Training Loss: 0.0262\n",
            "Epoch [4871/20000], Training Loss: 0.0286\n",
            "Epoch [4872/20000], Training Loss: 0.0259\n",
            "Epoch [4873/20000], Training Loss: 0.0251\n",
            "Epoch [4874/20000], Training Loss: 0.0252\n",
            "Epoch [4875/20000], Training Loss: 0.0277\n",
            "Epoch [4876/20000], Training Loss: 0.0271\n",
            "Epoch [4877/20000], Training Loss: 0.0245\n",
            "Epoch [4878/20000], Training Loss: 0.0266\n",
            "Epoch [4879/20000], Training Loss: 0.0283\n",
            "Epoch [4880/20000], Training Loss: 0.0255\n",
            "Epoch [4881/20000], Training Loss: 0.0267\n",
            "Epoch [4882/20000], Training Loss: 0.0270\n",
            "Epoch [4883/20000], Training Loss: 0.0249\n",
            "Epoch [4884/20000], Training Loss: 0.0276\n",
            "Epoch [4885/20000], Training Loss: 0.0280\n",
            "Epoch [4886/20000], Training Loss: 0.0269\n",
            "Epoch [4887/20000], Training Loss: 0.0258\n",
            "Epoch [4888/20000], Training Loss: 0.0253\n",
            "Epoch [4889/20000], Training Loss: 0.0262\n",
            "Epoch [4890/20000], Training Loss: 0.0265\n",
            "Epoch [4891/20000], Training Loss: 0.0256\n",
            "Epoch [4892/20000], Training Loss: 0.0264\n",
            "Epoch [4893/20000], Training Loss: 0.0275\n",
            "Epoch [4894/20000], Training Loss: 0.0256\n",
            "Epoch [4895/20000], Training Loss: 0.0252\n",
            "Epoch [4896/20000], Training Loss: 0.0268\n",
            "Epoch [4897/20000], Training Loss: 0.0267\n",
            "Epoch [4898/20000], Training Loss: 0.0257\n",
            "Epoch [4899/20000], Training Loss: 0.0254\n",
            "Epoch [4900/20000], Training Loss: 0.0303\n",
            "Epoch [4901/20000], Training Loss: 0.0269\n",
            "Epoch [4902/20000], Training Loss: 0.0253\n",
            "Epoch [4903/20000], Training Loss: 0.0265\n",
            "Epoch [4904/20000], Training Loss: 0.0267\n",
            "Epoch [4905/20000], Training Loss: 0.0277\n",
            "Epoch [4906/20000], Training Loss: 0.0273\n",
            "Epoch [4907/20000], Training Loss: 0.0265\n",
            "Epoch [4908/20000], Training Loss: 0.0262\n",
            "Epoch [4909/20000], Training Loss: 0.0252\n",
            "Epoch [4910/20000], Training Loss: 0.0261\n",
            "Epoch [4911/20000], Training Loss: 0.0257\n",
            "Epoch [4912/20000], Training Loss: 0.0258\n",
            "Epoch [4913/20000], Training Loss: 0.0254\n",
            "Epoch [4914/20000], Training Loss: 0.0270\n",
            "Epoch [4915/20000], Training Loss: 0.0266\n",
            "Epoch [4916/20000], Training Loss: 0.0258\n",
            "Epoch [4917/20000], Training Loss: 0.0266\n",
            "Epoch [4918/20000], Training Loss: 0.0277\n",
            "Epoch [4919/20000], Training Loss: 0.0273\n",
            "Epoch [4920/20000], Training Loss: 0.0256\n",
            "Epoch [4921/20000], Training Loss: 0.0285\n",
            "Epoch [4922/20000], Training Loss: 0.0262\n",
            "Epoch [4923/20000], Training Loss: 0.0254\n",
            "Epoch [4924/20000], Training Loss: 0.0271\n",
            "Epoch [4925/20000], Training Loss: 0.0264\n",
            "Epoch [4926/20000], Training Loss: 0.0274\n",
            "Epoch [4927/20000], Training Loss: 0.0261\n",
            "Epoch [4928/20000], Training Loss: 0.0258\n",
            "Epoch [4929/20000], Training Loss: 0.0288\n",
            "Epoch [4930/20000], Training Loss: 0.0254\n",
            "Epoch [4931/20000], Training Loss: 0.0272\n",
            "Epoch [4932/20000], Training Loss: 0.0261\n",
            "Epoch [4933/20000], Training Loss: 0.0260\n",
            "Epoch [4934/20000], Training Loss: 0.0252\n",
            "Epoch [4935/20000], Training Loss: 0.0277\n",
            "Epoch [4936/20000], Training Loss: 0.0271\n",
            "Epoch [4937/20000], Training Loss: 0.0255\n",
            "Epoch [4938/20000], Training Loss: 0.0285\n",
            "Epoch [4939/20000], Training Loss: 0.0255\n",
            "Epoch [4940/20000], Training Loss: 0.0272\n",
            "Epoch [4941/20000], Training Loss: 0.0254\n",
            "Epoch [4942/20000], Training Loss: 0.0256\n",
            "Epoch [4943/20000], Training Loss: 0.0258\n",
            "Epoch [4944/20000], Training Loss: 0.0287\n",
            "Epoch [4945/20000], Training Loss: 0.0259\n",
            "Epoch [4946/20000], Training Loss: 0.0270\n",
            "Epoch [4947/20000], Training Loss: 0.0267\n",
            "Epoch [4948/20000], Training Loss: 0.0249\n",
            "Epoch [4949/20000], Training Loss: 0.0291\n",
            "Epoch [4950/20000], Training Loss: 0.0278\n",
            "Epoch [4951/20000], Training Loss: 0.0276\n",
            "Epoch [4952/20000], Training Loss: 0.0261\n",
            "Epoch [4953/20000], Training Loss: 0.0282\n",
            "Epoch [4954/20000], Training Loss: 0.0266\n",
            "Epoch [4955/20000], Training Loss: 0.0261\n",
            "Epoch [4956/20000], Training Loss: 0.0252\n",
            "Epoch [4957/20000], Training Loss: 0.0272\n",
            "Epoch [4958/20000], Training Loss: 0.0263\n",
            "Epoch [4959/20000], Training Loss: 0.0257\n",
            "Epoch [4960/20000], Training Loss: 0.0266\n",
            "Epoch [4961/20000], Training Loss: 0.0279\n",
            "Epoch [4962/20000], Training Loss: 0.0276\n",
            "Epoch [4963/20000], Training Loss: 0.0278\n",
            "Epoch [4964/20000], Training Loss: 0.0278\n",
            "Epoch [4965/20000], Training Loss: 0.0278\n",
            "Epoch [4966/20000], Training Loss: 0.0261\n",
            "Epoch [4967/20000], Training Loss: 0.0247\n",
            "Epoch [4968/20000], Training Loss: 0.0258\n",
            "Epoch [4969/20000], Training Loss: 0.0277\n",
            "Epoch [4970/20000], Training Loss: 0.0287\n",
            "Epoch [4971/20000], Training Loss: 0.0260\n",
            "Epoch [4972/20000], Training Loss: 0.0259\n",
            "Epoch [4973/20000], Training Loss: 0.0270\n",
            "Epoch [4974/20000], Training Loss: 0.0268\n",
            "Epoch [4975/20000], Training Loss: 0.0269\n",
            "Epoch [4976/20000], Training Loss: 0.0275\n",
            "Epoch [4977/20000], Training Loss: 0.0282\n",
            "Epoch [4978/20000], Training Loss: 0.0264\n",
            "Epoch [4979/20000], Training Loss: 0.0263\n",
            "Epoch [4980/20000], Training Loss: 0.0256\n",
            "Epoch [4981/20000], Training Loss: 0.0263\n",
            "Epoch [4982/20000], Training Loss: 0.0265\n",
            "Epoch [4983/20000], Training Loss: 0.0267\n",
            "Epoch [4984/20000], Training Loss: 0.0260\n",
            "Epoch [4985/20000], Training Loss: 0.0267\n",
            "Epoch [4986/20000], Training Loss: 0.0273\n",
            "Epoch [4987/20000], Training Loss: 0.0250\n",
            "Epoch [4988/20000], Training Loss: 0.0275\n",
            "Epoch [4989/20000], Training Loss: 0.0256\n",
            "Epoch [4990/20000], Training Loss: 0.0252\n",
            "Epoch [4991/20000], Training Loss: 0.0274\n",
            "Epoch [4992/20000], Training Loss: 0.0271\n",
            "Epoch [4993/20000], Training Loss: 0.0259\n",
            "Epoch [4994/20000], Training Loss: 0.0265\n",
            "Epoch [4995/20000], Training Loss: 0.0261\n",
            "Epoch [4996/20000], Training Loss: 0.0278\n",
            "Epoch [4997/20000], Training Loss: 0.0285\n",
            "Epoch [4998/20000], Training Loss: 0.0277\n",
            "Epoch [4999/20000], Training Loss: 0.0291\n",
            "Epoch [5000/20000], Training Loss: 0.0259\n",
            "Epoch [5001/20000], Training Loss: 0.0279\n",
            "Epoch [5002/20000], Training Loss: 0.0256\n",
            "Epoch [5003/20000], Training Loss: 0.0247\n",
            "Epoch [5004/20000], Training Loss: 0.0247\n",
            "Epoch [5005/20000], Training Loss: 0.0256\n",
            "Epoch [5006/20000], Training Loss: 0.0272\n",
            "Epoch [5007/20000], Training Loss: 0.0274\n",
            "Epoch [5008/20000], Training Loss: 0.0253\n",
            "Epoch [5009/20000], Training Loss: 0.0269\n",
            "Epoch [5010/20000], Training Loss: 0.0259\n",
            "Epoch [5011/20000], Training Loss: 0.0272\n",
            "Epoch [5012/20000], Training Loss: 0.0276\n",
            "Epoch [5013/20000], Training Loss: 0.0260\n",
            "Epoch [5014/20000], Training Loss: 0.0248\n",
            "Epoch [5015/20000], Training Loss: 0.0265\n",
            "Epoch [5016/20000], Training Loss: 0.0254\n",
            "Epoch [5017/20000], Training Loss: 0.0265\n",
            "Epoch [5018/20000], Training Loss: 0.0282\n",
            "Epoch [5019/20000], Training Loss: 0.0255\n",
            "Epoch [5020/20000], Training Loss: 0.0280\n",
            "Epoch [5021/20000], Training Loss: 0.0277\n",
            "Epoch [5022/20000], Training Loss: 0.0256\n",
            "Epoch [5023/20000], Training Loss: 0.0252\n",
            "Epoch [5024/20000], Training Loss: 0.0248\n",
            "Epoch [5025/20000], Training Loss: 0.0274\n",
            "Epoch [5026/20000], Training Loss: 0.0268\n",
            "Epoch [5027/20000], Training Loss: 0.0272\n",
            "Epoch [5028/20000], Training Loss: 0.0260\n",
            "Epoch [5029/20000], Training Loss: 0.0274\n",
            "Epoch [5030/20000], Training Loss: 0.0279\n",
            "Epoch [5031/20000], Training Loss: 0.0280\n",
            "Epoch [5032/20000], Training Loss: 0.0258\n",
            "Epoch [5033/20000], Training Loss: 0.0266\n",
            "Epoch [5034/20000], Training Loss: 0.0264\n",
            "Epoch [5035/20000], Training Loss: 0.0244\n",
            "Epoch [5036/20000], Training Loss: 0.0270\n",
            "Epoch [5037/20000], Training Loss: 0.0246\n",
            "Epoch [5038/20000], Training Loss: 0.0238\n",
            "Epoch [5039/20000], Training Loss: 0.0264\n",
            "Epoch [5040/20000], Training Loss: 0.0259\n",
            "Epoch [5041/20000], Training Loss: 0.0263\n",
            "Epoch [5042/20000], Training Loss: 0.0255\n",
            "Epoch [5043/20000], Training Loss: 0.0245\n",
            "Epoch [5044/20000], Training Loss: 0.0249\n",
            "Epoch [5045/20000], Training Loss: 0.0264\n",
            "Epoch [5046/20000], Training Loss: 0.0267\n",
            "Epoch [5047/20000], Training Loss: 0.0288\n",
            "Epoch [5048/20000], Training Loss: 0.0246\n",
            "Epoch [5049/20000], Training Loss: 0.0283\n",
            "Epoch [5050/20000], Training Loss: 0.0245\n",
            "Epoch [5051/20000], Training Loss: 0.0280\n",
            "Epoch [5052/20000], Training Loss: 0.0255\n",
            "Epoch [5053/20000], Training Loss: 0.0258\n",
            "Epoch [5054/20000], Training Loss: 0.0258\n",
            "Epoch [5055/20000], Training Loss: 0.0254\n",
            "Epoch [5056/20000], Training Loss: 0.0275\n",
            "Epoch [5057/20000], Training Loss: 0.0275\n",
            "Epoch [5058/20000], Training Loss: 0.0270\n",
            "Epoch [5059/20000], Training Loss: 0.0262\n",
            "Epoch [5060/20000], Training Loss: 0.0264\n",
            "Epoch [5061/20000], Training Loss: 0.0247\n",
            "Epoch [5062/20000], Training Loss: 0.0273\n",
            "Epoch [5063/20000], Training Loss: 0.0253\n",
            "Epoch [5064/20000], Training Loss: 0.0264\n",
            "Epoch [5065/20000], Training Loss: 0.0271\n",
            "Epoch [5066/20000], Training Loss: 0.0256\n",
            "Epoch [5067/20000], Training Loss: 0.0268\n",
            "Epoch [5068/20000], Training Loss: 0.0256\n",
            "Epoch [5069/20000], Training Loss: 0.0266\n",
            "Epoch [5070/20000], Training Loss: 0.0273\n",
            "Epoch [5071/20000], Training Loss: 0.0273\n",
            "Epoch [5072/20000], Training Loss: 0.0262\n",
            "Epoch [5073/20000], Training Loss: 0.0271\n",
            "Epoch [5074/20000], Training Loss: 0.0247\n",
            "Epoch [5075/20000], Training Loss: 0.0284\n",
            "Epoch [5076/20000], Training Loss: 0.0277\n",
            "Epoch [5077/20000], Training Loss: 0.0260\n",
            "Epoch [5078/20000], Training Loss: 0.0275\n",
            "Epoch [5079/20000], Training Loss: 0.0277\n",
            "Epoch [5080/20000], Training Loss: 0.0260\n",
            "Epoch [5081/20000], Training Loss: 0.0261\n",
            "Epoch [5082/20000], Training Loss: 0.0264\n",
            "Epoch [5083/20000], Training Loss: 0.0285\n",
            "Epoch [5084/20000], Training Loss: 0.0282\n",
            "Epoch [5085/20000], Training Loss: 0.0288\n",
            "Epoch [5086/20000], Training Loss: 0.0250\n",
            "Epoch [5087/20000], Training Loss: 0.0278\n",
            "Epoch [5088/20000], Training Loss: 0.0269\n",
            "Epoch [5089/20000], Training Loss: 0.0279\n",
            "Epoch [5090/20000], Training Loss: 0.0261\n",
            "Epoch [5091/20000], Training Loss: 0.0283\n",
            "Epoch [5092/20000], Training Loss: 0.0276\n",
            "Epoch [5093/20000], Training Loss: 0.0237\n",
            "Epoch [5094/20000], Training Loss: 0.0258\n",
            "Epoch [5095/20000], Training Loss: 0.0270\n",
            "Epoch [5096/20000], Training Loss: 0.0266\n",
            "Epoch [5097/20000], Training Loss: 0.0265\n",
            "Epoch [5098/20000], Training Loss: 0.0279\n",
            "Epoch [5099/20000], Training Loss: 0.0254\n",
            "Epoch [5100/20000], Training Loss: 0.0269\n",
            "Epoch [5101/20000], Training Loss: 0.0261\n",
            "Epoch [5102/20000], Training Loss: 0.0246\n",
            "Epoch [5103/20000], Training Loss: 0.0261\n",
            "Epoch [5104/20000], Training Loss: 0.0264\n",
            "Epoch [5105/20000], Training Loss: 0.0279\n",
            "Epoch [5106/20000], Training Loss: 0.0262\n",
            "Epoch [5107/20000], Training Loss: 0.0260\n",
            "Epoch [5108/20000], Training Loss: 0.0268\n",
            "Epoch [5109/20000], Training Loss: 0.0268\n",
            "Epoch [5110/20000], Training Loss: 0.0274\n",
            "Epoch [5111/20000], Training Loss: 0.0268\n",
            "Epoch [5112/20000], Training Loss: 0.0256\n",
            "Epoch [5113/20000], Training Loss: 0.0270\n",
            "Epoch [5114/20000], Training Loss: 0.0292\n",
            "Epoch [5115/20000], Training Loss: 0.0260\n",
            "Epoch [5116/20000], Training Loss: 0.0271\n",
            "Epoch [5117/20000], Training Loss: 0.0262\n",
            "Epoch [5118/20000], Training Loss: 0.0268\n",
            "Epoch [5119/20000], Training Loss: 0.0262\n",
            "Epoch [5120/20000], Training Loss: 0.0275\n",
            "Epoch [5121/20000], Training Loss: 0.0273\n",
            "Epoch [5122/20000], Training Loss: 0.0256\n",
            "Epoch [5123/20000], Training Loss: 0.0264\n",
            "Epoch [5124/20000], Training Loss: 0.0255\n",
            "Epoch [5125/20000], Training Loss: 0.0269\n",
            "Epoch [5126/20000], Training Loss: 0.0270\n",
            "Epoch [5127/20000], Training Loss: 0.0254\n",
            "Epoch [5128/20000], Training Loss: 0.0250\n",
            "Epoch [5129/20000], Training Loss: 0.0255\n",
            "Epoch [5130/20000], Training Loss: 0.0253\n",
            "Epoch [5131/20000], Training Loss: 0.0279\n",
            "Epoch [5132/20000], Training Loss: 0.0256\n",
            "Epoch [5133/20000], Training Loss: 0.0250\n",
            "Epoch [5134/20000], Training Loss: 0.0278\n",
            "Epoch [5135/20000], Training Loss: 0.0263\n",
            "Epoch [5136/20000], Training Loss: 0.0254\n",
            "Epoch [5137/20000], Training Loss: 0.0257\n",
            "Epoch [5138/20000], Training Loss: 0.0270\n",
            "Epoch [5139/20000], Training Loss: 0.0271\n",
            "Epoch [5140/20000], Training Loss: 0.0270\n",
            "Epoch [5141/20000], Training Loss: 0.0268\n",
            "Epoch [5142/20000], Training Loss: 0.0245\n",
            "Epoch [5143/20000], Training Loss: 0.0263\n",
            "Epoch [5144/20000], Training Loss: 0.0260\n",
            "Epoch [5145/20000], Training Loss: 0.0285\n",
            "Epoch [5146/20000], Training Loss: 0.0265\n",
            "Epoch [5147/20000], Training Loss: 0.0268\n",
            "Epoch [5148/20000], Training Loss: 0.0253\n",
            "Epoch [5149/20000], Training Loss: 0.0252\n",
            "Epoch [5150/20000], Training Loss: 0.0256\n",
            "Epoch [5151/20000], Training Loss: 0.0264\n",
            "Epoch [5152/20000], Training Loss: 0.0271\n",
            "Epoch [5153/20000], Training Loss: 0.0255\n",
            "Epoch [5154/20000], Training Loss: 0.0276\n",
            "Epoch [5155/20000], Training Loss: 0.0287\n",
            "Epoch [5156/20000], Training Loss: 0.0272\n",
            "Epoch [5157/20000], Training Loss: 0.0288\n",
            "Epoch [5158/20000], Training Loss: 0.0258\n",
            "Epoch [5159/20000], Training Loss: 0.0251\n",
            "Epoch [5160/20000], Training Loss: 0.0246\n",
            "Epoch [5161/20000], Training Loss: 0.0257\n",
            "Epoch [5162/20000], Training Loss: 0.0275\n",
            "Epoch [5163/20000], Training Loss: 0.0262\n",
            "Epoch [5164/20000], Training Loss: 0.0256\n",
            "Epoch [5165/20000], Training Loss: 0.0260\n",
            "Epoch [5166/20000], Training Loss: 0.0281\n",
            "Epoch [5167/20000], Training Loss: 0.0279\n",
            "Epoch [5168/20000], Training Loss: 0.0267\n",
            "Epoch [5169/20000], Training Loss: 0.0263\n",
            "Epoch [5170/20000], Training Loss: 0.0281\n",
            "Epoch [5171/20000], Training Loss: 0.0293\n",
            "Epoch [5172/20000], Training Loss: 0.0274\n",
            "Epoch [5173/20000], Training Loss: 0.0257\n",
            "Epoch [5174/20000], Training Loss: 0.0270\n",
            "Epoch [5175/20000], Training Loss: 0.0263\n",
            "Epoch [5176/20000], Training Loss: 0.0262\n",
            "Epoch [5177/20000], Training Loss: 0.0278\n",
            "Epoch [5178/20000], Training Loss: 0.0251\n",
            "Epoch [5179/20000], Training Loss: 0.0255\n",
            "Epoch [5180/20000], Training Loss: 0.0272\n",
            "Epoch [5181/20000], Training Loss: 0.0271\n",
            "Epoch [5182/20000], Training Loss: 0.0260\n",
            "Epoch [5183/20000], Training Loss: 0.0277\n",
            "Epoch [5184/20000], Training Loss: 0.0278\n",
            "Epoch [5185/20000], Training Loss: 0.0251\n",
            "Epoch [5186/20000], Training Loss: 0.0271\n",
            "Epoch [5187/20000], Training Loss: 0.0283\n",
            "Epoch [5188/20000], Training Loss: 0.0287\n",
            "Epoch [5189/20000], Training Loss: 0.0266\n",
            "Epoch [5190/20000], Training Loss: 0.0257\n",
            "Epoch [5191/20000], Training Loss: 0.0280\n",
            "Epoch [5192/20000], Training Loss: 0.0259\n",
            "Epoch [5193/20000], Training Loss: 0.0250\n",
            "Epoch [5194/20000], Training Loss: 0.0282\n",
            "Epoch [5195/20000], Training Loss: 0.0243\n",
            "Epoch [5196/20000], Training Loss: 0.0270\n",
            "Epoch [5197/20000], Training Loss: 0.0264\n",
            "Epoch [5198/20000], Training Loss: 0.0264\n",
            "Epoch [5199/20000], Training Loss: 0.0266\n",
            "Epoch [5200/20000], Training Loss: 0.0241\n",
            "Epoch [5201/20000], Training Loss: 0.0275\n",
            "Epoch [5202/20000], Training Loss: 0.0273\n",
            "Epoch [5203/20000], Training Loss: 0.0270\n",
            "Epoch [5204/20000], Training Loss: 0.0269\n",
            "Epoch [5205/20000], Training Loss: 0.0257\n",
            "Epoch [5206/20000], Training Loss: 0.0263\n",
            "Epoch [5207/20000], Training Loss: 0.0270\n",
            "Epoch [5208/20000], Training Loss: 0.0265\n",
            "Epoch [5209/20000], Training Loss: 0.0271\n",
            "Epoch [5210/20000], Training Loss: 0.0246\n",
            "Epoch [5211/20000], Training Loss: 0.0263\n",
            "Epoch [5212/20000], Training Loss: 0.0277\n",
            "Epoch [5213/20000], Training Loss: 0.0261\n",
            "Epoch [5214/20000], Training Loss: 0.0261\n",
            "Epoch [5215/20000], Training Loss: 0.0269\n",
            "Epoch [5216/20000], Training Loss: 0.0276\n",
            "Epoch [5217/20000], Training Loss: 0.0264\n",
            "Epoch [5218/20000], Training Loss: 0.0250\n",
            "Epoch [5219/20000], Training Loss: 0.0286\n",
            "Epoch [5220/20000], Training Loss: 0.0258\n",
            "Epoch [5221/20000], Training Loss: 0.0269\n",
            "Epoch [5222/20000], Training Loss: 0.0258\n",
            "Epoch [5223/20000], Training Loss: 0.0261\n",
            "Epoch [5224/20000], Training Loss: 0.0278\n",
            "Epoch [5225/20000], Training Loss: 0.0244\n",
            "Epoch [5226/20000], Training Loss: 0.0264\n",
            "Epoch [5227/20000], Training Loss: 0.0270\n",
            "Epoch [5228/20000], Training Loss: 0.0281\n",
            "Epoch [5229/20000], Training Loss: 0.0259\n",
            "Epoch [5230/20000], Training Loss: 0.0269\n",
            "Epoch [5231/20000], Training Loss: 0.0254\n",
            "Epoch [5232/20000], Training Loss: 0.0271\n",
            "Epoch [5233/20000], Training Loss: 0.0248\n",
            "Epoch [5234/20000], Training Loss: 0.0271\n",
            "Epoch [5235/20000], Training Loss: 0.0260\n",
            "Epoch [5236/20000], Training Loss: 0.0260\n",
            "Epoch [5237/20000], Training Loss: 0.0269\n",
            "Epoch [5238/20000], Training Loss: 0.0264\n",
            "Epoch [5239/20000], Training Loss: 0.0262\n",
            "Epoch [5240/20000], Training Loss: 0.0274\n",
            "Epoch [5241/20000], Training Loss: 0.0269\n",
            "Epoch [5242/20000], Training Loss: 0.0262\n",
            "Epoch [5243/20000], Training Loss: 0.0268\n",
            "Epoch [5244/20000], Training Loss: 0.0265\n",
            "Epoch [5245/20000], Training Loss: 0.0257\n",
            "Epoch [5246/20000], Training Loss: 0.0254\n",
            "Epoch [5247/20000], Training Loss: 0.0276\n",
            "Epoch [5248/20000], Training Loss: 0.0262\n",
            "Epoch [5249/20000], Training Loss: 0.0267\n",
            "Epoch [5250/20000], Training Loss: 0.0268\n",
            "Epoch [5251/20000], Training Loss: 0.0263\n",
            "Epoch [5252/20000], Training Loss: 0.0267\n",
            "Epoch [5253/20000], Training Loss: 0.0280\n",
            "Epoch [5254/20000], Training Loss: 0.0277\n",
            "Epoch [5255/20000], Training Loss: 0.0260\n",
            "Epoch [5256/20000], Training Loss: 0.0254\n",
            "Epoch [5257/20000], Training Loss: 0.0255\n",
            "Epoch [5258/20000], Training Loss: 0.0271\n",
            "Epoch [5259/20000], Training Loss: 0.0263\n",
            "Epoch [5260/20000], Training Loss: 0.0249\n",
            "Epoch [5261/20000], Training Loss: 0.0276\n",
            "Epoch [5262/20000], Training Loss: 0.0270\n",
            "Epoch [5263/20000], Training Loss: 0.0267\n",
            "Epoch [5264/20000], Training Loss: 0.0276\n",
            "Epoch [5265/20000], Training Loss: 0.0270\n",
            "Epoch [5266/20000], Training Loss: 0.0277\n",
            "Epoch [5267/20000], Training Loss: 0.0266\n",
            "Epoch [5268/20000], Training Loss: 0.0278\n",
            "Epoch [5269/20000], Training Loss: 0.0272\n",
            "Epoch [5270/20000], Training Loss: 0.0283\n",
            "Epoch [5271/20000], Training Loss: 0.0248\n",
            "Epoch [5272/20000], Training Loss: 0.0258\n",
            "Epoch [5273/20000], Training Loss: 0.0254\n",
            "Epoch [5274/20000], Training Loss: 0.0270\n",
            "Epoch [5275/20000], Training Loss: 0.0277\n",
            "Epoch [5276/20000], Training Loss: 0.0240\n",
            "Epoch [5277/20000], Training Loss: 0.0285\n",
            "Epoch [5278/20000], Training Loss: 0.0265\n",
            "Epoch [5279/20000], Training Loss: 0.0280\n",
            "Epoch [5280/20000], Training Loss: 0.0271\n",
            "Epoch [5281/20000], Training Loss: 0.0253\n",
            "Epoch [5282/20000], Training Loss: 0.0272\n",
            "Epoch [5283/20000], Training Loss: 0.0252\n",
            "Epoch [5284/20000], Training Loss: 0.0260\n",
            "Epoch [5285/20000], Training Loss: 0.0282\n",
            "Epoch [5286/20000], Training Loss: 0.0267\n",
            "Epoch [5287/20000], Training Loss: 0.0260\n",
            "Epoch [5288/20000], Training Loss: 0.0269\n",
            "Epoch [5289/20000], Training Loss: 0.0260\n",
            "Epoch [5290/20000], Training Loss: 0.0270\n",
            "Epoch [5291/20000], Training Loss: 0.0255\n",
            "Epoch [5292/20000], Training Loss: 0.0260\n",
            "Epoch [5293/20000], Training Loss: 0.0258\n",
            "Epoch [5294/20000], Training Loss: 0.0275\n",
            "Epoch [5295/20000], Training Loss: 0.0259\n",
            "Epoch [5296/20000], Training Loss: 0.0258\n",
            "Epoch [5297/20000], Training Loss: 0.0271\n",
            "Epoch [5298/20000], Training Loss: 0.0277\n",
            "Epoch [5299/20000], Training Loss: 0.0286\n",
            "Epoch [5300/20000], Training Loss: 0.0266\n",
            "Epoch [5301/20000], Training Loss: 0.0266\n",
            "Epoch [5302/20000], Training Loss: 0.0283\n",
            "Epoch [5303/20000], Training Loss: 0.0259\n",
            "Epoch [5304/20000], Training Loss: 0.0262\n",
            "Epoch [5305/20000], Training Loss: 0.0264\n",
            "Epoch [5306/20000], Training Loss: 0.0265\n",
            "Epoch [5307/20000], Training Loss: 0.0255\n",
            "Epoch [5308/20000], Training Loss: 0.0273\n",
            "Epoch [5309/20000], Training Loss: 0.0271\n",
            "Epoch [5310/20000], Training Loss: 0.0251\n",
            "Epoch [5311/20000], Training Loss: 0.0256\n",
            "Epoch [5312/20000], Training Loss: 0.0283\n",
            "Epoch [5313/20000], Training Loss: 0.0261\n",
            "Epoch [5314/20000], Training Loss: 0.0249\n",
            "Epoch [5315/20000], Training Loss: 0.0283\n",
            "Epoch [5316/20000], Training Loss: 0.0258\n",
            "Epoch [5317/20000], Training Loss: 0.0267\n",
            "Epoch [5318/20000], Training Loss: 0.0269\n",
            "Epoch [5319/20000], Training Loss: 0.0259\n",
            "Epoch [5320/20000], Training Loss: 0.0257\n",
            "Epoch [5321/20000], Training Loss: 0.0268\n",
            "Epoch [5322/20000], Training Loss: 0.0239\n",
            "Epoch [5323/20000], Training Loss: 0.0263\n",
            "Epoch [5324/20000], Training Loss: 0.0271\n",
            "Epoch [5325/20000], Training Loss: 0.0284\n",
            "Epoch [5326/20000], Training Loss: 0.0254\n",
            "Epoch [5327/20000], Training Loss: 0.0248\n",
            "Epoch [5328/20000], Training Loss: 0.0263\n",
            "Epoch [5329/20000], Training Loss: 0.0271\n",
            "Epoch [5330/20000], Training Loss: 0.0246\n",
            "Epoch [5331/20000], Training Loss: 0.0267\n",
            "Epoch [5332/20000], Training Loss: 0.0261\n",
            "Epoch [5333/20000], Training Loss: 0.0268\n",
            "Epoch [5334/20000], Training Loss: 0.0288\n",
            "Epoch [5335/20000], Training Loss: 0.0258\n",
            "Epoch [5336/20000], Training Loss: 0.0267\n",
            "Epoch [5337/20000], Training Loss: 0.0264\n",
            "Epoch [5338/20000], Training Loss: 0.0275\n",
            "Epoch [5339/20000], Training Loss: 0.0279\n",
            "Epoch [5340/20000], Training Loss: 0.0284\n",
            "Epoch [5341/20000], Training Loss: 0.0252\n",
            "Epoch [5342/20000], Training Loss: 0.0267\n",
            "Epoch [5343/20000], Training Loss: 0.0275\n",
            "Epoch [5344/20000], Training Loss: 0.0244\n",
            "Epoch [5345/20000], Training Loss: 0.0281\n",
            "Epoch [5346/20000], Training Loss: 0.0266\n",
            "Epoch [5347/20000], Training Loss: 0.0279\n",
            "Epoch [5348/20000], Training Loss: 0.0249\n",
            "Epoch [5349/20000], Training Loss: 0.0240\n",
            "Epoch [5350/20000], Training Loss: 0.0253\n",
            "Epoch [5351/20000], Training Loss: 0.0287\n",
            "Epoch [5352/20000], Training Loss: 0.0264\n",
            "Epoch [5353/20000], Training Loss: 0.0281\n",
            "Epoch [5354/20000], Training Loss: 0.0265\n",
            "Epoch [5355/20000], Training Loss: 0.0267\n",
            "Epoch [5356/20000], Training Loss: 0.0276\n",
            "Epoch [5357/20000], Training Loss: 0.0272\n",
            "Epoch [5358/20000], Training Loss: 0.0267\n",
            "Epoch [5359/20000], Training Loss: 0.0257\n",
            "Epoch [5360/20000], Training Loss: 0.0271\n",
            "Epoch [5361/20000], Training Loss: 0.0257\n",
            "Epoch [5362/20000], Training Loss: 0.0262\n",
            "Epoch [5363/20000], Training Loss: 0.0273\n",
            "Epoch [5364/20000], Training Loss: 0.0249\n",
            "Epoch [5365/20000], Training Loss: 0.0255\n",
            "Epoch [5366/20000], Training Loss: 0.0251\n",
            "Epoch [5367/20000], Training Loss: 0.0281\n",
            "Epoch [5368/20000], Training Loss: 0.0262\n",
            "Epoch [5369/20000], Training Loss: 0.0261\n",
            "Epoch [5370/20000], Training Loss: 0.0269\n",
            "Epoch [5371/20000], Training Loss: 0.0260\n",
            "Epoch [5372/20000], Training Loss: 0.0273\n",
            "Epoch [5373/20000], Training Loss: 0.0263\n",
            "Epoch [5374/20000], Training Loss: 0.0267\n",
            "Epoch [5375/20000], Training Loss: 0.0249\n",
            "Epoch [5376/20000], Training Loss: 0.0257\n",
            "Epoch [5377/20000], Training Loss: 0.0262\n",
            "Epoch [5378/20000], Training Loss: 0.0280\n",
            "Epoch [5379/20000], Training Loss: 0.0265\n",
            "Epoch [5380/20000], Training Loss: 0.0258\n",
            "Epoch [5381/20000], Training Loss: 0.0299\n",
            "Epoch [5382/20000], Training Loss: 0.0274\n",
            "Epoch [5383/20000], Training Loss: 0.0270\n",
            "Epoch [5384/20000], Training Loss: 0.0284\n",
            "Epoch [5385/20000], Training Loss: 0.0269\n",
            "Epoch [5386/20000], Training Loss: 0.0270\n",
            "Epoch [5387/20000], Training Loss: 0.0253\n",
            "Epoch [5388/20000], Training Loss: 0.0246\n",
            "Epoch [5389/20000], Training Loss: 0.0245\n",
            "Epoch [5390/20000], Training Loss: 0.0258\n",
            "Epoch [5391/20000], Training Loss: 0.0254\n",
            "Epoch [5392/20000], Training Loss: 0.0258\n",
            "Epoch [5393/20000], Training Loss: 0.0252\n",
            "Epoch [5394/20000], Training Loss: 0.0245\n",
            "Epoch [5395/20000], Training Loss: 0.0276\n",
            "Epoch [5396/20000], Training Loss: 0.0257\n",
            "Epoch [5397/20000], Training Loss: 0.0257\n",
            "Epoch [5398/20000], Training Loss: 0.0265\n",
            "Epoch [5399/20000], Training Loss: 0.0277\n",
            "Epoch [5400/20000], Training Loss: 0.0270\n",
            "Epoch [5401/20000], Training Loss: 0.0261\n",
            "Epoch [5402/20000], Training Loss: 0.0278\n",
            "Epoch [5403/20000], Training Loss: 0.0269\n",
            "Epoch [5404/20000], Training Loss: 0.0276\n",
            "Epoch [5405/20000], Training Loss: 0.0280\n",
            "Epoch [5406/20000], Training Loss: 0.0257\n",
            "Epoch [5407/20000], Training Loss: 0.0267\n",
            "Epoch [5408/20000], Training Loss: 0.0268\n",
            "Epoch [5409/20000], Training Loss: 0.0265\n",
            "Epoch [5410/20000], Training Loss: 0.0274\n",
            "Epoch [5411/20000], Training Loss: 0.0271\n",
            "Epoch [5412/20000], Training Loss: 0.0275\n",
            "Epoch [5413/20000], Training Loss: 0.0273\n",
            "Epoch [5414/20000], Training Loss: 0.0275\n",
            "Epoch [5415/20000], Training Loss: 0.0259\n",
            "Epoch [5416/20000], Training Loss: 0.0248\n",
            "Epoch [5417/20000], Training Loss: 0.0255\n",
            "Epoch [5418/20000], Training Loss: 0.0276\n",
            "Epoch [5419/20000], Training Loss: 0.0256\n",
            "Epoch [5420/20000], Training Loss: 0.0260\n",
            "Epoch [5421/20000], Training Loss: 0.0265\n",
            "Epoch [5422/20000], Training Loss: 0.0235\n",
            "Epoch [5423/20000], Training Loss: 0.0258\n",
            "Epoch [5424/20000], Training Loss: 0.0257\n",
            "Epoch [5425/20000], Training Loss: 0.0254\n",
            "Epoch [5426/20000], Training Loss: 0.0256\n",
            "Epoch [5427/20000], Training Loss: 0.0285\n",
            "Epoch [5428/20000], Training Loss: 0.0290\n",
            "Epoch [5429/20000], Training Loss: 0.0257\n",
            "Epoch [5430/20000], Training Loss: 0.0269\n",
            "Epoch [5431/20000], Training Loss: 0.0267\n",
            "Epoch [5432/20000], Training Loss: 0.0276\n",
            "Epoch [5433/20000], Training Loss: 0.0243\n",
            "Epoch [5434/20000], Training Loss: 0.0256\n",
            "Epoch [5435/20000], Training Loss: 0.0264\n",
            "Epoch [5436/20000], Training Loss: 0.0289\n",
            "Epoch [5437/20000], Training Loss: 0.0251\n",
            "Epoch [5438/20000], Training Loss: 0.0277\n",
            "Epoch [5439/20000], Training Loss: 0.0284\n",
            "Epoch [5440/20000], Training Loss: 0.0257\n",
            "Epoch [5441/20000], Training Loss: 0.0285\n",
            "Epoch [5442/20000], Training Loss: 0.0279\n",
            "Epoch [5443/20000], Training Loss: 0.0261\n",
            "Epoch [5444/20000], Training Loss: 0.0244\n",
            "Epoch [5445/20000], Training Loss: 0.0275\n",
            "Epoch [5446/20000], Training Loss: 0.0253\n",
            "Epoch [5447/20000], Training Loss: 0.0261\n",
            "Epoch [5448/20000], Training Loss: 0.0279\n",
            "Epoch [5449/20000], Training Loss: 0.0287\n",
            "Epoch [5450/20000], Training Loss: 0.0269\n",
            "Epoch [5451/20000], Training Loss: 0.0253\n",
            "Epoch [5452/20000], Training Loss: 0.0293\n",
            "Epoch [5453/20000], Training Loss: 0.0279\n",
            "Epoch [5454/20000], Training Loss: 0.0299\n",
            "Epoch [5455/20000], Training Loss: 0.0254\n",
            "Epoch [5456/20000], Training Loss: 0.0281\n",
            "Epoch [5457/20000], Training Loss: 0.0247\n",
            "Epoch [5458/20000], Training Loss: 0.0263\n",
            "Epoch [5459/20000], Training Loss: 0.0260\n",
            "Epoch [5460/20000], Training Loss: 0.0245\n",
            "Epoch [5461/20000], Training Loss: 0.0267\n",
            "Epoch [5462/20000], Training Loss: 0.0255\n",
            "Epoch [5463/20000], Training Loss: 0.0273\n",
            "Epoch [5464/20000], Training Loss: 0.0271\n",
            "Epoch [5465/20000], Training Loss: 0.0260\n",
            "Epoch [5466/20000], Training Loss: 0.0265\n",
            "Epoch [5467/20000], Training Loss: 0.0263\n",
            "Epoch [5468/20000], Training Loss: 0.0272\n",
            "Epoch [5469/20000], Training Loss: 0.0249\n",
            "Epoch [5470/20000], Training Loss: 0.0256\n",
            "Epoch [5471/20000], Training Loss: 0.0250\n",
            "Epoch [5472/20000], Training Loss: 0.0247\n",
            "Epoch [5473/20000], Training Loss: 0.0277\n",
            "Epoch [5474/20000], Training Loss: 0.0259\n",
            "Epoch [5475/20000], Training Loss: 0.0256\n",
            "Epoch [5476/20000], Training Loss: 0.0259\n",
            "Epoch [5477/20000], Training Loss: 0.0254\n",
            "Epoch [5478/20000], Training Loss: 0.0283\n",
            "Epoch [5479/20000], Training Loss: 0.0269\n",
            "Epoch [5480/20000], Training Loss: 0.0271\n",
            "Epoch [5481/20000], Training Loss: 0.0277\n",
            "Epoch [5482/20000], Training Loss: 0.0273\n",
            "Epoch [5483/20000], Training Loss: 0.0259\n",
            "Epoch [5484/20000], Training Loss: 0.0282\n",
            "Epoch [5485/20000], Training Loss: 0.0260\n",
            "Epoch [5486/20000], Training Loss: 0.0261\n",
            "Epoch [5487/20000], Training Loss: 0.0260\n",
            "Epoch [5488/20000], Training Loss: 0.0281\n",
            "Epoch [5489/20000], Training Loss: 0.0266\n",
            "Epoch [5490/20000], Training Loss: 0.0280\n",
            "Epoch [5491/20000], Training Loss: 0.0250\n",
            "Epoch [5492/20000], Training Loss: 0.0251\n",
            "Epoch [5493/20000], Training Loss: 0.0281\n",
            "Epoch [5494/20000], Training Loss: 0.0252\n",
            "Epoch [5495/20000], Training Loss: 0.0264\n",
            "Epoch [5496/20000], Training Loss: 0.0250\n",
            "Epoch [5497/20000], Training Loss: 0.0264\n",
            "Epoch [5498/20000], Training Loss: 0.0264\n",
            "Epoch [5499/20000], Training Loss: 0.0249\n",
            "Epoch [5500/20000], Training Loss: 0.0273\n",
            "Epoch [5501/20000], Training Loss: 0.0254\n",
            "Epoch [5502/20000], Training Loss: 0.0258\n",
            "Epoch [5503/20000], Training Loss: 0.0278\n",
            "Epoch [5504/20000], Training Loss: 0.0257\n",
            "Epoch [5505/20000], Training Loss: 0.0262\n",
            "Epoch [5506/20000], Training Loss: 0.0253\n",
            "Epoch [5507/20000], Training Loss: 0.0268\n",
            "Epoch [5508/20000], Training Loss: 0.0270\n",
            "Epoch [5509/20000], Training Loss: 0.0290\n",
            "Epoch [5510/20000], Training Loss: 0.0262\n",
            "Epoch [5511/20000], Training Loss: 0.0280\n",
            "Epoch [5512/20000], Training Loss: 0.0269\n",
            "Epoch [5513/20000], Training Loss: 0.0273\n",
            "Epoch [5514/20000], Training Loss: 0.0260\n",
            "Epoch [5515/20000], Training Loss: 0.0254\n",
            "Epoch [5516/20000], Training Loss: 0.0260\n",
            "Epoch [5517/20000], Training Loss: 0.0279\n",
            "Epoch [5518/20000], Training Loss: 0.0273\n",
            "Epoch [5519/20000], Training Loss: 0.0261\n",
            "Epoch [5520/20000], Training Loss: 0.0270\n",
            "Epoch [5521/20000], Training Loss: 0.0265\n",
            "Epoch [5522/20000], Training Loss: 0.0284\n",
            "Epoch [5523/20000], Training Loss: 0.0275\n",
            "Epoch [5524/20000], Training Loss: 0.0263\n",
            "Epoch [5525/20000], Training Loss: 0.0246\n",
            "Epoch [5526/20000], Training Loss: 0.0259\n",
            "Epoch [5527/20000], Training Loss: 0.0275\n",
            "Epoch [5528/20000], Training Loss: 0.0278\n",
            "Epoch [5529/20000], Training Loss: 0.0267\n",
            "Epoch [5530/20000], Training Loss: 0.0238\n",
            "Epoch [5531/20000], Training Loss: 0.0266\n",
            "Epoch [5532/20000], Training Loss: 0.0245\n",
            "Epoch [5533/20000], Training Loss: 0.0276\n",
            "Epoch [5534/20000], Training Loss: 0.0259\n",
            "Epoch [5535/20000], Training Loss: 0.0252\n",
            "Epoch [5536/20000], Training Loss: 0.0282\n",
            "Epoch [5537/20000], Training Loss: 0.0266\n",
            "Epoch [5538/20000], Training Loss: 0.0258\n",
            "Epoch [5539/20000], Training Loss: 0.0288\n",
            "Epoch [5540/20000], Training Loss: 0.0269\n",
            "Epoch [5541/20000], Training Loss: 0.0245\n",
            "Epoch [5542/20000], Training Loss: 0.0283\n",
            "Epoch [5543/20000], Training Loss: 0.0265\n",
            "Epoch [5544/20000], Training Loss: 0.0283\n",
            "Epoch [5545/20000], Training Loss: 0.0260\n",
            "Epoch [5546/20000], Training Loss: 0.0267\n",
            "Epoch [5547/20000], Training Loss: 0.0258\n",
            "Epoch [5548/20000], Training Loss: 0.0254\n",
            "Epoch [5549/20000], Training Loss: 0.0272\n",
            "Epoch [5550/20000], Training Loss: 0.0270\n",
            "Epoch [5551/20000], Training Loss: 0.0263\n",
            "Epoch [5552/20000], Training Loss: 0.0265\n",
            "Epoch [5553/20000], Training Loss: 0.0257\n",
            "Epoch [5554/20000], Training Loss: 0.0252\n",
            "Epoch [5555/20000], Training Loss: 0.0267\n",
            "Epoch [5556/20000], Training Loss: 0.0256\n",
            "Epoch [5557/20000], Training Loss: 0.0268\n",
            "Epoch [5558/20000], Training Loss: 0.0259\n",
            "Epoch [5559/20000], Training Loss: 0.0281\n",
            "Epoch [5560/20000], Training Loss: 0.0264\n",
            "Epoch [5561/20000], Training Loss: 0.0264\n",
            "Epoch [5562/20000], Training Loss: 0.0284\n",
            "Epoch [5563/20000], Training Loss: 0.0278\n",
            "Epoch [5564/20000], Training Loss: 0.0256\n",
            "Epoch [5565/20000], Training Loss: 0.0242\n",
            "Epoch [5566/20000], Training Loss: 0.0261\n",
            "Epoch [5567/20000], Training Loss: 0.0253\n",
            "Epoch [5568/20000], Training Loss: 0.0265\n",
            "Epoch [5569/20000], Training Loss: 0.0259\n",
            "Epoch [5570/20000], Training Loss: 0.0265\n",
            "Epoch [5571/20000], Training Loss: 0.0264\n",
            "Epoch [5572/20000], Training Loss: 0.0260\n",
            "Epoch [5573/20000], Training Loss: 0.0275\n",
            "Epoch [5574/20000], Training Loss: 0.0267\n",
            "Epoch [5575/20000], Training Loss: 0.0249\n",
            "Epoch [5576/20000], Training Loss: 0.0261\n",
            "Epoch [5577/20000], Training Loss: 0.0268\n",
            "Epoch [5578/20000], Training Loss: 0.0244\n",
            "Epoch [5579/20000], Training Loss: 0.0265\n",
            "Epoch [5580/20000], Training Loss: 0.0274\n",
            "Epoch [5581/20000], Training Loss: 0.0246\n",
            "Epoch [5582/20000], Training Loss: 0.0252\n",
            "Epoch [5583/20000], Training Loss: 0.0252\n",
            "Epoch [5584/20000], Training Loss: 0.0264\n",
            "Epoch [5585/20000], Training Loss: 0.0261\n",
            "Epoch [5586/20000], Training Loss: 0.0261\n",
            "Epoch [5587/20000], Training Loss: 0.0266\n",
            "Epoch [5588/20000], Training Loss: 0.0248\n",
            "Epoch [5589/20000], Training Loss: 0.0274\n",
            "Epoch [5590/20000], Training Loss: 0.0244\n",
            "Epoch [5591/20000], Training Loss: 0.0269\n",
            "Epoch [5592/20000], Training Loss: 0.0274\n",
            "Epoch [5593/20000], Training Loss: 0.0273\n",
            "Epoch [5594/20000], Training Loss: 0.0276\n",
            "Epoch [5595/20000], Training Loss: 0.0274\n",
            "Epoch [5596/20000], Training Loss: 0.0268\n",
            "Epoch [5597/20000], Training Loss: 0.0276\n",
            "Epoch [5598/20000], Training Loss: 0.0242\n",
            "Epoch [5599/20000], Training Loss: 0.0273\n",
            "Epoch [5600/20000], Training Loss: 0.0271\n",
            "Epoch [5601/20000], Training Loss: 0.0270\n",
            "Epoch [5602/20000], Training Loss: 0.0254\n",
            "Epoch [5603/20000], Training Loss: 0.0269\n",
            "Epoch [5604/20000], Training Loss: 0.0272\n",
            "Epoch [5605/20000], Training Loss: 0.0261\n",
            "Epoch [5606/20000], Training Loss: 0.0246\n",
            "Epoch [5607/20000], Training Loss: 0.0251\n",
            "Epoch [5608/20000], Training Loss: 0.0258\n",
            "Epoch [5609/20000], Training Loss: 0.0259\n",
            "Epoch [5610/20000], Training Loss: 0.0287\n",
            "Epoch [5611/20000], Training Loss: 0.0248\n",
            "Epoch [5612/20000], Training Loss: 0.0277\n",
            "Epoch [5613/20000], Training Loss: 0.0282\n",
            "Epoch [5614/20000], Training Loss: 0.0269\n",
            "Epoch [5615/20000], Training Loss: 0.0265\n",
            "Epoch [5616/20000], Training Loss: 0.0282\n",
            "Epoch [5617/20000], Training Loss: 0.0271\n",
            "Epoch [5618/20000], Training Loss: 0.0254\n",
            "Epoch [5619/20000], Training Loss: 0.0255\n",
            "Epoch [5620/20000], Training Loss: 0.0255\n",
            "Epoch [5621/20000], Training Loss: 0.0262\n",
            "Epoch [5622/20000], Training Loss: 0.0254\n",
            "Epoch [5623/20000], Training Loss: 0.0258\n",
            "Epoch [5624/20000], Training Loss: 0.0278\n",
            "Epoch [5625/20000], Training Loss: 0.0267\n",
            "Epoch [5626/20000], Training Loss: 0.0273\n",
            "Epoch [5627/20000], Training Loss: 0.0260\n",
            "Epoch [5628/20000], Training Loss: 0.0289\n",
            "Epoch [5629/20000], Training Loss: 0.0257\n",
            "Epoch [5630/20000], Training Loss: 0.0278\n",
            "Epoch [5631/20000], Training Loss: 0.0251\n",
            "Epoch [5632/20000], Training Loss: 0.0262\n",
            "Epoch [5633/20000], Training Loss: 0.0251\n",
            "Epoch [5634/20000], Training Loss: 0.0264\n",
            "Epoch [5635/20000], Training Loss: 0.0267\n",
            "Epoch [5636/20000], Training Loss: 0.0260\n",
            "Epoch [5637/20000], Training Loss: 0.0265\n",
            "Epoch [5638/20000], Training Loss: 0.0279\n",
            "Epoch [5639/20000], Training Loss: 0.0261\n",
            "Epoch [5640/20000], Training Loss: 0.0272\n",
            "Epoch [5641/20000], Training Loss: 0.0282\n",
            "Epoch [5642/20000], Training Loss: 0.0271\n",
            "Epoch [5643/20000], Training Loss: 0.0261\n",
            "Epoch [5644/20000], Training Loss: 0.0259\n",
            "Epoch [5645/20000], Training Loss: 0.0285\n",
            "Epoch [5646/20000], Training Loss: 0.0262\n",
            "Epoch [5647/20000], Training Loss: 0.0265\n",
            "Epoch [5648/20000], Training Loss: 0.0260\n",
            "Epoch [5649/20000], Training Loss: 0.0250\n",
            "Epoch [5650/20000], Training Loss: 0.0286\n",
            "Epoch [5651/20000], Training Loss: 0.0287\n",
            "Epoch [5652/20000], Training Loss: 0.0277\n",
            "Epoch [5653/20000], Training Loss: 0.0264\n",
            "Epoch [5654/20000], Training Loss: 0.0269\n",
            "Epoch [5655/20000], Training Loss: 0.0267\n",
            "Epoch [5656/20000], Training Loss: 0.0246\n",
            "Epoch [5657/20000], Training Loss: 0.0281\n",
            "Epoch [5658/20000], Training Loss: 0.0252\n",
            "Epoch [5659/20000], Training Loss: 0.0268\n",
            "Epoch [5660/20000], Training Loss: 0.0241\n",
            "Epoch [5661/20000], Training Loss: 0.0258\n",
            "Epoch [5662/20000], Training Loss: 0.0266\n",
            "Epoch [5663/20000], Training Loss: 0.0252\n",
            "Epoch [5664/20000], Training Loss: 0.0251\n",
            "Epoch [5665/20000], Training Loss: 0.0271\n",
            "Epoch [5666/20000], Training Loss: 0.0285\n",
            "Epoch [5667/20000], Training Loss: 0.0280\n",
            "Epoch [5668/20000], Training Loss: 0.0258\n",
            "Epoch [5669/20000], Training Loss: 0.0250\n",
            "Epoch [5670/20000], Training Loss: 0.0270\n",
            "Epoch [5671/20000], Training Loss: 0.0258\n",
            "Epoch [5672/20000], Training Loss: 0.0270\n",
            "Epoch [5673/20000], Training Loss: 0.0248\n",
            "Epoch [5674/20000], Training Loss: 0.0239\n",
            "Epoch [5675/20000], Training Loss: 0.0282\n",
            "Epoch [5676/20000], Training Loss: 0.0244\n",
            "Epoch [5677/20000], Training Loss: 0.0274\n",
            "Epoch [5678/20000], Training Loss: 0.0252\n",
            "Epoch [5679/20000], Training Loss: 0.0265\n",
            "Epoch [5680/20000], Training Loss: 0.0247\n",
            "Epoch [5681/20000], Training Loss: 0.0270\n",
            "Epoch [5682/20000], Training Loss: 0.0255\n",
            "Epoch [5683/20000], Training Loss: 0.0264\n",
            "Epoch [5684/20000], Training Loss: 0.0248\n",
            "Epoch [5685/20000], Training Loss: 0.0271\n",
            "Epoch [5686/20000], Training Loss: 0.0291\n",
            "Epoch [5687/20000], Training Loss: 0.0244\n",
            "Epoch [5688/20000], Training Loss: 0.0259\n",
            "Epoch [5689/20000], Training Loss: 0.0266\n",
            "Epoch [5690/20000], Training Loss: 0.0263\n",
            "Epoch [5691/20000], Training Loss: 0.0264\n",
            "Epoch [5692/20000], Training Loss: 0.0292\n",
            "Epoch [5693/20000], Training Loss: 0.0247\n",
            "Epoch [5694/20000], Training Loss: 0.0257\n",
            "Epoch [5695/20000], Training Loss: 0.0280\n",
            "Epoch [5696/20000], Training Loss: 0.0269\n",
            "Epoch [5697/20000], Training Loss: 0.0268\n",
            "Epoch [5698/20000], Training Loss: 0.0263\n",
            "Epoch [5699/20000], Training Loss: 0.0270\n",
            "Epoch [5700/20000], Training Loss: 0.0255\n",
            "Epoch [5701/20000], Training Loss: 0.0266\n",
            "Epoch [5702/20000], Training Loss: 0.0276\n",
            "Epoch [5703/20000], Training Loss: 0.0300\n",
            "Epoch [5704/20000], Training Loss: 0.0267\n",
            "Epoch [5705/20000], Training Loss: 0.0263\n",
            "Epoch [5706/20000], Training Loss: 0.0247\n",
            "Epoch [5707/20000], Training Loss: 0.0261\n",
            "Epoch [5708/20000], Training Loss: 0.0278\n",
            "Epoch [5709/20000], Training Loss: 0.0275\n",
            "Epoch [5710/20000], Training Loss: 0.0255\n",
            "Epoch [5711/20000], Training Loss: 0.0252\n",
            "Epoch [5712/20000], Training Loss: 0.0261\n",
            "Epoch [5713/20000], Training Loss: 0.0257\n",
            "Epoch [5714/20000], Training Loss: 0.0255\n",
            "Epoch [5715/20000], Training Loss: 0.0270\n",
            "Epoch [5716/20000], Training Loss: 0.0265\n",
            "Epoch [5717/20000], Training Loss: 0.0295\n",
            "Epoch [5718/20000], Training Loss: 0.0268\n",
            "Epoch [5719/20000], Training Loss: 0.0259\n",
            "Epoch [5720/20000], Training Loss: 0.0259\n",
            "Epoch [5721/20000], Training Loss: 0.0246\n",
            "Epoch [5722/20000], Training Loss: 0.0273\n",
            "Epoch [5723/20000], Training Loss: 0.0254\n",
            "Epoch [5724/20000], Training Loss: 0.0248\n",
            "Epoch [5725/20000], Training Loss: 0.0277\n",
            "Epoch [5726/20000], Training Loss: 0.0256\n",
            "Epoch [5727/20000], Training Loss: 0.0275\n",
            "Epoch [5728/20000], Training Loss: 0.0257\n",
            "Epoch [5729/20000], Training Loss: 0.0270\n",
            "Epoch [5730/20000], Training Loss: 0.0263\n",
            "Epoch [5731/20000], Training Loss: 0.0266\n",
            "Epoch [5732/20000], Training Loss: 0.0267\n",
            "Epoch [5733/20000], Training Loss: 0.0252\n",
            "Epoch [5734/20000], Training Loss: 0.0290\n",
            "Epoch [5735/20000], Training Loss: 0.0252\n",
            "Epoch [5736/20000], Training Loss: 0.0259\n",
            "Epoch [5737/20000], Training Loss: 0.0250\n",
            "Epoch [5738/20000], Training Loss: 0.0271\n",
            "Epoch [5739/20000], Training Loss: 0.0275\n",
            "Epoch [5740/20000], Training Loss: 0.0272\n",
            "Epoch [5741/20000], Training Loss: 0.0270\n",
            "Epoch [5742/20000], Training Loss: 0.0259\n",
            "Epoch [5743/20000], Training Loss: 0.0244\n",
            "Epoch [5744/20000], Training Loss: 0.0259\n",
            "Epoch [5745/20000], Training Loss: 0.0271\n",
            "Epoch [5746/20000], Training Loss: 0.0255\n",
            "Epoch [5747/20000], Training Loss: 0.0278\n",
            "Epoch [5748/20000], Training Loss: 0.0269\n",
            "Epoch [5749/20000], Training Loss: 0.0270\n",
            "Epoch [5750/20000], Training Loss: 0.0248\n",
            "Epoch [5751/20000], Training Loss: 0.0282\n",
            "Epoch [5752/20000], Training Loss: 0.0251\n",
            "Epoch [5753/20000], Training Loss: 0.0267\n",
            "Epoch [5754/20000], Training Loss: 0.0274\n",
            "Epoch [5755/20000], Training Loss: 0.0256\n",
            "Epoch [5756/20000], Training Loss: 0.0263\n",
            "Epoch [5757/20000], Training Loss: 0.0285\n",
            "Epoch [5758/20000], Training Loss: 0.0250\n",
            "Epoch [5759/20000], Training Loss: 0.0275\n",
            "Epoch [5760/20000], Training Loss: 0.0277\n",
            "Epoch [5761/20000], Training Loss: 0.0245\n",
            "Epoch [5762/20000], Training Loss: 0.0281\n",
            "Epoch [5763/20000], Training Loss: 0.0258\n",
            "Epoch [5764/20000], Training Loss: 0.0262\n",
            "Epoch [5765/20000], Training Loss: 0.0247\n",
            "Epoch [5766/20000], Training Loss: 0.0264\n",
            "Epoch [5767/20000], Training Loss: 0.0269\n",
            "Epoch [5768/20000], Training Loss: 0.0288\n",
            "Epoch [5769/20000], Training Loss: 0.0275\n",
            "Epoch [5770/20000], Training Loss: 0.0273\n",
            "Epoch [5771/20000], Training Loss: 0.0262\n",
            "Epoch [5772/20000], Training Loss: 0.0251\n",
            "Epoch [5773/20000], Training Loss: 0.0259\n",
            "Epoch [5774/20000], Training Loss: 0.0272\n",
            "Epoch [5775/20000], Training Loss: 0.0260\n",
            "Epoch [5776/20000], Training Loss: 0.0253\n",
            "Epoch [5777/20000], Training Loss: 0.0260\n",
            "Epoch [5778/20000], Training Loss: 0.0257\n",
            "Epoch [5779/20000], Training Loss: 0.0261\n",
            "Epoch [5780/20000], Training Loss: 0.0252\n",
            "Epoch [5781/20000], Training Loss: 0.0264\n",
            "Epoch [5782/20000], Training Loss: 0.0265\n",
            "Epoch [5783/20000], Training Loss: 0.0262\n",
            "Epoch [5784/20000], Training Loss: 0.0256\n",
            "Epoch [5785/20000], Training Loss: 0.0279\n",
            "Epoch [5786/20000], Training Loss: 0.0277\n",
            "Epoch [5787/20000], Training Loss: 0.0258\n",
            "Epoch [5788/20000], Training Loss: 0.0258\n",
            "Epoch [5789/20000], Training Loss: 0.0273\n",
            "Epoch [5790/20000], Training Loss: 0.0272\n",
            "Epoch [5791/20000], Training Loss: 0.0279\n",
            "Epoch [5792/20000], Training Loss: 0.0256\n",
            "Epoch [5793/20000], Training Loss: 0.0263\n",
            "Epoch [5794/20000], Training Loss: 0.0273\n",
            "Epoch [5795/20000], Training Loss: 0.0253\n",
            "Epoch [5796/20000], Training Loss: 0.0282\n",
            "Epoch [5797/20000], Training Loss: 0.0259\n",
            "Epoch [5798/20000], Training Loss: 0.0265\n",
            "Epoch [5799/20000], Training Loss: 0.0258\n",
            "Epoch [5800/20000], Training Loss: 0.0251\n",
            "Epoch [5801/20000], Training Loss: 0.0250\n",
            "Epoch [5802/20000], Training Loss: 0.0266\n",
            "Epoch [5803/20000], Training Loss: 0.0270\n",
            "Epoch [5804/20000], Training Loss: 0.0264\n",
            "Epoch [5805/20000], Training Loss: 0.0256\n",
            "Epoch [5806/20000], Training Loss: 0.0248\n",
            "Epoch [5807/20000], Training Loss: 0.0253\n",
            "Epoch [5808/20000], Training Loss: 0.0268\n",
            "Epoch [5809/20000], Training Loss: 0.0288\n",
            "Epoch [5810/20000], Training Loss: 0.0251\n",
            "Epoch [5811/20000], Training Loss: 0.0285\n",
            "Epoch [5812/20000], Training Loss: 0.0255\n",
            "Epoch [5813/20000], Training Loss: 0.0265\n",
            "Epoch [5814/20000], Training Loss: 0.0276\n",
            "Epoch [5815/20000], Training Loss: 0.0267\n",
            "Epoch [5816/20000], Training Loss: 0.0271\n",
            "Epoch [5817/20000], Training Loss: 0.0248\n",
            "Epoch [5818/20000], Training Loss: 0.0271\n",
            "Epoch [5819/20000], Training Loss: 0.0289\n",
            "Epoch [5820/20000], Training Loss: 0.0268\n",
            "Epoch [5821/20000], Training Loss: 0.0259\n",
            "Epoch [5822/20000], Training Loss: 0.0266\n",
            "Epoch [5823/20000], Training Loss: 0.0264\n",
            "Epoch [5824/20000], Training Loss: 0.0262\n",
            "Epoch [5825/20000], Training Loss: 0.0262\n",
            "Epoch [5826/20000], Training Loss: 0.0280\n",
            "Epoch [5827/20000], Training Loss: 0.0289\n",
            "Epoch [5828/20000], Training Loss: 0.0258\n",
            "Epoch [5829/20000], Training Loss: 0.0243\n",
            "Epoch [5830/20000], Training Loss: 0.0254\n",
            "Epoch [5831/20000], Training Loss: 0.0268\n",
            "Epoch [5832/20000], Training Loss: 0.0269\n",
            "Epoch [5833/20000], Training Loss: 0.0256\n",
            "Epoch [5834/20000], Training Loss: 0.0255\n",
            "Epoch [5835/20000], Training Loss: 0.0256\n",
            "Epoch [5836/20000], Training Loss: 0.0255\n",
            "Epoch [5837/20000], Training Loss: 0.0255\n",
            "Epoch [5838/20000], Training Loss: 0.0270\n",
            "Epoch [5839/20000], Training Loss: 0.0269\n",
            "Epoch [5840/20000], Training Loss: 0.0242\n",
            "Epoch [5841/20000], Training Loss: 0.0262\n",
            "Epoch [5842/20000], Training Loss: 0.0256\n",
            "Epoch [5843/20000], Training Loss: 0.0270\n",
            "Epoch [5844/20000], Training Loss: 0.0248\n",
            "Epoch [5845/20000], Training Loss: 0.0259\n",
            "Epoch [5846/20000], Training Loss: 0.0257\n",
            "Epoch [5847/20000], Training Loss: 0.0249\n",
            "Epoch [5848/20000], Training Loss: 0.0266\n",
            "Epoch [5849/20000], Training Loss: 0.0264\n",
            "Epoch [5850/20000], Training Loss: 0.0288\n",
            "Epoch [5851/20000], Training Loss: 0.0253\n",
            "Epoch [5852/20000], Training Loss: 0.0278\n",
            "Epoch [5853/20000], Training Loss: 0.0241\n",
            "Epoch [5854/20000], Training Loss: 0.0276\n",
            "Epoch [5855/20000], Training Loss: 0.0286\n",
            "Epoch [5856/20000], Training Loss: 0.0263\n",
            "Epoch [5857/20000], Training Loss: 0.0265\n",
            "Epoch [5858/20000], Training Loss: 0.0264\n",
            "Epoch [5859/20000], Training Loss: 0.0263\n",
            "Epoch [5860/20000], Training Loss: 0.0267\n",
            "Epoch [5861/20000], Training Loss: 0.0253\n",
            "Epoch [5862/20000], Training Loss: 0.0277\n",
            "Epoch [5863/20000], Training Loss: 0.0253\n",
            "Epoch [5864/20000], Training Loss: 0.0270\n",
            "Epoch [5865/20000], Training Loss: 0.0268\n",
            "Epoch [5866/20000], Training Loss: 0.0269\n",
            "Epoch [5867/20000], Training Loss: 0.0267\n",
            "Epoch [5868/20000], Training Loss: 0.0277\n",
            "Epoch [5869/20000], Training Loss: 0.0265\n",
            "Epoch [5870/20000], Training Loss: 0.0263\n",
            "Epoch [5871/20000], Training Loss: 0.0271\n",
            "Epoch [5872/20000], Training Loss: 0.0274\n",
            "Epoch [5873/20000], Training Loss: 0.0263\n",
            "Epoch [5874/20000], Training Loss: 0.0252\n",
            "Epoch [5875/20000], Training Loss: 0.0256\n",
            "Epoch [5876/20000], Training Loss: 0.0273\n",
            "Epoch [5877/20000], Training Loss: 0.0248\n",
            "Epoch [5878/20000], Training Loss: 0.0269\n",
            "Epoch [5879/20000], Training Loss: 0.0270\n",
            "Epoch [5880/20000], Training Loss: 0.0250\n",
            "Epoch [5881/20000], Training Loss: 0.0271\n",
            "Epoch [5882/20000], Training Loss: 0.0253\n",
            "Epoch [5883/20000], Training Loss: 0.0284\n",
            "Epoch [5884/20000], Training Loss: 0.0274\n",
            "Epoch [5885/20000], Training Loss: 0.0261\n",
            "Epoch [5886/20000], Training Loss: 0.0254\n",
            "Epoch [5887/20000], Training Loss: 0.0262\n",
            "Epoch [5888/20000], Training Loss: 0.0249\n",
            "Epoch [5889/20000], Training Loss: 0.0265\n",
            "Epoch [5890/20000], Training Loss: 0.0268\n",
            "Epoch [5891/20000], Training Loss: 0.0258\n",
            "Epoch [5892/20000], Training Loss: 0.0279\n",
            "Epoch [5893/20000], Training Loss: 0.0254\n",
            "Epoch [5894/20000], Training Loss: 0.0280\n",
            "Epoch [5895/20000], Training Loss: 0.0245\n",
            "Epoch [5896/20000], Training Loss: 0.0267\n",
            "Epoch [5897/20000], Training Loss: 0.0262\n",
            "Epoch [5898/20000], Training Loss: 0.0267\n",
            "Epoch [5899/20000], Training Loss: 0.0275\n",
            "Epoch [5900/20000], Training Loss: 0.0274\n",
            "Epoch [5901/20000], Training Loss: 0.0274\n",
            "Epoch [5902/20000], Training Loss: 0.0257\n",
            "Epoch [5903/20000], Training Loss: 0.0286\n",
            "Epoch [5904/20000], Training Loss: 0.0268\n",
            "Epoch [5905/20000], Training Loss: 0.0266\n",
            "Epoch [5906/20000], Training Loss: 0.0277\n",
            "Epoch [5907/20000], Training Loss: 0.0268\n",
            "Epoch [5908/20000], Training Loss: 0.0249\n",
            "Epoch [5909/20000], Training Loss: 0.0255\n",
            "Epoch [5910/20000], Training Loss: 0.0271\n",
            "Epoch [5911/20000], Training Loss: 0.0250\n",
            "Epoch [5912/20000], Training Loss: 0.0279\n",
            "Epoch [5913/20000], Training Loss: 0.0266\n",
            "Epoch [5914/20000], Training Loss: 0.0275\n",
            "Epoch [5915/20000], Training Loss: 0.0270\n",
            "Epoch [5916/20000], Training Loss: 0.0276\n",
            "Epoch [5917/20000], Training Loss: 0.0262\n",
            "Epoch [5918/20000], Training Loss: 0.0258\n",
            "Epoch [5919/20000], Training Loss: 0.0261\n",
            "Epoch [5920/20000], Training Loss: 0.0245\n",
            "Epoch [5921/20000], Training Loss: 0.0268\n",
            "Epoch [5922/20000], Training Loss: 0.0279\n",
            "Epoch [5923/20000], Training Loss: 0.0267\n",
            "Epoch [5924/20000], Training Loss: 0.0265\n",
            "Epoch [5925/20000], Training Loss: 0.0262\n",
            "Epoch [5926/20000], Training Loss: 0.0261\n",
            "Epoch [5927/20000], Training Loss: 0.0262\n",
            "Epoch [5928/20000], Training Loss: 0.0276\n",
            "Epoch [5929/20000], Training Loss: 0.0251\n",
            "Epoch [5930/20000], Training Loss: 0.0271\n",
            "Epoch [5931/20000], Training Loss: 0.0295\n",
            "Epoch [5932/20000], Training Loss: 0.0269\n",
            "Epoch [5933/20000], Training Loss: 0.0260\n",
            "Epoch [5934/20000], Training Loss: 0.0255\n",
            "Epoch [5935/20000], Training Loss: 0.0253\n",
            "Epoch [5936/20000], Training Loss: 0.0257\n",
            "Epoch [5937/20000], Training Loss: 0.0258\n",
            "Epoch [5938/20000], Training Loss: 0.0258\n",
            "Epoch [5939/20000], Training Loss: 0.0258\n",
            "Epoch [5940/20000], Training Loss: 0.0255\n",
            "Epoch [5941/20000], Training Loss: 0.0278\n",
            "Epoch [5942/20000], Training Loss: 0.0247\n",
            "Epoch [5943/20000], Training Loss: 0.0265\n",
            "Epoch [5944/20000], Training Loss: 0.0258\n",
            "Epoch [5945/20000], Training Loss: 0.0270\n",
            "Epoch [5946/20000], Training Loss: 0.0274\n",
            "Epoch [5947/20000], Training Loss: 0.0267\n",
            "Epoch [5948/20000], Training Loss: 0.0259\n",
            "Epoch [5949/20000], Training Loss: 0.0258\n",
            "Epoch [5950/20000], Training Loss: 0.0256\n",
            "Epoch [5951/20000], Training Loss: 0.0254\n",
            "Epoch [5952/20000], Training Loss: 0.0280\n",
            "Epoch [5953/20000], Training Loss: 0.0264\n",
            "Epoch [5954/20000], Training Loss: 0.0275\n",
            "Epoch [5955/20000], Training Loss: 0.0264\n",
            "Epoch [5956/20000], Training Loss: 0.0269\n",
            "Epoch [5957/20000], Training Loss: 0.0276\n",
            "Epoch [5958/20000], Training Loss: 0.0273\n",
            "Epoch [5959/20000], Training Loss: 0.0263\n",
            "Epoch [5960/20000], Training Loss: 0.0286\n",
            "Epoch [5961/20000], Training Loss: 0.0261\n",
            "Epoch [5962/20000], Training Loss: 0.0283\n",
            "Epoch [5963/20000], Training Loss: 0.0284\n",
            "Epoch [5964/20000], Training Loss: 0.0246\n",
            "Epoch [5965/20000], Training Loss: 0.0265\n",
            "Epoch [5966/20000], Training Loss: 0.0288\n",
            "Epoch [5967/20000], Training Loss: 0.0256\n",
            "Epoch [5968/20000], Training Loss: 0.0246\n",
            "Epoch [5969/20000], Training Loss: 0.0256\n",
            "Epoch [5970/20000], Training Loss: 0.0249\n",
            "Epoch [5971/20000], Training Loss: 0.0265\n",
            "Epoch [5972/20000], Training Loss: 0.0264\n",
            "Epoch [5973/20000], Training Loss: 0.0260\n",
            "Epoch [5974/20000], Training Loss: 0.0271\n",
            "Epoch [5975/20000], Training Loss: 0.0260\n",
            "Epoch [5976/20000], Training Loss: 0.0262\n",
            "Epoch [5977/20000], Training Loss: 0.0277\n",
            "Epoch [5978/20000], Training Loss: 0.0277\n",
            "Epoch [5979/20000], Training Loss: 0.0283\n",
            "Epoch [5980/20000], Training Loss: 0.0243\n",
            "Epoch [5981/20000], Training Loss: 0.0240\n",
            "Epoch [5982/20000], Training Loss: 0.0266\n",
            "Epoch [5983/20000], Training Loss: 0.0287\n",
            "Epoch [5984/20000], Training Loss: 0.0243\n",
            "Epoch [5985/20000], Training Loss: 0.0269\n",
            "Epoch [5986/20000], Training Loss: 0.0267\n",
            "Epoch [5987/20000], Training Loss: 0.0274\n",
            "Epoch [5988/20000], Training Loss: 0.0253\n",
            "Epoch [5989/20000], Training Loss: 0.0268\n",
            "Epoch [5990/20000], Training Loss: 0.0281\n",
            "Epoch [5991/20000], Training Loss: 0.0275\n",
            "Epoch [5992/20000], Training Loss: 0.0269\n",
            "Epoch [5993/20000], Training Loss: 0.0243\n",
            "Epoch [5994/20000], Training Loss: 0.0280\n",
            "Epoch [5995/20000], Training Loss: 0.0261\n",
            "Epoch [5996/20000], Training Loss: 0.0250\n",
            "Epoch [5997/20000], Training Loss: 0.0248\n",
            "Epoch [5998/20000], Training Loss: 0.0273\n",
            "Epoch [5999/20000], Training Loss: 0.0241\n",
            "Epoch [6000/20000], Training Loss: 0.0250\n",
            "Epoch [6001/20000], Training Loss: 0.0244\n",
            "Epoch [6002/20000], Training Loss: 0.0266\n",
            "Epoch [6003/20000], Training Loss: 0.0250\n",
            "Epoch [6004/20000], Training Loss: 0.0274\n",
            "Epoch [6005/20000], Training Loss: 0.0265\n",
            "Epoch [6006/20000], Training Loss: 0.0266\n",
            "Epoch [6007/20000], Training Loss: 0.0236\n",
            "Epoch [6008/20000], Training Loss: 0.0272\n",
            "Epoch [6009/20000], Training Loss: 0.0248\n",
            "Epoch [6010/20000], Training Loss: 0.0258\n",
            "Epoch [6011/20000], Training Loss: 0.0254\n",
            "Epoch [6012/20000], Training Loss: 0.0283\n",
            "Epoch [6013/20000], Training Loss: 0.0251\n",
            "Epoch [6014/20000], Training Loss: 0.0259\n",
            "Epoch [6015/20000], Training Loss: 0.0287\n",
            "Epoch [6016/20000], Training Loss: 0.0252\n",
            "Epoch [6017/20000], Training Loss: 0.0254\n",
            "Epoch [6018/20000], Training Loss: 0.0240\n",
            "Epoch [6019/20000], Training Loss: 0.0260\n",
            "Epoch [6020/20000], Training Loss: 0.0281\n",
            "Epoch [6021/20000], Training Loss: 0.0268\n",
            "Epoch [6022/20000], Training Loss: 0.0274\n",
            "Epoch [6023/20000], Training Loss: 0.0260\n",
            "Epoch [6024/20000], Training Loss: 0.0249\n",
            "Epoch [6025/20000], Training Loss: 0.0254\n",
            "Epoch [6026/20000], Training Loss: 0.0282\n",
            "Epoch [6027/20000], Training Loss: 0.0289\n",
            "Epoch [6028/20000], Training Loss: 0.0251\n",
            "Epoch [6029/20000], Training Loss: 0.0259\n",
            "Epoch [6030/20000], Training Loss: 0.0272\n",
            "Epoch [6031/20000], Training Loss: 0.0260\n",
            "Epoch [6032/20000], Training Loss: 0.0256\n",
            "Epoch [6033/20000], Training Loss: 0.0261\n",
            "Epoch [6034/20000], Training Loss: 0.0271\n",
            "Epoch [6035/20000], Training Loss: 0.0285\n",
            "Epoch [6036/20000], Training Loss: 0.0267\n",
            "Epoch [6037/20000], Training Loss: 0.0260\n",
            "Epoch [6038/20000], Training Loss: 0.0263\n",
            "Epoch [6039/20000], Training Loss: 0.0280\n",
            "Epoch [6040/20000], Training Loss: 0.0269\n",
            "Epoch [6041/20000], Training Loss: 0.0257\n",
            "Epoch [6042/20000], Training Loss: 0.0281\n",
            "Epoch [6043/20000], Training Loss: 0.0266\n",
            "Epoch [6044/20000], Training Loss: 0.0258\n",
            "Epoch [6045/20000], Training Loss: 0.0272\n",
            "Epoch [6046/20000], Training Loss: 0.0267\n",
            "Epoch [6047/20000], Training Loss: 0.0267\n",
            "Epoch [6048/20000], Training Loss: 0.0288\n",
            "Epoch [6049/20000], Training Loss: 0.0283\n",
            "Epoch [6050/20000], Training Loss: 0.0251\n",
            "Epoch [6051/20000], Training Loss: 0.0242\n",
            "Epoch [6052/20000], Training Loss: 0.0278\n",
            "Epoch [6053/20000], Training Loss: 0.0254\n",
            "Epoch [6054/20000], Training Loss: 0.0257\n",
            "Epoch [6055/20000], Training Loss: 0.0273\n",
            "Epoch [6056/20000], Training Loss: 0.0259\n",
            "Epoch [6057/20000], Training Loss: 0.0265\n",
            "Epoch [6058/20000], Training Loss: 0.0284\n",
            "Epoch [6059/20000], Training Loss: 0.0247\n",
            "Epoch [6060/20000], Training Loss: 0.0261\n",
            "Epoch [6061/20000], Training Loss: 0.0276\n",
            "Epoch [6062/20000], Training Loss: 0.0272\n",
            "Epoch [6063/20000], Training Loss: 0.0272\n",
            "Epoch [6064/20000], Training Loss: 0.0265\n",
            "Epoch [6065/20000], Training Loss: 0.0268\n",
            "Epoch [6066/20000], Training Loss: 0.0295\n",
            "Epoch [6067/20000], Training Loss: 0.0259\n",
            "Epoch [6068/20000], Training Loss: 0.0275\n",
            "Epoch [6069/20000], Training Loss: 0.0274\n",
            "Epoch [6070/20000], Training Loss: 0.0266\n",
            "Epoch [6071/20000], Training Loss: 0.0250\n",
            "Epoch [6072/20000], Training Loss: 0.0257\n",
            "Epoch [6073/20000], Training Loss: 0.0272\n",
            "Epoch [6074/20000], Training Loss: 0.0268\n",
            "Epoch [6075/20000], Training Loss: 0.0254\n",
            "Epoch [6076/20000], Training Loss: 0.0249\n",
            "Epoch [6077/20000], Training Loss: 0.0250\n",
            "Epoch [6078/20000], Training Loss: 0.0262\n",
            "Epoch [6079/20000], Training Loss: 0.0244\n",
            "Epoch [6080/20000], Training Loss: 0.0243\n",
            "Epoch [6081/20000], Training Loss: 0.0258\n",
            "Epoch [6082/20000], Training Loss: 0.0254\n",
            "Epoch [6083/20000], Training Loss: 0.0261\n",
            "Epoch [6084/20000], Training Loss: 0.0266\n",
            "Epoch [6085/20000], Training Loss: 0.0276\n",
            "Epoch [6086/20000], Training Loss: 0.0269\n",
            "Epoch [6087/20000], Training Loss: 0.0284\n",
            "Epoch [6088/20000], Training Loss: 0.0270\n",
            "Epoch [6089/20000], Training Loss: 0.0259\n",
            "Epoch [6090/20000], Training Loss: 0.0269\n",
            "Epoch [6091/20000], Training Loss: 0.0286\n",
            "Epoch [6092/20000], Training Loss: 0.0259\n",
            "Epoch [6093/20000], Training Loss: 0.0271\n",
            "Epoch [6094/20000], Training Loss: 0.0253\n",
            "Epoch [6095/20000], Training Loss: 0.0256\n",
            "Epoch [6096/20000], Training Loss: 0.0252\n",
            "Epoch [6097/20000], Training Loss: 0.0264\n",
            "Epoch [6098/20000], Training Loss: 0.0285\n",
            "Epoch [6099/20000], Training Loss: 0.0264\n",
            "Epoch [6100/20000], Training Loss: 0.0270\n",
            "Epoch [6101/20000], Training Loss: 0.0274\n",
            "Epoch [6102/20000], Training Loss: 0.0258\n",
            "Epoch [6103/20000], Training Loss: 0.0279\n",
            "Epoch [6104/20000], Training Loss: 0.0269\n",
            "Epoch [6105/20000], Training Loss: 0.0246\n",
            "Epoch [6106/20000], Training Loss: 0.0256\n",
            "Epoch [6107/20000], Training Loss: 0.0261\n",
            "Epoch [6108/20000], Training Loss: 0.0256\n",
            "Epoch [6109/20000], Training Loss: 0.0265\n",
            "Epoch [6110/20000], Training Loss: 0.0272\n",
            "Epoch [6111/20000], Training Loss: 0.0276\n",
            "Epoch [6112/20000], Training Loss: 0.0283\n",
            "Epoch [6113/20000], Training Loss: 0.0276\n",
            "Epoch [6114/20000], Training Loss: 0.0270\n",
            "Epoch [6115/20000], Training Loss: 0.0269\n",
            "Epoch [6116/20000], Training Loss: 0.0261\n",
            "Epoch [6117/20000], Training Loss: 0.0274\n",
            "Epoch [6118/20000], Training Loss: 0.0266\n",
            "Epoch [6119/20000], Training Loss: 0.0294\n",
            "Epoch [6120/20000], Training Loss: 0.0246\n",
            "Epoch [6121/20000], Training Loss: 0.0273\n",
            "Epoch [6122/20000], Training Loss: 0.0291\n",
            "Epoch [6123/20000], Training Loss: 0.0281\n",
            "Epoch [6124/20000], Training Loss: 0.0244\n",
            "Epoch [6125/20000], Training Loss: 0.0258\n",
            "Epoch [6126/20000], Training Loss: 0.0275\n",
            "Epoch [6127/20000], Training Loss: 0.0254\n",
            "Epoch [6128/20000], Training Loss: 0.0265\n",
            "Epoch [6129/20000], Training Loss: 0.0275\n",
            "Epoch [6130/20000], Training Loss: 0.0255\n",
            "Epoch [6131/20000], Training Loss: 0.0251\n",
            "Epoch [6132/20000], Training Loss: 0.0268\n",
            "Epoch [6133/20000], Training Loss: 0.0273\n",
            "Epoch [6134/20000], Training Loss: 0.0264\n",
            "Epoch [6135/20000], Training Loss: 0.0256\n",
            "Epoch [6136/20000], Training Loss: 0.0274\n",
            "Epoch [6137/20000], Training Loss: 0.0261\n",
            "Epoch [6138/20000], Training Loss: 0.0263\n",
            "Epoch [6139/20000], Training Loss: 0.0270\n",
            "Epoch [6140/20000], Training Loss: 0.0285\n",
            "Epoch [6141/20000], Training Loss: 0.0281\n",
            "Epoch [6142/20000], Training Loss: 0.0268\n",
            "Epoch [6143/20000], Training Loss: 0.0257\n",
            "Epoch [6144/20000], Training Loss: 0.0260\n",
            "Epoch [6145/20000], Training Loss: 0.0283\n",
            "Epoch [6146/20000], Training Loss: 0.0249\n",
            "Epoch [6147/20000], Training Loss: 0.0247\n",
            "Epoch [6148/20000], Training Loss: 0.0267\n",
            "Epoch [6149/20000], Training Loss: 0.0271\n",
            "Epoch [6150/20000], Training Loss: 0.0279\n",
            "Epoch [6151/20000], Training Loss: 0.0289\n",
            "Epoch [6152/20000], Training Loss: 0.0285\n",
            "Epoch [6153/20000], Training Loss: 0.0272\n",
            "Epoch [6154/20000], Training Loss: 0.0264\n",
            "Epoch [6155/20000], Training Loss: 0.0261\n",
            "Epoch [6156/20000], Training Loss: 0.0291\n",
            "Epoch [6157/20000], Training Loss: 0.0262\n",
            "Epoch [6158/20000], Training Loss: 0.0263\n",
            "Epoch [6159/20000], Training Loss: 0.0270\n",
            "Epoch [6160/20000], Training Loss: 0.0256\n",
            "Epoch [6161/20000], Training Loss: 0.0271\n",
            "Epoch [6162/20000], Training Loss: 0.0268\n",
            "Epoch [6163/20000], Training Loss: 0.0240\n",
            "Epoch [6164/20000], Training Loss: 0.0249\n",
            "Epoch [6165/20000], Training Loss: 0.0292\n",
            "Epoch [6166/20000], Training Loss: 0.0237\n",
            "Epoch [6167/20000], Training Loss: 0.0269\n",
            "Epoch [6168/20000], Training Loss: 0.0284\n",
            "Epoch [6169/20000], Training Loss: 0.0279\n",
            "Epoch [6170/20000], Training Loss: 0.0259\n",
            "Epoch [6171/20000], Training Loss: 0.0268\n",
            "Epoch [6172/20000], Training Loss: 0.0268\n",
            "Epoch [6173/20000], Training Loss: 0.0276\n",
            "Epoch [6174/20000], Training Loss: 0.0270\n",
            "Epoch [6175/20000], Training Loss: 0.0261\n",
            "Epoch [6176/20000], Training Loss: 0.0246\n",
            "Epoch [6177/20000], Training Loss: 0.0271\n",
            "Epoch [6178/20000], Training Loss: 0.0252\n",
            "Epoch [6179/20000], Training Loss: 0.0258\n",
            "Epoch [6180/20000], Training Loss: 0.0256\n",
            "Epoch [6181/20000], Training Loss: 0.0260\n",
            "Epoch [6182/20000], Training Loss: 0.0253\n",
            "Epoch [6183/20000], Training Loss: 0.0277\n",
            "Epoch [6184/20000], Training Loss: 0.0255\n",
            "Epoch [6185/20000], Training Loss: 0.0282\n",
            "Epoch [6186/20000], Training Loss: 0.0265\n",
            "Epoch [6187/20000], Training Loss: 0.0286\n",
            "Epoch [6188/20000], Training Loss: 0.0280\n",
            "Epoch [6189/20000], Training Loss: 0.0270\n",
            "Epoch [6190/20000], Training Loss: 0.0276\n",
            "Epoch [6191/20000], Training Loss: 0.0267\n",
            "Epoch [6192/20000], Training Loss: 0.0269\n",
            "Epoch [6193/20000], Training Loss: 0.0250\n",
            "Epoch [6194/20000], Training Loss: 0.0275\n",
            "Epoch [6195/20000], Training Loss: 0.0251\n",
            "Epoch [6196/20000], Training Loss: 0.0257\n",
            "Epoch [6197/20000], Training Loss: 0.0266\n",
            "Epoch [6198/20000], Training Loss: 0.0250\n",
            "Epoch [6199/20000], Training Loss: 0.0281\n",
            "Epoch [6200/20000], Training Loss: 0.0262\n",
            "Epoch [6201/20000], Training Loss: 0.0253\n",
            "Epoch [6202/20000], Training Loss: 0.0287\n",
            "Epoch [6203/20000], Training Loss: 0.0265\n",
            "Epoch [6204/20000], Training Loss: 0.0265\n",
            "Epoch [6205/20000], Training Loss: 0.0260\n",
            "Epoch [6206/20000], Training Loss: 0.0272\n",
            "Epoch [6207/20000], Training Loss: 0.0255\n",
            "Epoch [6208/20000], Training Loss: 0.0246\n",
            "Epoch [6209/20000], Training Loss: 0.0273\n",
            "Epoch [6210/20000], Training Loss: 0.0269\n",
            "Epoch [6211/20000], Training Loss: 0.0271\n",
            "Epoch [6212/20000], Training Loss: 0.0266\n",
            "Epoch [6213/20000], Training Loss: 0.0266\n",
            "Epoch [6214/20000], Training Loss: 0.0270\n",
            "Epoch [6215/20000], Training Loss: 0.0292\n",
            "Epoch [6216/20000], Training Loss: 0.0276\n",
            "Epoch [6217/20000], Training Loss: 0.0284\n",
            "Epoch [6218/20000], Training Loss: 0.0274\n",
            "Epoch [6219/20000], Training Loss: 0.0277\n",
            "Epoch [6220/20000], Training Loss: 0.0282\n",
            "Epoch [6221/20000], Training Loss: 0.0266\n",
            "Epoch [6222/20000], Training Loss: 0.0275\n",
            "Epoch [6223/20000], Training Loss: 0.0255\n",
            "Epoch [6224/20000], Training Loss: 0.0260\n",
            "Epoch [6225/20000], Training Loss: 0.0259\n",
            "Epoch [6226/20000], Training Loss: 0.0249\n",
            "Epoch [6227/20000], Training Loss: 0.0258\n",
            "Epoch [6228/20000], Training Loss: 0.0264\n",
            "Epoch [6229/20000], Training Loss: 0.0279\n",
            "Epoch [6230/20000], Training Loss: 0.0257\n",
            "Epoch [6231/20000], Training Loss: 0.0268\n",
            "Epoch [6232/20000], Training Loss: 0.0264\n",
            "Epoch [6233/20000], Training Loss: 0.0283\n",
            "Epoch [6234/20000], Training Loss: 0.0277\n",
            "Epoch [6235/20000], Training Loss: 0.0267\n",
            "Epoch [6236/20000], Training Loss: 0.0278\n",
            "Epoch [6237/20000], Training Loss: 0.0266\n",
            "Epoch [6238/20000], Training Loss: 0.0258\n",
            "Epoch [6239/20000], Training Loss: 0.0273\n",
            "Epoch [6240/20000], Training Loss: 0.0266\n",
            "Epoch [6241/20000], Training Loss: 0.0250\n",
            "Epoch [6242/20000], Training Loss: 0.0258\n",
            "Epoch [6243/20000], Training Loss: 0.0262\n",
            "Epoch [6244/20000], Training Loss: 0.0282\n",
            "Epoch [6245/20000], Training Loss: 0.0251\n",
            "Epoch [6246/20000], Training Loss: 0.0264\n",
            "Epoch [6247/20000], Training Loss: 0.0251\n",
            "Epoch [6248/20000], Training Loss: 0.0281\n",
            "Epoch [6249/20000], Training Loss: 0.0249\n",
            "Epoch [6250/20000], Training Loss: 0.0272\n",
            "Epoch [6251/20000], Training Loss: 0.0262\n",
            "Epoch [6252/20000], Training Loss: 0.0254\n",
            "Epoch [6253/20000], Training Loss: 0.0264\n",
            "Epoch [6254/20000], Training Loss: 0.0273\n",
            "Epoch [6255/20000], Training Loss: 0.0271\n",
            "Epoch [6256/20000], Training Loss: 0.0287\n",
            "Epoch [6257/20000], Training Loss: 0.0274\n",
            "Epoch [6258/20000], Training Loss: 0.0249\n",
            "Epoch [6259/20000], Training Loss: 0.0260\n",
            "Epoch [6260/20000], Training Loss: 0.0258\n",
            "Epoch [6261/20000], Training Loss: 0.0246\n",
            "Epoch [6262/20000], Training Loss: 0.0258\n",
            "Epoch [6263/20000], Training Loss: 0.0269\n",
            "Epoch [6264/20000], Training Loss: 0.0251\n",
            "Epoch [6265/20000], Training Loss: 0.0251\n",
            "Epoch [6266/20000], Training Loss: 0.0260\n",
            "Epoch [6267/20000], Training Loss: 0.0256\n",
            "Epoch [6268/20000], Training Loss: 0.0275\n",
            "Epoch [6269/20000], Training Loss: 0.0267\n",
            "Epoch [6270/20000], Training Loss: 0.0259\n",
            "Epoch [6271/20000], Training Loss: 0.0260\n",
            "Epoch [6272/20000], Training Loss: 0.0281\n",
            "Epoch [6273/20000], Training Loss: 0.0276\n",
            "Epoch [6274/20000], Training Loss: 0.0261\n",
            "Epoch [6275/20000], Training Loss: 0.0259\n",
            "Epoch [6276/20000], Training Loss: 0.0259\n",
            "Epoch [6277/20000], Training Loss: 0.0281\n",
            "Epoch [6278/20000], Training Loss: 0.0247\n",
            "Epoch [6279/20000], Training Loss: 0.0268\n",
            "Epoch [6280/20000], Training Loss: 0.0264\n",
            "Epoch [6281/20000], Training Loss: 0.0262\n",
            "Epoch [6282/20000], Training Loss: 0.0289\n",
            "Epoch [6283/20000], Training Loss: 0.0258\n",
            "Epoch [6284/20000], Training Loss: 0.0265\n",
            "Epoch [6285/20000], Training Loss: 0.0273\n",
            "Epoch [6286/20000], Training Loss: 0.0256\n",
            "Epoch [6287/20000], Training Loss: 0.0283\n",
            "Epoch [6288/20000], Training Loss: 0.0255\n",
            "Epoch [6289/20000], Training Loss: 0.0259\n",
            "Epoch [6290/20000], Training Loss: 0.0263\n",
            "Epoch [6291/20000], Training Loss: 0.0255\n",
            "Epoch [6292/20000], Training Loss: 0.0267\n",
            "Epoch [6293/20000], Training Loss: 0.0255\n",
            "Epoch [6294/20000], Training Loss: 0.0264\n",
            "Epoch [6295/20000], Training Loss: 0.0253\n",
            "Epoch [6296/20000], Training Loss: 0.0277\n",
            "Epoch [6297/20000], Training Loss: 0.0278\n",
            "Epoch [6298/20000], Training Loss: 0.0264\n",
            "Epoch [6299/20000], Training Loss: 0.0289\n",
            "Epoch [6300/20000], Training Loss: 0.0278\n",
            "Epoch [6301/20000], Training Loss: 0.0273\n",
            "Epoch [6302/20000], Training Loss: 0.0257\n",
            "Epoch [6303/20000], Training Loss: 0.0289\n",
            "Epoch [6304/20000], Training Loss: 0.0272\n",
            "Epoch [6305/20000], Training Loss: 0.0252\n",
            "Epoch [6306/20000], Training Loss: 0.0259\n",
            "Epoch [6307/20000], Training Loss: 0.0246\n",
            "Epoch [6308/20000], Training Loss: 0.0264\n",
            "Epoch [6309/20000], Training Loss: 0.0256\n",
            "Epoch [6310/20000], Training Loss: 0.0272\n",
            "Epoch [6311/20000], Training Loss: 0.0274\n",
            "Epoch [6312/20000], Training Loss: 0.0265\n",
            "Epoch [6313/20000], Training Loss: 0.0262\n",
            "Epoch [6314/20000], Training Loss: 0.0258\n",
            "Epoch [6315/20000], Training Loss: 0.0245\n",
            "Epoch [6316/20000], Training Loss: 0.0255\n",
            "Epoch [6317/20000], Training Loss: 0.0275\n",
            "Epoch [6318/20000], Training Loss: 0.0273\n",
            "Epoch [6319/20000], Training Loss: 0.0267\n",
            "Epoch [6320/20000], Training Loss: 0.0261\n",
            "Epoch [6321/20000], Training Loss: 0.0262\n",
            "Epoch [6322/20000], Training Loss: 0.0291\n",
            "Epoch [6323/20000], Training Loss: 0.0267\n",
            "Epoch [6324/20000], Training Loss: 0.0260\n",
            "Epoch [6325/20000], Training Loss: 0.0273\n",
            "Epoch [6326/20000], Training Loss: 0.0273\n",
            "Epoch [6327/20000], Training Loss: 0.0265\n",
            "Epoch [6328/20000], Training Loss: 0.0281\n",
            "Epoch [6329/20000], Training Loss: 0.0253\n",
            "Epoch [6330/20000], Training Loss: 0.0263\n",
            "Epoch [6331/20000], Training Loss: 0.0266\n",
            "Epoch [6332/20000], Training Loss: 0.0286\n",
            "Epoch [6333/20000], Training Loss: 0.0250\n",
            "Epoch [6334/20000], Training Loss: 0.0251\n",
            "Epoch [6335/20000], Training Loss: 0.0252\n",
            "Epoch [6336/20000], Training Loss: 0.0261\n",
            "Epoch [6337/20000], Training Loss: 0.0272\n",
            "Epoch [6338/20000], Training Loss: 0.0264\n",
            "Epoch [6339/20000], Training Loss: 0.0267\n",
            "Epoch [6340/20000], Training Loss: 0.0247\n",
            "Epoch [6341/20000], Training Loss: 0.0269\n",
            "Epoch [6342/20000], Training Loss: 0.0275\n",
            "Epoch [6343/20000], Training Loss: 0.0271\n",
            "Epoch [6344/20000], Training Loss: 0.0245\n",
            "Epoch [6345/20000], Training Loss: 0.0291\n",
            "Epoch [6346/20000], Training Loss: 0.0260\n",
            "Epoch [6347/20000], Training Loss: 0.0262\n",
            "Epoch [6348/20000], Training Loss: 0.0259\n",
            "Epoch [6349/20000], Training Loss: 0.0251\n",
            "Epoch [6350/20000], Training Loss: 0.0261\n",
            "Epoch [6351/20000], Training Loss: 0.0281\n",
            "Epoch [6352/20000], Training Loss: 0.0253\n",
            "Epoch [6353/20000], Training Loss: 0.0250\n",
            "Epoch [6354/20000], Training Loss: 0.0266\n",
            "Epoch [6355/20000], Training Loss: 0.0252\n",
            "Epoch [6356/20000], Training Loss: 0.0256\n",
            "Epoch [6357/20000], Training Loss: 0.0261\n",
            "Epoch [6358/20000], Training Loss: 0.0272\n",
            "Epoch [6359/20000], Training Loss: 0.0264\n",
            "Epoch [6360/20000], Training Loss: 0.0255\n",
            "Epoch [6361/20000], Training Loss: 0.0283\n",
            "Epoch [6362/20000], Training Loss: 0.0277\n",
            "Epoch [6363/20000], Training Loss: 0.0266\n",
            "Epoch [6364/20000], Training Loss: 0.0269\n",
            "Epoch [6365/20000], Training Loss: 0.0271\n",
            "Epoch [6366/20000], Training Loss: 0.0260\n",
            "Epoch [6367/20000], Training Loss: 0.0252\n",
            "Epoch [6368/20000], Training Loss: 0.0261\n",
            "Epoch [6369/20000], Training Loss: 0.0259\n",
            "Epoch [6370/20000], Training Loss: 0.0288\n",
            "Epoch [6371/20000], Training Loss: 0.0279\n",
            "Epoch [6372/20000], Training Loss: 0.0262\n",
            "Epoch [6373/20000], Training Loss: 0.0263\n",
            "Epoch [6374/20000], Training Loss: 0.0291\n",
            "Epoch [6375/20000], Training Loss: 0.0274\n",
            "Epoch [6376/20000], Training Loss: 0.0277\n",
            "Epoch [6377/20000], Training Loss: 0.0256\n",
            "Epoch [6378/20000], Training Loss: 0.0256\n",
            "Epoch [6379/20000], Training Loss: 0.0281\n",
            "Epoch [6380/20000], Training Loss: 0.0246\n",
            "Epoch [6381/20000], Training Loss: 0.0279\n",
            "Epoch [6382/20000], Training Loss: 0.0277\n",
            "Epoch [6383/20000], Training Loss: 0.0265\n",
            "Epoch [6384/20000], Training Loss: 0.0263\n",
            "Epoch [6385/20000], Training Loss: 0.0259\n",
            "Epoch [6386/20000], Training Loss: 0.0256\n",
            "Epoch [6387/20000], Training Loss: 0.0266\n",
            "Epoch [6388/20000], Training Loss: 0.0244\n",
            "Epoch [6389/20000], Training Loss: 0.0266\n",
            "Epoch [6390/20000], Training Loss: 0.0260\n",
            "Epoch [6391/20000], Training Loss: 0.0277\n",
            "Epoch [6392/20000], Training Loss: 0.0255\n",
            "Epoch [6393/20000], Training Loss: 0.0248\n",
            "Epoch [6394/20000], Training Loss: 0.0254\n",
            "Epoch [6395/20000], Training Loss: 0.0257\n",
            "Epoch [6396/20000], Training Loss: 0.0244\n",
            "Epoch [6397/20000], Training Loss: 0.0265\n",
            "Epoch [6398/20000], Training Loss: 0.0264\n",
            "Epoch [6399/20000], Training Loss: 0.0285\n",
            "Epoch [6400/20000], Training Loss: 0.0257\n",
            "Epoch [6401/20000], Training Loss: 0.0264\n",
            "Epoch [6402/20000], Training Loss: 0.0258\n",
            "Epoch [6403/20000], Training Loss: 0.0275\n",
            "Epoch [6404/20000], Training Loss: 0.0259\n",
            "Epoch [6405/20000], Training Loss: 0.0262\n",
            "Epoch [6406/20000], Training Loss: 0.0269\n",
            "Epoch [6407/20000], Training Loss: 0.0259\n",
            "Epoch [6408/20000], Training Loss: 0.0270\n",
            "Epoch [6409/20000], Training Loss: 0.0274\n",
            "Epoch [6410/20000], Training Loss: 0.0274\n",
            "Epoch [6411/20000], Training Loss: 0.0248\n",
            "Epoch [6412/20000], Training Loss: 0.0280\n",
            "Epoch [6413/20000], Training Loss: 0.0244\n",
            "Epoch [6414/20000], Training Loss: 0.0276\n",
            "Epoch [6415/20000], Training Loss: 0.0263\n",
            "Epoch [6416/20000], Training Loss: 0.0279\n",
            "Epoch [6417/20000], Training Loss: 0.0272\n",
            "Epoch [6418/20000], Training Loss: 0.0279\n",
            "Epoch [6419/20000], Training Loss: 0.0253\n",
            "Epoch [6420/20000], Training Loss: 0.0255\n",
            "Epoch [6421/20000], Training Loss: 0.0259\n",
            "Epoch [6422/20000], Training Loss: 0.0286\n",
            "Epoch [6423/20000], Training Loss: 0.0248\n",
            "Epoch [6424/20000], Training Loss: 0.0275\n",
            "Epoch [6425/20000], Training Loss: 0.0269\n",
            "Epoch [6426/20000], Training Loss: 0.0252\n",
            "Epoch [6427/20000], Training Loss: 0.0268\n",
            "Epoch [6428/20000], Training Loss: 0.0262\n",
            "Epoch [6429/20000], Training Loss: 0.0259\n",
            "Epoch [6430/20000], Training Loss: 0.0258\n",
            "Epoch [6431/20000], Training Loss: 0.0272\n",
            "Epoch [6432/20000], Training Loss: 0.0250\n",
            "Epoch [6433/20000], Training Loss: 0.0282\n",
            "Epoch [6434/20000], Training Loss: 0.0273\n",
            "Epoch [6435/20000], Training Loss: 0.0262\n",
            "Epoch [6436/20000], Training Loss: 0.0274\n",
            "Epoch [6437/20000], Training Loss: 0.0263\n",
            "Epoch [6438/20000], Training Loss: 0.0283\n",
            "Epoch [6439/20000], Training Loss: 0.0242\n",
            "Epoch [6440/20000], Training Loss: 0.0280\n",
            "Epoch [6441/20000], Training Loss: 0.0269\n",
            "Epoch [6442/20000], Training Loss: 0.0252\n",
            "Epoch [6443/20000], Training Loss: 0.0257\n",
            "Epoch [6444/20000], Training Loss: 0.0253\n",
            "Epoch [6445/20000], Training Loss: 0.0246\n",
            "Epoch [6446/20000], Training Loss: 0.0265\n",
            "Epoch [6447/20000], Training Loss: 0.0262\n",
            "Epoch [6448/20000], Training Loss: 0.0269\n",
            "Epoch [6449/20000], Training Loss: 0.0279\n",
            "Epoch [6450/20000], Training Loss: 0.0278\n",
            "Epoch [6451/20000], Training Loss: 0.0278\n",
            "Epoch [6452/20000], Training Loss: 0.0256\n",
            "Epoch [6453/20000], Training Loss: 0.0264\n",
            "Epoch [6454/20000], Training Loss: 0.0266\n",
            "Epoch [6455/20000], Training Loss: 0.0260\n",
            "Epoch [6456/20000], Training Loss: 0.0252\n",
            "Epoch [6457/20000], Training Loss: 0.0257\n",
            "Epoch [6458/20000], Training Loss: 0.0269\n",
            "Epoch [6459/20000], Training Loss: 0.0245\n",
            "Epoch [6460/20000], Training Loss: 0.0259\n",
            "Epoch [6461/20000], Training Loss: 0.0277\n",
            "Epoch [6462/20000], Training Loss: 0.0272\n",
            "Epoch [6463/20000], Training Loss: 0.0281\n",
            "Epoch [6464/20000], Training Loss: 0.0257\n",
            "Epoch [6465/20000], Training Loss: 0.0253\n",
            "Epoch [6466/20000], Training Loss: 0.0254\n",
            "Epoch [6467/20000], Training Loss: 0.0267\n",
            "Epoch [6468/20000], Training Loss: 0.0286\n",
            "Epoch [6469/20000], Training Loss: 0.0273\n",
            "Epoch [6470/20000], Training Loss: 0.0280\n",
            "Epoch [6471/20000], Training Loss: 0.0253\n",
            "Epoch [6472/20000], Training Loss: 0.0266\n",
            "Epoch [6473/20000], Training Loss: 0.0278\n",
            "Epoch [6474/20000], Training Loss: 0.0258\n",
            "Epoch [6475/20000], Training Loss: 0.0285\n",
            "Epoch [6476/20000], Training Loss: 0.0241\n",
            "Epoch [6477/20000], Training Loss: 0.0258\n",
            "Epoch [6478/20000], Training Loss: 0.0253\n",
            "Epoch [6479/20000], Training Loss: 0.0279\n",
            "Epoch [6480/20000], Training Loss: 0.0277\n",
            "Epoch [6481/20000], Training Loss: 0.0247\n",
            "Epoch [6482/20000], Training Loss: 0.0272\n",
            "Epoch [6483/20000], Training Loss: 0.0276\n",
            "Epoch [6484/20000], Training Loss: 0.0279\n",
            "Epoch [6485/20000], Training Loss: 0.0270\n",
            "Epoch [6486/20000], Training Loss: 0.0269\n",
            "Epoch [6487/20000], Training Loss: 0.0269\n",
            "Epoch [6488/20000], Training Loss: 0.0241\n",
            "Epoch [6489/20000], Training Loss: 0.0285\n",
            "Epoch [6490/20000], Training Loss: 0.0258\n",
            "Epoch [6491/20000], Training Loss: 0.0276\n",
            "Epoch [6492/20000], Training Loss: 0.0257\n",
            "Epoch [6493/20000], Training Loss: 0.0261\n",
            "Epoch [6494/20000], Training Loss: 0.0260\n",
            "Epoch [6495/20000], Training Loss: 0.0274\n",
            "Epoch [6496/20000], Training Loss: 0.0257\n",
            "Epoch [6497/20000], Training Loss: 0.0243\n",
            "Epoch [6498/20000], Training Loss: 0.0257\n",
            "Epoch [6499/20000], Training Loss: 0.0276\n",
            "Epoch [6500/20000], Training Loss: 0.0268\n",
            "Epoch [6501/20000], Training Loss: 0.0257\n",
            "Epoch [6502/20000], Training Loss: 0.0270\n",
            "Epoch [6503/20000], Training Loss: 0.0250\n",
            "Epoch [6504/20000], Training Loss: 0.0290\n",
            "Epoch [6505/20000], Training Loss: 0.0258\n",
            "Epoch [6506/20000], Training Loss: 0.0258\n",
            "Epoch [6507/20000], Training Loss: 0.0260\n",
            "Epoch [6508/20000], Training Loss: 0.0287\n",
            "Epoch [6509/20000], Training Loss: 0.0284\n",
            "Epoch [6510/20000], Training Loss: 0.0275\n",
            "Epoch [6511/20000], Training Loss: 0.0258\n",
            "Epoch [6512/20000], Training Loss: 0.0256\n",
            "Epoch [6513/20000], Training Loss: 0.0243\n",
            "Epoch [6514/20000], Training Loss: 0.0260\n",
            "Epoch [6515/20000], Training Loss: 0.0279\n",
            "Epoch [6516/20000], Training Loss: 0.0286\n",
            "Epoch [6517/20000], Training Loss: 0.0262\n",
            "Epoch [6518/20000], Training Loss: 0.0287\n",
            "Epoch [6519/20000], Training Loss: 0.0269\n",
            "Epoch [6520/20000], Training Loss: 0.0271\n",
            "Epoch [6521/20000], Training Loss: 0.0257\n",
            "Epoch [6522/20000], Training Loss: 0.0265\n",
            "Epoch [6523/20000], Training Loss: 0.0285\n",
            "Epoch [6524/20000], Training Loss: 0.0270\n",
            "Epoch [6525/20000], Training Loss: 0.0261\n",
            "Epoch [6526/20000], Training Loss: 0.0292\n",
            "Epoch [6527/20000], Training Loss: 0.0280\n",
            "Epoch [6528/20000], Training Loss: 0.0279\n",
            "Epoch [6529/20000], Training Loss: 0.0258\n",
            "Epoch [6530/20000], Training Loss: 0.0267\n",
            "Epoch [6531/20000], Training Loss: 0.0276\n",
            "Epoch [6532/20000], Training Loss: 0.0256\n",
            "Epoch [6533/20000], Training Loss: 0.0240\n",
            "Epoch [6534/20000], Training Loss: 0.0231\n",
            "Epoch [6535/20000], Training Loss: 0.0252\n",
            "Epoch [6536/20000], Training Loss: 0.0284\n",
            "Epoch [6537/20000], Training Loss: 0.0273\n",
            "Epoch [6538/20000], Training Loss: 0.0262\n",
            "Epoch [6539/20000], Training Loss: 0.0254\n",
            "Epoch [6540/20000], Training Loss: 0.0263\n",
            "Epoch [6541/20000], Training Loss: 0.0248\n",
            "Epoch [6542/20000], Training Loss: 0.0262\n",
            "Epoch [6543/20000], Training Loss: 0.0251\n",
            "Epoch [6544/20000], Training Loss: 0.0280\n",
            "Epoch [6545/20000], Training Loss: 0.0249\n",
            "Epoch [6546/20000], Training Loss: 0.0273\n",
            "Epoch [6547/20000], Training Loss: 0.0270\n",
            "Epoch [6548/20000], Training Loss: 0.0268\n",
            "Epoch [6549/20000], Training Loss: 0.0277\n",
            "Epoch [6550/20000], Training Loss: 0.0248\n",
            "Epoch [6551/20000], Training Loss: 0.0277\n",
            "Epoch [6552/20000], Training Loss: 0.0262\n",
            "Epoch [6553/20000], Training Loss: 0.0280\n",
            "Epoch [6554/20000], Training Loss: 0.0260\n",
            "Epoch [6555/20000], Training Loss: 0.0249\n",
            "Epoch [6556/20000], Training Loss: 0.0281\n",
            "Epoch [6557/20000], Training Loss: 0.0250\n",
            "Epoch [6558/20000], Training Loss: 0.0259\n",
            "Epoch [6559/20000], Training Loss: 0.0290\n",
            "Epoch [6560/20000], Training Loss: 0.0270\n",
            "Epoch [6561/20000], Training Loss: 0.0278\n",
            "Epoch [6562/20000], Training Loss: 0.0252\n",
            "Epoch [6563/20000], Training Loss: 0.0263\n",
            "Epoch [6564/20000], Training Loss: 0.0285\n",
            "Epoch [6565/20000], Training Loss: 0.0250\n",
            "Epoch [6566/20000], Training Loss: 0.0270\n",
            "Epoch [6567/20000], Training Loss: 0.0248\n",
            "Epoch [6568/20000], Training Loss: 0.0247\n",
            "Epoch [6569/20000], Training Loss: 0.0268\n",
            "Epoch [6570/20000], Training Loss: 0.0270\n",
            "Epoch [6571/20000], Training Loss: 0.0265\n",
            "Epoch [6572/20000], Training Loss: 0.0258\n",
            "Epoch [6573/20000], Training Loss: 0.0257\n",
            "Epoch [6574/20000], Training Loss: 0.0262\n",
            "Epoch [6575/20000], Training Loss: 0.0274\n",
            "Epoch [6576/20000], Training Loss: 0.0255\n",
            "Epoch [6577/20000], Training Loss: 0.0272\n",
            "Epoch [6578/20000], Training Loss: 0.0254\n",
            "Epoch [6579/20000], Training Loss: 0.0259\n",
            "Epoch [6580/20000], Training Loss: 0.0262\n",
            "Epoch [6581/20000], Training Loss: 0.0263\n",
            "Epoch [6582/20000], Training Loss: 0.0268\n",
            "Epoch [6583/20000], Training Loss: 0.0265\n",
            "Epoch [6584/20000], Training Loss: 0.0270\n",
            "Epoch [6585/20000], Training Loss: 0.0274\n",
            "Epoch [6586/20000], Training Loss: 0.0276\n",
            "Epoch [6587/20000], Training Loss: 0.0253\n",
            "Epoch [6588/20000], Training Loss: 0.0252\n",
            "Epoch [6589/20000], Training Loss: 0.0265\n",
            "Epoch [6590/20000], Training Loss: 0.0270\n",
            "Epoch [6591/20000], Training Loss: 0.0277\n",
            "Epoch [6592/20000], Training Loss: 0.0278\n",
            "Epoch [6593/20000], Training Loss: 0.0267\n",
            "Epoch [6594/20000], Training Loss: 0.0291\n",
            "Epoch [6595/20000], Training Loss: 0.0276\n",
            "Epoch [6596/20000], Training Loss: 0.0270\n",
            "Epoch [6597/20000], Training Loss: 0.0263\n",
            "Epoch [6598/20000], Training Loss: 0.0274\n",
            "Epoch [6599/20000], Training Loss: 0.0243\n",
            "Epoch [6600/20000], Training Loss: 0.0245\n",
            "Epoch [6601/20000], Training Loss: 0.0247\n",
            "Epoch [6602/20000], Training Loss: 0.0264\n",
            "Epoch [6603/20000], Training Loss: 0.0278\n",
            "Epoch [6604/20000], Training Loss: 0.0283\n",
            "Epoch [6605/20000], Training Loss: 0.0271\n",
            "Epoch [6606/20000], Training Loss: 0.0260\n",
            "Epoch [6607/20000], Training Loss: 0.0281\n",
            "Epoch [6608/20000], Training Loss: 0.0276\n",
            "Epoch [6609/20000], Training Loss: 0.0282\n",
            "Epoch [6610/20000], Training Loss: 0.0262\n",
            "Epoch [6611/20000], Training Loss: 0.0249\n",
            "Epoch [6612/20000], Training Loss: 0.0275\n",
            "Epoch [6613/20000], Training Loss: 0.0263\n",
            "Epoch [6614/20000], Training Loss: 0.0272\n",
            "Epoch [6615/20000], Training Loss: 0.0255\n",
            "Epoch [6616/20000], Training Loss: 0.0274\n",
            "Epoch [6617/20000], Training Loss: 0.0243\n",
            "Epoch [6618/20000], Training Loss: 0.0281\n",
            "Epoch [6619/20000], Training Loss: 0.0263\n",
            "Epoch [6620/20000], Training Loss: 0.0270\n",
            "Epoch [6621/20000], Training Loss: 0.0277\n",
            "Epoch [6622/20000], Training Loss: 0.0259\n",
            "Epoch [6623/20000], Training Loss: 0.0267\n",
            "Epoch [6624/20000], Training Loss: 0.0252\n",
            "Epoch [6625/20000], Training Loss: 0.0257\n",
            "Epoch [6626/20000], Training Loss: 0.0243\n",
            "Epoch [6627/20000], Training Loss: 0.0258\n",
            "Epoch [6628/20000], Training Loss: 0.0257\n",
            "Epoch [6629/20000], Training Loss: 0.0255\n",
            "Epoch [6630/20000], Training Loss: 0.0282\n",
            "Epoch [6631/20000], Training Loss: 0.0276\n",
            "Epoch [6632/20000], Training Loss: 0.0261\n",
            "Epoch [6633/20000], Training Loss: 0.0271\n",
            "Epoch [6634/20000], Training Loss: 0.0273\n",
            "Epoch [6635/20000], Training Loss: 0.0256\n",
            "Epoch [6636/20000], Training Loss: 0.0249\n",
            "Epoch [6637/20000], Training Loss: 0.0268\n",
            "Epoch [6638/20000], Training Loss: 0.0281\n",
            "Epoch [6639/20000], Training Loss: 0.0256\n",
            "Epoch [6640/20000], Training Loss: 0.0253\n",
            "Epoch [6641/20000], Training Loss: 0.0270\n",
            "Epoch [6642/20000], Training Loss: 0.0255\n",
            "Epoch [6643/20000], Training Loss: 0.0262\n",
            "Epoch [6644/20000], Training Loss: 0.0250\n",
            "Epoch [6645/20000], Training Loss: 0.0249\n",
            "Epoch [6646/20000], Training Loss: 0.0270\n",
            "Epoch [6647/20000], Training Loss: 0.0286\n",
            "Epoch [6648/20000], Training Loss: 0.0259\n",
            "Epoch [6649/20000], Training Loss: 0.0266\n",
            "Epoch [6650/20000], Training Loss: 0.0250\n",
            "Epoch [6651/20000], Training Loss: 0.0292\n",
            "Epoch [6652/20000], Training Loss: 0.0275\n",
            "Epoch [6653/20000], Training Loss: 0.0258\n",
            "Epoch [6654/20000], Training Loss: 0.0254\n",
            "Epoch [6655/20000], Training Loss: 0.0268\n",
            "Epoch [6656/20000], Training Loss: 0.0253\n",
            "Epoch [6657/20000], Training Loss: 0.0248\n",
            "Epoch [6658/20000], Training Loss: 0.0259\n",
            "Epoch [6659/20000], Training Loss: 0.0271\n",
            "Epoch [6660/20000], Training Loss: 0.0264\n",
            "Epoch [6661/20000], Training Loss: 0.0268\n",
            "Epoch [6662/20000], Training Loss: 0.0273\n",
            "Epoch [6663/20000], Training Loss: 0.0252\n",
            "Epoch [6664/20000], Training Loss: 0.0269\n",
            "Epoch [6665/20000], Training Loss: 0.0279\n",
            "Epoch [6666/20000], Training Loss: 0.0269\n",
            "Epoch [6667/20000], Training Loss: 0.0267\n",
            "Epoch [6668/20000], Training Loss: 0.0266\n",
            "Epoch [6669/20000], Training Loss: 0.0283\n",
            "Epoch [6670/20000], Training Loss: 0.0275\n",
            "Epoch [6671/20000], Training Loss: 0.0250\n",
            "Epoch [6672/20000], Training Loss: 0.0253\n",
            "Epoch [6673/20000], Training Loss: 0.0255\n",
            "Epoch [6674/20000], Training Loss: 0.0270\n",
            "Epoch [6675/20000], Training Loss: 0.0269\n",
            "Epoch [6676/20000], Training Loss: 0.0268\n",
            "Epoch [6677/20000], Training Loss: 0.0274\n",
            "Epoch [6678/20000], Training Loss: 0.0259\n",
            "Epoch [6679/20000], Training Loss: 0.0244\n",
            "Epoch [6680/20000], Training Loss: 0.0270\n",
            "Epoch [6681/20000], Training Loss: 0.0261\n",
            "Epoch [6682/20000], Training Loss: 0.0246\n",
            "Epoch [6683/20000], Training Loss: 0.0262\n",
            "Epoch [6684/20000], Training Loss: 0.0272\n",
            "Epoch [6685/20000], Training Loss: 0.0249\n",
            "Epoch [6686/20000], Training Loss: 0.0271\n",
            "Epoch [6687/20000], Training Loss: 0.0253\n",
            "Epoch [6688/20000], Training Loss: 0.0250\n",
            "Epoch [6689/20000], Training Loss: 0.0263\n",
            "Epoch [6690/20000], Training Loss: 0.0258\n",
            "Epoch [6691/20000], Training Loss: 0.0252\n",
            "Epoch [6692/20000], Training Loss: 0.0269\n",
            "Epoch [6693/20000], Training Loss: 0.0266\n",
            "Epoch [6694/20000], Training Loss: 0.0274\n",
            "Epoch [6695/20000], Training Loss: 0.0281\n",
            "Epoch [6696/20000], Training Loss: 0.0249\n",
            "Epoch [6697/20000], Training Loss: 0.0271\n",
            "Epoch [6698/20000], Training Loss: 0.0254\n",
            "Epoch [6699/20000], Training Loss: 0.0270\n",
            "Epoch [6700/20000], Training Loss: 0.0265\n",
            "Epoch [6701/20000], Training Loss: 0.0263\n",
            "Epoch [6702/20000], Training Loss: 0.0257\n",
            "Epoch [6703/20000], Training Loss: 0.0266\n",
            "Epoch [6704/20000], Training Loss: 0.0273\n",
            "Epoch [6705/20000], Training Loss: 0.0272\n",
            "Epoch [6706/20000], Training Loss: 0.0279\n",
            "Epoch [6707/20000], Training Loss: 0.0257\n",
            "Epoch [6708/20000], Training Loss: 0.0247\n",
            "Epoch [6709/20000], Training Loss: 0.0255\n",
            "Epoch [6710/20000], Training Loss: 0.0256\n",
            "Epoch [6711/20000], Training Loss: 0.0269\n",
            "Epoch [6712/20000], Training Loss: 0.0272\n",
            "Epoch [6713/20000], Training Loss: 0.0265\n",
            "Epoch [6714/20000], Training Loss: 0.0286\n",
            "Epoch [6715/20000], Training Loss: 0.0267\n",
            "Epoch [6716/20000], Training Loss: 0.0288\n",
            "Epoch [6717/20000], Training Loss: 0.0285\n",
            "Epoch [6718/20000], Training Loss: 0.0263\n",
            "Epoch [6719/20000], Training Loss: 0.0268\n",
            "Epoch [6720/20000], Training Loss: 0.0262\n",
            "Epoch [6721/20000], Training Loss: 0.0273\n",
            "Epoch [6722/20000], Training Loss: 0.0256\n",
            "Epoch [6723/20000], Training Loss: 0.0249\n",
            "Epoch [6724/20000], Training Loss: 0.0283\n",
            "Epoch [6725/20000], Training Loss: 0.0241\n",
            "Epoch [6726/20000], Training Loss: 0.0280\n",
            "Epoch [6727/20000], Training Loss: 0.0262\n",
            "Epoch [6728/20000], Training Loss: 0.0252\n",
            "Epoch [6729/20000], Training Loss: 0.0258\n",
            "Epoch [6730/20000], Training Loss: 0.0266\n",
            "Epoch [6731/20000], Training Loss: 0.0268\n",
            "Epoch [6732/20000], Training Loss: 0.0261\n",
            "Epoch [6733/20000], Training Loss: 0.0267\n",
            "Epoch [6734/20000], Training Loss: 0.0265\n",
            "Epoch [6735/20000], Training Loss: 0.0261\n",
            "Epoch [6736/20000], Training Loss: 0.0271\n",
            "Epoch [6737/20000], Training Loss: 0.0281\n",
            "Epoch [6738/20000], Training Loss: 0.0272\n",
            "Epoch [6739/20000], Training Loss: 0.0246\n",
            "Epoch [6740/20000], Training Loss: 0.0266\n",
            "Epoch [6741/20000], Training Loss: 0.0275\n",
            "Epoch [6742/20000], Training Loss: 0.0267\n",
            "Epoch [6743/20000], Training Loss: 0.0278\n",
            "Epoch [6744/20000], Training Loss: 0.0245\n",
            "Epoch [6745/20000], Training Loss: 0.0251\n",
            "Epoch [6746/20000], Training Loss: 0.0265\n",
            "Epoch [6747/20000], Training Loss: 0.0261\n",
            "Epoch [6748/20000], Training Loss: 0.0264\n",
            "Epoch [6749/20000], Training Loss: 0.0269\n",
            "Epoch [6750/20000], Training Loss: 0.0269\n",
            "Epoch [6751/20000], Training Loss: 0.0254\n",
            "Epoch [6752/20000], Training Loss: 0.0260\n",
            "Epoch [6753/20000], Training Loss: 0.0260\n",
            "Epoch [6754/20000], Training Loss: 0.0272\n",
            "Epoch [6755/20000], Training Loss: 0.0276\n",
            "Epoch [6756/20000], Training Loss: 0.0261\n",
            "Epoch [6757/20000], Training Loss: 0.0275\n",
            "Epoch [6758/20000], Training Loss: 0.0254\n",
            "Epoch [6759/20000], Training Loss: 0.0269\n",
            "Epoch [6760/20000], Training Loss: 0.0250\n",
            "Epoch [6761/20000], Training Loss: 0.0265\n",
            "Epoch [6762/20000], Training Loss: 0.0276\n",
            "Epoch [6763/20000], Training Loss: 0.0274\n",
            "Epoch [6764/20000], Training Loss: 0.0257\n",
            "Epoch [6765/20000], Training Loss: 0.0279\n",
            "Epoch [6766/20000], Training Loss: 0.0272\n",
            "Epoch [6767/20000], Training Loss: 0.0272\n",
            "Epoch [6768/20000], Training Loss: 0.0277\n",
            "Epoch [6769/20000], Training Loss: 0.0273\n",
            "Epoch [6770/20000], Training Loss: 0.0261\n",
            "Epoch [6771/20000], Training Loss: 0.0289\n",
            "Epoch [6772/20000], Training Loss: 0.0247\n",
            "Epoch [6773/20000], Training Loss: 0.0272\n",
            "Epoch [6774/20000], Training Loss: 0.0266\n",
            "Epoch [6775/20000], Training Loss: 0.0282\n",
            "Epoch [6776/20000], Training Loss: 0.0270\n",
            "Epoch [6777/20000], Training Loss: 0.0267\n",
            "Epoch [6778/20000], Training Loss: 0.0247\n",
            "Epoch [6779/20000], Training Loss: 0.0257\n",
            "Epoch [6780/20000], Training Loss: 0.0269\n",
            "Epoch [6781/20000], Training Loss: 0.0277\n",
            "Epoch [6782/20000], Training Loss: 0.0249\n",
            "Epoch [6783/20000], Training Loss: 0.0277\n",
            "Epoch [6784/20000], Training Loss: 0.0247\n",
            "Epoch [6785/20000], Training Loss: 0.0256\n",
            "Epoch [6786/20000], Training Loss: 0.0275\n",
            "Epoch [6787/20000], Training Loss: 0.0264\n",
            "Epoch [6788/20000], Training Loss: 0.0249\n",
            "Epoch [6789/20000], Training Loss: 0.0266\n",
            "Epoch [6790/20000], Training Loss: 0.0252\n",
            "Epoch [6791/20000], Training Loss: 0.0250\n",
            "Epoch [6792/20000], Training Loss: 0.0275\n",
            "Epoch [6793/20000], Training Loss: 0.0251\n",
            "Epoch [6794/20000], Training Loss: 0.0249\n",
            "Epoch [6795/20000], Training Loss: 0.0267\n",
            "Epoch [6796/20000], Training Loss: 0.0262\n",
            "Epoch [6797/20000], Training Loss: 0.0278\n",
            "Epoch [6798/20000], Training Loss: 0.0269\n",
            "Epoch [6799/20000], Training Loss: 0.0268\n",
            "Epoch [6800/20000], Training Loss: 0.0254\n",
            "Epoch [6801/20000], Training Loss: 0.0255\n",
            "Epoch [6802/20000], Training Loss: 0.0254\n",
            "Epoch [6803/20000], Training Loss: 0.0286\n",
            "Epoch [6804/20000], Training Loss: 0.0266\n",
            "Epoch [6805/20000], Training Loss: 0.0276\n",
            "Epoch [6806/20000], Training Loss: 0.0267\n",
            "Epoch [6807/20000], Training Loss: 0.0275\n",
            "Epoch [6808/20000], Training Loss: 0.0259\n",
            "Epoch [6809/20000], Training Loss: 0.0282\n",
            "Epoch [6810/20000], Training Loss: 0.0282\n",
            "Epoch [6811/20000], Training Loss: 0.0250\n",
            "Epoch [6812/20000], Training Loss: 0.0272\n",
            "Epoch [6813/20000], Training Loss: 0.0267\n",
            "Epoch [6814/20000], Training Loss: 0.0274\n",
            "Epoch [6815/20000], Training Loss: 0.0258\n",
            "Epoch [6816/20000], Training Loss: 0.0277\n",
            "Epoch [6817/20000], Training Loss: 0.0255\n",
            "Epoch [6818/20000], Training Loss: 0.0255\n",
            "Epoch [6819/20000], Training Loss: 0.0250\n",
            "Epoch [6820/20000], Training Loss: 0.0271\n",
            "Epoch [6821/20000], Training Loss: 0.0256\n",
            "Epoch [6822/20000], Training Loss: 0.0282\n",
            "Epoch [6823/20000], Training Loss: 0.0285\n",
            "Epoch [6824/20000], Training Loss: 0.0264\n",
            "Epoch [6825/20000], Training Loss: 0.0275\n",
            "Epoch [6826/20000], Training Loss: 0.0253\n",
            "Epoch [6827/20000], Training Loss: 0.0276\n",
            "Epoch [6828/20000], Training Loss: 0.0261\n",
            "Epoch [6829/20000], Training Loss: 0.0263\n",
            "Epoch [6830/20000], Training Loss: 0.0262\n",
            "Epoch [6831/20000], Training Loss: 0.0260\n",
            "Epoch [6832/20000], Training Loss: 0.0253\n",
            "Epoch [6833/20000], Training Loss: 0.0256\n",
            "Epoch [6834/20000], Training Loss: 0.0249\n",
            "Epoch [6835/20000], Training Loss: 0.0275\n",
            "Epoch [6836/20000], Training Loss: 0.0235\n",
            "Epoch [6837/20000], Training Loss: 0.0265\n",
            "Epoch [6838/20000], Training Loss: 0.0273\n",
            "Epoch [6839/20000], Training Loss: 0.0261\n",
            "Epoch [6840/20000], Training Loss: 0.0276\n",
            "Epoch [6841/20000], Training Loss: 0.0272\n",
            "Epoch [6842/20000], Training Loss: 0.0273\n",
            "Epoch [6843/20000], Training Loss: 0.0284\n",
            "Epoch [6844/20000], Training Loss: 0.0264\n",
            "Epoch [6845/20000], Training Loss: 0.0250\n",
            "Epoch [6846/20000], Training Loss: 0.0258\n",
            "Epoch [6847/20000], Training Loss: 0.0266\n",
            "Epoch [6848/20000], Training Loss: 0.0281\n",
            "Epoch [6849/20000], Training Loss: 0.0243\n",
            "Epoch [6850/20000], Training Loss: 0.0240\n",
            "Epoch [6851/20000], Training Loss: 0.0272\n",
            "Epoch [6852/20000], Training Loss: 0.0255\n",
            "Epoch [6853/20000], Training Loss: 0.0255\n",
            "Epoch [6854/20000], Training Loss: 0.0259\n",
            "Epoch [6855/20000], Training Loss: 0.0252\n",
            "Epoch [6856/20000], Training Loss: 0.0264\n",
            "Epoch [6857/20000], Training Loss: 0.0260\n",
            "Epoch [6858/20000], Training Loss: 0.0247\n",
            "Epoch [6859/20000], Training Loss: 0.0246\n",
            "Epoch [6860/20000], Training Loss: 0.0272\n",
            "Epoch [6861/20000], Training Loss: 0.0261\n",
            "Epoch [6862/20000], Training Loss: 0.0254\n",
            "Epoch [6863/20000], Training Loss: 0.0252\n",
            "Epoch [6864/20000], Training Loss: 0.0253\n",
            "Epoch [6865/20000], Training Loss: 0.0262\n",
            "Epoch [6866/20000], Training Loss: 0.0274\n",
            "Epoch [6867/20000], Training Loss: 0.0277\n",
            "Epoch [6868/20000], Training Loss: 0.0270\n",
            "Epoch [6869/20000], Training Loss: 0.0262\n",
            "Epoch [6870/20000], Training Loss: 0.0267\n",
            "Epoch [6871/20000], Training Loss: 0.0286\n",
            "Epoch [6872/20000], Training Loss: 0.0276\n",
            "Epoch [6873/20000], Training Loss: 0.0258\n",
            "Epoch [6874/20000], Training Loss: 0.0276\n",
            "Epoch [6875/20000], Training Loss: 0.0272\n",
            "Epoch [6876/20000], Training Loss: 0.0245\n",
            "Epoch [6877/20000], Training Loss: 0.0263\n",
            "Epoch [6878/20000], Training Loss: 0.0266\n",
            "Epoch [6879/20000], Training Loss: 0.0271\n",
            "Epoch [6880/20000], Training Loss: 0.0255\n",
            "Epoch [6881/20000], Training Loss: 0.0280\n",
            "Epoch [6882/20000], Training Loss: 0.0261\n",
            "Epoch [6883/20000], Training Loss: 0.0255\n",
            "Epoch [6884/20000], Training Loss: 0.0250\n",
            "Epoch [6885/20000], Training Loss: 0.0275\n",
            "Epoch [6886/20000], Training Loss: 0.0268\n",
            "Epoch [6887/20000], Training Loss: 0.0267\n",
            "Epoch [6888/20000], Training Loss: 0.0264\n",
            "Epoch [6889/20000], Training Loss: 0.0290\n",
            "Epoch [6890/20000], Training Loss: 0.0266\n",
            "Epoch [6891/20000], Training Loss: 0.0275\n",
            "Epoch [6892/20000], Training Loss: 0.0271\n",
            "Epoch [6893/20000], Training Loss: 0.0262\n",
            "Epoch [6894/20000], Training Loss: 0.0263\n",
            "Epoch [6895/20000], Training Loss: 0.0243\n",
            "Epoch [6896/20000], Training Loss: 0.0265\n",
            "Epoch [6897/20000], Training Loss: 0.0278\n",
            "Epoch [6898/20000], Training Loss: 0.0272\n",
            "Epoch [6899/20000], Training Loss: 0.0257\n",
            "Epoch [6900/20000], Training Loss: 0.0257\n",
            "Epoch [6901/20000], Training Loss: 0.0256\n",
            "Epoch [6902/20000], Training Loss: 0.0266\n",
            "Epoch [6903/20000], Training Loss: 0.0283\n",
            "Epoch [6904/20000], Training Loss: 0.0270\n",
            "Epoch [6905/20000], Training Loss: 0.0266\n",
            "Epoch [6906/20000], Training Loss: 0.0248\n",
            "Epoch [6907/20000], Training Loss: 0.0253\n",
            "Epoch [6908/20000], Training Loss: 0.0280\n",
            "Epoch [6909/20000], Training Loss: 0.0258\n",
            "Epoch [6910/20000], Training Loss: 0.0245\n",
            "Epoch [6911/20000], Training Loss: 0.0258\n",
            "Epoch [6912/20000], Training Loss: 0.0267\n",
            "Epoch [6913/20000], Training Loss: 0.0270\n",
            "Epoch [6914/20000], Training Loss: 0.0262\n",
            "Epoch [6915/20000], Training Loss: 0.0261\n",
            "Epoch [6916/20000], Training Loss: 0.0260\n",
            "Epoch [6917/20000], Training Loss: 0.0262\n",
            "Epoch [6918/20000], Training Loss: 0.0269\n",
            "Epoch [6919/20000], Training Loss: 0.0250\n",
            "Epoch [6920/20000], Training Loss: 0.0287\n",
            "Epoch [6921/20000], Training Loss: 0.0261\n",
            "Epoch [6922/20000], Training Loss: 0.0264\n",
            "Epoch [6923/20000], Training Loss: 0.0263\n",
            "Epoch [6924/20000], Training Loss: 0.0254\n",
            "Epoch [6925/20000], Training Loss: 0.0263\n",
            "Epoch [6926/20000], Training Loss: 0.0277\n",
            "Epoch [6927/20000], Training Loss: 0.0279\n",
            "Epoch [6928/20000], Training Loss: 0.0245\n",
            "Epoch [6929/20000], Training Loss: 0.0265\n",
            "Epoch [6930/20000], Training Loss: 0.0281\n",
            "Epoch [6931/20000], Training Loss: 0.0261\n",
            "Epoch [6932/20000], Training Loss: 0.0251\n",
            "Epoch [6933/20000], Training Loss: 0.0262\n",
            "Epoch [6934/20000], Training Loss: 0.0270\n",
            "Epoch [6935/20000], Training Loss: 0.0271\n",
            "Epoch [6936/20000], Training Loss: 0.0284\n",
            "Epoch [6937/20000], Training Loss: 0.0270\n",
            "Epoch [6938/20000], Training Loss: 0.0258\n",
            "Epoch [6939/20000], Training Loss: 0.0263\n",
            "Epoch [6940/20000], Training Loss: 0.0277\n",
            "Epoch [6941/20000], Training Loss: 0.0288\n",
            "Epoch [6942/20000], Training Loss: 0.0268\n",
            "Epoch [6943/20000], Training Loss: 0.0247\n",
            "Epoch [6944/20000], Training Loss: 0.0254\n",
            "Epoch [6945/20000], Training Loss: 0.0263\n",
            "Epoch [6946/20000], Training Loss: 0.0265\n",
            "Epoch [6947/20000], Training Loss: 0.0273\n",
            "Epoch [6948/20000], Training Loss: 0.0276\n",
            "Epoch [6949/20000], Training Loss: 0.0249\n",
            "Epoch [6950/20000], Training Loss: 0.0278\n",
            "Epoch [6951/20000], Training Loss: 0.0269\n",
            "Epoch [6952/20000], Training Loss: 0.0250\n",
            "Epoch [6953/20000], Training Loss: 0.0272\n",
            "Epoch [6954/20000], Training Loss: 0.0276\n",
            "Epoch [6955/20000], Training Loss: 0.0256\n",
            "Epoch [6956/20000], Training Loss: 0.0284\n",
            "Epoch [6957/20000], Training Loss: 0.0251\n",
            "Epoch [6958/20000], Training Loss: 0.0270\n",
            "Epoch [6959/20000], Training Loss: 0.0261\n",
            "Epoch [6960/20000], Training Loss: 0.0260\n",
            "Epoch [6961/20000], Training Loss: 0.0286\n",
            "Epoch [6962/20000], Training Loss: 0.0261\n",
            "Epoch [6963/20000], Training Loss: 0.0244\n",
            "Epoch [6964/20000], Training Loss: 0.0264\n",
            "Epoch [6965/20000], Training Loss: 0.0261\n",
            "Epoch [6966/20000], Training Loss: 0.0279\n",
            "Epoch [6967/20000], Training Loss: 0.0250\n",
            "Epoch [6968/20000], Training Loss: 0.0278\n",
            "Epoch [6969/20000], Training Loss: 0.0268\n",
            "Epoch [6970/20000], Training Loss: 0.0296\n",
            "Epoch [6971/20000], Training Loss: 0.0262\n",
            "Epoch [6972/20000], Training Loss: 0.0281\n",
            "Epoch [6973/20000], Training Loss: 0.0255\n",
            "Epoch [6974/20000], Training Loss: 0.0250\n",
            "Epoch [6975/20000], Training Loss: 0.0248\n",
            "Epoch [6976/20000], Training Loss: 0.0275\n",
            "Epoch [6977/20000], Training Loss: 0.0283\n",
            "Epoch [6978/20000], Training Loss: 0.0237\n",
            "Epoch [6979/20000], Training Loss: 0.0269\n",
            "Epoch [6980/20000], Training Loss: 0.0271\n",
            "Epoch [6981/20000], Training Loss: 0.0269\n",
            "Epoch [6982/20000], Training Loss: 0.0262\n",
            "Epoch [6983/20000], Training Loss: 0.0258\n",
            "Epoch [6984/20000], Training Loss: 0.0284\n",
            "Epoch [6985/20000], Training Loss: 0.0250\n",
            "Epoch [6986/20000], Training Loss: 0.0257\n",
            "Epoch [6987/20000], Training Loss: 0.0258\n",
            "Epoch [6988/20000], Training Loss: 0.0257\n",
            "Epoch [6989/20000], Training Loss: 0.0246\n",
            "Epoch [6990/20000], Training Loss: 0.0266\n",
            "Epoch [6991/20000], Training Loss: 0.0277\n",
            "Epoch [6992/20000], Training Loss: 0.0256\n",
            "Epoch [6993/20000], Training Loss: 0.0285\n",
            "Epoch [6994/20000], Training Loss: 0.0274\n",
            "Epoch [6995/20000], Training Loss: 0.0243\n",
            "Epoch [6996/20000], Training Loss: 0.0268\n",
            "Epoch [6997/20000], Training Loss: 0.0245\n",
            "Epoch [6998/20000], Training Loss: 0.0265\n",
            "Epoch [6999/20000], Training Loss: 0.0270\n",
            "Epoch [7000/20000], Training Loss: 0.0270\n",
            "Epoch [7001/20000], Training Loss: 0.0275\n",
            "Epoch [7002/20000], Training Loss: 0.0257\n",
            "Epoch [7003/20000], Training Loss: 0.0268\n",
            "Epoch [7004/20000], Training Loss: 0.0283\n",
            "Epoch [7005/20000], Training Loss: 0.0250\n",
            "Epoch [7006/20000], Training Loss: 0.0252\n",
            "Epoch [7007/20000], Training Loss: 0.0269\n",
            "Epoch [7008/20000], Training Loss: 0.0251\n",
            "Epoch [7009/20000], Training Loss: 0.0263\n",
            "Epoch [7010/20000], Training Loss: 0.0257\n",
            "Epoch [7011/20000], Training Loss: 0.0266\n",
            "Epoch [7012/20000], Training Loss: 0.0262\n",
            "Epoch [7013/20000], Training Loss: 0.0284\n",
            "Epoch [7014/20000], Training Loss: 0.0263\n",
            "Epoch [7015/20000], Training Loss: 0.0274\n",
            "Epoch [7016/20000], Training Loss: 0.0271\n",
            "Epoch [7017/20000], Training Loss: 0.0264\n",
            "Epoch [7018/20000], Training Loss: 0.0255\n",
            "Epoch [7019/20000], Training Loss: 0.0260\n",
            "Epoch [7020/20000], Training Loss: 0.0268\n",
            "Epoch [7021/20000], Training Loss: 0.0258\n",
            "Epoch [7022/20000], Training Loss: 0.0271\n",
            "Epoch [7023/20000], Training Loss: 0.0244\n",
            "Epoch [7024/20000], Training Loss: 0.0255\n",
            "Epoch [7025/20000], Training Loss: 0.0283\n",
            "Epoch [7026/20000], Training Loss: 0.0251\n",
            "Epoch [7027/20000], Training Loss: 0.0258\n",
            "Epoch [7028/20000], Training Loss: 0.0268\n",
            "Epoch [7029/20000], Training Loss: 0.0276\n",
            "Epoch [7030/20000], Training Loss: 0.0259\n",
            "Epoch [7031/20000], Training Loss: 0.0256\n",
            "Epoch [7032/20000], Training Loss: 0.0252\n",
            "Epoch [7033/20000], Training Loss: 0.0259\n",
            "Epoch [7034/20000], Training Loss: 0.0265\n",
            "Epoch [7035/20000], Training Loss: 0.0263\n",
            "Epoch [7036/20000], Training Loss: 0.0285\n",
            "Epoch [7037/20000], Training Loss: 0.0257\n",
            "Epoch [7038/20000], Training Loss: 0.0276\n",
            "Epoch [7039/20000], Training Loss: 0.0271\n",
            "Epoch [7040/20000], Training Loss: 0.0287\n",
            "Epoch [7041/20000], Training Loss: 0.0265\n",
            "Epoch [7042/20000], Training Loss: 0.0250\n",
            "Epoch [7043/20000], Training Loss: 0.0260\n",
            "Epoch [7044/20000], Training Loss: 0.0254\n",
            "Epoch [7045/20000], Training Loss: 0.0266\n",
            "Epoch [7046/20000], Training Loss: 0.0255\n",
            "Epoch [7047/20000], Training Loss: 0.0248\n",
            "Epoch [7048/20000], Training Loss: 0.0280\n",
            "Epoch [7049/20000], Training Loss: 0.0249\n",
            "Epoch [7050/20000], Training Loss: 0.0263\n",
            "Epoch [7051/20000], Training Loss: 0.0274\n",
            "Epoch [7052/20000], Training Loss: 0.0245\n",
            "Epoch [7053/20000], Training Loss: 0.0273\n",
            "Epoch [7054/20000], Training Loss: 0.0269\n",
            "Epoch [7055/20000], Training Loss: 0.0259\n",
            "Epoch [7056/20000], Training Loss: 0.0289\n",
            "Epoch [7057/20000], Training Loss: 0.0250\n",
            "Epoch [7058/20000], Training Loss: 0.0276\n",
            "Epoch [7059/20000], Training Loss: 0.0290\n",
            "Epoch [7060/20000], Training Loss: 0.0272\n",
            "Epoch [7061/20000], Training Loss: 0.0278\n",
            "Epoch [7062/20000], Training Loss: 0.0268\n",
            "Epoch [7063/20000], Training Loss: 0.0253\n",
            "Epoch [7064/20000], Training Loss: 0.0258\n",
            "Epoch [7065/20000], Training Loss: 0.0269\n",
            "Epoch [7066/20000], Training Loss: 0.0252\n",
            "Epoch [7067/20000], Training Loss: 0.0263\n",
            "Epoch [7068/20000], Training Loss: 0.0258\n",
            "Epoch [7069/20000], Training Loss: 0.0255\n",
            "Epoch [7070/20000], Training Loss: 0.0282\n",
            "Epoch [7071/20000], Training Loss: 0.0254\n",
            "Epoch [7072/20000], Training Loss: 0.0284\n",
            "Epoch [7073/20000], Training Loss: 0.0258\n",
            "Epoch [7074/20000], Training Loss: 0.0248\n",
            "Epoch [7075/20000], Training Loss: 0.0259\n",
            "Epoch [7076/20000], Training Loss: 0.0264\n",
            "Epoch [7077/20000], Training Loss: 0.0262\n",
            "Epoch [7078/20000], Training Loss: 0.0272\n",
            "Epoch [7079/20000], Training Loss: 0.0283\n",
            "Epoch [7080/20000], Training Loss: 0.0265\n",
            "Epoch [7081/20000], Training Loss: 0.0276\n",
            "Epoch [7082/20000], Training Loss: 0.0284\n",
            "Epoch [7083/20000], Training Loss: 0.0248\n",
            "Epoch [7084/20000], Training Loss: 0.0276\n",
            "Epoch [7085/20000], Training Loss: 0.0273\n",
            "Epoch [7086/20000], Training Loss: 0.0258\n",
            "Epoch [7087/20000], Training Loss: 0.0268\n",
            "Epoch [7088/20000], Training Loss: 0.0240\n",
            "Epoch [7089/20000], Training Loss: 0.0255\n",
            "Epoch [7090/20000], Training Loss: 0.0254\n",
            "Epoch [7091/20000], Training Loss: 0.0268\n",
            "Epoch [7092/20000], Training Loss: 0.0271\n",
            "Epoch [7093/20000], Training Loss: 0.0272\n",
            "Epoch [7094/20000], Training Loss: 0.0265\n",
            "Epoch [7095/20000], Training Loss: 0.0264\n",
            "Epoch [7096/20000], Training Loss: 0.0290\n",
            "Epoch [7097/20000], Training Loss: 0.0253\n",
            "Epoch [7098/20000], Training Loss: 0.0258\n",
            "Epoch [7099/20000], Training Loss: 0.0254\n",
            "Epoch [7100/20000], Training Loss: 0.0274\n",
            "Epoch [7101/20000], Training Loss: 0.0276\n",
            "Epoch [7102/20000], Training Loss: 0.0259\n",
            "Epoch [7103/20000], Training Loss: 0.0254\n",
            "Epoch [7104/20000], Training Loss: 0.0265\n",
            "Epoch [7105/20000], Training Loss: 0.0235\n",
            "Epoch [7106/20000], Training Loss: 0.0259\n",
            "Epoch [7107/20000], Training Loss: 0.0260\n",
            "Epoch [7108/20000], Training Loss: 0.0262\n",
            "Epoch [7109/20000], Training Loss: 0.0270\n",
            "Epoch [7110/20000], Training Loss: 0.0268\n",
            "Epoch [7111/20000], Training Loss: 0.0245\n",
            "Epoch [7112/20000], Training Loss: 0.0258\n",
            "Epoch [7113/20000], Training Loss: 0.0260\n",
            "Epoch [7114/20000], Training Loss: 0.0245\n",
            "Epoch [7115/20000], Training Loss: 0.0258\n",
            "Epoch [7116/20000], Training Loss: 0.0252\n",
            "Epoch [7117/20000], Training Loss: 0.0251\n",
            "Epoch [7118/20000], Training Loss: 0.0264\n",
            "Epoch [7119/20000], Training Loss: 0.0253\n",
            "Epoch [7120/20000], Training Loss: 0.0282\n",
            "Epoch [7121/20000], Training Loss: 0.0271\n",
            "Epoch [7122/20000], Training Loss: 0.0264\n",
            "Epoch [7123/20000], Training Loss: 0.0262\n",
            "Epoch [7124/20000], Training Loss: 0.0250\n",
            "Epoch [7125/20000], Training Loss: 0.0283\n",
            "Epoch [7126/20000], Training Loss: 0.0250\n",
            "Epoch [7127/20000], Training Loss: 0.0279\n",
            "Epoch [7128/20000], Training Loss: 0.0267\n",
            "Epoch [7129/20000], Training Loss: 0.0281\n",
            "Epoch [7130/20000], Training Loss: 0.0280\n",
            "Epoch [7131/20000], Training Loss: 0.0271\n",
            "Epoch [7132/20000], Training Loss: 0.0278\n",
            "Epoch [7133/20000], Training Loss: 0.0267\n",
            "Epoch [7134/20000], Training Loss: 0.0275\n",
            "Epoch [7135/20000], Training Loss: 0.0268\n",
            "Epoch [7136/20000], Training Loss: 0.0284\n",
            "Epoch [7137/20000], Training Loss: 0.0278\n",
            "Epoch [7138/20000], Training Loss: 0.0274\n",
            "Epoch [7139/20000], Training Loss: 0.0264\n",
            "Epoch [7140/20000], Training Loss: 0.0265\n",
            "Epoch [7141/20000], Training Loss: 0.0288\n",
            "Epoch [7142/20000], Training Loss: 0.0275\n",
            "Epoch [7143/20000], Training Loss: 0.0257\n",
            "Epoch [7144/20000], Training Loss: 0.0253\n",
            "Epoch [7145/20000], Training Loss: 0.0269\n",
            "Epoch [7146/20000], Training Loss: 0.0265\n",
            "Epoch [7147/20000], Training Loss: 0.0263\n",
            "Epoch [7148/20000], Training Loss: 0.0256\n",
            "Epoch [7149/20000], Training Loss: 0.0291\n",
            "Epoch [7150/20000], Training Loss: 0.0268\n",
            "Epoch [7151/20000], Training Loss: 0.0245\n",
            "Epoch [7152/20000], Training Loss: 0.0254\n",
            "Epoch [7153/20000], Training Loss: 0.0269\n",
            "Epoch [7154/20000], Training Loss: 0.0262\n",
            "Epoch [7155/20000], Training Loss: 0.0263\n",
            "Epoch [7156/20000], Training Loss: 0.0273\n",
            "Epoch [7157/20000], Training Loss: 0.0271\n",
            "Epoch [7158/20000], Training Loss: 0.0264\n",
            "Epoch [7159/20000], Training Loss: 0.0239\n",
            "Epoch [7160/20000], Training Loss: 0.0259\n",
            "Epoch [7161/20000], Training Loss: 0.0256\n",
            "Epoch [7162/20000], Training Loss: 0.0242\n",
            "Epoch [7163/20000], Training Loss: 0.0266\n",
            "Epoch [7164/20000], Training Loss: 0.0259\n",
            "Epoch [7165/20000], Training Loss: 0.0263\n",
            "Epoch [7166/20000], Training Loss: 0.0270\n",
            "Epoch [7167/20000], Training Loss: 0.0248\n",
            "Epoch [7168/20000], Training Loss: 0.0273\n",
            "Epoch [7169/20000], Training Loss: 0.0275\n",
            "Epoch [7170/20000], Training Loss: 0.0266\n",
            "Epoch [7171/20000], Training Loss: 0.0264\n",
            "Epoch [7172/20000], Training Loss: 0.0264\n",
            "Epoch [7173/20000], Training Loss: 0.0270\n",
            "Epoch [7174/20000], Training Loss: 0.0268\n",
            "Epoch [7175/20000], Training Loss: 0.0272\n",
            "Epoch [7176/20000], Training Loss: 0.0263\n",
            "Epoch [7177/20000], Training Loss: 0.0265\n",
            "Epoch [7178/20000], Training Loss: 0.0281\n",
            "Epoch [7179/20000], Training Loss: 0.0273\n",
            "Epoch [7180/20000], Training Loss: 0.0276\n",
            "Epoch [7181/20000], Training Loss: 0.0272\n",
            "Epoch [7182/20000], Training Loss: 0.0273\n",
            "Epoch [7183/20000], Training Loss: 0.0247\n",
            "Epoch [7184/20000], Training Loss: 0.0264\n",
            "Epoch [7185/20000], Training Loss: 0.0279\n",
            "Epoch [7186/20000], Training Loss: 0.0264\n",
            "Epoch [7187/20000], Training Loss: 0.0264\n",
            "Epoch [7188/20000], Training Loss: 0.0275\n",
            "Epoch [7189/20000], Training Loss: 0.0267\n",
            "Epoch [7190/20000], Training Loss: 0.0274\n",
            "Epoch [7191/20000], Training Loss: 0.0274\n",
            "Epoch [7192/20000], Training Loss: 0.0273\n",
            "Epoch [7193/20000], Training Loss: 0.0268\n",
            "Epoch [7194/20000], Training Loss: 0.0257\n",
            "Epoch [7195/20000], Training Loss: 0.0252\n",
            "Epoch [7196/20000], Training Loss: 0.0273\n",
            "Epoch [7197/20000], Training Loss: 0.0269\n",
            "Epoch [7198/20000], Training Loss: 0.0272\n",
            "Epoch [7199/20000], Training Loss: 0.0265\n",
            "Epoch [7200/20000], Training Loss: 0.0258\n",
            "Epoch [7201/20000], Training Loss: 0.0282\n",
            "Epoch [7202/20000], Training Loss: 0.0264\n",
            "Epoch [7203/20000], Training Loss: 0.0248\n",
            "Epoch [7204/20000], Training Loss: 0.0239\n",
            "Epoch [7205/20000], Training Loss: 0.0277\n",
            "Epoch [7206/20000], Training Loss: 0.0273\n",
            "Epoch [7207/20000], Training Loss: 0.0276\n",
            "Epoch [7208/20000], Training Loss: 0.0270\n",
            "Epoch [7209/20000], Training Loss: 0.0251\n",
            "Epoch [7210/20000], Training Loss: 0.0265\n",
            "Epoch [7211/20000], Training Loss: 0.0269\n",
            "Epoch [7212/20000], Training Loss: 0.0266\n",
            "Epoch [7213/20000], Training Loss: 0.0277\n",
            "Epoch [7214/20000], Training Loss: 0.0251\n",
            "Epoch [7215/20000], Training Loss: 0.0241\n",
            "Epoch [7216/20000], Training Loss: 0.0256\n",
            "Epoch [7217/20000], Training Loss: 0.0268\n",
            "Epoch [7218/20000], Training Loss: 0.0285\n",
            "Epoch [7219/20000], Training Loss: 0.0293\n",
            "Epoch [7220/20000], Training Loss: 0.0270\n",
            "Epoch [7221/20000], Training Loss: 0.0278\n",
            "Epoch [7222/20000], Training Loss: 0.0264\n",
            "Epoch [7223/20000], Training Loss: 0.0275\n",
            "Epoch [7224/20000], Training Loss: 0.0287\n",
            "Epoch [7225/20000], Training Loss: 0.0268\n",
            "Epoch [7226/20000], Training Loss: 0.0287\n",
            "Epoch [7227/20000], Training Loss: 0.0276\n",
            "Epoch [7228/20000], Training Loss: 0.0254\n",
            "Epoch [7229/20000], Training Loss: 0.0251\n",
            "Epoch [7230/20000], Training Loss: 0.0245\n",
            "Epoch [7231/20000], Training Loss: 0.0273\n",
            "Epoch [7232/20000], Training Loss: 0.0251\n",
            "Epoch [7233/20000], Training Loss: 0.0264\n",
            "Epoch [7234/20000], Training Loss: 0.0261\n",
            "Epoch [7235/20000], Training Loss: 0.0260\n",
            "Epoch [7236/20000], Training Loss: 0.0278\n",
            "Epoch [7237/20000], Training Loss: 0.0259\n",
            "Epoch [7238/20000], Training Loss: 0.0257\n",
            "Epoch [7239/20000], Training Loss: 0.0264\n",
            "Epoch [7240/20000], Training Loss: 0.0258\n",
            "Epoch [7241/20000], Training Loss: 0.0262\n",
            "Epoch [7242/20000], Training Loss: 0.0306\n",
            "Epoch [7243/20000], Training Loss: 0.0249\n",
            "Epoch [7244/20000], Training Loss: 0.0280\n",
            "Epoch [7245/20000], Training Loss: 0.0263\n",
            "Epoch [7246/20000], Training Loss: 0.0265\n",
            "Epoch [7247/20000], Training Loss: 0.0267\n",
            "Epoch [7248/20000], Training Loss: 0.0248\n",
            "Epoch [7249/20000], Training Loss: 0.0274\n",
            "Epoch [7250/20000], Training Loss: 0.0267\n",
            "Epoch [7251/20000], Training Loss: 0.0256\n",
            "Epoch [7252/20000], Training Loss: 0.0251\n",
            "Epoch [7253/20000], Training Loss: 0.0269\n",
            "Epoch [7254/20000], Training Loss: 0.0271\n",
            "Epoch [7255/20000], Training Loss: 0.0254\n",
            "Epoch [7256/20000], Training Loss: 0.0261\n",
            "Epoch [7257/20000], Training Loss: 0.0249\n",
            "Epoch [7258/20000], Training Loss: 0.0245\n",
            "Epoch [7259/20000], Training Loss: 0.0256\n",
            "Epoch [7260/20000], Training Loss: 0.0267\n",
            "Epoch [7261/20000], Training Loss: 0.0277\n",
            "Epoch [7262/20000], Training Loss: 0.0262\n",
            "Epoch [7263/20000], Training Loss: 0.0272\n",
            "Epoch [7264/20000], Training Loss: 0.0262\n",
            "Epoch [7265/20000], Training Loss: 0.0279\n",
            "Epoch [7266/20000], Training Loss: 0.0284\n",
            "Epoch [7267/20000], Training Loss: 0.0272\n",
            "Epoch [7268/20000], Training Loss: 0.0281\n",
            "Epoch [7269/20000], Training Loss: 0.0248\n",
            "Epoch [7270/20000], Training Loss: 0.0267\n",
            "Epoch [7271/20000], Training Loss: 0.0248\n",
            "Epoch [7272/20000], Training Loss: 0.0269\n",
            "Epoch [7273/20000], Training Loss: 0.0249\n",
            "Epoch [7274/20000], Training Loss: 0.0275\n",
            "Epoch [7275/20000], Training Loss: 0.0245\n",
            "Epoch [7276/20000], Training Loss: 0.0273\n",
            "Epoch [7277/20000], Training Loss: 0.0286\n",
            "Epoch [7278/20000], Training Loss: 0.0259\n",
            "Epoch [7279/20000], Training Loss: 0.0260\n",
            "Epoch [7280/20000], Training Loss: 0.0273\n",
            "Epoch [7281/20000], Training Loss: 0.0278\n",
            "Epoch [7282/20000], Training Loss: 0.0251\n",
            "Epoch [7283/20000], Training Loss: 0.0269\n",
            "Epoch [7284/20000], Training Loss: 0.0238\n",
            "Epoch [7285/20000], Training Loss: 0.0256\n",
            "Epoch [7286/20000], Training Loss: 0.0242\n",
            "Epoch [7287/20000], Training Loss: 0.0266\n",
            "Epoch [7288/20000], Training Loss: 0.0275\n",
            "Epoch [7289/20000], Training Loss: 0.0251\n",
            "Epoch [7290/20000], Training Loss: 0.0273\n",
            "Epoch [7291/20000], Training Loss: 0.0268\n",
            "Epoch [7292/20000], Training Loss: 0.0270\n",
            "Epoch [7293/20000], Training Loss: 0.0273\n",
            "Epoch [7294/20000], Training Loss: 0.0236\n",
            "Epoch [7295/20000], Training Loss: 0.0251\n",
            "Epoch [7296/20000], Training Loss: 0.0284\n",
            "Epoch [7297/20000], Training Loss: 0.0276\n",
            "Epoch [7298/20000], Training Loss: 0.0249\n",
            "Epoch [7299/20000], Training Loss: 0.0269\n",
            "Epoch [7300/20000], Training Loss: 0.0272\n",
            "Epoch [7301/20000], Training Loss: 0.0254\n",
            "Epoch [7302/20000], Training Loss: 0.0263\n",
            "Epoch [7303/20000], Training Loss: 0.0253\n",
            "Epoch [7304/20000], Training Loss: 0.0257\n",
            "Epoch [7305/20000], Training Loss: 0.0285\n",
            "Epoch [7306/20000], Training Loss: 0.0273\n",
            "Epoch [7307/20000], Training Loss: 0.0267\n",
            "Epoch [7308/20000], Training Loss: 0.0257\n",
            "Epoch [7309/20000], Training Loss: 0.0247\n",
            "Epoch [7310/20000], Training Loss: 0.0271\n",
            "Epoch [7311/20000], Training Loss: 0.0260\n",
            "Epoch [7312/20000], Training Loss: 0.0265\n",
            "Epoch [7313/20000], Training Loss: 0.0278\n",
            "Epoch [7314/20000], Training Loss: 0.0284\n",
            "Epoch [7315/20000], Training Loss: 0.0286\n",
            "Epoch [7316/20000], Training Loss: 0.0275\n",
            "Epoch [7317/20000], Training Loss: 0.0278\n",
            "Epoch [7318/20000], Training Loss: 0.0284\n",
            "Epoch [7319/20000], Training Loss: 0.0286\n",
            "Epoch [7320/20000], Training Loss: 0.0249\n",
            "Epoch [7321/20000], Training Loss: 0.0255\n",
            "Epoch [7322/20000], Training Loss: 0.0262\n",
            "Epoch [7323/20000], Training Loss: 0.0272\n",
            "Epoch [7324/20000], Training Loss: 0.0267\n",
            "Epoch [7325/20000], Training Loss: 0.0253\n",
            "Epoch [7326/20000], Training Loss: 0.0258\n",
            "Epoch [7327/20000], Training Loss: 0.0286\n",
            "Epoch [7328/20000], Training Loss: 0.0252\n",
            "Epoch [7329/20000], Training Loss: 0.0248\n",
            "Epoch [7330/20000], Training Loss: 0.0278\n",
            "Epoch [7331/20000], Training Loss: 0.0266\n",
            "Epoch [7332/20000], Training Loss: 0.0278\n",
            "Epoch [7333/20000], Training Loss: 0.0272\n",
            "Epoch [7334/20000], Training Loss: 0.0265\n",
            "Epoch [7335/20000], Training Loss: 0.0242\n",
            "Epoch [7336/20000], Training Loss: 0.0266\n",
            "Epoch [7337/20000], Training Loss: 0.0277\n",
            "Epoch [7338/20000], Training Loss: 0.0274\n",
            "Epoch [7339/20000], Training Loss: 0.0257\n",
            "Epoch [7340/20000], Training Loss: 0.0264\n",
            "Epoch [7341/20000], Training Loss: 0.0265\n",
            "Epoch [7342/20000], Training Loss: 0.0248\n",
            "Epoch [7343/20000], Training Loss: 0.0267\n",
            "Epoch [7344/20000], Training Loss: 0.0273\n",
            "Epoch [7345/20000], Training Loss: 0.0248\n",
            "Epoch [7346/20000], Training Loss: 0.0272\n",
            "Epoch [7347/20000], Training Loss: 0.0277\n",
            "Epoch [7348/20000], Training Loss: 0.0264\n",
            "Epoch [7349/20000], Training Loss: 0.0256\n",
            "Epoch [7350/20000], Training Loss: 0.0262\n",
            "Epoch [7351/20000], Training Loss: 0.0252\n",
            "Epoch [7352/20000], Training Loss: 0.0295\n",
            "Epoch [7353/20000], Training Loss: 0.0257\n",
            "Epoch [7354/20000], Training Loss: 0.0253\n",
            "Epoch [7355/20000], Training Loss: 0.0249\n",
            "Epoch [7356/20000], Training Loss: 0.0264\n",
            "Epoch [7357/20000], Training Loss: 0.0289\n",
            "Epoch [7358/20000], Training Loss: 0.0262\n",
            "Epoch [7359/20000], Training Loss: 0.0248\n",
            "Epoch [7360/20000], Training Loss: 0.0287\n",
            "Epoch [7361/20000], Training Loss: 0.0266\n",
            "Epoch [7362/20000], Training Loss: 0.0271\n",
            "Epoch [7363/20000], Training Loss: 0.0286\n",
            "Epoch [7364/20000], Training Loss: 0.0264\n",
            "Epoch [7365/20000], Training Loss: 0.0261\n",
            "Epoch [7366/20000], Training Loss: 0.0251\n",
            "Epoch [7367/20000], Training Loss: 0.0267\n",
            "Epoch [7368/20000], Training Loss: 0.0254\n",
            "Epoch [7369/20000], Training Loss: 0.0260\n",
            "Epoch [7370/20000], Training Loss: 0.0268\n",
            "Epoch [7371/20000], Training Loss: 0.0271\n",
            "Epoch [7372/20000], Training Loss: 0.0264\n",
            "Epoch [7373/20000], Training Loss: 0.0273\n",
            "Epoch [7374/20000], Training Loss: 0.0247\n",
            "Epoch [7375/20000], Training Loss: 0.0287\n",
            "Epoch [7376/20000], Training Loss: 0.0260\n",
            "Epoch [7377/20000], Training Loss: 0.0255\n",
            "Epoch [7378/20000], Training Loss: 0.0259\n",
            "Epoch [7379/20000], Training Loss: 0.0245\n",
            "Epoch [7380/20000], Training Loss: 0.0267\n",
            "Epoch [7381/20000], Training Loss: 0.0267\n",
            "Epoch [7382/20000], Training Loss: 0.0281\n",
            "Epoch [7383/20000], Training Loss: 0.0240\n",
            "Epoch [7384/20000], Training Loss: 0.0266\n",
            "Epoch [7385/20000], Training Loss: 0.0284\n",
            "Epoch [7386/20000], Training Loss: 0.0260\n",
            "Epoch [7387/20000], Training Loss: 0.0274\n",
            "Epoch [7388/20000], Training Loss: 0.0257\n",
            "Epoch [7389/20000], Training Loss: 0.0274\n",
            "Epoch [7390/20000], Training Loss: 0.0262\n",
            "Epoch [7391/20000], Training Loss: 0.0268\n",
            "Epoch [7392/20000], Training Loss: 0.0258\n",
            "Epoch [7393/20000], Training Loss: 0.0282\n",
            "Epoch [7394/20000], Training Loss: 0.0277\n",
            "Epoch [7395/20000], Training Loss: 0.0272\n",
            "Epoch [7396/20000], Training Loss: 0.0274\n",
            "Epoch [7397/20000], Training Loss: 0.0271\n",
            "Epoch [7398/20000], Training Loss: 0.0288\n",
            "Epoch [7399/20000], Training Loss: 0.0252\n",
            "Epoch [7400/20000], Training Loss: 0.0252\n",
            "Epoch [7401/20000], Training Loss: 0.0259\n",
            "Epoch [7402/20000], Training Loss: 0.0249\n",
            "Epoch [7403/20000], Training Loss: 0.0262\n",
            "Epoch [7404/20000], Training Loss: 0.0283\n",
            "Epoch [7405/20000], Training Loss: 0.0262\n",
            "Epoch [7406/20000], Training Loss: 0.0256\n",
            "Epoch [7407/20000], Training Loss: 0.0260\n",
            "Epoch [7408/20000], Training Loss: 0.0271\n",
            "Epoch [7409/20000], Training Loss: 0.0253\n",
            "Epoch [7410/20000], Training Loss: 0.0272\n",
            "Epoch [7411/20000], Training Loss: 0.0278\n",
            "Epoch [7412/20000], Training Loss: 0.0285\n",
            "Epoch [7413/20000], Training Loss: 0.0259\n",
            "Epoch [7414/20000], Training Loss: 0.0280\n",
            "Epoch [7415/20000], Training Loss: 0.0270\n",
            "Epoch [7416/20000], Training Loss: 0.0271\n",
            "Epoch [7417/20000], Training Loss: 0.0266\n",
            "Epoch [7418/20000], Training Loss: 0.0262\n",
            "Epoch [7419/20000], Training Loss: 0.0261\n",
            "Epoch [7420/20000], Training Loss: 0.0282\n",
            "Epoch [7421/20000], Training Loss: 0.0255\n",
            "Epoch [7422/20000], Training Loss: 0.0261\n",
            "Epoch [7423/20000], Training Loss: 0.0266\n",
            "Epoch [7424/20000], Training Loss: 0.0249\n",
            "Epoch [7425/20000], Training Loss: 0.0275\n",
            "Epoch [7426/20000], Training Loss: 0.0273\n",
            "Epoch [7427/20000], Training Loss: 0.0251\n",
            "Epoch [7428/20000], Training Loss: 0.0277\n",
            "Epoch [7429/20000], Training Loss: 0.0268\n",
            "Epoch [7430/20000], Training Loss: 0.0290\n",
            "Epoch [7431/20000], Training Loss: 0.0281\n",
            "Epoch [7432/20000], Training Loss: 0.0262\n",
            "Epoch [7433/20000], Training Loss: 0.0244\n",
            "Epoch [7434/20000], Training Loss: 0.0240\n",
            "Epoch [7435/20000], Training Loss: 0.0278\n",
            "Epoch [7436/20000], Training Loss: 0.0266\n",
            "Epoch [7437/20000], Training Loss: 0.0275\n",
            "Epoch [7438/20000], Training Loss: 0.0257\n",
            "Epoch [7439/20000], Training Loss: 0.0270\n",
            "Epoch [7440/20000], Training Loss: 0.0267\n",
            "Epoch [7441/20000], Training Loss: 0.0262\n",
            "Epoch [7442/20000], Training Loss: 0.0265\n",
            "Epoch [7443/20000], Training Loss: 0.0251\n",
            "Epoch [7444/20000], Training Loss: 0.0257\n",
            "Epoch [7445/20000], Training Loss: 0.0280\n",
            "Epoch [7446/20000], Training Loss: 0.0268\n",
            "Epoch [7447/20000], Training Loss: 0.0253\n",
            "Epoch [7448/20000], Training Loss: 0.0276\n",
            "Epoch [7449/20000], Training Loss: 0.0250\n",
            "Epoch [7450/20000], Training Loss: 0.0246\n",
            "Epoch [7451/20000], Training Loss: 0.0252\n",
            "Epoch [7452/20000], Training Loss: 0.0257\n",
            "Epoch [7453/20000], Training Loss: 0.0261\n",
            "Epoch [7454/20000], Training Loss: 0.0275\n",
            "Epoch [7455/20000], Training Loss: 0.0244\n",
            "Epoch [7456/20000], Training Loss: 0.0272\n",
            "Epoch [7457/20000], Training Loss: 0.0259\n",
            "Epoch [7458/20000], Training Loss: 0.0260\n",
            "Epoch [7459/20000], Training Loss: 0.0249\n",
            "Epoch [7460/20000], Training Loss: 0.0250\n",
            "Epoch [7461/20000], Training Loss: 0.0274\n",
            "Epoch [7462/20000], Training Loss: 0.0274\n",
            "Epoch [7463/20000], Training Loss: 0.0263\n",
            "Epoch [7464/20000], Training Loss: 0.0268\n",
            "Epoch [7465/20000], Training Loss: 0.0276\n",
            "Epoch [7466/20000], Training Loss: 0.0253\n",
            "Epoch [7467/20000], Training Loss: 0.0276\n",
            "Epoch [7468/20000], Training Loss: 0.0257\n",
            "Epoch [7469/20000], Training Loss: 0.0280\n",
            "Epoch [7470/20000], Training Loss: 0.0267\n",
            "Epoch [7471/20000], Training Loss: 0.0271\n",
            "Epoch [7472/20000], Training Loss: 0.0252\n",
            "Epoch [7473/20000], Training Loss: 0.0275\n",
            "Epoch [7474/20000], Training Loss: 0.0265\n",
            "Epoch [7475/20000], Training Loss: 0.0261\n",
            "Epoch [7476/20000], Training Loss: 0.0257\n",
            "Epoch [7477/20000], Training Loss: 0.0269\n",
            "Epoch [7478/20000], Training Loss: 0.0247\n",
            "Epoch [7479/20000], Training Loss: 0.0243\n",
            "Epoch [7480/20000], Training Loss: 0.0265\n",
            "Epoch [7481/20000], Training Loss: 0.0258\n",
            "Epoch [7482/20000], Training Loss: 0.0278\n",
            "Epoch [7483/20000], Training Loss: 0.0253\n",
            "Epoch [7484/20000], Training Loss: 0.0242\n",
            "Epoch [7485/20000], Training Loss: 0.0277\n",
            "Epoch [7486/20000], Training Loss: 0.0269\n",
            "Epoch [7487/20000], Training Loss: 0.0291\n",
            "Epoch [7488/20000], Training Loss: 0.0255\n",
            "Epoch [7489/20000], Training Loss: 0.0264\n",
            "Epoch [7490/20000], Training Loss: 0.0273\n",
            "Epoch [7491/20000], Training Loss: 0.0274\n",
            "Epoch [7492/20000], Training Loss: 0.0264\n",
            "Epoch [7493/20000], Training Loss: 0.0263\n",
            "Epoch [7494/20000], Training Loss: 0.0264\n",
            "Epoch [7495/20000], Training Loss: 0.0255\n",
            "Epoch [7496/20000], Training Loss: 0.0295\n",
            "Epoch [7497/20000], Training Loss: 0.0265\n",
            "Epoch [7498/20000], Training Loss: 0.0254\n",
            "Epoch [7499/20000], Training Loss: 0.0260\n",
            "Epoch [7500/20000], Training Loss: 0.0256\n",
            "Epoch [7501/20000], Training Loss: 0.0246\n",
            "Epoch [7502/20000], Training Loss: 0.0269\n",
            "Epoch [7503/20000], Training Loss: 0.0269\n",
            "Epoch [7504/20000], Training Loss: 0.0260\n",
            "Epoch [7505/20000], Training Loss: 0.0278\n",
            "Epoch [7506/20000], Training Loss: 0.0259\n",
            "Epoch [7507/20000], Training Loss: 0.0266\n",
            "Epoch [7508/20000], Training Loss: 0.0264\n",
            "Epoch [7509/20000], Training Loss: 0.0274\n",
            "Epoch [7510/20000], Training Loss: 0.0242\n",
            "Epoch [7511/20000], Training Loss: 0.0270\n",
            "Epoch [7512/20000], Training Loss: 0.0264\n",
            "Epoch [7513/20000], Training Loss: 0.0248\n",
            "Epoch [7514/20000], Training Loss: 0.0267\n",
            "Epoch [7515/20000], Training Loss: 0.0258\n",
            "Epoch [7516/20000], Training Loss: 0.0292\n",
            "Epoch [7517/20000], Training Loss: 0.0276\n",
            "Epoch [7518/20000], Training Loss: 0.0271\n",
            "Epoch [7519/20000], Training Loss: 0.0262\n",
            "Epoch [7520/20000], Training Loss: 0.0269\n",
            "Epoch [7521/20000], Training Loss: 0.0305\n",
            "Epoch [7522/20000], Training Loss: 0.0247\n",
            "Epoch [7523/20000], Training Loss: 0.0252\n",
            "Epoch [7524/20000], Training Loss: 0.0264\n",
            "Epoch [7525/20000], Training Loss: 0.0271\n",
            "Epoch [7526/20000], Training Loss: 0.0266\n",
            "Epoch [7527/20000], Training Loss: 0.0263\n",
            "Epoch [7528/20000], Training Loss: 0.0274\n",
            "Epoch [7529/20000], Training Loss: 0.0300\n",
            "Epoch [7530/20000], Training Loss: 0.0278\n",
            "Epoch [7531/20000], Training Loss: 0.0257\n",
            "Epoch [7532/20000], Training Loss: 0.0263\n",
            "Epoch [7533/20000], Training Loss: 0.0276\n",
            "Epoch [7534/20000], Training Loss: 0.0262\n",
            "Epoch [7535/20000], Training Loss: 0.0278\n",
            "Epoch [7536/20000], Training Loss: 0.0241\n",
            "Epoch [7537/20000], Training Loss: 0.0281\n",
            "Epoch [7538/20000], Training Loss: 0.0250\n",
            "Epoch [7539/20000], Training Loss: 0.0268\n",
            "Epoch [7540/20000], Training Loss: 0.0251\n",
            "Epoch [7541/20000], Training Loss: 0.0250\n",
            "Epoch [7542/20000], Training Loss: 0.0255\n",
            "Epoch [7543/20000], Training Loss: 0.0283\n",
            "Epoch [7544/20000], Training Loss: 0.0274\n",
            "Epoch [7545/20000], Training Loss: 0.0275\n",
            "Epoch [7546/20000], Training Loss: 0.0253\n",
            "Epoch [7547/20000], Training Loss: 0.0257\n",
            "Epoch [7548/20000], Training Loss: 0.0265\n",
            "Epoch [7549/20000], Training Loss: 0.0259\n",
            "Epoch [7550/20000], Training Loss: 0.0253\n",
            "Epoch [7551/20000], Training Loss: 0.0257\n",
            "Epoch [7552/20000], Training Loss: 0.0271\n",
            "Epoch [7553/20000], Training Loss: 0.0279\n",
            "Epoch [7554/20000], Training Loss: 0.0264\n",
            "Epoch [7555/20000], Training Loss: 0.0248\n",
            "Epoch [7556/20000], Training Loss: 0.0260\n",
            "Epoch [7557/20000], Training Loss: 0.0253\n",
            "Epoch [7558/20000], Training Loss: 0.0265\n",
            "Epoch [7559/20000], Training Loss: 0.0254\n",
            "Epoch [7560/20000], Training Loss: 0.0267\n",
            "Epoch [7561/20000], Training Loss: 0.0258\n",
            "Epoch [7562/20000], Training Loss: 0.0260\n",
            "Epoch [7563/20000], Training Loss: 0.0250\n",
            "Epoch [7564/20000], Training Loss: 0.0254\n",
            "Epoch [7565/20000], Training Loss: 0.0270\n",
            "Epoch [7566/20000], Training Loss: 0.0290\n",
            "Epoch [7567/20000], Training Loss: 0.0257\n",
            "Epoch [7568/20000], Training Loss: 0.0264\n",
            "Epoch [7569/20000], Training Loss: 0.0262\n",
            "Epoch [7570/20000], Training Loss: 0.0284\n",
            "Epoch [7571/20000], Training Loss: 0.0275\n",
            "Epoch [7572/20000], Training Loss: 0.0266\n",
            "Epoch [7573/20000], Training Loss: 0.0270\n",
            "Epoch [7574/20000], Training Loss: 0.0261\n",
            "Epoch [7575/20000], Training Loss: 0.0262\n",
            "Epoch [7576/20000], Training Loss: 0.0273\n",
            "Epoch [7577/20000], Training Loss: 0.0259\n",
            "Epoch [7578/20000], Training Loss: 0.0281\n",
            "Epoch [7579/20000], Training Loss: 0.0252\n",
            "Epoch [7580/20000], Training Loss: 0.0276\n",
            "Epoch [7581/20000], Training Loss: 0.0269\n",
            "Epoch [7582/20000], Training Loss: 0.0250\n",
            "Epoch [7583/20000], Training Loss: 0.0279\n",
            "Epoch [7584/20000], Training Loss: 0.0263\n",
            "Epoch [7585/20000], Training Loss: 0.0267\n",
            "Epoch [7586/20000], Training Loss: 0.0257\n",
            "Epoch [7587/20000], Training Loss: 0.0269\n",
            "Epoch [7588/20000], Training Loss: 0.0263\n",
            "Epoch [7589/20000], Training Loss: 0.0248\n",
            "Epoch [7590/20000], Training Loss: 0.0274\n",
            "Epoch [7591/20000], Training Loss: 0.0257\n",
            "Epoch [7592/20000], Training Loss: 0.0252\n",
            "Epoch [7593/20000], Training Loss: 0.0259\n",
            "Epoch [7594/20000], Training Loss: 0.0287\n",
            "Epoch [7595/20000], Training Loss: 0.0247\n",
            "Epoch [7596/20000], Training Loss: 0.0273\n",
            "Epoch [7597/20000], Training Loss: 0.0272\n",
            "Epoch [7598/20000], Training Loss: 0.0255\n",
            "Epoch [7599/20000], Training Loss: 0.0281\n",
            "Epoch [7600/20000], Training Loss: 0.0247\n",
            "Epoch [7601/20000], Training Loss: 0.0278\n",
            "Epoch [7602/20000], Training Loss: 0.0261\n",
            "Epoch [7603/20000], Training Loss: 0.0255\n",
            "Epoch [7604/20000], Training Loss: 0.0263\n",
            "Epoch [7605/20000], Training Loss: 0.0255\n",
            "Epoch [7606/20000], Training Loss: 0.0269\n",
            "Epoch [7607/20000], Training Loss: 0.0253\n",
            "Epoch [7608/20000], Training Loss: 0.0272\n",
            "Epoch [7609/20000], Training Loss: 0.0259\n",
            "Epoch [7610/20000], Training Loss: 0.0273\n",
            "Epoch [7611/20000], Training Loss: 0.0277\n",
            "Epoch [7612/20000], Training Loss: 0.0278\n",
            "Epoch [7613/20000], Training Loss: 0.0260\n",
            "Epoch [7614/20000], Training Loss: 0.0242\n",
            "Epoch [7615/20000], Training Loss: 0.0259\n",
            "Epoch [7616/20000], Training Loss: 0.0277\n",
            "Epoch [7617/20000], Training Loss: 0.0256\n",
            "Epoch [7618/20000], Training Loss: 0.0282\n",
            "Epoch [7619/20000], Training Loss: 0.0280\n",
            "Epoch [7620/20000], Training Loss: 0.0285\n",
            "Epoch [7621/20000], Training Loss: 0.0271\n",
            "Epoch [7622/20000], Training Loss: 0.0268\n",
            "Epoch [7623/20000], Training Loss: 0.0268\n",
            "Epoch [7624/20000], Training Loss: 0.0246\n",
            "Epoch [7625/20000], Training Loss: 0.0259\n",
            "Epoch [7626/20000], Training Loss: 0.0256\n",
            "Epoch [7627/20000], Training Loss: 0.0272\n",
            "Epoch [7628/20000], Training Loss: 0.0281\n",
            "Epoch [7629/20000], Training Loss: 0.0258\n",
            "Epoch [7630/20000], Training Loss: 0.0279\n",
            "Epoch [7631/20000], Training Loss: 0.0268\n",
            "Epoch [7632/20000], Training Loss: 0.0272\n",
            "Epoch [7633/20000], Training Loss: 0.0270\n",
            "Epoch [7634/20000], Training Loss: 0.0275\n",
            "Epoch [7635/20000], Training Loss: 0.0282\n",
            "Epoch [7636/20000], Training Loss: 0.0244\n",
            "Epoch [7637/20000], Training Loss: 0.0247\n",
            "Epoch [7638/20000], Training Loss: 0.0253\n",
            "Epoch [7639/20000], Training Loss: 0.0271\n",
            "Epoch [7640/20000], Training Loss: 0.0259\n",
            "Epoch [7641/20000], Training Loss: 0.0241\n",
            "Epoch [7642/20000], Training Loss: 0.0248\n",
            "Epoch [7643/20000], Training Loss: 0.0272\n",
            "Epoch [7644/20000], Training Loss: 0.0281\n",
            "Epoch [7645/20000], Training Loss: 0.0273\n",
            "Epoch [7646/20000], Training Loss: 0.0253\n",
            "Epoch [7647/20000], Training Loss: 0.0239\n",
            "Epoch [7648/20000], Training Loss: 0.0264\n",
            "Epoch [7649/20000], Training Loss: 0.0252\n",
            "Epoch [7650/20000], Training Loss: 0.0266\n",
            "Epoch [7651/20000], Training Loss: 0.0276\n",
            "Epoch [7652/20000], Training Loss: 0.0281\n",
            "Epoch [7653/20000], Training Loss: 0.0261\n",
            "Epoch [7654/20000], Training Loss: 0.0272\n",
            "Epoch [7655/20000], Training Loss: 0.0252\n",
            "Epoch [7656/20000], Training Loss: 0.0269\n",
            "Epoch [7657/20000], Training Loss: 0.0286\n",
            "Epoch [7658/20000], Training Loss: 0.0272\n",
            "Epoch [7659/20000], Training Loss: 0.0280\n",
            "Epoch [7660/20000], Training Loss: 0.0277\n",
            "Epoch [7661/20000], Training Loss: 0.0254\n",
            "Epoch [7662/20000], Training Loss: 0.0264\n",
            "Epoch [7663/20000], Training Loss: 0.0275\n",
            "Epoch [7664/20000], Training Loss: 0.0270\n",
            "Epoch [7665/20000], Training Loss: 0.0263\n",
            "Epoch [7666/20000], Training Loss: 0.0253\n",
            "Epoch [7667/20000], Training Loss: 0.0280\n",
            "Epoch [7668/20000], Training Loss: 0.0254\n",
            "Epoch [7669/20000], Training Loss: 0.0279\n",
            "Epoch [7670/20000], Training Loss: 0.0262\n",
            "Epoch [7671/20000], Training Loss: 0.0262\n",
            "Epoch [7672/20000], Training Loss: 0.0249\n",
            "Epoch [7673/20000], Training Loss: 0.0273\n",
            "Epoch [7674/20000], Training Loss: 0.0257\n",
            "Epoch [7675/20000], Training Loss: 0.0271\n",
            "Epoch [7676/20000], Training Loss: 0.0253\n",
            "Epoch [7677/20000], Training Loss: 0.0252\n",
            "Epoch [7678/20000], Training Loss: 0.0267\n",
            "Epoch [7679/20000], Training Loss: 0.0262\n",
            "Epoch [7680/20000], Training Loss: 0.0253\n",
            "Epoch [7681/20000], Training Loss: 0.0260\n",
            "Epoch [7682/20000], Training Loss: 0.0268\n",
            "Epoch [7683/20000], Training Loss: 0.0252\n",
            "Epoch [7684/20000], Training Loss: 0.0263\n",
            "Epoch [7685/20000], Training Loss: 0.0264\n",
            "Epoch [7686/20000], Training Loss: 0.0262\n",
            "Epoch [7687/20000], Training Loss: 0.0250\n",
            "Epoch [7688/20000], Training Loss: 0.0275\n",
            "Epoch [7689/20000], Training Loss: 0.0269\n",
            "Epoch [7690/20000], Training Loss: 0.0274\n",
            "Epoch [7691/20000], Training Loss: 0.0269\n",
            "Epoch [7692/20000], Training Loss: 0.0264\n",
            "Epoch [7693/20000], Training Loss: 0.0264\n",
            "Epoch [7694/20000], Training Loss: 0.0249\n",
            "Epoch [7695/20000], Training Loss: 0.0282\n",
            "Epoch [7696/20000], Training Loss: 0.0254\n",
            "Epoch [7697/20000], Training Loss: 0.0269\n",
            "Epoch [7698/20000], Training Loss: 0.0285\n",
            "Epoch [7699/20000], Training Loss: 0.0255\n",
            "Epoch [7700/20000], Training Loss: 0.0281\n",
            "Epoch [7701/20000], Training Loss: 0.0251\n",
            "Epoch [7702/20000], Training Loss: 0.0279\n",
            "Epoch [7703/20000], Training Loss: 0.0268\n",
            "Epoch [7704/20000], Training Loss: 0.0261\n",
            "Epoch [7705/20000], Training Loss: 0.0287\n",
            "Epoch [7706/20000], Training Loss: 0.0267\n",
            "Epoch [7707/20000], Training Loss: 0.0254\n",
            "Epoch [7708/20000], Training Loss: 0.0264\n",
            "Epoch [7709/20000], Training Loss: 0.0277\n",
            "Epoch [7710/20000], Training Loss: 0.0277\n",
            "Epoch [7711/20000], Training Loss: 0.0267\n",
            "Epoch [7712/20000], Training Loss: 0.0274\n",
            "Epoch [7713/20000], Training Loss: 0.0281\n",
            "Epoch [7714/20000], Training Loss: 0.0272\n",
            "Epoch [7715/20000], Training Loss: 0.0295\n",
            "Epoch [7716/20000], Training Loss: 0.0272\n",
            "Epoch [7717/20000], Training Loss: 0.0253\n",
            "Epoch [7718/20000], Training Loss: 0.0244\n",
            "Epoch [7719/20000], Training Loss: 0.0272\n",
            "Epoch [7720/20000], Training Loss: 0.0275\n",
            "Epoch [7721/20000], Training Loss: 0.0248\n",
            "Epoch [7722/20000], Training Loss: 0.0250\n",
            "Epoch [7723/20000], Training Loss: 0.0259\n",
            "Epoch [7724/20000], Training Loss: 0.0273\n",
            "Epoch [7725/20000], Training Loss: 0.0270\n",
            "Epoch [7726/20000], Training Loss: 0.0291\n",
            "Epoch [7727/20000], Training Loss: 0.0260\n",
            "Epoch [7728/20000], Training Loss: 0.0283\n",
            "Epoch [7729/20000], Training Loss: 0.0286\n",
            "Epoch [7730/20000], Training Loss: 0.0278\n",
            "Epoch [7731/20000], Training Loss: 0.0261\n",
            "Epoch [7732/20000], Training Loss: 0.0269\n",
            "Epoch [7733/20000], Training Loss: 0.0249\n",
            "Epoch [7734/20000], Training Loss: 0.0269\n",
            "Epoch [7735/20000], Training Loss: 0.0285\n",
            "Epoch [7736/20000], Training Loss: 0.0287\n",
            "Epoch [7737/20000], Training Loss: 0.0256\n",
            "Epoch [7738/20000], Training Loss: 0.0255\n",
            "Epoch [7739/20000], Training Loss: 0.0278\n",
            "Epoch [7740/20000], Training Loss: 0.0256\n",
            "Epoch [7741/20000], Training Loss: 0.0262\n",
            "Epoch [7742/20000], Training Loss: 0.0263\n",
            "Epoch [7743/20000], Training Loss: 0.0276\n",
            "Epoch [7744/20000], Training Loss: 0.0286\n",
            "Epoch [7745/20000], Training Loss: 0.0266\n",
            "Epoch [7746/20000], Training Loss: 0.0287\n",
            "Epoch [7747/20000], Training Loss: 0.0266\n",
            "Epoch [7748/20000], Training Loss: 0.0279\n",
            "Epoch [7749/20000], Training Loss: 0.0252\n",
            "Epoch [7750/20000], Training Loss: 0.0276\n",
            "Epoch [7751/20000], Training Loss: 0.0256\n",
            "Epoch [7752/20000], Training Loss: 0.0292\n",
            "Epoch [7753/20000], Training Loss: 0.0247\n",
            "Epoch [7754/20000], Training Loss: 0.0290\n",
            "Epoch [7755/20000], Training Loss: 0.0256\n",
            "Epoch [7756/20000], Training Loss: 0.0255\n",
            "Epoch [7757/20000], Training Loss: 0.0262\n",
            "Epoch [7758/20000], Training Loss: 0.0258\n",
            "Epoch [7759/20000], Training Loss: 0.0281\n",
            "Epoch [7760/20000], Training Loss: 0.0287\n",
            "Epoch [7761/20000], Training Loss: 0.0254\n",
            "Epoch [7762/20000], Training Loss: 0.0257\n",
            "Epoch [7763/20000], Training Loss: 0.0282\n",
            "Epoch [7764/20000], Training Loss: 0.0261\n",
            "Epoch [7765/20000], Training Loss: 0.0263\n",
            "Epoch [7766/20000], Training Loss: 0.0273\n",
            "Epoch [7767/20000], Training Loss: 0.0278\n",
            "Epoch [7768/20000], Training Loss: 0.0256\n",
            "Epoch [7769/20000], Training Loss: 0.0258\n",
            "Epoch [7770/20000], Training Loss: 0.0284\n",
            "Epoch [7771/20000], Training Loss: 0.0257\n",
            "Epoch [7772/20000], Training Loss: 0.0252\n",
            "Epoch [7773/20000], Training Loss: 0.0263\n",
            "Epoch [7774/20000], Training Loss: 0.0259\n",
            "Epoch [7775/20000], Training Loss: 0.0273\n",
            "Epoch [7776/20000], Training Loss: 0.0270\n",
            "Epoch [7777/20000], Training Loss: 0.0277\n",
            "Epoch [7778/20000], Training Loss: 0.0281\n",
            "Epoch [7779/20000], Training Loss: 0.0256\n",
            "Epoch [7780/20000], Training Loss: 0.0276\n",
            "Epoch [7781/20000], Training Loss: 0.0267\n",
            "Epoch [7782/20000], Training Loss: 0.0281\n",
            "Epoch [7783/20000], Training Loss: 0.0265\n",
            "Epoch [7784/20000], Training Loss: 0.0275\n",
            "Epoch [7785/20000], Training Loss: 0.0250\n",
            "Epoch [7786/20000], Training Loss: 0.0256\n",
            "Epoch [7787/20000], Training Loss: 0.0259\n",
            "Epoch [7788/20000], Training Loss: 0.0287\n",
            "Epoch [7789/20000], Training Loss: 0.0248\n",
            "Epoch [7790/20000], Training Loss: 0.0269\n",
            "Epoch [7791/20000], Training Loss: 0.0255\n",
            "Epoch [7792/20000], Training Loss: 0.0255\n",
            "Epoch [7793/20000], Training Loss: 0.0247\n",
            "Epoch [7794/20000], Training Loss: 0.0262\n",
            "Epoch [7795/20000], Training Loss: 0.0261\n",
            "Epoch [7796/20000], Training Loss: 0.0257\n",
            "Epoch [7797/20000], Training Loss: 0.0272\n",
            "Epoch [7798/20000], Training Loss: 0.0274\n",
            "Epoch [7799/20000], Training Loss: 0.0268\n",
            "Epoch [7800/20000], Training Loss: 0.0260\n",
            "Epoch [7801/20000], Training Loss: 0.0265\n",
            "Epoch [7802/20000], Training Loss: 0.0280\n",
            "Epoch [7803/20000], Training Loss: 0.0263\n",
            "Epoch [7804/20000], Training Loss: 0.0266\n",
            "Epoch [7805/20000], Training Loss: 0.0255\n",
            "Epoch [7806/20000], Training Loss: 0.0274\n",
            "Epoch [7807/20000], Training Loss: 0.0253\n",
            "Epoch [7808/20000], Training Loss: 0.0267\n",
            "Epoch [7809/20000], Training Loss: 0.0257\n",
            "Epoch [7810/20000], Training Loss: 0.0249\n",
            "Epoch [7811/20000], Training Loss: 0.0266\n",
            "Epoch [7812/20000], Training Loss: 0.0255\n",
            "Epoch [7813/20000], Training Loss: 0.0262\n",
            "Epoch [7814/20000], Training Loss: 0.0269\n",
            "Epoch [7815/20000], Training Loss: 0.0278\n",
            "Epoch [7816/20000], Training Loss: 0.0262\n",
            "Epoch [7817/20000], Training Loss: 0.0261\n",
            "Epoch [7818/20000], Training Loss: 0.0249\n",
            "Epoch [7819/20000], Training Loss: 0.0285\n",
            "Epoch [7820/20000], Training Loss: 0.0263\n",
            "Epoch [7821/20000], Training Loss: 0.0248\n",
            "Epoch [7822/20000], Training Loss: 0.0262\n",
            "Epoch [7823/20000], Training Loss: 0.0274\n",
            "Epoch [7824/20000], Training Loss: 0.0272\n",
            "Epoch [7825/20000], Training Loss: 0.0280\n",
            "Epoch [7826/20000], Training Loss: 0.0279\n",
            "Epoch [7827/20000], Training Loss: 0.0272\n",
            "Epoch [7828/20000], Training Loss: 0.0268\n",
            "Epoch [7829/20000], Training Loss: 0.0265\n",
            "Epoch [7830/20000], Training Loss: 0.0251\n",
            "Epoch [7831/20000], Training Loss: 0.0246\n",
            "Epoch [7832/20000], Training Loss: 0.0252\n",
            "Epoch [7833/20000], Training Loss: 0.0259\n",
            "Epoch [7834/20000], Training Loss: 0.0284\n",
            "Epoch [7835/20000], Training Loss: 0.0255\n",
            "Epoch [7836/20000], Training Loss: 0.0265\n",
            "Epoch [7837/20000], Training Loss: 0.0258\n",
            "Epoch [7838/20000], Training Loss: 0.0250\n",
            "Epoch [7839/20000], Training Loss: 0.0253\n",
            "Epoch [7840/20000], Training Loss: 0.0266\n",
            "Epoch [7841/20000], Training Loss: 0.0269\n",
            "Epoch [7842/20000], Training Loss: 0.0255\n",
            "Epoch [7843/20000], Training Loss: 0.0260\n",
            "Epoch [7844/20000], Training Loss: 0.0267\n",
            "Epoch [7845/20000], Training Loss: 0.0248\n",
            "Epoch [7846/20000], Training Loss: 0.0267\n",
            "Epoch [7847/20000], Training Loss: 0.0274\n",
            "Epoch [7848/20000], Training Loss: 0.0250\n",
            "Epoch [7849/20000], Training Loss: 0.0267\n",
            "Epoch [7850/20000], Training Loss: 0.0267\n",
            "Epoch [7851/20000], Training Loss: 0.0256\n",
            "Epoch [7852/20000], Training Loss: 0.0274\n",
            "Epoch [7853/20000], Training Loss: 0.0258\n",
            "Epoch [7854/20000], Training Loss: 0.0264\n",
            "Epoch [7855/20000], Training Loss: 0.0268\n",
            "Epoch [7856/20000], Training Loss: 0.0261\n",
            "Epoch [7857/20000], Training Loss: 0.0266\n",
            "Epoch [7858/20000], Training Loss: 0.0260\n",
            "Epoch [7859/20000], Training Loss: 0.0257\n",
            "Epoch [7860/20000], Training Loss: 0.0254\n",
            "Epoch [7861/20000], Training Loss: 0.0271\n",
            "Epoch [7862/20000], Training Loss: 0.0245\n",
            "Epoch [7863/20000], Training Loss: 0.0253\n",
            "Epoch [7864/20000], Training Loss: 0.0259\n",
            "Epoch [7865/20000], Training Loss: 0.0254\n",
            "Epoch [7866/20000], Training Loss: 0.0260\n",
            "Epoch [7867/20000], Training Loss: 0.0280\n",
            "Epoch [7868/20000], Training Loss: 0.0258\n",
            "Epoch [7869/20000], Training Loss: 0.0273\n",
            "Epoch [7870/20000], Training Loss: 0.0282\n",
            "Epoch [7871/20000], Training Loss: 0.0288\n",
            "Epoch [7872/20000], Training Loss: 0.0266\n",
            "Epoch [7873/20000], Training Loss: 0.0277\n",
            "Epoch [7874/20000], Training Loss: 0.0236\n",
            "Epoch [7875/20000], Training Loss: 0.0268\n",
            "Epoch [7876/20000], Training Loss: 0.0265\n",
            "Epoch [7877/20000], Training Loss: 0.0261\n",
            "Epoch [7878/20000], Training Loss: 0.0260\n",
            "Epoch [7879/20000], Training Loss: 0.0245\n",
            "Epoch [7880/20000], Training Loss: 0.0248\n",
            "Epoch [7881/20000], Training Loss: 0.0292\n",
            "Epoch [7882/20000], Training Loss: 0.0255\n",
            "Epoch [7883/20000], Training Loss: 0.0266\n",
            "Epoch [7884/20000], Training Loss: 0.0262\n",
            "Epoch [7885/20000], Training Loss: 0.0280\n",
            "Epoch [7886/20000], Training Loss: 0.0275\n",
            "Epoch [7887/20000], Training Loss: 0.0271\n",
            "Epoch [7888/20000], Training Loss: 0.0250\n",
            "Epoch [7889/20000], Training Loss: 0.0246\n",
            "Epoch [7890/20000], Training Loss: 0.0277\n",
            "Epoch [7891/20000], Training Loss: 0.0267\n",
            "Epoch [7892/20000], Training Loss: 0.0290\n",
            "Epoch [7893/20000], Training Loss: 0.0257\n",
            "Epoch [7894/20000], Training Loss: 0.0275\n",
            "Epoch [7895/20000], Training Loss: 0.0267\n",
            "Epoch [7896/20000], Training Loss: 0.0265\n",
            "Epoch [7897/20000], Training Loss: 0.0260\n",
            "Epoch [7898/20000], Training Loss: 0.0251\n",
            "Epoch [7899/20000], Training Loss: 0.0238\n",
            "Epoch [7900/20000], Training Loss: 0.0250\n",
            "Epoch [7901/20000], Training Loss: 0.0258\n",
            "Epoch [7902/20000], Training Loss: 0.0262\n",
            "Epoch [7903/20000], Training Loss: 0.0266\n",
            "Epoch [7904/20000], Training Loss: 0.0293\n",
            "Epoch [7905/20000], Training Loss: 0.0288\n",
            "Epoch [7906/20000], Training Loss: 0.0267\n",
            "Epoch [7907/20000], Training Loss: 0.0253\n",
            "Epoch [7908/20000], Training Loss: 0.0240\n",
            "Epoch [7909/20000], Training Loss: 0.0293\n",
            "Epoch [7910/20000], Training Loss: 0.0277\n",
            "Epoch [7911/20000], Training Loss: 0.0270\n",
            "Epoch [7912/20000], Training Loss: 0.0266\n",
            "Epoch [7913/20000], Training Loss: 0.0256\n",
            "Epoch [7914/20000], Training Loss: 0.0273\n",
            "Epoch [7915/20000], Training Loss: 0.0266\n",
            "Epoch [7916/20000], Training Loss: 0.0281\n",
            "Epoch [7917/20000], Training Loss: 0.0254\n",
            "Epoch [7918/20000], Training Loss: 0.0260\n",
            "Epoch [7919/20000], Training Loss: 0.0277\n",
            "Epoch [7920/20000], Training Loss: 0.0285\n",
            "Epoch [7921/20000], Training Loss: 0.0280\n",
            "Epoch [7922/20000], Training Loss: 0.0254\n",
            "Epoch [7923/20000], Training Loss: 0.0252\n",
            "Epoch [7924/20000], Training Loss: 0.0278\n",
            "Epoch [7925/20000], Training Loss: 0.0275\n",
            "Epoch [7926/20000], Training Loss: 0.0281\n",
            "Epoch [7927/20000], Training Loss: 0.0249\n",
            "Epoch [7928/20000], Training Loss: 0.0278\n",
            "Epoch [7929/20000], Training Loss: 0.0248\n",
            "Epoch [7930/20000], Training Loss: 0.0285\n",
            "Epoch [7931/20000], Training Loss: 0.0284\n",
            "Epoch [7932/20000], Training Loss: 0.0259\n",
            "Epoch [7933/20000], Training Loss: 0.0274\n",
            "Epoch [7934/20000], Training Loss: 0.0280\n",
            "Epoch [7935/20000], Training Loss: 0.0245\n",
            "Epoch [7936/20000], Training Loss: 0.0258\n",
            "Epoch [7937/20000], Training Loss: 0.0242\n",
            "Epoch [7938/20000], Training Loss: 0.0269\n",
            "Epoch [7939/20000], Training Loss: 0.0272\n",
            "Epoch [7940/20000], Training Loss: 0.0275\n",
            "Epoch [7941/20000], Training Loss: 0.0268\n",
            "Epoch [7942/20000], Training Loss: 0.0256\n",
            "Epoch [7943/20000], Training Loss: 0.0260\n",
            "Epoch [7944/20000], Training Loss: 0.0260\n",
            "Epoch [7945/20000], Training Loss: 0.0272\n",
            "Epoch [7946/20000], Training Loss: 0.0266\n",
            "Epoch [7947/20000], Training Loss: 0.0255\n",
            "Epoch [7948/20000], Training Loss: 0.0249\n",
            "Epoch [7949/20000], Training Loss: 0.0250\n",
            "Epoch [7950/20000], Training Loss: 0.0273\n",
            "Epoch [7951/20000], Training Loss: 0.0286\n",
            "Epoch [7952/20000], Training Loss: 0.0291\n",
            "Epoch [7953/20000], Training Loss: 0.0259\n",
            "Epoch [7954/20000], Training Loss: 0.0257\n",
            "Epoch [7955/20000], Training Loss: 0.0280\n",
            "Epoch [7956/20000], Training Loss: 0.0287\n",
            "Epoch [7957/20000], Training Loss: 0.0269\n",
            "Epoch [7958/20000], Training Loss: 0.0268\n",
            "Epoch [7959/20000], Training Loss: 0.0262\n",
            "Epoch [7960/20000], Training Loss: 0.0260\n",
            "Epoch [7961/20000], Training Loss: 0.0261\n",
            "Epoch [7962/20000], Training Loss: 0.0267\n",
            "Epoch [7963/20000], Training Loss: 0.0263\n",
            "Epoch [7964/20000], Training Loss: 0.0259\n",
            "Epoch [7965/20000], Training Loss: 0.0259\n",
            "Epoch [7966/20000], Training Loss: 0.0270\n",
            "Epoch [7967/20000], Training Loss: 0.0278\n",
            "Epoch [7968/20000], Training Loss: 0.0259\n",
            "Epoch [7969/20000], Training Loss: 0.0261\n",
            "Epoch [7970/20000], Training Loss: 0.0278\n",
            "Epoch [7971/20000], Training Loss: 0.0272\n",
            "Epoch [7972/20000], Training Loss: 0.0276\n",
            "Epoch [7973/20000], Training Loss: 0.0270\n",
            "Epoch [7974/20000], Training Loss: 0.0263\n",
            "Epoch [7975/20000], Training Loss: 0.0264\n",
            "Epoch [7976/20000], Training Loss: 0.0258\n",
            "Epoch [7977/20000], Training Loss: 0.0269\n",
            "Epoch [7978/20000], Training Loss: 0.0263\n",
            "Epoch [7979/20000], Training Loss: 0.0262\n",
            "Epoch [7980/20000], Training Loss: 0.0254\n",
            "Epoch [7981/20000], Training Loss: 0.0289\n",
            "Epoch [7982/20000], Training Loss: 0.0273\n",
            "Epoch [7983/20000], Training Loss: 0.0275\n",
            "Epoch [7984/20000], Training Loss: 0.0259\n",
            "Epoch [7985/20000], Training Loss: 0.0284\n",
            "Epoch [7986/20000], Training Loss: 0.0263\n",
            "Epoch [7987/20000], Training Loss: 0.0281\n",
            "Epoch [7988/20000], Training Loss: 0.0247\n",
            "Epoch [7989/20000], Training Loss: 0.0262\n",
            "Epoch [7990/20000], Training Loss: 0.0289\n",
            "Epoch [7991/20000], Training Loss: 0.0294\n",
            "Epoch [7992/20000], Training Loss: 0.0272\n",
            "Epoch [7993/20000], Training Loss: 0.0279\n",
            "Epoch [7994/20000], Training Loss: 0.0273\n",
            "Epoch [7995/20000], Training Loss: 0.0245\n",
            "Epoch [7996/20000], Training Loss: 0.0270\n",
            "Epoch [7997/20000], Training Loss: 0.0256\n",
            "Epoch [7998/20000], Training Loss: 0.0246\n",
            "Epoch [7999/20000], Training Loss: 0.0260\n",
            "Epoch [8000/20000], Training Loss: 0.0273\n",
            "Epoch [8001/20000], Training Loss: 0.0258\n",
            "Epoch [8002/20000], Training Loss: 0.0274\n",
            "Epoch [8003/20000], Training Loss: 0.0254\n",
            "Epoch [8004/20000], Training Loss: 0.0287\n",
            "Epoch [8005/20000], Training Loss: 0.0261\n",
            "Epoch [8006/20000], Training Loss: 0.0259\n",
            "Epoch [8007/20000], Training Loss: 0.0269\n",
            "Epoch [8008/20000], Training Loss: 0.0271\n",
            "Epoch [8009/20000], Training Loss: 0.0284\n",
            "Epoch [8010/20000], Training Loss: 0.0260\n",
            "Epoch [8011/20000], Training Loss: 0.0278\n",
            "Epoch [8012/20000], Training Loss: 0.0261\n",
            "Epoch [8013/20000], Training Loss: 0.0252\n",
            "Epoch [8014/20000], Training Loss: 0.0270\n",
            "Epoch [8015/20000], Training Loss: 0.0252\n",
            "Epoch [8016/20000], Training Loss: 0.0260\n",
            "Epoch [8017/20000], Training Loss: 0.0264\n",
            "Epoch [8018/20000], Training Loss: 0.0258\n",
            "Epoch [8019/20000], Training Loss: 0.0265\n",
            "Epoch [8020/20000], Training Loss: 0.0274\n",
            "Epoch [8021/20000], Training Loss: 0.0257\n",
            "Epoch [8022/20000], Training Loss: 0.0276\n",
            "Epoch [8023/20000], Training Loss: 0.0255\n",
            "Epoch [8024/20000], Training Loss: 0.0263\n",
            "Epoch [8025/20000], Training Loss: 0.0258\n",
            "Epoch [8026/20000], Training Loss: 0.0248\n",
            "Epoch [8027/20000], Training Loss: 0.0259\n",
            "Epoch [8028/20000], Training Loss: 0.0255\n",
            "Epoch [8029/20000], Training Loss: 0.0242\n",
            "Epoch [8030/20000], Training Loss: 0.0262\n",
            "Epoch [8031/20000], Training Loss: 0.0263\n",
            "Epoch [8032/20000], Training Loss: 0.0255\n",
            "Epoch [8033/20000], Training Loss: 0.0253\n",
            "Epoch [8034/20000], Training Loss: 0.0242\n",
            "Epoch [8035/20000], Training Loss: 0.0259\n",
            "Epoch [8036/20000], Training Loss: 0.0257\n",
            "Epoch [8037/20000], Training Loss: 0.0276\n",
            "Epoch [8038/20000], Training Loss: 0.0252\n",
            "Epoch [8039/20000], Training Loss: 0.0262\n",
            "Epoch [8040/20000], Training Loss: 0.0257\n",
            "Epoch [8041/20000], Training Loss: 0.0247\n",
            "Epoch [8042/20000], Training Loss: 0.0252\n",
            "Epoch [8043/20000], Training Loss: 0.0246\n",
            "Epoch [8044/20000], Training Loss: 0.0273\n",
            "Epoch [8045/20000], Training Loss: 0.0270\n",
            "Epoch [8046/20000], Training Loss: 0.0292\n",
            "Epoch [8047/20000], Training Loss: 0.0259\n",
            "Epoch [8048/20000], Training Loss: 0.0282\n",
            "Epoch [8049/20000], Training Loss: 0.0276\n",
            "Epoch [8050/20000], Training Loss: 0.0261\n",
            "Epoch [8051/20000], Training Loss: 0.0254\n",
            "Epoch [8052/20000], Training Loss: 0.0282\n",
            "Epoch [8053/20000], Training Loss: 0.0265\n",
            "Epoch [8054/20000], Training Loss: 0.0258\n",
            "Epoch [8055/20000], Training Loss: 0.0271\n",
            "Epoch [8056/20000], Training Loss: 0.0257\n",
            "Epoch [8057/20000], Training Loss: 0.0241\n",
            "Epoch [8058/20000], Training Loss: 0.0285\n",
            "Epoch [8059/20000], Training Loss: 0.0257\n",
            "Epoch [8060/20000], Training Loss: 0.0276\n",
            "Epoch [8061/20000], Training Loss: 0.0269\n",
            "Epoch [8062/20000], Training Loss: 0.0243\n",
            "Epoch [8063/20000], Training Loss: 0.0241\n",
            "Epoch [8064/20000], Training Loss: 0.0254\n",
            "Epoch [8065/20000], Training Loss: 0.0286\n",
            "Epoch [8066/20000], Training Loss: 0.0263\n",
            "Epoch [8067/20000], Training Loss: 0.0277\n",
            "Epoch [8068/20000], Training Loss: 0.0281\n",
            "Epoch [8069/20000], Training Loss: 0.0259\n",
            "Epoch [8070/20000], Training Loss: 0.0265\n",
            "Epoch [8071/20000], Training Loss: 0.0251\n",
            "Epoch [8072/20000], Training Loss: 0.0270\n",
            "Epoch [8073/20000], Training Loss: 0.0271\n",
            "Epoch [8074/20000], Training Loss: 0.0255\n",
            "Epoch [8075/20000], Training Loss: 0.0246\n",
            "Epoch [8076/20000], Training Loss: 0.0254\n",
            "Epoch [8077/20000], Training Loss: 0.0272\n",
            "Epoch [8078/20000], Training Loss: 0.0297\n",
            "Epoch [8079/20000], Training Loss: 0.0269\n",
            "Epoch [8080/20000], Training Loss: 0.0254\n",
            "Epoch [8081/20000], Training Loss: 0.0273\n",
            "Epoch [8082/20000], Training Loss: 0.0265\n",
            "Epoch [8083/20000], Training Loss: 0.0283\n",
            "Epoch [8084/20000], Training Loss: 0.0255\n",
            "Epoch [8085/20000], Training Loss: 0.0248\n",
            "Epoch [8086/20000], Training Loss: 0.0268\n",
            "Epoch [8087/20000], Training Loss: 0.0266\n",
            "Epoch [8088/20000], Training Loss: 0.0286\n",
            "Epoch [8089/20000], Training Loss: 0.0250\n",
            "Epoch [8090/20000], Training Loss: 0.0265\n",
            "Epoch [8091/20000], Training Loss: 0.0254\n",
            "Epoch [8092/20000], Training Loss: 0.0281\n",
            "Epoch [8093/20000], Training Loss: 0.0281\n",
            "Epoch [8094/20000], Training Loss: 0.0258\n",
            "Epoch [8095/20000], Training Loss: 0.0285\n",
            "Epoch [8096/20000], Training Loss: 0.0269\n",
            "Epoch [8097/20000], Training Loss: 0.0252\n",
            "Epoch [8098/20000], Training Loss: 0.0265\n",
            "Epoch [8099/20000], Training Loss: 0.0246\n",
            "Epoch [8100/20000], Training Loss: 0.0270\n",
            "Epoch [8101/20000], Training Loss: 0.0279\n",
            "Epoch [8102/20000], Training Loss: 0.0257\n",
            "Epoch [8103/20000], Training Loss: 0.0257\n",
            "Epoch [8104/20000], Training Loss: 0.0274\n",
            "Epoch [8105/20000], Training Loss: 0.0247\n",
            "Epoch [8106/20000], Training Loss: 0.0260\n",
            "Epoch [8107/20000], Training Loss: 0.0248\n",
            "Epoch [8108/20000], Training Loss: 0.0266\n",
            "Epoch [8109/20000], Training Loss: 0.0279\n",
            "Epoch [8110/20000], Training Loss: 0.0263\n",
            "Epoch [8111/20000], Training Loss: 0.0263\n",
            "Epoch [8112/20000], Training Loss: 0.0253\n",
            "Epoch [8113/20000], Training Loss: 0.0261\n",
            "Epoch [8114/20000], Training Loss: 0.0251\n",
            "Epoch [8115/20000], Training Loss: 0.0280\n",
            "Epoch [8116/20000], Training Loss: 0.0262\n",
            "Epoch [8117/20000], Training Loss: 0.0251\n",
            "Epoch [8118/20000], Training Loss: 0.0249\n",
            "Epoch [8119/20000], Training Loss: 0.0263\n",
            "Epoch [8120/20000], Training Loss: 0.0254\n",
            "Epoch [8121/20000], Training Loss: 0.0254\n",
            "Epoch [8122/20000], Training Loss: 0.0260\n",
            "Epoch [8123/20000], Training Loss: 0.0284\n",
            "Epoch [8124/20000], Training Loss: 0.0272\n",
            "Epoch [8125/20000], Training Loss: 0.0246\n",
            "Epoch [8126/20000], Training Loss: 0.0269\n",
            "Epoch [8127/20000], Training Loss: 0.0261\n",
            "Epoch [8128/20000], Training Loss: 0.0262\n",
            "Epoch [8129/20000], Training Loss: 0.0261\n",
            "Epoch [8130/20000], Training Loss: 0.0274\n",
            "Epoch [8131/20000], Training Loss: 0.0268\n",
            "Epoch [8132/20000], Training Loss: 0.0269\n",
            "Epoch [8133/20000], Training Loss: 0.0288\n",
            "Epoch [8134/20000], Training Loss: 0.0263\n",
            "Epoch [8135/20000], Training Loss: 0.0247\n",
            "Epoch [8136/20000], Training Loss: 0.0272\n",
            "Epoch [8137/20000], Training Loss: 0.0266\n",
            "Epoch [8138/20000], Training Loss: 0.0246\n",
            "Epoch [8139/20000], Training Loss: 0.0256\n",
            "Epoch [8140/20000], Training Loss: 0.0261\n",
            "Epoch [8141/20000], Training Loss: 0.0256\n",
            "Epoch [8142/20000], Training Loss: 0.0287\n",
            "Epoch [8143/20000], Training Loss: 0.0263\n",
            "Epoch [8144/20000], Training Loss: 0.0276\n",
            "Epoch [8145/20000], Training Loss: 0.0258\n",
            "Epoch [8146/20000], Training Loss: 0.0287\n",
            "Epoch [8147/20000], Training Loss: 0.0272\n",
            "Epoch [8148/20000], Training Loss: 0.0264\n",
            "Epoch [8149/20000], Training Loss: 0.0264\n",
            "Epoch [8150/20000], Training Loss: 0.0274\n",
            "Epoch [8151/20000], Training Loss: 0.0269\n",
            "Epoch [8152/20000], Training Loss: 0.0290\n",
            "Epoch [8153/20000], Training Loss: 0.0258\n",
            "Epoch [8154/20000], Training Loss: 0.0250\n",
            "Epoch [8155/20000], Training Loss: 0.0267\n",
            "Epoch [8156/20000], Training Loss: 0.0276\n",
            "Epoch [8157/20000], Training Loss: 0.0268\n",
            "Epoch [8158/20000], Training Loss: 0.0254\n",
            "Epoch [8159/20000], Training Loss: 0.0273\n",
            "Epoch [8160/20000], Training Loss: 0.0255\n",
            "Epoch [8161/20000], Training Loss: 0.0253\n",
            "Epoch [8162/20000], Training Loss: 0.0250\n",
            "Epoch [8163/20000], Training Loss: 0.0262\n",
            "Epoch [8164/20000], Training Loss: 0.0270\n",
            "Epoch [8165/20000], Training Loss: 0.0255\n",
            "Epoch [8166/20000], Training Loss: 0.0269\n",
            "Epoch [8167/20000], Training Loss: 0.0258\n",
            "Epoch [8168/20000], Training Loss: 0.0266\n",
            "Epoch [8169/20000], Training Loss: 0.0263\n",
            "Epoch [8170/20000], Training Loss: 0.0260\n",
            "Epoch [8171/20000], Training Loss: 0.0270\n",
            "Epoch [8172/20000], Training Loss: 0.0260\n",
            "Epoch [8173/20000], Training Loss: 0.0246\n",
            "Epoch [8174/20000], Training Loss: 0.0277\n",
            "Epoch [8175/20000], Training Loss: 0.0258\n",
            "Epoch [8176/20000], Training Loss: 0.0280\n",
            "Epoch [8177/20000], Training Loss: 0.0260\n",
            "Epoch [8178/20000], Training Loss: 0.0267\n",
            "Epoch [8179/20000], Training Loss: 0.0260\n",
            "Epoch [8180/20000], Training Loss: 0.0255\n",
            "Epoch [8181/20000], Training Loss: 0.0256\n",
            "Epoch [8182/20000], Training Loss: 0.0277\n",
            "Epoch [8183/20000], Training Loss: 0.0286\n",
            "Epoch [8184/20000], Training Loss: 0.0272\n",
            "Epoch [8185/20000], Training Loss: 0.0270\n",
            "Epoch [8186/20000], Training Loss: 0.0264\n",
            "Epoch [8187/20000], Training Loss: 0.0256\n",
            "Epoch [8188/20000], Training Loss: 0.0264\n",
            "Epoch [8189/20000], Training Loss: 0.0280\n",
            "Epoch [8190/20000], Training Loss: 0.0259\n",
            "Epoch [8191/20000], Training Loss: 0.0259\n",
            "Epoch [8192/20000], Training Loss: 0.0255\n",
            "Epoch [8193/20000], Training Loss: 0.0259\n",
            "Epoch [8194/20000], Training Loss: 0.0267\n",
            "Epoch [8195/20000], Training Loss: 0.0267\n",
            "Epoch [8196/20000], Training Loss: 0.0280\n",
            "Epoch [8197/20000], Training Loss: 0.0270\n",
            "Epoch [8198/20000], Training Loss: 0.0267\n",
            "Epoch [8199/20000], Training Loss: 0.0267\n",
            "Epoch [8200/20000], Training Loss: 0.0258\n",
            "Epoch [8201/20000], Training Loss: 0.0243\n",
            "Epoch [8202/20000], Training Loss: 0.0285\n",
            "Epoch [8203/20000], Training Loss: 0.0257\n",
            "Epoch [8204/20000], Training Loss: 0.0266\n",
            "Epoch [8205/20000], Training Loss: 0.0273\n",
            "Epoch [8206/20000], Training Loss: 0.0261\n",
            "Epoch [8207/20000], Training Loss: 0.0283\n",
            "Epoch [8208/20000], Training Loss: 0.0244\n",
            "Epoch [8209/20000], Training Loss: 0.0269\n",
            "Epoch [8210/20000], Training Loss: 0.0262\n",
            "Epoch [8211/20000], Training Loss: 0.0260\n",
            "Epoch [8212/20000], Training Loss: 0.0263\n",
            "Epoch [8213/20000], Training Loss: 0.0266\n",
            "Epoch [8214/20000], Training Loss: 0.0248\n",
            "Epoch [8215/20000], Training Loss: 0.0239\n",
            "Epoch [8216/20000], Training Loss: 0.0271\n",
            "Epoch [8217/20000], Training Loss: 0.0267\n",
            "Epoch [8218/20000], Training Loss: 0.0260\n",
            "Epoch [8219/20000], Training Loss: 0.0263\n",
            "Epoch [8220/20000], Training Loss: 0.0253\n",
            "Epoch [8221/20000], Training Loss: 0.0254\n",
            "Epoch [8222/20000], Training Loss: 0.0266\n",
            "Epoch [8223/20000], Training Loss: 0.0259\n",
            "Epoch [8224/20000], Training Loss: 0.0261\n",
            "Epoch [8225/20000], Training Loss: 0.0275\n",
            "Epoch [8226/20000], Training Loss: 0.0276\n",
            "Epoch [8227/20000], Training Loss: 0.0271\n",
            "Epoch [8228/20000], Training Loss: 0.0275\n",
            "Epoch [8229/20000], Training Loss: 0.0271\n",
            "Epoch [8230/20000], Training Loss: 0.0271\n",
            "Epoch [8231/20000], Training Loss: 0.0262\n",
            "Epoch [8232/20000], Training Loss: 0.0249\n",
            "Epoch [8233/20000], Training Loss: 0.0282\n",
            "Epoch [8234/20000], Training Loss: 0.0258\n",
            "Epoch [8235/20000], Training Loss: 0.0278\n",
            "Epoch [8236/20000], Training Loss: 0.0254\n",
            "Epoch [8237/20000], Training Loss: 0.0244\n",
            "Epoch [8238/20000], Training Loss: 0.0246\n",
            "Epoch [8239/20000], Training Loss: 0.0266\n",
            "Epoch [8240/20000], Training Loss: 0.0248\n",
            "Epoch [8241/20000], Training Loss: 0.0260\n",
            "Epoch [8242/20000], Training Loss: 0.0268\n",
            "Epoch [8243/20000], Training Loss: 0.0251\n",
            "Epoch [8244/20000], Training Loss: 0.0279\n",
            "Epoch [8245/20000], Training Loss: 0.0280\n",
            "Epoch [8246/20000], Training Loss: 0.0252\n",
            "Epoch [8247/20000], Training Loss: 0.0249\n",
            "Epoch [8248/20000], Training Loss: 0.0252\n",
            "Epoch [8249/20000], Training Loss: 0.0254\n",
            "Epoch [8250/20000], Training Loss: 0.0263\n",
            "Epoch [8251/20000], Training Loss: 0.0261\n",
            "Epoch [8252/20000], Training Loss: 0.0284\n",
            "Epoch [8253/20000], Training Loss: 0.0265\n",
            "Epoch [8254/20000], Training Loss: 0.0256\n",
            "Epoch [8255/20000], Training Loss: 0.0271\n",
            "Epoch [8256/20000], Training Loss: 0.0268\n",
            "Epoch [8257/20000], Training Loss: 0.0261\n",
            "Epoch [8258/20000], Training Loss: 0.0277\n",
            "Epoch [8259/20000], Training Loss: 0.0292\n",
            "Epoch [8260/20000], Training Loss: 0.0242\n",
            "Epoch [8261/20000], Training Loss: 0.0259\n",
            "Epoch [8262/20000], Training Loss: 0.0267\n",
            "Epoch [8263/20000], Training Loss: 0.0260\n",
            "Epoch [8264/20000], Training Loss: 0.0264\n",
            "Epoch [8265/20000], Training Loss: 0.0273\n",
            "Epoch [8266/20000], Training Loss: 0.0258\n",
            "Epoch [8267/20000], Training Loss: 0.0266\n",
            "Epoch [8268/20000], Training Loss: 0.0247\n",
            "Epoch [8269/20000], Training Loss: 0.0267\n",
            "Epoch [8270/20000], Training Loss: 0.0261\n",
            "Epoch [8271/20000], Training Loss: 0.0257\n",
            "Epoch [8272/20000], Training Loss: 0.0245\n",
            "Epoch [8273/20000], Training Loss: 0.0269\n",
            "Epoch [8274/20000], Training Loss: 0.0273\n",
            "Epoch [8275/20000], Training Loss: 0.0278\n",
            "Epoch [8276/20000], Training Loss: 0.0249\n",
            "Epoch [8277/20000], Training Loss: 0.0278\n",
            "Epoch [8278/20000], Training Loss: 0.0271\n",
            "Epoch [8279/20000], Training Loss: 0.0271\n",
            "Epoch [8280/20000], Training Loss: 0.0268\n",
            "Epoch [8281/20000], Training Loss: 0.0263\n",
            "Epoch [8282/20000], Training Loss: 0.0271\n",
            "Epoch [8283/20000], Training Loss: 0.0254\n",
            "Epoch [8284/20000], Training Loss: 0.0267\n",
            "Epoch [8285/20000], Training Loss: 0.0272\n",
            "Epoch [8286/20000], Training Loss: 0.0275\n",
            "Epoch [8287/20000], Training Loss: 0.0262\n",
            "Epoch [8288/20000], Training Loss: 0.0284\n",
            "Epoch [8289/20000], Training Loss: 0.0264\n",
            "Epoch [8290/20000], Training Loss: 0.0274\n",
            "Epoch [8291/20000], Training Loss: 0.0256\n",
            "Epoch [8292/20000], Training Loss: 0.0258\n",
            "Epoch [8293/20000], Training Loss: 0.0255\n",
            "Epoch [8294/20000], Training Loss: 0.0263\n",
            "Epoch [8295/20000], Training Loss: 0.0255\n",
            "Epoch [8296/20000], Training Loss: 0.0273\n",
            "Epoch [8297/20000], Training Loss: 0.0249\n",
            "Epoch [8298/20000], Training Loss: 0.0255\n",
            "Epoch [8299/20000], Training Loss: 0.0259\n",
            "Epoch [8300/20000], Training Loss: 0.0266\n",
            "Epoch [8301/20000], Training Loss: 0.0255\n",
            "Epoch [8302/20000], Training Loss: 0.0260\n",
            "Epoch [8303/20000], Training Loss: 0.0255\n",
            "Epoch [8304/20000], Training Loss: 0.0268\n",
            "Epoch [8305/20000], Training Loss: 0.0294\n",
            "Epoch [8306/20000], Training Loss: 0.0266\n",
            "Epoch [8307/20000], Training Loss: 0.0273\n",
            "Epoch [8308/20000], Training Loss: 0.0276\n",
            "Epoch [8309/20000], Training Loss: 0.0265\n",
            "Epoch [8310/20000], Training Loss: 0.0280\n",
            "Epoch [8311/20000], Training Loss: 0.0276\n",
            "Epoch [8312/20000], Training Loss: 0.0244\n",
            "Epoch [8313/20000], Training Loss: 0.0272\n",
            "Epoch [8314/20000], Training Loss: 0.0250\n",
            "Epoch [8315/20000], Training Loss: 0.0298\n",
            "Epoch [8316/20000], Training Loss: 0.0266\n",
            "Epoch [8317/20000], Training Loss: 0.0296\n",
            "Epoch [8318/20000], Training Loss: 0.0288\n",
            "Epoch [8319/20000], Training Loss: 0.0268\n",
            "Epoch [8320/20000], Training Loss: 0.0285\n",
            "Epoch [8321/20000], Training Loss: 0.0256\n",
            "Epoch [8322/20000], Training Loss: 0.0256\n",
            "Epoch [8323/20000], Training Loss: 0.0251\n",
            "Epoch [8324/20000], Training Loss: 0.0260\n",
            "Epoch [8325/20000], Training Loss: 0.0264\n",
            "Epoch [8326/20000], Training Loss: 0.0277\n",
            "Epoch [8327/20000], Training Loss: 0.0259\n",
            "Epoch [8328/20000], Training Loss: 0.0270\n",
            "Epoch [8329/20000], Training Loss: 0.0252\n",
            "Epoch [8330/20000], Training Loss: 0.0276\n",
            "Epoch [8331/20000], Training Loss: 0.0240\n",
            "Epoch [8332/20000], Training Loss: 0.0248\n",
            "Epoch [8333/20000], Training Loss: 0.0267\n",
            "Epoch [8334/20000], Training Loss: 0.0292\n",
            "Epoch [8335/20000], Training Loss: 0.0252\n",
            "Epoch [8336/20000], Training Loss: 0.0277\n",
            "Epoch [8337/20000], Training Loss: 0.0264\n",
            "Epoch [8338/20000], Training Loss: 0.0272\n",
            "Epoch [8339/20000], Training Loss: 0.0271\n",
            "Epoch [8340/20000], Training Loss: 0.0253\n",
            "Epoch [8341/20000], Training Loss: 0.0290\n",
            "Epoch [8342/20000], Training Loss: 0.0280\n",
            "Epoch [8343/20000], Training Loss: 0.0265\n",
            "Epoch [8344/20000], Training Loss: 0.0293\n",
            "Epoch [8345/20000], Training Loss: 0.0276\n",
            "Epoch [8346/20000], Training Loss: 0.0269\n",
            "Epoch [8347/20000], Training Loss: 0.0271\n",
            "Epoch [8348/20000], Training Loss: 0.0263\n",
            "Epoch [8349/20000], Training Loss: 0.0257\n",
            "Epoch [8350/20000], Training Loss: 0.0273\n",
            "Epoch [8351/20000], Training Loss: 0.0244\n",
            "Epoch [8352/20000], Training Loss: 0.0279\n",
            "Epoch [8353/20000], Training Loss: 0.0270\n",
            "Epoch [8354/20000], Training Loss: 0.0268\n",
            "Epoch [8355/20000], Training Loss: 0.0269\n",
            "Epoch [8356/20000], Training Loss: 0.0265\n",
            "Epoch [8357/20000], Training Loss: 0.0244\n",
            "Epoch [8358/20000], Training Loss: 0.0257\n",
            "Epoch [8359/20000], Training Loss: 0.0250\n",
            "Epoch [8360/20000], Training Loss: 0.0251\n",
            "Epoch [8361/20000], Training Loss: 0.0265\n",
            "Epoch [8362/20000], Training Loss: 0.0268\n",
            "Epoch [8363/20000], Training Loss: 0.0250\n",
            "Epoch [8364/20000], Training Loss: 0.0253\n",
            "Epoch [8365/20000], Training Loss: 0.0269\n",
            "Epoch [8366/20000], Training Loss: 0.0255\n",
            "Epoch [8367/20000], Training Loss: 0.0251\n",
            "Epoch [8368/20000], Training Loss: 0.0252\n",
            "Epoch [8369/20000], Training Loss: 0.0258\n",
            "Epoch [8370/20000], Training Loss: 0.0289\n",
            "Epoch [8371/20000], Training Loss: 0.0264\n",
            "Epoch [8372/20000], Training Loss: 0.0265\n",
            "Epoch [8373/20000], Training Loss: 0.0259\n",
            "Epoch [8374/20000], Training Loss: 0.0253\n",
            "Epoch [8375/20000], Training Loss: 0.0266\n",
            "Epoch [8376/20000], Training Loss: 0.0266\n",
            "Epoch [8377/20000], Training Loss: 0.0270\n",
            "Epoch [8378/20000], Training Loss: 0.0281\n",
            "Epoch [8379/20000], Training Loss: 0.0266\n",
            "Epoch [8380/20000], Training Loss: 0.0251\n",
            "Epoch [8381/20000], Training Loss: 0.0242\n",
            "Epoch [8382/20000], Training Loss: 0.0266\n",
            "Epoch [8383/20000], Training Loss: 0.0249\n",
            "Epoch [8384/20000], Training Loss: 0.0271\n",
            "Epoch [8385/20000], Training Loss: 0.0271\n",
            "Epoch [8386/20000], Training Loss: 0.0281\n",
            "Epoch [8387/20000], Training Loss: 0.0277\n",
            "Epoch [8388/20000], Training Loss: 0.0275\n",
            "Epoch [8389/20000], Training Loss: 0.0245\n",
            "Epoch [8390/20000], Training Loss: 0.0251\n",
            "Epoch [8391/20000], Training Loss: 0.0275\n",
            "Epoch [8392/20000], Training Loss: 0.0268\n",
            "Epoch [8393/20000], Training Loss: 0.0270\n",
            "Epoch [8394/20000], Training Loss: 0.0264\n",
            "Epoch [8395/20000], Training Loss: 0.0259\n",
            "Epoch [8396/20000], Training Loss: 0.0269\n",
            "Epoch [8397/20000], Training Loss: 0.0261\n",
            "Epoch [8398/20000], Training Loss: 0.0270\n",
            "Epoch [8399/20000], Training Loss: 0.0292\n",
            "Epoch [8400/20000], Training Loss: 0.0266\n",
            "Epoch [8401/20000], Training Loss: 0.0273\n",
            "Epoch [8402/20000], Training Loss: 0.0267\n",
            "Epoch [8403/20000], Training Loss: 0.0269\n",
            "Epoch [8404/20000], Training Loss: 0.0281\n",
            "Epoch [8405/20000], Training Loss: 0.0262\n",
            "Epoch [8406/20000], Training Loss: 0.0255\n",
            "Epoch [8407/20000], Training Loss: 0.0272\n",
            "Epoch [8408/20000], Training Loss: 0.0257\n",
            "Epoch [8409/20000], Training Loss: 0.0263\n",
            "Epoch [8410/20000], Training Loss: 0.0258\n",
            "Epoch [8411/20000], Training Loss: 0.0268\n",
            "Epoch [8412/20000], Training Loss: 0.0266\n",
            "Epoch [8413/20000], Training Loss: 0.0252\n",
            "Epoch [8414/20000], Training Loss: 0.0253\n",
            "Epoch [8415/20000], Training Loss: 0.0286\n",
            "Epoch [8416/20000], Training Loss: 0.0254\n",
            "Epoch [8417/20000], Training Loss: 0.0276\n",
            "Epoch [8418/20000], Training Loss: 0.0273\n",
            "Epoch [8419/20000], Training Loss: 0.0248\n",
            "Epoch [8420/20000], Training Loss: 0.0263\n",
            "Epoch [8421/20000], Training Loss: 0.0265\n",
            "Epoch [8422/20000], Training Loss: 0.0248\n",
            "Epoch [8423/20000], Training Loss: 0.0278\n",
            "Epoch [8424/20000], Training Loss: 0.0277\n",
            "Epoch [8425/20000], Training Loss: 0.0263\n",
            "Epoch [8426/20000], Training Loss: 0.0275\n",
            "Epoch [8427/20000], Training Loss: 0.0280\n",
            "Epoch [8428/20000], Training Loss: 0.0238\n",
            "Epoch [8429/20000], Training Loss: 0.0256\n",
            "Epoch [8430/20000], Training Loss: 0.0265\n",
            "Epoch [8431/20000], Training Loss: 0.0267\n",
            "Epoch [8432/20000], Training Loss: 0.0294\n",
            "Epoch [8433/20000], Training Loss: 0.0258\n",
            "Epoch [8434/20000], Training Loss: 0.0276\n",
            "Epoch [8435/20000], Training Loss: 0.0252\n",
            "Epoch [8436/20000], Training Loss: 0.0269\n",
            "Epoch [8437/20000], Training Loss: 0.0269\n",
            "Epoch [8438/20000], Training Loss: 0.0248\n",
            "Epoch [8439/20000], Training Loss: 0.0250\n",
            "Epoch [8440/20000], Training Loss: 0.0259\n",
            "Epoch [8441/20000], Training Loss: 0.0289\n",
            "Epoch [8442/20000], Training Loss: 0.0254\n",
            "Epoch [8443/20000], Training Loss: 0.0243\n",
            "Epoch [8444/20000], Training Loss: 0.0272\n",
            "Epoch [8445/20000], Training Loss: 0.0255\n",
            "Epoch [8446/20000], Training Loss: 0.0252\n",
            "Epoch [8447/20000], Training Loss: 0.0246\n",
            "Epoch [8448/20000], Training Loss: 0.0266\n",
            "Epoch [8449/20000], Training Loss: 0.0274\n",
            "Epoch [8450/20000], Training Loss: 0.0256\n",
            "Epoch [8451/20000], Training Loss: 0.0276\n",
            "Epoch [8452/20000], Training Loss: 0.0260\n",
            "Epoch [8453/20000], Training Loss: 0.0256\n",
            "Epoch [8454/20000], Training Loss: 0.0271\n",
            "Epoch [8455/20000], Training Loss: 0.0265\n",
            "Epoch [8456/20000], Training Loss: 0.0273\n",
            "Epoch [8457/20000], Training Loss: 0.0273\n",
            "Epoch [8458/20000], Training Loss: 0.0282\n",
            "Epoch [8459/20000], Training Loss: 0.0255\n",
            "Epoch [8460/20000], Training Loss: 0.0258\n",
            "Epoch [8461/20000], Training Loss: 0.0253\n",
            "Epoch [8462/20000], Training Loss: 0.0273\n",
            "Epoch [8463/20000], Training Loss: 0.0262\n",
            "Epoch [8464/20000], Training Loss: 0.0258\n",
            "Epoch [8465/20000], Training Loss: 0.0256\n",
            "Epoch [8466/20000], Training Loss: 0.0265\n",
            "Epoch [8467/20000], Training Loss: 0.0302\n",
            "Epoch [8468/20000], Training Loss: 0.0254\n",
            "Epoch [8469/20000], Training Loss: 0.0261\n",
            "Epoch [8470/20000], Training Loss: 0.0285\n",
            "Epoch [8471/20000], Training Loss: 0.0280\n",
            "Epoch [8472/20000], Training Loss: 0.0281\n",
            "Epoch [8473/20000], Training Loss: 0.0271\n",
            "Epoch [8474/20000], Training Loss: 0.0270\n",
            "Epoch [8475/20000], Training Loss: 0.0256\n",
            "Epoch [8476/20000], Training Loss: 0.0275\n",
            "Epoch [8477/20000], Training Loss: 0.0257\n",
            "Epoch [8478/20000], Training Loss: 0.0257\n",
            "Epoch [8479/20000], Training Loss: 0.0288\n",
            "Epoch [8480/20000], Training Loss: 0.0252\n",
            "Epoch [8481/20000], Training Loss: 0.0271\n",
            "Epoch [8482/20000], Training Loss: 0.0256\n",
            "Epoch [8483/20000], Training Loss: 0.0264\n",
            "Epoch [8484/20000], Training Loss: 0.0272\n",
            "Epoch [8485/20000], Training Loss: 0.0278\n",
            "Epoch [8486/20000], Training Loss: 0.0245\n",
            "Epoch [8487/20000], Training Loss: 0.0274\n",
            "Epoch [8488/20000], Training Loss: 0.0257\n",
            "Epoch [8489/20000], Training Loss: 0.0272\n",
            "Epoch [8490/20000], Training Loss: 0.0276\n",
            "Epoch [8491/20000], Training Loss: 0.0260\n",
            "Epoch [8492/20000], Training Loss: 0.0259\n",
            "Epoch [8493/20000], Training Loss: 0.0269\n",
            "Epoch [8494/20000], Training Loss: 0.0264\n",
            "Epoch [8495/20000], Training Loss: 0.0260\n",
            "Epoch [8496/20000], Training Loss: 0.0261\n",
            "Epoch [8497/20000], Training Loss: 0.0263\n",
            "Epoch [8498/20000], Training Loss: 0.0265\n",
            "Epoch [8499/20000], Training Loss: 0.0269\n",
            "Epoch [8500/20000], Training Loss: 0.0248\n",
            "Epoch [8501/20000], Training Loss: 0.0267\n",
            "Epoch [8502/20000], Training Loss: 0.0253\n",
            "Epoch [8503/20000], Training Loss: 0.0261\n",
            "Epoch [8504/20000], Training Loss: 0.0271\n",
            "Epoch [8505/20000], Training Loss: 0.0264\n",
            "Epoch [8506/20000], Training Loss: 0.0276\n",
            "Epoch [8507/20000], Training Loss: 0.0261\n",
            "Epoch [8508/20000], Training Loss: 0.0247\n",
            "Epoch [8509/20000], Training Loss: 0.0264\n",
            "Epoch [8510/20000], Training Loss: 0.0253\n",
            "Epoch [8511/20000], Training Loss: 0.0260\n",
            "Epoch [8512/20000], Training Loss: 0.0260\n",
            "Epoch [8513/20000], Training Loss: 0.0259\n",
            "Epoch [8514/20000], Training Loss: 0.0278\n",
            "Epoch [8515/20000], Training Loss: 0.0253\n",
            "Epoch [8516/20000], Training Loss: 0.0253\n",
            "Epoch [8517/20000], Training Loss: 0.0261\n",
            "Epoch [8518/20000], Training Loss: 0.0267\n",
            "Epoch [8519/20000], Training Loss: 0.0276\n",
            "Epoch [8520/20000], Training Loss: 0.0252\n",
            "Epoch [8521/20000], Training Loss: 0.0253\n",
            "Epoch [8522/20000], Training Loss: 0.0268\n",
            "Epoch [8523/20000], Training Loss: 0.0247\n",
            "Epoch [8524/20000], Training Loss: 0.0268\n",
            "Epoch [8525/20000], Training Loss: 0.0283\n",
            "Epoch [8526/20000], Training Loss: 0.0256\n",
            "Epoch [8527/20000], Training Loss: 0.0260\n",
            "Epoch [8528/20000], Training Loss: 0.0248\n",
            "Epoch [8529/20000], Training Loss: 0.0287\n",
            "Epoch [8530/20000], Training Loss: 0.0269\n",
            "Epoch [8531/20000], Training Loss: 0.0277\n",
            "Epoch [8532/20000], Training Loss: 0.0258\n",
            "Epoch [8533/20000], Training Loss: 0.0260\n",
            "Epoch [8534/20000], Training Loss: 0.0252\n",
            "Epoch [8535/20000], Training Loss: 0.0265\n",
            "Epoch [8536/20000], Training Loss: 0.0282\n",
            "Epoch [8537/20000], Training Loss: 0.0261\n",
            "Epoch [8538/20000], Training Loss: 0.0239\n",
            "Epoch [8539/20000], Training Loss: 0.0266\n",
            "Epoch [8540/20000], Training Loss: 0.0250\n",
            "Epoch [8541/20000], Training Loss: 0.0273\n",
            "Epoch [8542/20000], Training Loss: 0.0256\n",
            "Epoch [8543/20000], Training Loss: 0.0284\n",
            "Epoch [8544/20000], Training Loss: 0.0273\n",
            "Epoch [8545/20000], Training Loss: 0.0257\n",
            "Epoch [8546/20000], Training Loss: 0.0263\n",
            "Epoch [8547/20000], Training Loss: 0.0264\n",
            "Epoch [8548/20000], Training Loss: 0.0263\n",
            "Epoch [8549/20000], Training Loss: 0.0258\n",
            "Epoch [8550/20000], Training Loss: 0.0286\n",
            "Epoch [8551/20000], Training Loss: 0.0276\n",
            "Epoch [8552/20000], Training Loss: 0.0291\n",
            "Epoch [8553/20000], Training Loss: 0.0240\n",
            "Epoch [8554/20000], Training Loss: 0.0283\n",
            "Epoch [8555/20000], Training Loss: 0.0259\n",
            "Epoch [8556/20000], Training Loss: 0.0260\n",
            "Epoch [8557/20000], Training Loss: 0.0269\n",
            "Epoch [8558/20000], Training Loss: 0.0255\n",
            "Epoch [8559/20000], Training Loss: 0.0253\n",
            "Epoch [8560/20000], Training Loss: 0.0256\n",
            "Epoch [8561/20000], Training Loss: 0.0247\n",
            "Epoch [8562/20000], Training Loss: 0.0259\n",
            "Epoch [8563/20000], Training Loss: 0.0273\n",
            "Epoch [8564/20000], Training Loss: 0.0256\n",
            "Epoch [8565/20000], Training Loss: 0.0254\n",
            "Epoch [8566/20000], Training Loss: 0.0291\n",
            "Epoch [8567/20000], Training Loss: 0.0252\n",
            "Epoch [8568/20000], Training Loss: 0.0249\n",
            "Epoch [8569/20000], Training Loss: 0.0256\n",
            "Epoch [8570/20000], Training Loss: 0.0270\n",
            "Epoch [8571/20000], Training Loss: 0.0273\n",
            "Epoch [8572/20000], Training Loss: 0.0262\n",
            "Epoch [8573/20000], Training Loss: 0.0260\n",
            "Epoch [8574/20000], Training Loss: 0.0264\n",
            "Epoch [8575/20000], Training Loss: 0.0288\n",
            "Epoch [8576/20000], Training Loss: 0.0266\n",
            "Epoch [8577/20000], Training Loss: 0.0283\n",
            "Epoch [8578/20000], Training Loss: 0.0285\n",
            "Epoch [8579/20000], Training Loss: 0.0246\n",
            "Epoch [8580/20000], Training Loss: 0.0258\n",
            "Epoch [8581/20000], Training Loss: 0.0282\n",
            "Epoch [8582/20000], Training Loss: 0.0250\n",
            "Epoch [8583/20000], Training Loss: 0.0242\n",
            "Epoch [8584/20000], Training Loss: 0.0252\n",
            "Epoch [8585/20000], Training Loss: 0.0298\n",
            "Epoch [8586/20000], Training Loss: 0.0273\n",
            "Epoch [8587/20000], Training Loss: 0.0272\n",
            "Epoch [8588/20000], Training Loss: 0.0295\n",
            "Epoch [8589/20000], Training Loss: 0.0276\n",
            "Epoch [8590/20000], Training Loss: 0.0267\n",
            "Epoch [8591/20000], Training Loss: 0.0264\n",
            "Epoch [8592/20000], Training Loss: 0.0276\n",
            "Epoch [8593/20000], Training Loss: 0.0246\n",
            "Epoch [8594/20000], Training Loss: 0.0277\n",
            "Epoch [8595/20000], Training Loss: 0.0263\n",
            "Epoch [8596/20000], Training Loss: 0.0252\n",
            "Epoch [8597/20000], Training Loss: 0.0257\n",
            "Epoch [8598/20000], Training Loss: 0.0264\n",
            "Epoch [8599/20000], Training Loss: 0.0278\n",
            "Epoch [8600/20000], Training Loss: 0.0266\n",
            "Epoch [8601/20000], Training Loss: 0.0261\n",
            "Epoch [8602/20000], Training Loss: 0.0249\n",
            "Epoch [8603/20000], Training Loss: 0.0237\n",
            "Epoch [8604/20000], Training Loss: 0.0249\n",
            "Epoch [8605/20000], Training Loss: 0.0250\n",
            "Epoch [8606/20000], Training Loss: 0.0272\n",
            "Epoch [8607/20000], Training Loss: 0.0257\n",
            "Epoch [8608/20000], Training Loss: 0.0252\n",
            "Epoch [8609/20000], Training Loss: 0.0265\n",
            "Epoch [8610/20000], Training Loss: 0.0245\n",
            "Epoch [8611/20000], Training Loss: 0.0263\n",
            "Epoch [8612/20000], Training Loss: 0.0261\n",
            "Epoch [8613/20000], Training Loss: 0.0250\n",
            "Epoch [8614/20000], Training Loss: 0.0267\n",
            "Epoch [8615/20000], Training Loss: 0.0272\n",
            "Epoch [8616/20000], Training Loss: 0.0281\n",
            "Epoch [8617/20000], Training Loss: 0.0258\n",
            "Epoch [8618/20000], Training Loss: 0.0268\n",
            "Epoch [8619/20000], Training Loss: 0.0248\n",
            "Epoch [8620/20000], Training Loss: 0.0267\n",
            "Epoch [8621/20000], Training Loss: 0.0261\n",
            "Epoch [8622/20000], Training Loss: 0.0267\n",
            "Epoch [8623/20000], Training Loss: 0.0278\n",
            "Epoch [8624/20000], Training Loss: 0.0249\n",
            "Epoch [8625/20000], Training Loss: 0.0268\n",
            "Epoch [8626/20000], Training Loss: 0.0256\n",
            "Epoch [8627/20000], Training Loss: 0.0268\n",
            "Epoch [8628/20000], Training Loss: 0.0285\n",
            "Epoch [8629/20000], Training Loss: 0.0271\n",
            "Epoch [8630/20000], Training Loss: 0.0284\n",
            "Epoch [8631/20000], Training Loss: 0.0266\n",
            "Epoch [8632/20000], Training Loss: 0.0256\n",
            "Epoch [8633/20000], Training Loss: 0.0274\n",
            "Epoch [8634/20000], Training Loss: 0.0271\n",
            "Epoch [8635/20000], Training Loss: 0.0244\n",
            "Epoch [8636/20000], Training Loss: 0.0255\n",
            "Epoch [8637/20000], Training Loss: 0.0266\n",
            "Epoch [8638/20000], Training Loss: 0.0245\n",
            "Epoch [8639/20000], Training Loss: 0.0274\n",
            "Epoch [8640/20000], Training Loss: 0.0268\n",
            "Epoch [8641/20000], Training Loss: 0.0278\n",
            "Epoch [8642/20000], Training Loss: 0.0261\n",
            "Epoch [8643/20000], Training Loss: 0.0259\n",
            "Epoch [8644/20000], Training Loss: 0.0279\n",
            "Epoch [8645/20000], Training Loss: 0.0268\n",
            "Epoch [8646/20000], Training Loss: 0.0261\n",
            "Epoch [8647/20000], Training Loss: 0.0262\n",
            "Epoch [8648/20000], Training Loss: 0.0242\n",
            "Epoch [8649/20000], Training Loss: 0.0292\n",
            "Epoch [8650/20000], Training Loss: 0.0279\n",
            "Epoch [8651/20000], Training Loss: 0.0268\n",
            "Epoch [8652/20000], Training Loss: 0.0274\n",
            "Epoch [8653/20000], Training Loss: 0.0263\n",
            "Epoch [8654/20000], Training Loss: 0.0286\n",
            "Epoch [8655/20000], Training Loss: 0.0257\n",
            "Epoch [8656/20000], Training Loss: 0.0256\n",
            "Epoch [8657/20000], Training Loss: 0.0274\n",
            "Epoch [8658/20000], Training Loss: 0.0267\n",
            "Epoch [8659/20000], Training Loss: 0.0247\n",
            "Epoch [8660/20000], Training Loss: 0.0274\n",
            "Epoch [8661/20000], Training Loss: 0.0278\n",
            "Epoch [8662/20000], Training Loss: 0.0264\n",
            "Epoch [8663/20000], Training Loss: 0.0249\n",
            "Epoch [8664/20000], Training Loss: 0.0254\n",
            "Epoch [8665/20000], Training Loss: 0.0281\n",
            "Epoch [8666/20000], Training Loss: 0.0258\n",
            "Epoch [8667/20000], Training Loss: 0.0261\n",
            "Epoch [8668/20000], Training Loss: 0.0278\n",
            "Epoch [8669/20000], Training Loss: 0.0275\n",
            "Epoch [8670/20000], Training Loss: 0.0266\n",
            "Epoch [8671/20000], Training Loss: 0.0277\n",
            "Epoch [8672/20000], Training Loss: 0.0267\n",
            "Epoch [8673/20000], Training Loss: 0.0258\n",
            "Epoch [8674/20000], Training Loss: 0.0283\n",
            "Epoch [8675/20000], Training Loss: 0.0262\n",
            "Epoch [8676/20000], Training Loss: 0.0268\n",
            "Epoch [8677/20000], Training Loss: 0.0262\n",
            "Epoch [8678/20000], Training Loss: 0.0257\n",
            "Epoch [8679/20000], Training Loss: 0.0270\n",
            "Epoch [8680/20000], Training Loss: 0.0251\n",
            "Epoch [8681/20000], Training Loss: 0.0279\n",
            "Epoch [8682/20000], Training Loss: 0.0267\n",
            "Epoch [8683/20000], Training Loss: 0.0264\n",
            "Epoch [8684/20000], Training Loss: 0.0291\n",
            "Epoch [8685/20000], Training Loss: 0.0258\n",
            "Epoch [8686/20000], Training Loss: 0.0250\n",
            "Epoch [8687/20000], Training Loss: 0.0246\n",
            "Epoch [8688/20000], Training Loss: 0.0263\n",
            "Epoch [8689/20000], Training Loss: 0.0263\n",
            "Epoch [8690/20000], Training Loss: 0.0274\n",
            "Epoch [8691/20000], Training Loss: 0.0260\n",
            "Epoch [8692/20000], Training Loss: 0.0273\n",
            "Epoch [8693/20000], Training Loss: 0.0261\n",
            "Epoch [8694/20000], Training Loss: 0.0258\n",
            "Epoch [8695/20000], Training Loss: 0.0261\n",
            "Epoch [8696/20000], Training Loss: 0.0264\n",
            "Epoch [8697/20000], Training Loss: 0.0265\n",
            "Epoch [8698/20000], Training Loss: 0.0271\n",
            "Epoch [8699/20000], Training Loss: 0.0280\n",
            "Epoch [8700/20000], Training Loss: 0.0285\n",
            "Epoch [8701/20000], Training Loss: 0.0258\n",
            "Epoch [8702/20000], Training Loss: 0.0279\n",
            "Epoch [8703/20000], Training Loss: 0.0254\n",
            "Epoch [8704/20000], Training Loss: 0.0253\n",
            "Epoch [8705/20000], Training Loss: 0.0273\n",
            "Epoch [8706/20000], Training Loss: 0.0270\n",
            "Epoch [8707/20000], Training Loss: 0.0282\n",
            "Epoch [8708/20000], Training Loss: 0.0245\n",
            "Epoch [8709/20000], Training Loss: 0.0250\n",
            "Epoch [8710/20000], Training Loss: 0.0249\n",
            "Epoch [8711/20000], Training Loss: 0.0262\n",
            "Epoch [8712/20000], Training Loss: 0.0272\n",
            "Epoch [8713/20000], Training Loss: 0.0246\n",
            "Epoch [8714/20000], Training Loss: 0.0263\n",
            "Epoch [8715/20000], Training Loss: 0.0273\n",
            "Epoch [8716/20000], Training Loss: 0.0263\n",
            "Epoch [8717/20000], Training Loss: 0.0272\n",
            "Epoch [8718/20000], Training Loss: 0.0268\n",
            "Epoch [8719/20000], Training Loss: 0.0273\n",
            "Epoch [8720/20000], Training Loss: 0.0256\n",
            "Epoch [8721/20000], Training Loss: 0.0256\n",
            "Epoch [8722/20000], Training Loss: 0.0277\n",
            "Epoch [8723/20000], Training Loss: 0.0269\n",
            "Epoch [8724/20000], Training Loss: 0.0268\n",
            "Epoch [8725/20000], Training Loss: 0.0291\n",
            "Epoch [8726/20000], Training Loss: 0.0269\n",
            "Epoch [8727/20000], Training Loss: 0.0282\n",
            "Epoch [8728/20000], Training Loss: 0.0283\n",
            "Epoch [8729/20000], Training Loss: 0.0266\n",
            "Epoch [8730/20000], Training Loss: 0.0268\n",
            "Epoch [8731/20000], Training Loss: 0.0287\n",
            "Epoch [8732/20000], Training Loss: 0.0240\n",
            "Epoch [8733/20000], Training Loss: 0.0261\n",
            "Epoch [8734/20000], Training Loss: 0.0260\n",
            "Epoch [8735/20000], Training Loss: 0.0270\n",
            "Epoch [8736/20000], Training Loss: 0.0278\n",
            "Epoch [8737/20000], Training Loss: 0.0278\n",
            "Epoch [8738/20000], Training Loss: 0.0241\n",
            "Epoch [8739/20000], Training Loss: 0.0250\n",
            "Epoch [8740/20000], Training Loss: 0.0291\n",
            "Epoch [8741/20000], Training Loss: 0.0268\n",
            "Epoch [8742/20000], Training Loss: 0.0248\n",
            "Epoch [8743/20000], Training Loss: 0.0247\n",
            "Epoch [8744/20000], Training Loss: 0.0264\n",
            "Epoch [8745/20000], Training Loss: 0.0258\n",
            "Epoch [8746/20000], Training Loss: 0.0268\n",
            "Epoch [8747/20000], Training Loss: 0.0258\n",
            "Epoch [8748/20000], Training Loss: 0.0263\n",
            "Epoch [8749/20000], Training Loss: 0.0267\n",
            "Epoch [8750/20000], Training Loss: 0.0264\n",
            "Epoch [8751/20000], Training Loss: 0.0269\n",
            "Epoch [8752/20000], Training Loss: 0.0285\n",
            "Epoch [8753/20000], Training Loss: 0.0253\n",
            "Epoch [8754/20000], Training Loss: 0.0253\n",
            "Epoch [8755/20000], Training Loss: 0.0245\n",
            "Epoch [8756/20000], Training Loss: 0.0247\n",
            "Epoch [8757/20000], Training Loss: 0.0265\n",
            "Epoch [8758/20000], Training Loss: 0.0265\n",
            "Epoch [8759/20000], Training Loss: 0.0266\n",
            "Epoch [8760/20000], Training Loss: 0.0270\n",
            "Epoch [8761/20000], Training Loss: 0.0270\n",
            "Epoch [8762/20000], Training Loss: 0.0261\n",
            "Epoch [8763/20000], Training Loss: 0.0253\n",
            "Epoch [8764/20000], Training Loss: 0.0240\n",
            "Epoch [8765/20000], Training Loss: 0.0273\n",
            "Epoch [8766/20000], Training Loss: 0.0252\n",
            "Epoch [8767/20000], Training Loss: 0.0271\n",
            "Epoch [8768/20000], Training Loss: 0.0253\n",
            "Epoch [8769/20000], Training Loss: 0.0268\n",
            "Epoch [8770/20000], Training Loss: 0.0265\n",
            "Epoch [8771/20000], Training Loss: 0.0272\n",
            "Epoch [8772/20000], Training Loss: 0.0258\n",
            "Epoch [8773/20000], Training Loss: 0.0265\n",
            "Epoch [8774/20000], Training Loss: 0.0250\n",
            "Epoch [8775/20000], Training Loss: 0.0271\n",
            "Epoch [8776/20000], Training Loss: 0.0266\n",
            "Epoch [8777/20000], Training Loss: 0.0275\n",
            "Epoch [8778/20000], Training Loss: 0.0276\n",
            "Epoch [8779/20000], Training Loss: 0.0260\n",
            "Epoch [8780/20000], Training Loss: 0.0262\n",
            "Epoch [8781/20000], Training Loss: 0.0258\n",
            "Epoch [8782/20000], Training Loss: 0.0254\n",
            "Epoch [8783/20000], Training Loss: 0.0258\n",
            "Epoch [8784/20000], Training Loss: 0.0268\n",
            "Epoch [8785/20000], Training Loss: 0.0246\n",
            "Epoch [8786/20000], Training Loss: 0.0250\n",
            "Epoch [8787/20000], Training Loss: 0.0270\n",
            "Epoch [8788/20000], Training Loss: 0.0273\n",
            "Epoch [8789/20000], Training Loss: 0.0280\n",
            "Epoch [8790/20000], Training Loss: 0.0276\n",
            "Epoch [8791/20000], Training Loss: 0.0249\n",
            "Epoch [8792/20000], Training Loss: 0.0255\n",
            "Epoch [8793/20000], Training Loss: 0.0266\n",
            "Epoch [8794/20000], Training Loss: 0.0253\n",
            "Epoch [8795/20000], Training Loss: 0.0275\n",
            "Epoch [8796/20000], Training Loss: 0.0254\n",
            "Epoch [8797/20000], Training Loss: 0.0276\n",
            "Epoch [8798/20000], Training Loss: 0.0269\n",
            "Epoch [8799/20000], Training Loss: 0.0252\n",
            "Epoch [8800/20000], Training Loss: 0.0262\n",
            "Epoch [8801/20000], Training Loss: 0.0253\n",
            "Epoch [8802/20000], Training Loss: 0.0246\n",
            "Epoch [8803/20000], Training Loss: 0.0257\n",
            "Epoch [8804/20000], Training Loss: 0.0243\n",
            "Epoch [8805/20000], Training Loss: 0.0253\n",
            "Epoch [8806/20000], Training Loss: 0.0274\n",
            "Epoch [8807/20000], Training Loss: 0.0267\n",
            "Epoch [8808/20000], Training Loss: 0.0287\n",
            "Epoch [8809/20000], Training Loss: 0.0268\n",
            "Epoch [8810/20000], Training Loss: 0.0272\n",
            "Epoch [8811/20000], Training Loss: 0.0258\n",
            "Epoch [8812/20000], Training Loss: 0.0266\n",
            "Epoch [8813/20000], Training Loss: 0.0272\n",
            "Epoch [8814/20000], Training Loss: 0.0261\n",
            "Epoch [8815/20000], Training Loss: 0.0262\n",
            "Epoch [8816/20000], Training Loss: 0.0263\n",
            "Epoch [8817/20000], Training Loss: 0.0278\n",
            "Epoch [8818/20000], Training Loss: 0.0279\n",
            "Epoch [8819/20000], Training Loss: 0.0278\n",
            "Epoch [8820/20000], Training Loss: 0.0276\n",
            "Epoch [8821/20000], Training Loss: 0.0264\n",
            "Epoch [8822/20000], Training Loss: 0.0250\n",
            "Epoch [8823/20000], Training Loss: 0.0258\n",
            "Epoch [8824/20000], Training Loss: 0.0250\n",
            "Epoch [8825/20000], Training Loss: 0.0272\n",
            "Epoch [8826/20000], Training Loss: 0.0245\n",
            "Epoch [8827/20000], Training Loss: 0.0263\n",
            "Epoch [8828/20000], Training Loss: 0.0277\n",
            "Epoch [8829/20000], Training Loss: 0.0291\n",
            "Epoch [8830/20000], Training Loss: 0.0250\n",
            "Epoch [8831/20000], Training Loss: 0.0264\n",
            "Epoch [8832/20000], Training Loss: 0.0256\n",
            "Epoch [8833/20000], Training Loss: 0.0272\n",
            "Epoch [8834/20000], Training Loss: 0.0266\n",
            "Epoch [8835/20000], Training Loss: 0.0263\n",
            "Epoch [8836/20000], Training Loss: 0.0248\n",
            "Epoch [8837/20000], Training Loss: 0.0272\n",
            "Epoch [8838/20000], Training Loss: 0.0281\n",
            "Epoch [8839/20000], Training Loss: 0.0263\n",
            "Epoch [8840/20000], Training Loss: 0.0296\n",
            "Epoch [8841/20000], Training Loss: 0.0264\n",
            "Epoch [8842/20000], Training Loss: 0.0265\n",
            "Epoch [8843/20000], Training Loss: 0.0282\n",
            "Epoch [8844/20000], Training Loss: 0.0282\n",
            "Epoch [8845/20000], Training Loss: 0.0259\n",
            "Epoch [8846/20000], Training Loss: 0.0256\n",
            "Epoch [8847/20000], Training Loss: 0.0266\n",
            "Epoch [8848/20000], Training Loss: 0.0255\n",
            "Epoch [8849/20000], Training Loss: 0.0264\n",
            "Epoch [8850/20000], Training Loss: 0.0268\n",
            "Epoch [8851/20000], Training Loss: 0.0256\n",
            "Epoch [8852/20000], Training Loss: 0.0281\n",
            "Epoch [8853/20000], Training Loss: 0.0246\n",
            "Epoch [8854/20000], Training Loss: 0.0254\n",
            "Epoch [8855/20000], Training Loss: 0.0274\n",
            "Epoch [8856/20000], Training Loss: 0.0277\n",
            "Epoch [8857/20000], Training Loss: 0.0249\n",
            "Epoch [8858/20000], Training Loss: 0.0247\n",
            "Epoch [8859/20000], Training Loss: 0.0259\n",
            "Epoch [8860/20000], Training Loss: 0.0286\n",
            "Epoch [8861/20000], Training Loss: 0.0258\n",
            "Epoch [8862/20000], Training Loss: 0.0269\n",
            "Epoch [8863/20000], Training Loss: 0.0289\n",
            "Epoch [8864/20000], Training Loss: 0.0265\n",
            "Epoch [8865/20000], Training Loss: 0.0273\n",
            "Epoch [8866/20000], Training Loss: 0.0265\n",
            "Epoch [8867/20000], Training Loss: 0.0282\n",
            "Epoch [8868/20000], Training Loss: 0.0246\n",
            "Epoch [8869/20000], Training Loss: 0.0261\n",
            "Epoch [8870/20000], Training Loss: 0.0272\n",
            "Epoch [8871/20000], Training Loss: 0.0251\n",
            "Epoch [8872/20000], Training Loss: 0.0260\n",
            "Epoch [8873/20000], Training Loss: 0.0277\n",
            "Epoch [8874/20000], Training Loss: 0.0255\n",
            "Epoch [8875/20000], Training Loss: 0.0263\n",
            "Epoch [8876/20000], Training Loss: 0.0283\n",
            "Epoch [8877/20000], Training Loss: 0.0250\n",
            "Epoch [8878/20000], Training Loss: 0.0240\n",
            "Epoch [8879/20000], Training Loss: 0.0266\n",
            "Epoch [8880/20000], Training Loss: 0.0273\n",
            "Epoch [8881/20000], Training Loss: 0.0272\n",
            "Epoch [8882/20000], Training Loss: 0.0270\n",
            "Epoch [8883/20000], Training Loss: 0.0283\n",
            "Epoch [8884/20000], Training Loss: 0.0278\n",
            "Epoch [8885/20000], Training Loss: 0.0268\n",
            "Epoch [8886/20000], Training Loss: 0.0273\n",
            "Epoch [8887/20000], Training Loss: 0.0263\n",
            "Epoch [8888/20000], Training Loss: 0.0256\n",
            "Epoch [8889/20000], Training Loss: 0.0252\n",
            "Epoch [8890/20000], Training Loss: 0.0271\n",
            "Epoch [8891/20000], Training Loss: 0.0268\n",
            "Epoch [8892/20000], Training Loss: 0.0272\n",
            "Epoch [8893/20000], Training Loss: 0.0250\n",
            "Epoch [8894/20000], Training Loss: 0.0249\n",
            "Epoch [8895/20000], Training Loss: 0.0268\n",
            "Epoch [8896/20000], Training Loss: 0.0270\n",
            "Epoch [8897/20000], Training Loss: 0.0286\n",
            "Epoch [8898/20000], Training Loss: 0.0274\n",
            "Epoch [8899/20000], Training Loss: 0.0253\n",
            "Epoch [8900/20000], Training Loss: 0.0264\n",
            "Epoch [8901/20000], Training Loss: 0.0274\n",
            "Epoch [8902/20000], Training Loss: 0.0245\n",
            "Epoch [8903/20000], Training Loss: 0.0267\n",
            "Epoch [8904/20000], Training Loss: 0.0240\n",
            "Epoch [8905/20000], Training Loss: 0.0258\n",
            "Epoch [8906/20000], Training Loss: 0.0275\n",
            "Epoch [8907/20000], Training Loss: 0.0281\n",
            "Epoch [8908/20000], Training Loss: 0.0258\n",
            "Epoch [8909/20000], Training Loss: 0.0264\n",
            "Epoch [8910/20000], Training Loss: 0.0264\n",
            "Epoch [8911/20000], Training Loss: 0.0269\n",
            "Epoch [8912/20000], Training Loss: 0.0243\n",
            "Epoch [8913/20000], Training Loss: 0.0261\n",
            "Epoch [8914/20000], Training Loss: 0.0259\n",
            "Epoch [8915/20000], Training Loss: 0.0273\n",
            "Epoch [8916/20000], Training Loss: 0.0288\n",
            "Epoch [8917/20000], Training Loss: 0.0258\n",
            "Epoch [8918/20000], Training Loss: 0.0268\n",
            "Epoch [8919/20000], Training Loss: 0.0279\n",
            "Epoch [8920/20000], Training Loss: 0.0264\n",
            "Epoch [8921/20000], Training Loss: 0.0255\n",
            "Epoch [8922/20000], Training Loss: 0.0264\n",
            "Epoch [8923/20000], Training Loss: 0.0262\n",
            "Epoch [8924/20000], Training Loss: 0.0263\n",
            "Epoch [8925/20000], Training Loss: 0.0269\n",
            "Epoch [8926/20000], Training Loss: 0.0276\n",
            "Epoch [8927/20000], Training Loss: 0.0266\n",
            "Epoch [8928/20000], Training Loss: 0.0257\n",
            "Epoch [8929/20000], Training Loss: 0.0276\n",
            "Epoch [8930/20000], Training Loss: 0.0262\n",
            "Epoch [8931/20000], Training Loss: 0.0230\n",
            "Epoch [8932/20000], Training Loss: 0.0255\n",
            "Epoch [8933/20000], Training Loss: 0.0278\n",
            "Epoch [8934/20000], Training Loss: 0.0275\n",
            "Epoch [8935/20000], Training Loss: 0.0261\n",
            "Epoch [8936/20000], Training Loss: 0.0261\n",
            "Epoch [8937/20000], Training Loss: 0.0275\n",
            "Epoch [8938/20000], Training Loss: 0.0252\n",
            "Epoch [8939/20000], Training Loss: 0.0247\n",
            "Epoch [8940/20000], Training Loss: 0.0261\n",
            "Epoch [8941/20000], Training Loss: 0.0261\n",
            "Epoch [8942/20000], Training Loss: 0.0260\n",
            "Epoch [8943/20000], Training Loss: 0.0260\n",
            "Epoch [8944/20000], Training Loss: 0.0249\n",
            "Epoch [8945/20000], Training Loss: 0.0268\n",
            "Epoch [8946/20000], Training Loss: 0.0270\n",
            "Epoch [8947/20000], Training Loss: 0.0273\n",
            "Epoch [8948/20000], Training Loss: 0.0252\n",
            "Epoch [8949/20000], Training Loss: 0.0253\n",
            "Epoch [8950/20000], Training Loss: 0.0281\n",
            "Epoch [8951/20000], Training Loss: 0.0275\n",
            "Epoch [8952/20000], Training Loss: 0.0277\n",
            "Epoch [8953/20000], Training Loss: 0.0274\n",
            "Epoch [8954/20000], Training Loss: 0.0284\n",
            "Epoch [8955/20000], Training Loss: 0.0250\n",
            "Epoch [8956/20000], Training Loss: 0.0271\n",
            "Epoch [8957/20000], Training Loss: 0.0259\n",
            "Epoch [8958/20000], Training Loss: 0.0292\n",
            "Epoch [8959/20000], Training Loss: 0.0261\n",
            "Epoch [8960/20000], Training Loss: 0.0267\n",
            "Epoch [8961/20000], Training Loss: 0.0265\n",
            "Epoch [8962/20000], Training Loss: 0.0275\n",
            "Epoch [8963/20000], Training Loss: 0.0271\n",
            "Epoch [8964/20000], Training Loss: 0.0268\n",
            "Epoch [8965/20000], Training Loss: 0.0279\n",
            "Epoch [8966/20000], Training Loss: 0.0251\n",
            "Epoch [8967/20000], Training Loss: 0.0271\n",
            "Epoch [8968/20000], Training Loss: 0.0254\n",
            "Epoch [8969/20000], Training Loss: 0.0278\n",
            "Epoch [8970/20000], Training Loss: 0.0267\n",
            "Epoch [8971/20000], Training Loss: 0.0274\n",
            "Epoch [8972/20000], Training Loss: 0.0270\n",
            "Epoch [8973/20000], Training Loss: 0.0270\n",
            "Epoch [8974/20000], Training Loss: 0.0254\n",
            "Epoch [8975/20000], Training Loss: 0.0275\n",
            "Epoch [8976/20000], Training Loss: 0.0260\n",
            "Epoch [8977/20000], Training Loss: 0.0265\n",
            "Epoch [8978/20000], Training Loss: 0.0256\n",
            "Epoch [8979/20000], Training Loss: 0.0256\n",
            "Epoch [8980/20000], Training Loss: 0.0273\n",
            "Epoch [8981/20000], Training Loss: 0.0290\n",
            "Epoch [8982/20000], Training Loss: 0.0268\n",
            "Epoch [8983/20000], Training Loss: 0.0254\n",
            "Epoch [8984/20000], Training Loss: 0.0259\n",
            "Epoch [8985/20000], Training Loss: 0.0287\n",
            "Epoch [8986/20000], Training Loss: 0.0257\n",
            "Epoch [8987/20000], Training Loss: 0.0276\n",
            "Epoch [8988/20000], Training Loss: 0.0261\n",
            "Epoch [8989/20000], Training Loss: 0.0283\n",
            "Epoch [8990/20000], Training Loss: 0.0253\n",
            "Epoch [8991/20000], Training Loss: 0.0284\n",
            "Epoch [8992/20000], Training Loss: 0.0263\n",
            "Epoch [8993/20000], Training Loss: 0.0248\n",
            "Epoch [8994/20000], Training Loss: 0.0250\n",
            "Epoch [8995/20000], Training Loss: 0.0246\n",
            "Epoch [8996/20000], Training Loss: 0.0273\n",
            "Epoch [8997/20000], Training Loss: 0.0258\n",
            "Epoch [8998/20000], Training Loss: 0.0270\n",
            "Epoch [8999/20000], Training Loss: 0.0248\n",
            "Epoch [9000/20000], Training Loss: 0.0283\n",
            "Epoch [9001/20000], Training Loss: 0.0278\n",
            "Epoch [9002/20000], Training Loss: 0.0259\n",
            "Epoch [9003/20000], Training Loss: 0.0261\n",
            "Epoch [9004/20000], Training Loss: 0.0269\n",
            "Epoch [9005/20000], Training Loss: 0.0256\n",
            "Epoch [9006/20000], Training Loss: 0.0253\n",
            "Epoch [9007/20000], Training Loss: 0.0263\n",
            "Epoch [9008/20000], Training Loss: 0.0259\n",
            "Epoch [9009/20000], Training Loss: 0.0259\n",
            "Epoch [9010/20000], Training Loss: 0.0249\n",
            "Epoch [9011/20000], Training Loss: 0.0255\n",
            "Epoch [9012/20000], Training Loss: 0.0266\n",
            "Epoch [9013/20000], Training Loss: 0.0257\n",
            "Epoch [9014/20000], Training Loss: 0.0257\n",
            "Epoch [9015/20000], Training Loss: 0.0288\n",
            "Epoch [9016/20000], Training Loss: 0.0258\n",
            "Epoch [9017/20000], Training Loss: 0.0263\n",
            "Epoch [9018/20000], Training Loss: 0.0285\n",
            "Epoch [9019/20000], Training Loss: 0.0270\n",
            "Epoch [9020/20000], Training Loss: 0.0268\n",
            "Epoch [9021/20000], Training Loss: 0.0266\n",
            "Epoch [9022/20000], Training Loss: 0.0282\n",
            "Epoch [9023/20000], Training Loss: 0.0247\n",
            "Epoch [9024/20000], Training Loss: 0.0245\n",
            "Epoch [9025/20000], Training Loss: 0.0272\n",
            "Epoch [9026/20000], Training Loss: 0.0269\n",
            "Epoch [9027/20000], Training Loss: 0.0276\n",
            "Epoch [9028/20000], Training Loss: 0.0257\n",
            "Epoch [9029/20000], Training Loss: 0.0252\n",
            "Epoch [9030/20000], Training Loss: 0.0263\n",
            "Epoch [9031/20000], Training Loss: 0.0255\n",
            "Epoch [9032/20000], Training Loss: 0.0251\n",
            "Epoch [9033/20000], Training Loss: 0.0269\n",
            "Epoch [9034/20000], Training Loss: 0.0272\n",
            "Epoch [9035/20000], Training Loss: 0.0247\n",
            "Epoch [9036/20000], Training Loss: 0.0265\n",
            "Epoch [9037/20000], Training Loss: 0.0269\n",
            "Epoch [9038/20000], Training Loss: 0.0257\n",
            "Epoch [9039/20000], Training Loss: 0.0285\n",
            "Epoch [9040/20000], Training Loss: 0.0267\n",
            "Epoch [9041/20000], Training Loss: 0.0238\n",
            "Epoch [9042/20000], Training Loss: 0.0260\n",
            "Epoch [9043/20000], Training Loss: 0.0293\n",
            "Epoch [9044/20000], Training Loss: 0.0272\n",
            "Epoch [9045/20000], Training Loss: 0.0257\n",
            "Epoch [9046/20000], Training Loss: 0.0261\n",
            "Epoch [9047/20000], Training Loss: 0.0254\n",
            "Epoch [9048/20000], Training Loss: 0.0271\n",
            "Epoch [9049/20000], Training Loss: 0.0273\n",
            "Epoch [9050/20000], Training Loss: 0.0257\n",
            "Epoch [9051/20000], Training Loss: 0.0253\n",
            "Epoch [9052/20000], Training Loss: 0.0262\n",
            "Epoch [9053/20000], Training Loss: 0.0266\n",
            "Epoch [9054/20000], Training Loss: 0.0279\n",
            "Epoch [9055/20000], Training Loss: 0.0254\n",
            "Epoch [9056/20000], Training Loss: 0.0272\n",
            "Epoch [9057/20000], Training Loss: 0.0285\n",
            "Epoch [9058/20000], Training Loss: 0.0268\n",
            "Epoch [9059/20000], Training Loss: 0.0267\n",
            "Epoch [9060/20000], Training Loss: 0.0276\n",
            "Epoch [9061/20000], Training Loss: 0.0250\n",
            "Epoch [9062/20000], Training Loss: 0.0245\n",
            "Epoch [9063/20000], Training Loss: 0.0256\n",
            "Epoch [9064/20000], Training Loss: 0.0260\n",
            "Epoch [9065/20000], Training Loss: 0.0272\n",
            "Epoch [9066/20000], Training Loss: 0.0256\n",
            "Epoch [9067/20000], Training Loss: 0.0279\n",
            "Epoch [9068/20000], Training Loss: 0.0294\n",
            "Epoch [9069/20000], Training Loss: 0.0253\n",
            "Epoch [9070/20000], Training Loss: 0.0290\n",
            "Epoch [9071/20000], Training Loss: 0.0276\n",
            "Epoch [9072/20000], Training Loss: 0.0250\n",
            "Epoch [9073/20000], Training Loss: 0.0270\n",
            "Epoch [9074/20000], Training Loss: 0.0256\n",
            "Epoch [9075/20000], Training Loss: 0.0279\n",
            "Epoch [9076/20000], Training Loss: 0.0256\n",
            "Epoch [9077/20000], Training Loss: 0.0267\n",
            "Epoch [9078/20000], Training Loss: 0.0244\n",
            "Epoch [9079/20000], Training Loss: 0.0272\n",
            "Epoch [9080/20000], Training Loss: 0.0254\n",
            "Epoch [9081/20000], Training Loss: 0.0246\n",
            "Epoch [9082/20000], Training Loss: 0.0248\n",
            "Epoch [9083/20000], Training Loss: 0.0245\n",
            "Epoch [9084/20000], Training Loss: 0.0242\n",
            "Epoch [9085/20000], Training Loss: 0.0247\n",
            "Epoch [9086/20000], Training Loss: 0.0269\n",
            "Epoch [9087/20000], Training Loss: 0.0272\n",
            "Epoch [9088/20000], Training Loss: 0.0263\n",
            "Epoch [9089/20000], Training Loss: 0.0255\n",
            "Epoch [9090/20000], Training Loss: 0.0248\n",
            "Epoch [9091/20000], Training Loss: 0.0254\n",
            "Epoch [9092/20000], Training Loss: 0.0276\n",
            "Epoch [9093/20000], Training Loss: 0.0271\n",
            "Epoch [9094/20000], Training Loss: 0.0289\n",
            "Epoch [9095/20000], Training Loss: 0.0250\n",
            "Epoch [9096/20000], Training Loss: 0.0283\n",
            "Epoch [9097/20000], Training Loss: 0.0289\n",
            "Epoch [9098/20000], Training Loss: 0.0255\n",
            "Epoch [9099/20000], Training Loss: 0.0259\n",
            "Epoch [9100/20000], Training Loss: 0.0258\n",
            "Epoch [9101/20000], Training Loss: 0.0282\n",
            "Epoch [9102/20000], Training Loss: 0.0263\n",
            "Epoch [9103/20000], Training Loss: 0.0265\n",
            "Epoch [9104/20000], Training Loss: 0.0258\n",
            "Epoch [9105/20000], Training Loss: 0.0288\n",
            "Epoch [9106/20000], Training Loss: 0.0254\n",
            "Epoch [9107/20000], Training Loss: 0.0241\n",
            "Epoch [9108/20000], Training Loss: 0.0254\n",
            "Epoch [9109/20000], Training Loss: 0.0270\n",
            "Epoch [9110/20000], Training Loss: 0.0241\n",
            "Epoch [9111/20000], Training Loss: 0.0268\n",
            "Epoch [9112/20000], Training Loss: 0.0266\n",
            "Epoch [9113/20000], Training Loss: 0.0247\n",
            "Epoch [9114/20000], Training Loss: 0.0260\n",
            "Epoch [9115/20000], Training Loss: 0.0261\n",
            "Epoch [9116/20000], Training Loss: 0.0263\n",
            "Epoch [9117/20000], Training Loss: 0.0272\n",
            "Epoch [9118/20000], Training Loss: 0.0270\n",
            "Epoch [9119/20000], Training Loss: 0.0267\n",
            "Epoch [9120/20000], Training Loss: 0.0283\n",
            "Epoch [9121/20000], Training Loss: 0.0281\n",
            "Epoch [9122/20000], Training Loss: 0.0257\n",
            "Epoch [9123/20000], Training Loss: 0.0273\n",
            "Epoch [9124/20000], Training Loss: 0.0235\n",
            "Epoch [9125/20000], Training Loss: 0.0287\n",
            "Epoch [9126/20000], Training Loss: 0.0289\n",
            "Epoch [9127/20000], Training Loss: 0.0281\n",
            "Epoch [9128/20000], Training Loss: 0.0286\n",
            "Epoch [9129/20000], Training Loss: 0.0258\n",
            "Epoch [9130/20000], Training Loss: 0.0277\n",
            "Epoch [9131/20000], Training Loss: 0.0263\n",
            "Epoch [9132/20000], Training Loss: 0.0266\n",
            "Epoch [9133/20000], Training Loss: 0.0240\n",
            "Epoch [9134/20000], Training Loss: 0.0258\n",
            "Epoch [9135/20000], Training Loss: 0.0249\n",
            "Epoch [9136/20000], Training Loss: 0.0271\n",
            "Epoch [9137/20000], Training Loss: 0.0256\n",
            "Epoch [9138/20000], Training Loss: 0.0256\n",
            "Epoch [9139/20000], Training Loss: 0.0276\n",
            "Epoch [9140/20000], Training Loss: 0.0266\n",
            "Epoch [9141/20000], Training Loss: 0.0261\n",
            "Epoch [9142/20000], Training Loss: 0.0260\n",
            "Epoch [9143/20000], Training Loss: 0.0267\n",
            "Epoch [9144/20000], Training Loss: 0.0257\n",
            "Epoch [9145/20000], Training Loss: 0.0276\n",
            "Epoch [9146/20000], Training Loss: 0.0257\n",
            "Epoch [9147/20000], Training Loss: 0.0276\n",
            "Epoch [9148/20000], Training Loss: 0.0250\n",
            "Epoch [9149/20000], Training Loss: 0.0263\n",
            "Epoch [9150/20000], Training Loss: 0.0261\n",
            "Epoch [9151/20000], Training Loss: 0.0267\n",
            "Epoch [9152/20000], Training Loss: 0.0255\n",
            "Epoch [9153/20000], Training Loss: 0.0293\n",
            "Epoch [9154/20000], Training Loss: 0.0268\n",
            "Epoch [9155/20000], Training Loss: 0.0279\n",
            "Epoch [9156/20000], Training Loss: 0.0263\n",
            "Epoch [9157/20000], Training Loss: 0.0287\n",
            "Epoch [9158/20000], Training Loss: 0.0237\n",
            "Epoch [9159/20000], Training Loss: 0.0263\n",
            "Epoch [9160/20000], Training Loss: 0.0264\n",
            "Epoch [9161/20000], Training Loss: 0.0267\n",
            "Epoch [9162/20000], Training Loss: 0.0257\n",
            "Epoch [9163/20000], Training Loss: 0.0260\n",
            "Epoch [9164/20000], Training Loss: 0.0265\n",
            "Epoch [9165/20000], Training Loss: 0.0247\n",
            "Epoch [9166/20000], Training Loss: 0.0245\n",
            "Epoch [9167/20000], Training Loss: 0.0264\n",
            "Epoch [9168/20000], Training Loss: 0.0274\n",
            "Epoch [9169/20000], Training Loss: 0.0263\n",
            "Epoch [9170/20000], Training Loss: 0.0265\n",
            "Epoch [9171/20000], Training Loss: 0.0264\n",
            "Epoch [9172/20000], Training Loss: 0.0265\n",
            "Epoch [9173/20000], Training Loss: 0.0269\n",
            "Epoch [9174/20000], Training Loss: 0.0250\n",
            "Epoch [9175/20000], Training Loss: 0.0279\n",
            "Epoch [9176/20000], Training Loss: 0.0266\n",
            "Epoch [9177/20000], Training Loss: 0.0263\n",
            "Epoch [9178/20000], Training Loss: 0.0267\n",
            "Epoch [9179/20000], Training Loss: 0.0253\n",
            "Epoch [9180/20000], Training Loss: 0.0286\n",
            "Epoch [9181/20000], Training Loss: 0.0272\n",
            "Epoch [9182/20000], Training Loss: 0.0260\n",
            "Epoch [9183/20000], Training Loss: 0.0258\n",
            "Epoch [9184/20000], Training Loss: 0.0260\n",
            "Epoch [9185/20000], Training Loss: 0.0272\n",
            "Epoch [9186/20000], Training Loss: 0.0274\n",
            "Epoch [9187/20000], Training Loss: 0.0264\n",
            "Epoch [9188/20000], Training Loss: 0.0269\n",
            "Epoch [9189/20000], Training Loss: 0.0268\n",
            "Epoch [9190/20000], Training Loss: 0.0248\n",
            "Epoch [9191/20000], Training Loss: 0.0262\n",
            "Epoch [9192/20000], Training Loss: 0.0246\n",
            "Epoch [9193/20000], Training Loss: 0.0271\n",
            "Epoch [9194/20000], Training Loss: 0.0262\n",
            "Epoch [9195/20000], Training Loss: 0.0268\n",
            "Epoch [9196/20000], Training Loss: 0.0272\n",
            "Epoch [9197/20000], Training Loss: 0.0263\n",
            "Epoch [9198/20000], Training Loss: 0.0259\n",
            "Epoch [9199/20000], Training Loss: 0.0270\n",
            "Epoch [9200/20000], Training Loss: 0.0255\n",
            "Epoch [9201/20000], Training Loss: 0.0257\n",
            "Epoch [9202/20000], Training Loss: 0.0258\n",
            "Epoch [9203/20000], Training Loss: 0.0284\n",
            "Epoch [9204/20000], Training Loss: 0.0273\n",
            "Epoch [9205/20000], Training Loss: 0.0252\n",
            "Epoch [9206/20000], Training Loss: 0.0269\n",
            "Epoch [9207/20000], Training Loss: 0.0252\n",
            "Epoch [9208/20000], Training Loss: 0.0269\n",
            "Epoch [9209/20000], Training Loss: 0.0258\n",
            "Epoch [9210/20000], Training Loss: 0.0253\n",
            "Epoch [9211/20000], Training Loss: 0.0279\n",
            "Epoch [9212/20000], Training Loss: 0.0257\n",
            "Epoch [9213/20000], Training Loss: 0.0254\n",
            "Epoch [9214/20000], Training Loss: 0.0275\n",
            "Epoch [9215/20000], Training Loss: 0.0283\n",
            "Epoch [9216/20000], Training Loss: 0.0272\n",
            "Epoch [9217/20000], Training Loss: 0.0260\n",
            "Epoch [9218/20000], Training Loss: 0.0280\n",
            "Epoch [9219/20000], Training Loss: 0.0258\n",
            "Epoch [9220/20000], Training Loss: 0.0268\n",
            "Epoch [9221/20000], Training Loss: 0.0267\n",
            "Epoch [9222/20000], Training Loss: 0.0279\n",
            "Epoch [9223/20000], Training Loss: 0.0248\n",
            "Epoch [9224/20000], Training Loss: 0.0261\n",
            "Epoch [9225/20000], Training Loss: 0.0247\n",
            "Epoch [9226/20000], Training Loss: 0.0257\n",
            "Epoch [9227/20000], Training Loss: 0.0258\n",
            "Epoch [9228/20000], Training Loss: 0.0264\n",
            "Epoch [9229/20000], Training Loss: 0.0254\n",
            "Epoch [9230/20000], Training Loss: 0.0267\n",
            "Epoch [9231/20000], Training Loss: 0.0263\n",
            "Epoch [9232/20000], Training Loss: 0.0268\n",
            "Epoch [9233/20000], Training Loss: 0.0281\n",
            "Epoch [9234/20000], Training Loss: 0.0264\n",
            "Epoch [9235/20000], Training Loss: 0.0255\n",
            "Epoch [9236/20000], Training Loss: 0.0239\n",
            "Epoch [9237/20000], Training Loss: 0.0278\n",
            "Epoch [9238/20000], Training Loss: 0.0262\n",
            "Epoch [9239/20000], Training Loss: 0.0267\n",
            "Epoch [9240/20000], Training Loss: 0.0254\n",
            "Epoch [9241/20000], Training Loss: 0.0271\n",
            "Epoch [9242/20000], Training Loss: 0.0252\n",
            "Epoch [9243/20000], Training Loss: 0.0258\n",
            "Epoch [9244/20000], Training Loss: 0.0250\n",
            "Epoch [9245/20000], Training Loss: 0.0278\n",
            "Epoch [9246/20000], Training Loss: 0.0267\n",
            "Epoch [9247/20000], Training Loss: 0.0256\n",
            "Epoch [9248/20000], Training Loss: 0.0254\n",
            "Epoch [9249/20000], Training Loss: 0.0254\n",
            "Epoch [9250/20000], Training Loss: 0.0277\n",
            "Epoch [9251/20000], Training Loss: 0.0283\n",
            "Epoch [9252/20000], Training Loss: 0.0265\n",
            "Epoch [9253/20000], Training Loss: 0.0252\n",
            "Epoch [9254/20000], Training Loss: 0.0253\n",
            "Epoch [9255/20000], Training Loss: 0.0269\n",
            "Epoch [9256/20000], Training Loss: 0.0251\n",
            "Epoch [9257/20000], Training Loss: 0.0249\n",
            "Epoch [9258/20000], Training Loss: 0.0258\n",
            "Epoch [9259/20000], Training Loss: 0.0259\n",
            "Epoch [9260/20000], Training Loss: 0.0255\n",
            "Epoch [9261/20000], Training Loss: 0.0253\n",
            "Epoch [9262/20000], Training Loss: 0.0249\n",
            "Epoch [9263/20000], Training Loss: 0.0275\n",
            "Epoch [9264/20000], Training Loss: 0.0247\n",
            "Epoch [9265/20000], Training Loss: 0.0272\n",
            "Epoch [9266/20000], Training Loss: 0.0267\n",
            "Epoch [9267/20000], Training Loss: 0.0253\n",
            "Epoch [9268/20000], Training Loss: 0.0246\n",
            "Epoch [9269/20000], Training Loss: 0.0271\n",
            "Epoch [9270/20000], Training Loss: 0.0253\n",
            "Epoch [9271/20000], Training Loss: 0.0271\n",
            "Epoch [9272/20000], Training Loss: 0.0264\n",
            "Epoch [9273/20000], Training Loss: 0.0266\n",
            "Epoch [9274/20000], Training Loss: 0.0276\n",
            "Epoch [9275/20000], Training Loss: 0.0274\n",
            "Epoch [9276/20000], Training Loss: 0.0281\n",
            "Epoch [9277/20000], Training Loss: 0.0264\n",
            "Epoch [9278/20000], Training Loss: 0.0252\n",
            "Epoch [9279/20000], Training Loss: 0.0288\n",
            "Epoch [9280/20000], Training Loss: 0.0267\n",
            "Epoch [9281/20000], Training Loss: 0.0271\n",
            "Epoch [9282/20000], Training Loss: 0.0251\n",
            "Epoch [9283/20000], Training Loss: 0.0247\n",
            "Epoch [9284/20000], Training Loss: 0.0257\n",
            "Epoch [9285/20000], Training Loss: 0.0264\n",
            "Epoch [9286/20000], Training Loss: 0.0271\n",
            "Epoch [9287/20000], Training Loss: 0.0251\n",
            "Epoch [9288/20000], Training Loss: 0.0257\n",
            "Epoch [9289/20000], Training Loss: 0.0271\n",
            "Epoch [9290/20000], Training Loss: 0.0258\n",
            "Epoch [9291/20000], Training Loss: 0.0279\n",
            "Epoch [9292/20000], Training Loss: 0.0263\n",
            "Epoch [9293/20000], Training Loss: 0.0252\n",
            "Epoch [9294/20000], Training Loss: 0.0259\n",
            "Epoch [9295/20000], Training Loss: 0.0250\n",
            "Epoch [9296/20000], Training Loss: 0.0273\n",
            "Epoch [9297/20000], Training Loss: 0.0262\n",
            "Epoch [9298/20000], Training Loss: 0.0251\n",
            "Epoch [9299/20000], Training Loss: 0.0247\n",
            "Epoch [9300/20000], Training Loss: 0.0271\n",
            "Epoch [9301/20000], Training Loss: 0.0275\n",
            "Epoch [9302/20000], Training Loss: 0.0267\n",
            "Epoch [9303/20000], Training Loss: 0.0259\n",
            "Epoch [9304/20000], Training Loss: 0.0250\n",
            "Epoch [9305/20000], Training Loss: 0.0248\n",
            "Epoch [9306/20000], Training Loss: 0.0254\n",
            "Epoch [9307/20000], Training Loss: 0.0268\n",
            "Epoch [9308/20000], Training Loss: 0.0250\n",
            "Epoch [9309/20000], Training Loss: 0.0261\n",
            "Epoch [9310/20000], Training Loss: 0.0265\n",
            "Epoch [9311/20000], Training Loss: 0.0260\n",
            "Epoch [9312/20000], Training Loss: 0.0258\n",
            "Epoch [9313/20000], Training Loss: 0.0266\n",
            "Epoch [9314/20000], Training Loss: 0.0262\n",
            "Epoch [9315/20000], Training Loss: 0.0274\n",
            "Epoch [9316/20000], Training Loss: 0.0262\n",
            "Epoch [9317/20000], Training Loss: 0.0269\n",
            "Epoch [9318/20000], Training Loss: 0.0244\n",
            "Epoch [9319/20000], Training Loss: 0.0280\n",
            "Epoch [9320/20000], Training Loss: 0.0259\n",
            "Epoch [9321/20000], Training Loss: 0.0268\n",
            "Epoch [9322/20000], Training Loss: 0.0250\n",
            "Epoch [9323/20000], Training Loss: 0.0254\n",
            "Epoch [9324/20000], Training Loss: 0.0253\n",
            "Epoch [9325/20000], Training Loss: 0.0271\n",
            "Epoch [9326/20000], Training Loss: 0.0254\n",
            "Epoch [9327/20000], Training Loss: 0.0254\n",
            "Epoch [9328/20000], Training Loss: 0.0281\n",
            "Epoch [9329/20000], Training Loss: 0.0261\n",
            "Epoch [9330/20000], Training Loss: 0.0264\n",
            "Epoch [9331/20000], Training Loss: 0.0288\n",
            "Epoch [9332/20000], Training Loss: 0.0277\n",
            "Epoch [9333/20000], Training Loss: 0.0264\n",
            "Epoch [9334/20000], Training Loss: 0.0274\n",
            "Epoch [9335/20000], Training Loss: 0.0268\n",
            "Epoch [9336/20000], Training Loss: 0.0276\n",
            "Epoch [9337/20000], Training Loss: 0.0275\n",
            "Epoch [9338/20000], Training Loss: 0.0268\n",
            "Epoch [9339/20000], Training Loss: 0.0268\n",
            "Epoch [9340/20000], Training Loss: 0.0275\n",
            "Epoch [9341/20000], Training Loss: 0.0250\n",
            "Epoch [9342/20000], Training Loss: 0.0272\n",
            "Epoch [9343/20000], Training Loss: 0.0268\n",
            "Epoch [9344/20000], Training Loss: 0.0245\n",
            "Epoch [9345/20000], Training Loss: 0.0262\n",
            "Epoch [9346/20000], Training Loss: 0.0255\n",
            "Epoch [9347/20000], Training Loss: 0.0258\n",
            "Epoch [9348/20000], Training Loss: 0.0266\n",
            "Epoch [9349/20000], Training Loss: 0.0265\n",
            "Epoch [9350/20000], Training Loss: 0.0291\n",
            "Epoch [9351/20000], Training Loss: 0.0278\n",
            "Epoch [9352/20000], Training Loss: 0.0264\n",
            "Epoch [9353/20000], Training Loss: 0.0249\n",
            "Epoch [9354/20000], Training Loss: 0.0259\n",
            "Epoch [9355/20000], Training Loss: 0.0281\n",
            "Epoch [9356/20000], Training Loss: 0.0260\n",
            "Epoch [9357/20000], Training Loss: 0.0252\n",
            "Epoch [9358/20000], Training Loss: 0.0253\n",
            "Epoch [9359/20000], Training Loss: 0.0248\n",
            "Epoch [9360/20000], Training Loss: 0.0264\n",
            "Epoch [9361/20000], Training Loss: 0.0271\n",
            "Epoch [9362/20000], Training Loss: 0.0291\n",
            "Epoch [9363/20000], Training Loss: 0.0273\n",
            "Epoch [9364/20000], Training Loss: 0.0283\n",
            "Epoch [9365/20000], Training Loss: 0.0252\n",
            "Epoch [9366/20000], Training Loss: 0.0248\n",
            "Epoch [9367/20000], Training Loss: 0.0246\n",
            "Epoch [9368/20000], Training Loss: 0.0240\n",
            "Epoch [9369/20000], Training Loss: 0.0287\n",
            "Epoch [9370/20000], Training Loss: 0.0258\n",
            "Epoch [9371/20000], Training Loss: 0.0240\n",
            "Epoch [9372/20000], Training Loss: 0.0259\n",
            "Epoch [9373/20000], Training Loss: 0.0263\n",
            "Epoch [9374/20000], Training Loss: 0.0261\n",
            "Epoch [9375/20000], Training Loss: 0.0258\n",
            "Epoch [9376/20000], Training Loss: 0.0267\n",
            "Epoch [9377/20000], Training Loss: 0.0268\n",
            "Epoch [9378/20000], Training Loss: 0.0256\n",
            "Epoch [9379/20000], Training Loss: 0.0273\n",
            "Epoch [9380/20000], Training Loss: 0.0260\n",
            "Epoch [9381/20000], Training Loss: 0.0270\n",
            "Epoch [9382/20000], Training Loss: 0.0283\n",
            "Epoch [9383/20000], Training Loss: 0.0256\n",
            "Epoch [9384/20000], Training Loss: 0.0253\n",
            "Epoch [9385/20000], Training Loss: 0.0266\n",
            "Epoch [9386/20000], Training Loss: 0.0267\n",
            "Epoch [9387/20000], Training Loss: 0.0264\n",
            "Epoch [9388/20000], Training Loss: 0.0268\n",
            "Epoch [9389/20000], Training Loss: 0.0246\n",
            "Epoch [9390/20000], Training Loss: 0.0270\n",
            "Epoch [9391/20000], Training Loss: 0.0259\n",
            "Epoch [9392/20000], Training Loss: 0.0267\n",
            "Epoch [9393/20000], Training Loss: 0.0264\n",
            "Epoch [9394/20000], Training Loss: 0.0262\n",
            "Epoch [9395/20000], Training Loss: 0.0253\n",
            "Epoch [9396/20000], Training Loss: 0.0256\n",
            "Epoch [9397/20000], Training Loss: 0.0252\n",
            "Epoch [9398/20000], Training Loss: 0.0258\n",
            "Epoch [9399/20000], Training Loss: 0.0265\n",
            "Epoch [9400/20000], Training Loss: 0.0247\n",
            "Epoch [9401/20000], Training Loss: 0.0279\n",
            "Epoch [9402/20000], Training Loss: 0.0258\n",
            "Epoch [9403/20000], Training Loss: 0.0253\n",
            "Epoch [9404/20000], Training Loss: 0.0268\n",
            "Epoch [9405/20000], Training Loss: 0.0262\n",
            "Epoch [9406/20000], Training Loss: 0.0251\n",
            "Epoch [9407/20000], Training Loss: 0.0270\n",
            "Epoch [9408/20000], Training Loss: 0.0266\n",
            "Epoch [9409/20000], Training Loss: 0.0258\n",
            "Epoch [9410/20000], Training Loss: 0.0268\n",
            "Epoch [9411/20000], Training Loss: 0.0275\n",
            "Epoch [9412/20000], Training Loss: 0.0255\n",
            "Epoch [9413/20000], Training Loss: 0.0267\n",
            "Epoch [9414/20000], Training Loss: 0.0259\n",
            "Epoch [9415/20000], Training Loss: 0.0253\n",
            "Epoch [9416/20000], Training Loss: 0.0253\n",
            "Epoch [9417/20000], Training Loss: 0.0272\n",
            "Epoch [9418/20000], Training Loss: 0.0269\n",
            "Epoch [9419/20000], Training Loss: 0.0255\n",
            "Epoch [9420/20000], Training Loss: 0.0267\n",
            "Epoch [9421/20000], Training Loss: 0.0254\n",
            "Epoch [9422/20000], Training Loss: 0.0273\n",
            "Epoch [9423/20000], Training Loss: 0.0300\n",
            "Epoch [9424/20000], Training Loss: 0.0270\n",
            "Epoch [9425/20000], Training Loss: 0.0267\n",
            "Epoch [9426/20000], Training Loss: 0.0265\n",
            "Epoch [9427/20000], Training Loss: 0.0274\n",
            "Epoch [9428/20000], Training Loss: 0.0260\n",
            "Epoch [9429/20000], Training Loss: 0.0269\n",
            "Epoch [9430/20000], Training Loss: 0.0268\n",
            "Epoch [9431/20000], Training Loss: 0.0279\n",
            "Epoch [9432/20000], Training Loss: 0.0250\n",
            "Epoch [9433/20000], Training Loss: 0.0255\n",
            "Epoch [9434/20000], Training Loss: 0.0269\n",
            "Epoch [9435/20000], Training Loss: 0.0288\n",
            "Epoch [9436/20000], Training Loss: 0.0256\n",
            "Epoch [9437/20000], Training Loss: 0.0247\n",
            "Epoch [9438/20000], Training Loss: 0.0257\n",
            "Epoch [9439/20000], Training Loss: 0.0255\n",
            "Epoch [9440/20000], Training Loss: 0.0249\n",
            "Epoch [9441/20000], Training Loss: 0.0264\n",
            "Epoch [9442/20000], Training Loss: 0.0243\n",
            "Epoch [9443/20000], Training Loss: 0.0270\n",
            "Epoch [9444/20000], Training Loss: 0.0260\n",
            "Epoch [9445/20000], Training Loss: 0.0271\n",
            "Epoch [9446/20000], Training Loss: 0.0271\n",
            "Epoch [9447/20000], Training Loss: 0.0256\n",
            "Epoch [9448/20000], Training Loss: 0.0277\n",
            "Epoch [9449/20000], Training Loss: 0.0257\n",
            "Epoch [9450/20000], Training Loss: 0.0270\n",
            "Epoch [9451/20000], Training Loss: 0.0261\n",
            "Epoch [9452/20000], Training Loss: 0.0253\n",
            "Epoch [9453/20000], Training Loss: 0.0273\n",
            "Epoch [9454/20000], Training Loss: 0.0292\n",
            "Epoch [9455/20000], Training Loss: 0.0244\n",
            "Epoch [9456/20000], Training Loss: 0.0251\n",
            "Epoch [9457/20000], Training Loss: 0.0261\n",
            "Epoch [9458/20000], Training Loss: 0.0266\n",
            "Epoch [9459/20000], Training Loss: 0.0257\n",
            "Epoch [9460/20000], Training Loss: 0.0266\n",
            "Epoch [9461/20000], Training Loss: 0.0270\n",
            "Epoch [9462/20000], Training Loss: 0.0285\n",
            "Epoch [9463/20000], Training Loss: 0.0246\n",
            "Epoch [9464/20000], Training Loss: 0.0250\n",
            "Epoch [9465/20000], Training Loss: 0.0269\n",
            "Epoch [9466/20000], Training Loss: 0.0244\n",
            "Epoch [9467/20000], Training Loss: 0.0270\n",
            "Epoch [9468/20000], Training Loss: 0.0279\n",
            "Epoch [9469/20000], Training Loss: 0.0272\n",
            "Epoch [9470/20000], Training Loss: 0.0282\n",
            "Epoch [9471/20000], Training Loss: 0.0249\n",
            "Epoch [9472/20000], Training Loss: 0.0254\n",
            "Epoch [9473/20000], Training Loss: 0.0267\n",
            "Epoch [9474/20000], Training Loss: 0.0257\n",
            "Epoch [9475/20000], Training Loss: 0.0242\n",
            "Epoch [9476/20000], Training Loss: 0.0269\n",
            "Epoch [9477/20000], Training Loss: 0.0252\n",
            "Epoch [9478/20000], Training Loss: 0.0261\n",
            "Epoch [9479/20000], Training Loss: 0.0254\n",
            "Epoch [9480/20000], Training Loss: 0.0284\n",
            "Epoch [9481/20000], Training Loss: 0.0276\n",
            "Epoch [9482/20000], Training Loss: 0.0253\n",
            "Epoch [9483/20000], Training Loss: 0.0256\n",
            "Epoch [9484/20000], Training Loss: 0.0256\n",
            "Epoch [9485/20000], Training Loss: 0.0256\n",
            "Epoch [9486/20000], Training Loss: 0.0269\n",
            "Epoch [9487/20000], Training Loss: 0.0255\n",
            "Epoch [9488/20000], Training Loss: 0.0255\n",
            "Epoch [9489/20000], Training Loss: 0.0263\n",
            "Epoch [9490/20000], Training Loss: 0.0263\n",
            "Epoch [9491/20000], Training Loss: 0.0249\n",
            "Epoch [9492/20000], Training Loss: 0.0282\n",
            "Epoch [9493/20000], Training Loss: 0.0265\n",
            "Epoch [9494/20000], Training Loss: 0.0273\n",
            "Epoch [9495/20000], Training Loss: 0.0284\n",
            "Epoch [9496/20000], Training Loss: 0.0274\n",
            "Epoch [9497/20000], Training Loss: 0.0274\n",
            "Epoch [9498/20000], Training Loss: 0.0244\n",
            "Epoch [9499/20000], Training Loss: 0.0273\n",
            "Epoch [9500/20000], Training Loss: 0.0266\n",
            "Epoch [9501/20000], Training Loss: 0.0272\n",
            "Epoch [9502/20000], Training Loss: 0.0263\n",
            "Epoch [9503/20000], Training Loss: 0.0287\n",
            "Epoch [9504/20000], Training Loss: 0.0271\n",
            "Epoch [9505/20000], Training Loss: 0.0271\n",
            "Epoch [9506/20000], Training Loss: 0.0264\n",
            "Epoch [9507/20000], Training Loss: 0.0274\n",
            "Epoch [9508/20000], Training Loss: 0.0265\n",
            "Epoch [9509/20000], Training Loss: 0.0266\n",
            "Epoch [9510/20000], Training Loss: 0.0265\n",
            "Epoch [9511/20000], Training Loss: 0.0249\n",
            "Epoch [9512/20000], Training Loss: 0.0259\n",
            "Epoch [9513/20000], Training Loss: 0.0281\n",
            "Epoch [9514/20000], Training Loss: 0.0265\n",
            "Epoch [9515/20000], Training Loss: 0.0291\n",
            "Epoch [9516/20000], Training Loss: 0.0259\n",
            "Epoch [9517/20000], Training Loss: 0.0268\n",
            "Epoch [9518/20000], Training Loss: 0.0268\n",
            "Epoch [9519/20000], Training Loss: 0.0286\n",
            "Epoch [9520/20000], Training Loss: 0.0274\n",
            "Epoch [9521/20000], Training Loss: 0.0257\n",
            "Epoch [9522/20000], Training Loss: 0.0274\n",
            "Epoch [9523/20000], Training Loss: 0.0251\n",
            "Epoch [9524/20000], Training Loss: 0.0245\n",
            "Epoch [9525/20000], Training Loss: 0.0255\n",
            "Epoch [9526/20000], Training Loss: 0.0250\n",
            "Epoch [9527/20000], Training Loss: 0.0256\n",
            "Epoch [9528/20000], Training Loss: 0.0262\n",
            "Epoch [9529/20000], Training Loss: 0.0276\n",
            "Epoch [9530/20000], Training Loss: 0.0253\n",
            "Epoch [9531/20000], Training Loss: 0.0249\n",
            "Epoch [9532/20000], Training Loss: 0.0282\n",
            "Epoch [9533/20000], Training Loss: 0.0273\n",
            "Epoch [9534/20000], Training Loss: 0.0266\n",
            "Epoch [9535/20000], Training Loss: 0.0272\n",
            "Epoch [9536/20000], Training Loss: 0.0279\n",
            "Epoch [9537/20000], Training Loss: 0.0283\n",
            "Epoch [9538/20000], Training Loss: 0.0260\n",
            "Epoch [9539/20000], Training Loss: 0.0268\n",
            "Epoch [9540/20000], Training Loss: 0.0272\n",
            "Epoch [9541/20000], Training Loss: 0.0267\n",
            "Epoch [9542/20000], Training Loss: 0.0265\n",
            "Epoch [9543/20000], Training Loss: 0.0274\n",
            "Epoch [9544/20000], Training Loss: 0.0258\n",
            "Epoch [9545/20000], Training Loss: 0.0274\n",
            "Epoch [9546/20000], Training Loss: 0.0237\n",
            "Epoch [9547/20000], Training Loss: 0.0275\n",
            "Epoch [9548/20000], Training Loss: 0.0274\n",
            "Epoch [9549/20000], Training Loss: 0.0270\n",
            "Epoch [9550/20000], Training Loss: 0.0291\n",
            "Epoch [9551/20000], Training Loss: 0.0258\n",
            "Epoch [9552/20000], Training Loss: 0.0278\n",
            "Epoch [9553/20000], Training Loss: 0.0260\n",
            "Epoch [9554/20000], Training Loss: 0.0275\n",
            "Epoch [9555/20000], Training Loss: 0.0270\n",
            "Epoch [9556/20000], Training Loss: 0.0270\n",
            "Epoch [9557/20000], Training Loss: 0.0244\n",
            "Epoch [9558/20000], Training Loss: 0.0247\n",
            "Epoch [9559/20000], Training Loss: 0.0266\n",
            "Epoch [9560/20000], Training Loss: 0.0264\n",
            "Epoch [9561/20000], Training Loss: 0.0272\n",
            "Epoch [9562/20000], Training Loss: 0.0252\n",
            "Epoch [9563/20000], Training Loss: 0.0276\n",
            "Epoch [9564/20000], Training Loss: 0.0252\n",
            "Epoch [9565/20000], Training Loss: 0.0291\n",
            "Epoch [9566/20000], Training Loss: 0.0243\n",
            "Epoch [9567/20000], Training Loss: 0.0251\n",
            "Epoch [9568/20000], Training Loss: 0.0270\n",
            "Epoch [9569/20000], Training Loss: 0.0264\n",
            "Epoch [9570/20000], Training Loss: 0.0273\n",
            "Epoch [9571/20000], Training Loss: 0.0258\n",
            "Epoch [9572/20000], Training Loss: 0.0251\n",
            "Epoch [9573/20000], Training Loss: 0.0268\n",
            "Epoch [9574/20000], Training Loss: 0.0246\n",
            "Epoch [9575/20000], Training Loss: 0.0272\n",
            "Epoch [9576/20000], Training Loss: 0.0259\n",
            "Epoch [9577/20000], Training Loss: 0.0266\n",
            "Epoch [9578/20000], Training Loss: 0.0268\n",
            "Epoch [9579/20000], Training Loss: 0.0261\n",
            "Epoch [9580/20000], Training Loss: 0.0259\n",
            "Epoch [9581/20000], Training Loss: 0.0268\n",
            "Epoch [9582/20000], Training Loss: 0.0283\n",
            "Epoch [9583/20000], Training Loss: 0.0277\n",
            "Epoch [9584/20000], Training Loss: 0.0257\n",
            "Epoch [9585/20000], Training Loss: 0.0265\n",
            "Epoch [9586/20000], Training Loss: 0.0282\n",
            "Epoch [9587/20000], Training Loss: 0.0246\n",
            "Epoch [9588/20000], Training Loss: 0.0290\n",
            "Epoch [9589/20000], Training Loss: 0.0272\n",
            "Epoch [9590/20000], Training Loss: 0.0277\n",
            "Epoch [9591/20000], Training Loss: 0.0258\n",
            "Epoch [9592/20000], Training Loss: 0.0246\n",
            "Epoch [9593/20000], Training Loss: 0.0256\n",
            "Epoch [9594/20000], Training Loss: 0.0261\n",
            "Epoch [9595/20000], Training Loss: 0.0265\n",
            "Epoch [9596/20000], Training Loss: 0.0262\n",
            "Epoch [9597/20000], Training Loss: 0.0255\n",
            "Epoch [9598/20000], Training Loss: 0.0279\n",
            "Epoch [9599/20000], Training Loss: 0.0253\n",
            "Epoch [9600/20000], Training Loss: 0.0245\n",
            "Epoch [9601/20000], Training Loss: 0.0281\n",
            "Epoch [9602/20000], Training Loss: 0.0287\n",
            "Epoch [9603/20000], Training Loss: 0.0268\n",
            "Epoch [9604/20000], Training Loss: 0.0258\n",
            "Epoch [9605/20000], Training Loss: 0.0268\n",
            "Epoch [9606/20000], Training Loss: 0.0257\n",
            "Epoch [9607/20000], Training Loss: 0.0263\n",
            "Epoch [9608/20000], Training Loss: 0.0254\n",
            "Epoch [9609/20000], Training Loss: 0.0269\n",
            "Epoch [9610/20000], Training Loss: 0.0260\n",
            "Epoch [9611/20000], Training Loss: 0.0267\n",
            "Epoch [9612/20000], Training Loss: 0.0260\n",
            "Epoch [9613/20000], Training Loss: 0.0244\n",
            "Epoch [9614/20000], Training Loss: 0.0260\n",
            "Epoch [9615/20000], Training Loss: 0.0256\n",
            "Epoch [9616/20000], Training Loss: 0.0246\n",
            "Epoch [9617/20000], Training Loss: 0.0265\n",
            "Epoch [9618/20000], Training Loss: 0.0257\n",
            "Epoch [9619/20000], Training Loss: 0.0263\n",
            "Epoch [9620/20000], Training Loss: 0.0262\n",
            "Epoch [9621/20000], Training Loss: 0.0261\n",
            "Epoch [9622/20000], Training Loss: 0.0250\n",
            "Epoch [9623/20000], Training Loss: 0.0246\n",
            "Epoch [9624/20000], Training Loss: 0.0258\n",
            "Epoch [9625/20000], Training Loss: 0.0262\n",
            "Epoch [9626/20000], Training Loss: 0.0283\n",
            "Epoch [9627/20000], Training Loss: 0.0284\n",
            "Epoch [9628/20000], Training Loss: 0.0255\n",
            "Epoch [9629/20000], Training Loss: 0.0252\n",
            "Epoch [9630/20000], Training Loss: 0.0281\n",
            "Epoch [9631/20000], Training Loss: 0.0251\n",
            "Epoch [9632/20000], Training Loss: 0.0263\n",
            "Epoch [9633/20000], Training Loss: 0.0271\n",
            "Epoch [9634/20000], Training Loss: 0.0269\n",
            "Epoch [9635/20000], Training Loss: 0.0264\n",
            "Epoch [9636/20000], Training Loss: 0.0264\n",
            "Epoch [9637/20000], Training Loss: 0.0269\n",
            "Epoch [9638/20000], Training Loss: 0.0252\n",
            "Epoch [9639/20000], Training Loss: 0.0254\n",
            "Epoch [9640/20000], Training Loss: 0.0260\n",
            "Epoch [9641/20000], Training Loss: 0.0258\n",
            "Epoch [9642/20000], Training Loss: 0.0260\n",
            "Epoch [9643/20000], Training Loss: 0.0258\n",
            "Epoch [9644/20000], Training Loss: 0.0261\n",
            "Epoch [9645/20000], Training Loss: 0.0271\n",
            "Epoch [9646/20000], Training Loss: 0.0259\n",
            "Epoch [9647/20000], Training Loss: 0.0248\n",
            "Epoch [9648/20000], Training Loss: 0.0254\n",
            "Epoch [9649/20000], Training Loss: 0.0276\n",
            "Epoch [9650/20000], Training Loss: 0.0277\n",
            "Epoch [9651/20000], Training Loss: 0.0256\n",
            "Epoch [9652/20000], Training Loss: 0.0263\n",
            "Epoch [9653/20000], Training Loss: 0.0257\n",
            "Epoch [9654/20000], Training Loss: 0.0264\n",
            "Epoch [9655/20000], Training Loss: 0.0271\n",
            "Epoch [9656/20000], Training Loss: 0.0277\n",
            "Epoch [9657/20000], Training Loss: 0.0278\n",
            "Epoch [9658/20000], Training Loss: 0.0256\n",
            "Epoch [9659/20000], Training Loss: 0.0260\n",
            "Epoch [9660/20000], Training Loss: 0.0285\n",
            "Epoch [9661/20000], Training Loss: 0.0270\n",
            "Epoch [9662/20000], Training Loss: 0.0254\n",
            "Epoch [9663/20000], Training Loss: 0.0267\n",
            "Epoch [9664/20000], Training Loss: 0.0272\n",
            "Epoch [9665/20000], Training Loss: 0.0271\n",
            "Epoch [9666/20000], Training Loss: 0.0267\n",
            "Epoch [9667/20000], Training Loss: 0.0254\n",
            "Epoch [9668/20000], Training Loss: 0.0250\n",
            "Epoch [9669/20000], Training Loss: 0.0245\n",
            "Epoch [9670/20000], Training Loss: 0.0261\n",
            "Epoch [9671/20000], Training Loss: 0.0260\n",
            "Epoch [9672/20000], Training Loss: 0.0255\n",
            "Epoch [9673/20000], Training Loss: 0.0268\n",
            "Epoch [9674/20000], Training Loss: 0.0259\n",
            "Epoch [9675/20000], Training Loss: 0.0269\n",
            "Epoch [9676/20000], Training Loss: 0.0267\n",
            "Epoch [9677/20000], Training Loss: 0.0262\n",
            "Epoch [9678/20000], Training Loss: 0.0264\n",
            "Epoch [9679/20000], Training Loss: 0.0261\n",
            "Epoch [9680/20000], Training Loss: 0.0273\n",
            "Epoch [9681/20000], Training Loss: 0.0256\n",
            "Epoch [9682/20000], Training Loss: 0.0275\n",
            "Epoch [9683/20000], Training Loss: 0.0262\n",
            "Epoch [9684/20000], Training Loss: 0.0271\n",
            "Epoch [9685/20000], Training Loss: 0.0260\n",
            "Epoch [9686/20000], Training Loss: 0.0283\n",
            "Epoch [9687/20000], Training Loss: 0.0257\n",
            "Epoch [9688/20000], Training Loss: 0.0258\n",
            "Epoch [9689/20000], Training Loss: 0.0259\n",
            "Epoch [9690/20000], Training Loss: 0.0273\n",
            "Epoch [9691/20000], Training Loss: 0.0260\n",
            "Epoch [9692/20000], Training Loss: 0.0262\n",
            "Epoch [9693/20000], Training Loss: 0.0260\n",
            "Epoch [9694/20000], Training Loss: 0.0255\n",
            "Epoch [9695/20000], Training Loss: 0.0254\n",
            "Epoch [9696/20000], Training Loss: 0.0250\n",
            "Epoch [9697/20000], Training Loss: 0.0276\n",
            "Epoch [9698/20000], Training Loss: 0.0288\n",
            "Epoch [9699/20000], Training Loss: 0.0269\n",
            "Epoch [9700/20000], Training Loss: 0.0254\n",
            "Epoch [9701/20000], Training Loss: 0.0276\n",
            "Epoch [9702/20000], Training Loss: 0.0264\n",
            "Epoch [9703/20000], Training Loss: 0.0259\n",
            "Epoch [9704/20000], Training Loss: 0.0260\n",
            "Epoch [9705/20000], Training Loss: 0.0262\n",
            "Epoch [9706/20000], Training Loss: 0.0285\n",
            "Epoch [9707/20000], Training Loss: 0.0265\n",
            "Epoch [9708/20000], Training Loss: 0.0251\n",
            "Epoch [9709/20000], Training Loss: 0.0278\n",
            "Epoch [9710/20000], Training Loss: 0.0261\n",
            "Epoch [9711/20000], Training Loss: 0.0239\n",
            "Epoch [9712/20000], Training Loss: 0.0262\n",
            "Epoch [9713/20000], Training Loss: 0.0269\n",
            "Epoch [9714/20000], Training Loss: 0.0268\n",
            "Epoch [9715/20000], Training Loss: 0.0281\n",
            "Epoch [9716/20000], Training Loss: 0.0257\n",
            "Epoch [9717/20000], Training Loss: 0.0259\n",
            "Epoch [9718/20000], Training Loss: 0.0254\n",
            "Epoch [9719/20000], Training Loss: 0.0272\n",
            "Epoch [9720/20000], Training Loss: 0.0269\n",
            "Epoch [9721/20000], Training Loss: 0.0256\n",
            "Epoch [9722/20000], Training Loss: 0.0271\n",
            "Epoch [9723/20000], Training Loss: 0.0275\n",
            "Epoch [9724/20000], Training Loss: 0.0272\n",
            "Epoch [9725/20000], Training Loss: 0.0253\n",
            "Epoch [9726/20000], Training Loss: 0.0254\n",
            "Epoch [9727/20000], Training Loss: 0.0264\n",
            "Epoch [9728/20000], Training Loss: 0.0253\n",
            "Epoch [9729/20000], Training Loss: 0.0271\n",
            "Epoch [9730/20000], Training Loss: 0.0267\n",
            "Epoch [9731/20000], Training Loss: 0.0263\n",
            "Epoch [9732/20000], Training Loss: 0.0275\n",
            "Epoch [9733/20000], Training Loss: 0.0265\n",
            "Epoch [9734/20000], Training Loss: 0.0269\n",
            "Epoch [9735/20000], Training Loss: 0.0257\n",
            "Epoch [9736/20000], Training Loss: 0.0265\n",
            "Epoch [9737/20000], Training Loss: 0.0260\n",
            "Epoch [9738/20000], Training Loss: 0.0276\n",
            "Epoch [9739/20000], Training Loss: 0.0253\n",
            "Epoch [9740/20000], Training Loss: 0.0265\n",
            "Epoch [9741/20000], Training Loss: 0.0278\n",
            "Epoch [9742/20000], Training Loss: 0.0279\n",
            "Epoch [9743/20000], Training Loss: 0.0267\n",
            "Epoch [9744/20000], Training Loss: 0.0258\n",
            "Epoch [9745/20000], Training Loss: 0.0251\n",
            "Epoch [9746/20000], Training Loss: 0.0274\n",
            "Epoch [9747/20000], Training Loss: 0.0259\n",
            "Epoch [9748/20000], Training Loss: 0.0251\n",
            "Epoch [9749/20000], Training Loss: 0.0272\n",
            "Epoch [9750/20000], Training Loss: 0.0248\n",
            "Epoch [9751/20000], Training Loss: 0.0241\n",
            "Epoch [9752/20000], Training Loss: 0.0279\n",
            "Epoch [9753/20000], Training Loss: 0.0257\n",
            "Epoch [9754/20000], Training Loss: 0.0252\n",
            "Epoch [9755/20000], Training Loss: 0.0282\n",
            "Epoch [9756/20000], Training Loss: 0.0254\n",
            "Epoch [9757/20000], Training Loss: 0.0263\n",
            "Epoch [9758/20000], Training Loss: 0.0265\n",
            "Epoch [9759/20000], Training Loss: 0.0273\n",
            "Epoch [9760/20000], Training Loss: 0.0252\n",
            "Epoch [9761/20000], Training Loss: 0.0266\n",
            "Epoch [9762/20000], Training Loss: 0.0268\n",
            "Epoch [9763/20000], Training Loss: 0.0255\n",
            "Epoch [9764/20000], Training Loss: 0.0262\n",
            "Epoch [9765/20000], Training Loss: 0.0290\n",
            "Epoch [9766/20000], Training Loss: 0.0267\n",
            "Epoch [9767/20000], Training Loss: 0.0297\n",
            "Epoch [9768/20000], Training Loss: 0.0290\n",
            "Epoch [9769/20000], Training Loss: 0.0284\n",
            "Epoch [9770/20000], Training Loss: 0.0255\n",
            "Epoch [9771/20000], Training Loss: 0.0274\n",
            "Epoch [9772/20000], Training Loss: 0.0270\n",
            "Epoch [9773/20000], Training Loss: 0.0261\n",
            "Epoch [9774/20000], Training Loss: 0.0249\n",
            "Epoch [9775/20000], Training Loss: 0.0269\n",
            "Epoch [9776/20000], Training Loss: 0.0261\n",
            "Epoch [9777/20000], Training Loss: 0.0258\n",
            "Epoch [9778/20000], Training Loss: 0.0258\n",
            "Epoch [9779/20000], Training Loss: 0.0260\n",
            "Epoch [9780/20000], Training Loss: 0.0264\n",
            "Epoch [9781/20000], Training Loss: 0.0242\n",
            "Epoch [9782/20000], Training Loss: 0.0281\n",
            "Epoch [9783/20000], Training Loss: 0.0275\n",
            "Epoch [9784/20000], Training Loss: 0.0267\n",
            "Epoch [9785/20000], Training Loss: 0.0268\n",
            "Epoch [9786/20000], Training Loss: 0.0271\n",
            "Epoch [9787/20000], Training Loss: 0.0263\n",
            "Epoch [9788/20000], Training Loss: 0.0271\n",
            "Epoch [9789/20000], Training Loss: 0.0277\n",
            "Epoch [9790/20000], Training Loss: 0.0283\n",
            "Epoch [9791/20000], Training Loss: 0.0261\n",
            "Epoch [9792/20000], Training Loss: 0.0272\n",
            "Epoch [9793/20000], Training Loss: 0.0274\n",
            "Epoch [9794/20000], Training Loss: 0.0262\n",
            "Epoch [9795/20000], Training Loss: 0.0278\n",
            "Epoch [9796/20000], Training Loss: 0.0291\n",
            "Epoch [9797/20000], Training Loss: 0.0255\n",
            "Epoch [9798/20000], Training Loss: 0.0252\n",
            "Epoch [9799/20000], Training Loss: 0.0291\n",
            "Epoch [9800/20000], Training Loss: 0.0264\n",
            "Epoch [9801/20000], Training Loss: 0.0258\n",
            "Epoch [9802/20000], Training Loss: 0.0281\n",
            "Epoch [9803/20000], Training Loss: 0.0282\n",
            "Epoch [9804/20000], Training Loss: 0.0288\n",
            "Epoch [9805/20000], Training Loss: 0.0252\n",
            "Epoch [9806/20000], Training Loss: 0.0262\n",
            "Epoch [9807/20000], Training Loss: 0.0267\n",
            "Epoch [9808/20000], Training Loss: 0.0277\n",
            "Epoch [9809/20000], Training Loss: 0.0285\n",
            "Epoch [9810/20000], Training Loss: 0.0264\n",
            "Epoch [9811/20000], Training Loss: 0.0259\n",
            "Epoch [9812/20000], Training Loss: 0.0278\n",
            "Epoch [9813/20000], Training Loss: 0.0239\n",
            "Epoch [9814/20000], Training Loss: 0.0265\n",
            "Epoch [9815/20000], Training Loss: 0.0282\n",
            "Epoch [9816/20000], Training Loss: 0.0263\n",
            "Epoch [9817/20000], Training Loss: 0.0288\n",
            "Epoch [9818/20000], Training Loss: 0.0260\n",
            "Epoch [9819/20000], Training Loss: 0.0287\n",
            "Epoch [9820/20000], Training Loss: 0.0277\n",
            "Epoch [9821/20000], Training Loss: 0.0250\n",
            "Epoch [9822/20000], Training Loss: 0.0255\n",
            "Epoch [9823/20000], Training Loss: 0.0265\n",
            "Epoch [9824/20000], Training Loss: 0.0266\n",
            "Epoch [9825/20000], Training Loss: 0.0267\n",
            "Epoch [9826/20000], Training Loss: 0.0253\n",
            "Epoch [9827/20000], Training Loss: 0.0281\n",
            "Epoch [9828/20000], Training Loss: 0.0258\n",
            "Epoch [9829/20000], Training Loss: 0.0274\n",
            "Epoch [9830/20000], Training Loss: 0.0249\n",
            "Epoch [9831/20000], Training Loss: 0.0264\n",
            "Epoch [9832/20000], Training Loss: 0.0268\n",
            "Epoch [9833/20000], Training Loss: 0.0266\n",
            "Epoch [9834/20000], Training Loss: 0.0264\n",
            "Epoch [9835/20000], Training Loss: 0.0260\n",
            "Epoch [9836/20000], Training Loss: 0.0266\n",
            "Epoch [9837/20000], Training Loss: 0.0255\n",
            "Epoch [9838/20000], Training Loss: 0.0253\n",
            "Epoch [9839/20000], Training Loss: 0.0281\n",
            "Epoch [9840/20000], Training Loss: 0.0246\n",
            "Epoch [9841/20000], Training Loss: 0.0259\n",
            "Epoch [9842/20000], Training Loss: 0.0280\n",
            "Epoch [9843/20000], Training Loss: 0.0266\n",
            "Epoch [9844/20000], Training Loss: 0.0265\n",
            "Epoch [9845/20000], Training Loss: 0.0253\n",
            "Epoch [9846/20000], Training Loss: 0.0248\n",
            "Epoch [9847/20000], Training Loss: 0.0246\n",
            "Epoch [9848/20000], Training Loss: 0.0255\n",
            "Epoch [9849/20000], Training Loss: 0.0238\n",
            "Epoch [9850/20000], Training Loss: 0.0251\n",
            "Epoch [9851/20000], Training Loss: 0.0260\n",
            "Epoch [9852/20000], Training Loss: 0.0252\n",
            "Epoch [9853/20000], Training Loss: 0.0292\n",
            "Epoch [9854/20000], Training Loss: 0.0255\n",
            "Epoch [9855/20000], Training Loss: 0.0248\n",
            "Epoch [9856/20000], Training Loss: 0.0266\n",
            "Epoch [9857/20000], Training Loss: 0.0255\n",
            "Epoch [9858/20000], Training Loss: 0.0272\n",
            "Epoch [9859/20000], Training Loss: 0.0257\n",
            "Epoch [9860/20000], Training Loss: 0.0271\n",
            "Epoch [9861/20000], Training Loss: 0.0288\n",
            "Epoch [9862/20000], Training Loss: 0.0245\n",
            "Epoch [9863/20000], Training Loss: 0.0254\n",
            "Epoch [9864/20000], Training Loss: 0.0273\n",
            "Epoch [9865/20000], Training Loss: 0.0266\n",
            "Epoch [9866/20000], Training Loss: 0.0245\n",
            "Epoch [9867/20000], Training Loss: 0.0250\n",
            "Epoch [9868/20000], Training Loss: 0.0265\n",
            "Epoch [9869/20000], Training Loss: 0.0256\n",
            "Epoch [9870/20000], Training Loss: 0.0262\n",
            "Epoch [9871/20000], Training Loss: 0.0248\n",
            "Epoch [9872/20000], Training Loss: 0.0264\n",
            "Epoch [9873/20000], Training Loss: 0.0264\n",
            "Epoch [9874/20000], Training Loss: 0.0282\n",
            "Epoch [9875/20000], Training Loss: 0.0251\n",
            "Epoch [9876/20000], Training Loss: 0.0268\n",
            "Epoch [9877/20000], Training Loss: 0.0287\n",
            "Epoch [9878/20000], Training Loss: 0.0263\n",
            "Epoch [9879/20000], Training Loss: 0.0258\n",
            "Epoch [9880/20000], Training Loss: 0.0272\n",
            "Epoch [9881/20000], Training Loss: 0.0261\n",
            "Epoch [9882/20000], Training Loss: 0.0259\n",
            "Epoch [9883/20000], Training Loss: 0.0270\n",
            "Epoch [9884/20000], Training Loss: 0.0273\n",
            "Epoch [9885/20000], Training Loss: 0.0255\n",
            "Epoch [9886/20000], Training Loss: 0.0271\n",
            "Epoch [9887/20000], Training Loss: 0.0270\n",
            "Epoch [9888/20000], Training Loss: 0.0252\n",
            "Epoch [9889/20000], Training Loss: 0.0263\n",
            "Epoch [9890/20000], Training Loss: 0.0273\n",
            "Epoch [9891/20000], Training Loss: 0.0244\n",
            "Epoch [9892/20000], Training Loss: 0.0256\n",
            "Epoch [9893/20000], Training Loss: 0.0260\n",
            "Epoch [9894/20000], Training Loss: 0.0252\n",
            "Epoch [9895/20000], Training Loss: 0.0264\n",
            "Epoch [9896/20000], Training Loss: 0.0255\n",
            "Epoch [9897/20000], Training Loss: 0.0257\n",
            "Epoch [9898/20000], Training Loss: 0.0259\n",
            "Epoch [9899/20000], Training Loss: 0.0269\n",
            "Epoch [9900/20000], Training Loss: 0.0282\n",
            "Epoch [9901/20000], Training Loss: 0.0262\n",
            "Epoch [9902/20000], Training Loss: 0.0248\n",
            "Epoch [9903/20000], Training Loss: 0.0273\n",
            "Epoch [9904/20000], Training Loss: 0.0255\n",
            "Epoch [9905/20000], Training Loss: 0.0291\n",
            "Epoch [9906/20000], Training Loss: 0.0257\n",
            "Epoch [9907/20000], Training Loss: 0.0248\n",
            "Epoch [9908/20000], Training Loss: 0.0246\n",
            "Epoch [9909/20000], Training Loss: 0.0264\n",
            "Epoch [9910/20000], Training Loss: 0.0276\n",
            "Epoch [9911/20000], Training Loss: 0.0260\n",
            "Epoch [9912/20000], Training Loss: 0.0285\n",
            "Epoch [9913/20000], Training Loss: 0.0258\n",
            "Epoch [9914/20000], Training Loss: 0.0257\n",
            "Epoch [9915/20000], Training Loss: 0.0254\n",
            "Epoch [9916/20000], Training Loss: 0.0287\n",
            "Epoch [9917/20000], Training Loss: 0.0275\n",
            "Epoch [9918/20000], Training Loss: 0.0269\n",
            "Epoch [9919/20000], Training Loss: 0.0258\n",
            "Epoch [9920/20000], Training Loss: 0.0281\n",
            "Epoch [9921/20000], Training Loss: 0.0282\n",
            "Epoch [9922/20000], Training Loss: 0.0285\n",
            "Epoch [9923/20000], Training Loss: 0.0275\n",
            "Epoch [9924/20000], Training Loss: 0.0261\n",
            "Epoch [9925/20000], Training Loss: 0.0249\n",
            "Epoch [9926/20000], Training Loss: 0.0263\n",
            "Epoch [9927/20000], Training Loss: 0.0262\n",
            "Epoch [9928/20000], Training Loss: 0.0257\n",
            "Epoch [9929/20000], Training Loss: 0.0258\n",
            "Epoch [9930/20000], Training Loss: 0.0271\n",
            "Epoch [9931/20000], Training Loss: 0.0266\n",
            "Epoch [9932/20000], Training Loss: 0.0259\n",
            "Epoch [9933/20000], Training Loss: 0.0242\n",
            "Epoch [9934/20000], Training Loss: 0.0276\n",
            "Epoch [9935/20000], Training Loss: 0.0254\n",
            "Epoch [9936/20000], Training Loss: 0.0255\n",
            "Epoch [9937/20000], Training Loss: 0.0279\n",
            "Epoch [9938/20000], Training Loss: 0.0253\n",
            "Epoch [9939/20000], Training Loss: 0.0246\n",
            "Epoch [9940/20000], Training Loss: 0.0263\n",
            "Epoch [9941/20000], Training Loss: 0.0251\n",
            "Epoch [9942/20000], Training Loss: 0.0272\n",
            "Epoch [9943/20000], Training Loss: 0.0251\n",
            "Epoch [9944/20000], Training Loss: 0.0260\n",
            "Epoch [9945/20000], Training Loss: 0.0259\n",
            "Epoch [9946/20000], Training Loss: 0.0250\n",
            "Epoch [9947/20000], Training Loss: 0.0260\n",
            "Epoch [9948/20000], Training Loss: 0.0265\n",
            "Epoch [9949/20000], Training Loss: 0.0277\n",
            "Epoch [9950/20000], Training Loss: 0.0262\n",
            "Epoch [9951/20000], Training Loss: 0.0264\n",
            "Epoch [9952/20000], Training Loss: 0.0258\n",
            "Epoch [9953/20000], Training Loss: 0.0251\n",
            "Epoch [9954/20000], Training Loss: 0.0273\n",
            "Epoch [9955/20000], Training Loss: 0.0267\n",
            "Epoch [9956/20000], Training Loss: 0.0264\n",
            "Epoch [9957/20000], Training Loss: 0.0270\n",
            "Epoch [9958/20000], Training Loss: 0.0261\n",
            "Epoch [9959/20000], Training Loss: 0.0269\n",
            "Epoch [9960/20000], Training Loss: 0.0258\n",
            "Epoch [9961/20000], Training Loss: 0.0250\n",
            "Epoch [9962/20000], Training Loss: 0.0270\n",
            "Epoch [9963/20000], Training Loss: 0.0280\n",
            "Epoch [9964/20000], Training Loss: 0.0258\n",
            "Epoch [9965/20000], Training Loss: 0.0264\n",
            "Epoch [9966/20000], Training Loss: 0.0287\n",
            "Epoch [9967/20000], Training Loss: 0.0261\n",
            "Epoch [9968/20000], Training Loss: 0.0261\n",
            "Epoch [9969/20000], Training Loss: 0.0257\n",
            "Epoch [9970/20000], Training Loss: 0.0262\n",
            "Epoch [9971/20000], Training Loss: 0.0268\n",
            "Epoch [9972/20000], Training Loss: 0.0271\n",
            "Epoch [9973/20000], Training Loss: 0.0255\n",
            "Epoch [9974/20000], Training Loss: 0.0277\n",
            "Epoch [9975/20000], Training Loss: 0.0271\n",
            "Epoch [9976/20000], Training Loss: 0.0269\n",
            "Epoch [9977/20000], Training Loss: 0.0269\n",
            "Epoch [9978/20000], Training Loss: 0.0241\n",
            "Epoch [9979/20000], Training Loss: 0.0270\n",
            "Epoch [9980/20000], Training Loss: 0.0250\n",
            "Epoch [9981/20000], Training Loss: 0.0249\n",
            "Epoch [9982/20000], Training Loss: 0.0255\n",
            "Epoch [9983/20000], Training Loss: 0.0265\n",
            "Epoch [9984/20000], Training Loss: 0.0276\n",
            "Epoch [9985/20000], Training Loss: 0.0258\n",
            "Epoch [9986/20000], Training Loss: 0.0258\n",
            "Epoch [9987/20000], Training Loss: 0.0256\n",
            "Epoch [9988/20000], Training Loss: 0.0268\n",
            "Epoch [9989/20000], Training Loss: 0.0264\n",
            "Epoch [9990/20000], Training Loss: 0.0248\n",
            "Epoch [9991/20000], Training Loss: 0.0248\n",
            "Epoch [9992/20000], Training Loss: 0.0287\n",
            "Epoch [9993/20000], Training Loss: 0.0270\n",
            "Epoch [9994/20000], Training Loss: 0.0257\n",
            "Epoch [9995/20000], Training Loss: 0.0247\n",
            "Epoch [9996/20000], Training Loss: 0.0255\n",
            "Epoch [9997/20000], Training Loss: 0.0278\n",
            "Epoch [9998/20000], Training Loss: 0.0265\n",
            "Epoch [9999/20000], Training Loss: 0.0248\n",
            "Epoch [10000/20000], Training Loss: 0.0267\n",
            "Epoch [10001/20000], Training Loss: 0.0249\n",
            "Epoch [10002/20000], Training Loss: 0.0246\n",
            "Epoch [10003/20000], Training Loss: 0.0284\n",
            "Epoch [10004/20000], Training Loss: 0.0260\n",
            "Epoch [10005/20000], Training Loss: 0.0255\n",
            "Epoch [10006/20000], Training Loss: 0.0259\n",
            "Epoch [10007/20000], Training Loss: 0.0263\n",
            "Epoch [10008/20000], Training Loss: 0.0269\n",
            "Epoch [10009/20000], Training Loss: 0.0288\n",
            "Epoch [10010/20000], Training Loss: 0.0257\n",
            "Epoch [10011/20000], Training Loss: 0.0264\n",
            "Epoch [10012/20000], Training Loss: 0.0263\n",
            "Epoch [10013/20000], Training Loss: 0.0256\n",
            "Epoch [10014/20000], Training Loss: 0.0264\n",
            "Epoch [10015/20000], Training Loss: 0.0255\n",
            "Epoch [10016/20000], Training Loss: 0.0247\n",
            "Epoch [10017/20000], Training Loss: 0.0249\n",
            "Epoch [10018/20000], Training Loss: 0.0256\n",
            "Epoch [10019/20000], Training Loss: 0.0268\n",
            "Epoch [10020/20000], Training Loss: 0.0254\n",
            "Epoch [10021/20000], Training Loss: 0.0267\n",
            "Epoch [10022/20000], Training Loss: 0.0253\n",
            "Epoch [10023/20000], Training Loss: 0.0283\n",
            "Epoch [10024/20000], Training Loss: 0.0279\n",
            "Epoch [10025/20000], Training Loss: 0.0251\n",
            "Epoch [10026/20000], Training Loss: 0.0284\n",
            "Epoch [10027/20000], Training Loss: 0.0256\n",
            "Epoch [10028/20000], Training Loss: 0.0251\n",
            "Epoch [10029/20000], Training Loss: 0.0247\n",
            "Epoch [10030/20000], Training Loss: 0.0272\n",
            "Epoch [10031/20000], Training Loss: 0.0267\n",
            "Epoch [10032/20000], Training Loss: 0.0247\n",
            "Epoch [10033/20000], Training Loss: 0.0244\n",
            "Epoch [10034/20000], Training Loss: 0.0244\n",
            "Epoch [10035/20000], Training Loss: 0.0263\n",
            "Epoch [10036/20000], Training Loss: 0.0262\n",
            "Epoch [10037/20000], Training Loss: 0.0249\n",
            "Epoch [10038/20000], Training Loss: 0.0258\n",
            "Epoch [10039/20000], Training Loss: 0.0267\n",
            "Epoch [10040/20000], Training Loss: 0.0257\n",
            "Epoch [10041/20000], Training Loss: 0.0282\n",
            "Epoch [10042/20000], Training Loss: 0.0247\n",
            "Epoch [10043/20000], Training Loss: 0.0276\n",
            "Epoch [10044/20000], Training Loss: 0.0279\n",
            "Epoch [10045/20000], Training Loss: 0.0254\n",
            "Epoch [10046/20000], Training Loss: 0.0265\n",
            "Epoch [10047/20000], Training Loss: 0.0251\n",
            "Epoch [10048/20000], Training Loss: 0.0259\n",
            "Epoch [10049/20000], Training Loss: 0.0268\n",
            "Epoch [10050/20000], Training Loss: 0.0284\n",
            "Epoch [10051/20000], Training Loss: 0.0263\n",
            "Epoch [10052/20000], Training Loss: 0.0254\n",
            "Epoch [10053/20000], Training Loss: 0.0284\n",
            "Epoch [10054/20000], Training Loss: 0.0254\n",
            "Epoch [10055/20000], Training Loss: 0.0257\n",
            "Epoch [10056/20000], Training Loss: 0.0252\n",
            "Epoch [10057/20000], Training Loss: 0.0276\n",
            "Epoch [10058/20000], Training Loss: 0.0281\n",
            "Epoch [10059/20000], Training Loss: 0.0253\n",
            "Epoch [10060/20000], Training Loss: 0.0258\n",
            "Epoch [10061/20000], Training Loss: 0.0269\n",
            "Epoch [10062/20000], Training Loss: 0.0274\n",
            "Epoch [10063/20000], Training Loss: 0.0291\n",
            "Epoch [10064/20000], Training Loss: 0.0249\n",
            "Epoch [10065/20000], Training Loss: 0.0289\n",
            "Epoch [10066/20000], Training Loss: 0.0275\n",
            "Epoch [10067/20000], Training Loss: 0.0273\n",
            "Epoch [10068/20000], Training Loss: 0.0271\n",
            "Epoch [10069/20000], Training Loss: 0.0278\n",
            "Epoch [10070/20000], Training Loss: 0.0270\n",
            "Epoch [10071/20000], Training Loss: 0.0287\n",
            "Epoch [10072/20000], Training Loss: 0.0274\n",
            "Epoch [10073/20000], Training Loss: 0.0262\n",
            "Epoch [10074/20000], Training Loss: 0.0272\n",
            "Epoch [10075/20000], Training Loss: 0.0272\n",
            "Epoch [10076/20000], Training Loss: 0.0286\n",
            "Epoch [10077/20000], Training Loss: 0.0267\n",
            "Epoch [10078/20000], Training Loss: 0.0271\n",
            "Epoch [10079/20000], Training Loss: 0.0284\n",
            "Epoch [10080/20000], Training Loss: 0.0243\n",
            "Epoch [10081/20000], Training Loss: 0.0288\n",
            "Epoch [10082/20000], Training Loss: 0.0272\n",
            "Epoch [10083/20000], Training Loss: 0.0259\n",
            "Epoch [10084/20000], Training Loss: 0.0285\n",
            "Epoch [10085/20000], Training Loss: 0.0269\n",
            "Epoch [10086/20000], Training Loss: 0.0294\n",
            "Epoch [10087/20000], Training Loss: 0.0268\n",
            "Epoch [10088/20000], Training Loss: 0.0263\n",
            "Epoch [10089/20000], Training Loss: 0.0266\n",
            "Epoch [10090/20000], Training Loss: 0.0286\n",
            "Epoch [10091/20000], Training Loss: 0.0276\n",
            "Epoch [10092/20000], Training Loss: 0.0262\n",
            "Epoch [10093/20000], Training Loss: 0.0272\n",
            "Epoch [10094/20000], Training Loss: 0.0279\n",
            "Epoch [10095/20000], Training Loss: 0.0262\n",
            "Epoch [10096/20000], Training Loss: 0.0251\n",
            "Epoch [10097/20000], Training Loss: 0.0255\n",
            "Epoch [10098/20000], Training Loss: 0.0264\n",
            "Epoch [10099/20000], Training Loss: 0.0259\n",
            "Epoch [10100/20000], Training Loss: 0.0265\n",
            "Epoch [10101/20000], Training Loss: 0.0285\n",
            "Epoch [10102/20000], Training Loss: 0.0253\n",
            "Epoch [10103/20000], Training Loss: 0.0263\n",
            "Epoch [10104/20000], Training Loss: 0.0243\n",
            "Epoch [10105/20000], Training Loss: 0.0260\n",
            "Epoch [10106/20000], Training Loss: 0.0273\n",
            "Epoch [10107/20000], Training Loss: 0.0243\n",
            "Epoch [10108/20000], Training Loss: 0.0264\n",
            "Epoch [10109/20000], Training Loss: 0.0275\n",
            "Epoch [10110/20000], Training Loss: 0.0263\n",
            "Epoch [10111/20000], Training Loss: 0.0255\n",
            "Epoch [10112/20000], Training Loss: 0.0278\n",
            "Epoch [10113/20000], Training Loss: 0.0258\n",
            "Epoch [10114/20000], Training Loss: 0.0258\n",
            "Epoch [10115/20000], Training Loss: 0.0264\n",
            "Epoch [10116/20000], Training Loss: 0.0252\n",
            "Epoch [10117/20000], Training Loss: 0.0269\n",
            "Epoch [10118/20000], Training Loss: 0.0262\n",
            "Epoch [10119/20000], Training Loss: 0.0260\n",
            "Epoch [10120/20000], Training Loss: 0.0274\n",
            "Epoch [10121/20000], Training Loss: 0.0290\n",
            "Epoch [10122/20000], Training Loss: 0.0262\n",
            "Epoch [10123/20000], Training Loss: 0.0267\n",
            "Epoch [10124/20000], Training Loss: 0.0263\n",
            "Epoch [10125/20000], Training Loss: 0.0260\n",
            "Epoch [10126/20000], Training Loss: 0.0260\n",
            "Epoch [10127/20000], Training Loss: 0.0276\n",
            "Epoch [10128/20000], Training Loss: 0.0276\n",
            "Epoch [10129/20000], Training Loss: 0.0250\n",
            "Epoch [10130/20000], Training Loss: 0.0248\n",
            "Epoch [10131/20000], Training Loss: 0.0281\n",
            "Epoch [10132/20000], Training Loss: 0.0271\n",
            "Epoch [10133/20000], Training Loss: 0.0259\n",
            "Epoch [10134/20000], Training Loss: 0.0251\n",
            "Epoch [10135/20000], Training Loss: 0.0251\n",
            "Epoch [10136/20000], Training Loss: 0.0263\n",
            "Epoch [10137/20000], Training Loss: 0.0268\n",
            "Epoch [10138/20000], Training Loss: 0.0290\n",
            "Epoch [10139/20000], Training Loss: 0.0267\n",
            "Epoch [10140/20000], Training Loss: 0.0282\n",
            "Epoch [10141/20000], Training Loss: 0.0263\n",
            "Epoch [10142/20000], Training Loss: 0.0266\n",
            "Epoch [10143/20000], Training Loss: 0.0261\n",
            "Epoch [10144/20000], Training Loss: 0.0254\n",
            "Epoch [10145/20000], Training Loss: 0.0258\n",
            "Epoch [10146/20000], Training Loss: 0.0278\n",
            "Epoch [10147/20000], Training Loss: 0.0282\n",
            "Epoch [10148/20000], Training Loss: 0.0270\n",
            "Epoch [10149/20000], Training Loss: 0.0280\n",
            "Epoch [10150/20000], Training Loss: 0.0270\n",
            "Epoch [10151/20000], Training Loss: 0.0253\n",
            "Epoch [10152/20000], Training Loss: 0.0265\n",
            "Epoch [10153/20000], Training Loss: 0.0272\n",
            "Epoch [10154/20000], Training Loss: 0.0275\n",
            "Epoch [10155/20000], Training Loss: 0.0266\n",
            "Epoch [10156/20000], Training Loss: 0.0247\n",
            "Epoch [10157/20000], Training Loss: 0.0271\n",
            "Epoch [10158/20000], Training Loss: 0.0263\n",
            "Epoch [10159/20000], Training Loss: 0.0250\n",
            "Epoch [10160/20000], Training Loss: 0.0245\n",
            "Epoch [10161/20000], Training Loss: 0.0251\n",
            "Epoch [10162/20000], Training Loss: 0.0273\n",
            "Epoch [10163/20000], Training Loss: 0.0267\n",
            "Epoch [10164/20000], Training Loss: 0.0268\n",
            "Epoch [10165/20000], Training Loss: 0.0276\n",
            "Epoch [10166/20000], Training Loss: 0.0288\n",
            "Epoch [10167/20000], Training Loss: 0.0270\n",
            "Epoch [10168/20000], Training Loss: 0.0255\n",
            "Epoch [10169/20000], Training Loss: 0.0241\n",
            "Epoch [10170/20000], Training Loss: 0.0255\n",
            "Epoch [10171/20000], Training Loss: 0.0273\n",
            "Epoch [10172/20000], Training Loss: 0.0261\n",
            "Epoch [10173/20000], Training Loss: 0.0254\n",
            "Epoch [10174/20000], Training Loss: 0.0267\n",
            "Epoch [10175/20000], Training Loss: 0.0260\n",
            "Epoch [10176/20000], Training Loss: 0.0265\n",
            "Epoch [10177/20000], Training Loss: 0.0261\n",
            "Epoch [10178/20000], Training Loss: 0.0254\n",
            "Epoch [10179/20000], Training Loss: 0.0282\n",
            "Epoch [10180/20000], Training Loss: 0.0277\n",
            "Epoch [10181/20000], Training Loss: 0.0298\n",
            "Epoch [10182/20000], Training Loss: 0.0261\n",
            "Epoch [10183/20000], Training Loss: 0.0255\n",
            "Epoch [10184/20000], Training Loss: 0.0256\n",
            "Epoch [10185/20000], Training Loss: 0.0254\n",
            "Epoch [10186/20000], Training Loss: 0.0260\n",
            "Epoch [10187/20000], Training Loss: 0.0264\n",
            "Epoch [10188/20000], Training Loss: 0.0251\n",
            "Epoch [10189/20000], Training Loss: 0.0285\n",
            "Epoch [10190/20000], Training Loss: 0.0282\n",
            "Epoch [10191/20000], Training Loss: 0.0243\n",
            "Epoch [10192/20000], Training Loss: 0.0264\n",
            "Epoch [10193/20000], Training Loss: 0.0254\n",
            "Epoch [10194/20000], Training Loss: 0.0262\n",
            "Epoch [10195/20000], Training Loss: 0.0249\n",
            "Epoch [10196/20000], Training Loss: 0.0253\n",
            "Epoch [10197/20000], Training Loss: 0.0261\n",
            "Epoch [10198/20000], Training Loss: 0.0251\n",
            "Epoch [10199/20000], Training Loss: 0.0261\n",
            "Epoch [10200/20000], Training Loss: 0.0257\n",
            "Epoch [10201/20000], Training Loss: 0.0292\n",
            "Epoch [10202/20000], Training Loss: 0.0261\n",
            "Epoch [10203/20000], Training Loss: 0.0261\n",
            "Epoch [10204/20000], Training Loss: 0.0257\n",
            "Epoch [10205/20000], Training Loss: 0.0262\n",
            "Epoch [10206/20000], Training Loss: 0.0276\n",
            "Epoch [10207/20000], Training Loss: 0.0291\n",
            "Epoch [10208/20000], Training Loss: 0.0260\n",
            "Epoch [10209/20000], Training Loss: 0.0267\n",
            "Epoch [10210/20000], Training Loss: 0.0265\n",
            "Epoch [10211/20000], Training Loss: 0.0279\n",
            "Epoch [10212/20000], Training Loss: 0.0281\n",
            "Epoch [10213/20000], Training Loss: 0.0267\n",
            "Epoch [10214/20000], Training Loss: 0.0261\n",
            "Epoch [10215/20000], Training Loss: 0.0263\n",
            "Epoch [10216/20000], Training Loss: 0.0288\n",
            "Epoch [10217/20000], Training Loss: 0.0260\n",
            "Epoch [10218/20000], Training Loss: 0.0269\n",
            "Epoch [10219/20000], Training Loss: 0.0267\n",
            "Epoch [10220/20000], Training Loss: 0.0256\n",
            "Epoch [10221/20000], Training Loss: 0.0259\n",
            "Epoch [10222/20000], Training Loss: 0.0269\n",
            "Epoch [10223/20000], Training Loss: 0.0281\n",
            "Epoch [10224/20000], Training Loss: 0.0257\n",
            "Epoch [10225/20000], Training Loss: 0.0258\n",
            "Epoch [10226/20000], Training Loss: 0.0268\n",
            "Epoch [10227/20000], Training Loss: 0.0254\n",
            "Epoch [10228/20000], Training Loss: 0.0256\n",
            "Epoch [10229/20000], Training Loss: 0.0267\n",
            "Epoch [10230/20000], Training Loss: 0.0271\n",
            "Epoch [10231/20000], Training Loss: 0.0274\n",
            "Epoch [10232/20000], Training Loss: 0.0268\n",
            "Epoch [10233/20000], Training Loss: 0.0245\n",
            "Epoch [10234/20000], Training Loss: 0.0257\n",
            "Epoch [10235/20000], Training Loss: 0.0260\n",
            "Epoch [10236/20000], Training Loss: 0.0253\n",
            "Epoch [10237/20000], Training Loss: 0.0265\n",
            "Epoch [10238/20000], Training Loss: 0.0284\n",
            "Epoch [10239/20000], Training Loss: 0.0265\n",
            "Epoch [10240/20000], Training Loss: 0.0255\n",
            "Epoch [10241/20000], Training Loss: 0.0272\n",
            "Epoch [10242/20000], Training Loss: 0.0275\n",
            "Epoch [10243/20000], Training Loss: 0.0270\n",
            "Epoch [10244/20000], Training Loss: 0.0267\n",
            "Epoch [10245/20000], Training Loss: 0.0248\n",
            "Epoch [10246/20000], Training Loss: 0.0253\n",
            "Epoch [10247/20000], Training Loss: 0.0281\n",
            "Epoch [10248/20000], Training Loss: 0.0266\n",
            "Epoch [10249/20000], Training Loss: 0.0264\n",
            "Epoch [10250/20000], Training Loss: 0.0284\n",
            "Epoch [10251/20000], Training Loss: 0.0275\n",
            "Epoch [10252/20000], Training Loss: 0.0239\n",
            "Epoch [10253/20000], Training Loss: 0.0269\n",
            "Epoch [10254/20000], Training Loss: 0.0275\n",
            "Epoch [10255/20000], Training Loss: 0.0263\n",
            "Epoch [10256/20000], Training Loss: 0.0257\n",
            "Epoch [10257/20000], Training Loss: 0.0265\n",
            "Epoch [10258/20000], Training Loss: 0.0284\n",
            "Epoch [10259/20000], Training Loss: 0.0245\n",
            "Epoch [10260/20000], Training Loss: 0.0265\n",
            "Epoch [10261/20000], Training Loss: 0.0265\n",
            "Epoch [10262/20000], Training Loss: 0.0274\n",
            "Epoch [10263/20000], Training Loss: 0.0275\n",
            "Epoch [10264/20000], Training Loss: 0.0265\n",
            "Epoch [10265/20000], Training Loss: 0.0254\n",
            "Epoch [10266/20000], Training Loss: 0.0275\n",
            "Epoch [10267/20000], Training Loss: 0.0262\n",
            "Epoch [10268/20000], Training Loss: 0.0235\n",
            "Epoch [10269/20000], Training Loss: 0.0274\n",
            "Epoch [10270/20000], Training Loss: 0.0274\n",
            "Epoch [10271/20000], Training Loss: 0.0264\n",
            "Epoch [10272/20000], Training Loss: 0.0255\n",
            "Epoch [10273/20000], Training Loss: 0.0262\n",
            "Epoch [10274/20000], Training Loss: 0.0269\n",
            "Epoch [10275/20000], Training Loss: 0.0270\n",
            "Epoch [10276/20000], Training Loss: 0.0262\n",
            "Epoch [10277/20000], Training Loss: 0.0266\n",
            "Epoch [10278/20000], Training Loss: 0.0275\n",
            "Epoch [10279/20000], Training Loss: 0.0247\n",
            "Epoch [10280/20000], Training Loss: 0.0252\n",
            "Epoch [10281/20000], Training Loss: 0.0243\n",
            "Epoch [10282/20000], Training Loss: 0.0269\n",
            "Epoch [10283/20000], Training Loss: 0.0260\n",
            "Epoch [10284/20000], Training Loss: 0.0265\n",
            "Epoch [10285/20000], Training Loss: 0.0246\n",
            "Epoch [10286/20000], Training Loss: 0.0262\n",
            "Epoch [10287/20000], Training Loss: 0.0261\n",
            "Epoch [10288/20000], Training Loss: 0.0263\n",
            "Epoch [10289/20000], Training Loss: 0.0239\n",
            "Epoch [10290/20000], Training Loss: 0.0278\n",
            "Epoch [10291/20000], Training Loss: 0.0287\n",
            "Epoch [10292/20000], Training Loss: 0.0270\n",
            "Epoch [10293/20000], Training Loss: 0.0266\n",
            "Epoch [10294/20000], Training Loss: 0.0261\n",
            "Epoch [10295/20000], Training Loss: 0.0264\n",
            "Epoch [10296/20000], Training Loss: 0.0274\n",
            "Epoch [10297/20000], Training Loss: 0.0269\n",
            "Epoch [10298/20000], Training Loss: 0.0252\n",
            "Epoch [10299/20000], Training Loss: 0.0267\n",
            "Epoch [10300/20000], Training Loss: 0.0267\n",
            "Epoch [10301/20000], Training Loss: 0.0267\n",
            "Epoch [10302/20000], Training Loss: 0.0283\n",
            "Epoch [10303/20000], Training Loss: 0.0274\n",
            "Epoch [10304/20000], Training Loss: 0.0289\n",
            "Epoch [10305/20000], Training Loss: 0.0283\n",
            "Epoch [10306/20000], Training Loss: 0.0260\n",
            "Epoch [10307/20000], Training Loss: 0.0265\n",
            "Epoch [10308/20000], Training Loss: 0.0275\n",
            "Epoch [10309/20000], Training Loss: 0.0258\n",
            "Epoch [10310/20000], Training Loss: 0.0259\n",
            "Epoch [10311/20000], Training Loss: 0.0240\n",
            "Epoch [10312/20000], Training Loss: 0.0248\n",
            "Epoch [10313/20000], Training Loss: 0.0250\n",
            "Epoch [10314/20000], Training Loss: 0.0255\n",
            "Epoch [10315/20000], Training Loss: 0.0265\n",
            "Epoch [10316/20000], Training Loss: 0.0275\n",
            "Epoch [10317/20000], Training Loss: 0.0247\n",
            "Epoch [10318/20000], Training Loss: 0.0241\n",
            "Epoch [10319/20000], Training Loss: 0.0255\n",
            "Epoch [10320/20000], Training Loss: 0.0262\n",
            "Epoch [10321/20000], Training Loss: 0.0253\n",
            "Epoch [10322/20000], Training Loss: 0.0272\n",
            "Epoch [10323/20000], Training Loss: 0.0260\n",
            "Epoch [10324/20000], Training Loss: 0.0276\n",
            "Epoch [10325/20000], Training Loss: 0.0279\n",
            "Epoch [10326/20000], Training Loss: 0.0264\n",
            "Epoch [10327/20000], Training Loss: 0.0254\n",
            "Epoch [10328/20000], Training Loss: 0.0254\n",
            "Epoch [10329/20000], Training Loss: 0.0252\n",
            "Epoch [10330/20000], Training Loss: 0.0259\n",
            "Epoch [10331/20000], Training Loss: 0.0277\n",
            "Epoch [10332/20000], Training Loss: 0.0269\n",
            "Epoch [10333/20000], Training Loss: 0.0243\n",
            "Epoch [10334/20000], Training Loss: 0.0251\n",
            "Epoch [10335/20000], Training Loss: 0.0261\n",
            "Epoch [10336/20000], Training Loss: 0.0254\n",
            "Epoch [10337/20000], Training Loss: 0.0268\n",
            "Epoch [10338/20000], Training Loss: 0.0267\n",
            "Epoch [10339/20000], Training Loss: 0.0257\n",
            "Epoch [10340/20000], Training Loss: 0.0253\n",
            "Epoch [10341/20000], Training Loss: 0.0274\n",
            "Epoch [10342/20000], Training Loss: 0.0274\n",
            "Epoch [10343/20000], Training Loss: 0.0268\n",
            "Epoch [10344/20000], Training Loss: 0.0232\n",
            "Epoch [10345/20000], Training Loss: 0.0270\n",
            "Epoch [10346/20000], Training Loss: 0.0268\n",
            "Epoch [10347/20000], Training Loss: 0.0272\n",
            "Epoch [10348/20000], Training Loss: 0.0246\n",
            "Epoch [10349/20000], Training Loss: 0.0255\n",
            "Epoch [10350/20000], Training Loss: 0.0265\n",
            "Epoch [10351/20000], Training Loss: 0.0266\n",
            "Epoch [10352/20000], Training Loss: 0.0285\n",
            "Epoch [10353/20000], Training Loss: 0.0264\n",
            "Epoch [10354/20000], Training Loss: 0.0249\n",
            "Epoch [10355/20000], Training Loss: 0.0268\n",
            "Epoch [10356/20000], Training Loss: 0.0272\n",
            "Epoch [10357/20000], Training Loss: 0.0267\n",
            "Epoch [10358/20000], Training Loss: 0.0259\n",
            "Epoch [10359/20000], Training Loss: 0.0276\n",
            "Epoch [10360/20000], Training Loss: 0.0248\n",
            "Epoch [10361/20000], Training Loss: 0.0275\n",
            "Epoch [10362/20000], Training Loss: 0.0255\n",
            "Epoch [10363/20000], Training Loss: 0.0242\n",
            "Epoch [10364/20000], Training Loss: 0.0267\n",
            "Epoch [10365/20000], Training Loss: 0.0260\n",
            "Epoch [10366/20000], Training Loss: 0.0266\n",
            "Epoch [10367/20000], Training Loss: 0.0264\n",
            "Epoch [10368/20000], Training Loss: 0.0261\n",
            "Epoch [10369/20000], Training Loss: 0.0290\n",
            "Epoch [10370/20000], Training Loss: 0.0272\n",
            "Epoch [10371/20000], Training Loss: 0.0279\n",
            "Epoch [10372/20000], Training Loss: 0.0257\n",
            "Epoch [10373/20000], Training Loss: 0.0270\n",
            "Epoch [10374/20000], Training Loss: 0.0248\n",
            "Epoch [10375/20000], Training Loss: 0.0284\n",
            "Epoch [10376/20000], Training Loss: 0.0268\n",
            "Epoch [10377/20000], Training Loss: 0.0263\n",
            "Epoch [10378/20000], Training Loss: 0.0269\n",
            "Epoch [10379/20000], Training Loss: 0.0251\n",
            "Epoch [10380/20000], Training Loss: 0.0264\n",
            "Epoch [10381/20000], Training Loss: 0.0257\n",
            "Epoch [10382/20000], Training Loss: 0.0251\n",
            "Epoch [10383/20000], Training Loss: 0.0269\n",
            "Epoch [10384/20000], Training Loss: 0.0257\n",
            "Epoch [10385/20000], Training Loss: 0.0274\n",
            "Epoch [10386/20000], Training Loss: 0.0246\n",
            "Epoch [10387/20000], Training Loss: 0.0273\n",
            "Epoch [10388/20000], Training Loss: 0.0259\n",
            "Epoch [10389/20000], Training Loss: 0.0266\n",
            "Epoch [10390/20000], Training Loss: 0.0265\n",
            "Epoch [10391/20000], Training Loss: 0.0290\n",
            "Epoch [10392/20000], Training Loss: 0.0253\n",
            "Epoch [10393/20000], Training Loss: 0.0257\n",
            "Epoch [10394/20000], Training Loss: 0.0267\n",
            "Epoch [10395/20000], Training Loss: 0.0254\n",
            "Epoch [10396/20000], Training Loss: 0.0253\n",
            "Epoch [10397/20000], Training Loss: 0.0278\n",
            "Epoch [10398/20000], Training Loss: 0.0259\n",
            "Epoch [10399/20000], Training Loss: 0.0245\n",
            "Epoch [10400/20000], Training Loss: 0.0272\n",
            "Epoch [10401/20000], Training Loss: 0.0241\n",
            "Epoch [10402/20000], Training Loss: 0.0255\n",
            "Epoch [10403/20000], Training Loss: 0.0261\n",
            "Epoch [10404/20000], Training Loss: 0.0264\n",
            "Epoch [10405/20000], Training Loss: 0.0269\n",
            "Epoch [10406/20000], Training Loss: 0.0279\n",
            "Epoch [10407/20000], Training Loss: 0.0281\n",
            "Epoch [10408/20000], Training Loss: 0.0259\n",
            "Epoch [10409/20000], Training Loss: 0.0259\n",
            "Epoch [10410/20000], Training Loss: 0.0272\n",
            "Epoch [10411/20000], Training Loss: 0.0284\n",
            "Epoch [10412/20000], Training Loss: 0.0289\n",
            "Epoch [10413/20000], Training Loss: 0.0273\n",
            "Epoch [10414/20000], Training Loss: 0.0264\n",
            "Epoch [10415/20000], Training Loss: 0.0264\n",
            "Epoch [10416/20000], Training Loss: 0.0270\n",
            "Epoch [10417/20000], Training Loss: 0.0255\n",
            "Epoch [10418/20000], Training Loss: 0.0277\n",
            "Epoch [10419/20000], Training Loss: 0.0283\n",
            "Epoch [10420/20000], Training Loss: 0.0291\n",
            "Epoch [10421/20000], Training Loss: 0.0274\n",
            "Epoch [10422/20000], Training Loss: 0.0252\n",
            "Epoch [10423/20000], Training Loss: 0.0263\n",
            "Epoch [10424/20000], Training Loss: 0.0278\n",
            "Epoch [10425/20000], Training Loss: 0.0259\n",
            "Epoch [10426/20000], Training Loss: 0.0276\n",
            "Epoch [10427/20000], Training Loss: 0.0275\n",
            "Epoch [10428/20000], Training Loss: 0.0260\n",
            "Epoch [10429/20000], Training Loss: 0.0256\n",
            "Epoch [10430/20000], Training Loss: 0.0262\n",
            "Epoch [10431/20000], Training Loss: 0.0257\n",
            "Epoch [10432/20000], Training Loss: 0.0277\n",
            "Epoch [10433/20000], Training Loss: 0.0247\n",
            "Epoch [10434/20000], Training Loss: 0.0274\n",
            "Epoch [10435/20000], Training Loss: 0.0251\n",
            "Epoch [10436/20000], Training Loss: 0.0241\n",
            "Epoch [10437/20000], Training Loss: 0.0277\n",
            "Epoch [10438/20000], Training Loss: 0.0262\n",
            "Epoch [10439/20000], Training Loss: 0.0270\n",
            "Epoch [10440/20000], Training Loss: 0.0275\n",
            "Epoch [10441/20000], Training Loss: 0.0285\n",
            "Epoch [10442/20000], Training Loss: 0.0263\n",
            "Epoch [10443/20000], Training Loss: 0.0244\n",
            "Epoch [10444/20000], Training Loss: 0.0257\n",
            "Epoch [10445/20000], Training Loss: 0.0268\n",
            "Epoch [10446/20000], Training Loss: 0.0263\n",
            "Epoch [10447/20000], Training Loss: 0.0265\n",
            "Epoch [10448/20000], Training Loss: 0.0249\n",
            "Epoch [10449/20000], Training Loss: 0.0259\n",
            "Epoch [10450/20000], Training Loss: 0.0256\n",
            "Epoch [10451/20000], Training Loss: 0.0253\n",
            "Epoch [10452/20000], Training Loss: 0.0250\n",
            "Epoch [10453/20000], Training Loss: 0.0283\n",
            "Epoch [10454/20000], Training Loss: 0.0260\n",
            "Epoch [10455/20000], Training Loss: 0.0291\n",
            "Epoch [10456/20000], Training Loss: 0.0253\n",
            "Epoch [10457/20000], Training Loss: 0.0280\n",
            "Epoch [10458/20000], Training Loss: 0.0256\n",
            "Epoch [10459/20000], Training Loss: 0.0276\n",
            "Epoch [10460/20000], Training Loss: 0.0287\n",
            "Epoch [10461/20000], Training Loss: 0.0264\n",
            "Epoch [10462/20000], Training Loss: 0.0252\n",
            "Epoch [10463/20000], Training Loss: 0.0254\n",
            "Epoch [10464/20000], Training Loss: 0.0258\n",
            "Epoch [10465/20000], Training Loss: 0.0283\n",
            "Epoch [10466/20000], Training Loss: 0.0275\n",
            "Epoch [10467/20000], Training Loss: 0.0269\n",
            "Epoch [10468/20000], Training Loss: 0.0251\n",
            "Epoch [10469/20000], Training Loss: 0.0255\n",
            "Epoch [10470/20000], Training Loss: 0.0256\n",
            "Epoch [10471/20000], Training Loss: 0.0249\n",
            "Epoch [10472/20000], Training Loss: 0.0274\n",
            "Epoch [10473/20000], Training Loss: 0.0276\n",
            "Epoch [10474/20000], Training Loss: 0.0271\n",
            "Epoch [10475/20000], Training Loss: 0.0263\n",
            "Epoch [10476/20000], Training Loss: 0.0256\n",
            "Epoch [10477/20000], Training Loss: 0.0244\n",
            "Epoch [10478/20000], Training Loss: 0.0253\n",
            "Epoch [10479/20000], Training Loss: 0.0250\n",
            "Epoch [10480/20000], Training Loss: 0.0266\n",
            "Epoch [10481/20000], Training Loss: 0.0252\n",
            "Epoch [10482/20000], Training Loss: 0.0265\n",
            "Epoch [10483/20000], Training Loss: 0.0264\n",
            "Epoch [10484/20000], Training Loss: 0.0271\n",
            "Epoch [10485/20000], Training Loss: 0.0257\n",
            "Epoch [10486/20000], Training Loss: 0.0275\n",
            "Epoch [10487/20000], Training Loss: 0.0249\n",
            "Epoch [10488/20000], Training Loss: 0.0280\n",
            "Epoch [10489/20000], Training Loss: 0.0274\n",
            "Epoch [10490/20000], Training Loss: 0.0259\n",
            "Epoch [10491/20000], Training Loss: 0.0258\n",
            "Epoch [10492/20000], Training Loss: 0.0280\n",
            "Epoch [10493/20000], Training Loss: 0.0265\n",
            "Epoch [10494/20000], Training Loss: 0.0269\n",
            "Epoch [10495/20000], Training Loss: 0.0256\n",
            "Epoch [10496/20000], Training Loss: 0.0264\n",
            "Epoch [10497/20000], Training Loss: 0.0245\n",
            "Epoch [10498/20000], Training Loss: 0.0267\n",
            "Epoch [10499/20000], Training Loss: 0.0251\n",
            "Epoch [10500/20000], Training Loss: 0.0262\n",
            "Epoch [10501/20000], Training Loss: 0.0258\n",
            "Epoch [10502/20000], Training Loss: 0.0269\n",
            "Epoch [10503/20000], Training Loss: 0.0258\n",
            "Epoch [10504/20000], Training Loss: 0.0270\n",
            "Epoch [10505/20000], Training Loss: 0.0278\n",
            "Epoch [10506/20000], Training Loss: 0.0270\n",
            "Epoch [10507/20000], Training Loss: 0.0262\n",
            "Epoch [10508/20000], Training Loss: 0.0269\n",
            "Epoch [10509/20000], Training Loss: 0.0264\n",
            "Epoch [10510/20000], Training Loss: 0.0270\n",
            "Epoch [10511/20000], Training Loss: 0.0243\n",
            "Epoch [10512/20000], Training Loss: 0.0278\n",
            "Epoch [10513/20000], Training Loss: 0.0266\n",
            "Epoch [10514/20000], Training Loss: 0.0268\n",
            "Epoch [10515/20000], Training Loss: 0.0256\n",
            "Epoch [10516/20000], Training Loss: 0.0276\n",
            "Epoch [10517/20000], Training Loss: 0.0259\n",
            "Epoch [10518/20000], Training Loss: 0.0255\n",
            "Epoch [10519/20000], Training Loss: 0.0257\n",
            "Epoch [10520/20000], Training Loss: 0.0255\n",
            "Epoch [10521/20000], Training Loss: 0.0265\n",
            "Epoch [10522/20000], Training Loss: 0.0266\n",
            "Epoch [10523/20000], Training Loss: 0.0244\n",
            "Epoch [10524/20000], Training Loss: 0.0250\n",
            "Epoch [10525/20000], Training Loss: 0.0248\n",
            "Epoch [10526/20000], Training Loss: 0.0265\n",
            "Epoch [10527/20000], Training Loss: 0.0261\n",
            "Epoch [10528/20000], Training Loss: 0.0270\n",
            "Epoch [10529/20000], Training Loss: 0.0259\n",
            "Epoch [10530/20000], Training Loss: 0.0271\n",
            "Epoch [10531/20000], Training Loss: 0.0273\n",
            "Epoch [10532/20000], Training Loss: 0.0254\n",
            "Epoch [10533/20000], Training Loss: 0.0262\n",
            "Epoch [10534/20000], Training Loss: 0.0258\n",
            "Epoch [10535/20000], Training Loss: 0.0267\n",
            "Epoch [10536/20000], Training Loss: 0.0252\n",
            "Epoch [10537/20000], Training Loss: 0.0280\n",
            "Epoch [10538/20000], Training Loss: 0.0271\n",
            "Epoch [10539/20000], Training Loss: 0.0264\n",
            "Epoch [10540/20000], Training Loss: 0.0264\n",
            "Epoch [10541/20000], Training Loss: 0.0252\n",
            "Epoch [10542/20000], Training Loss: 0.0272\n",
            "Epoch [10543/20000], Training Loss: 0.0273\n",
            "Epoch [10544/20000], Training Loss: 0.0244\n",
            "Epoch [10545/20000], Training Loss: 0.0254\n",
            "Epoch [10546/20000], Training Loss: 0.0265\n",
            "Epoch [10547/20000], Training Loss: 0.0248\n",
            "Epoch [10548/20000], Training Loss: 0.0251\n",
            "Epoch [10549/20000], Training Loss: 0.0258\n",
            "Epoch [10550/20000], Training Loss: 0.0271\n",
            "Epoch [10551/20000], Training Loss: 0.0244\n",
            "Epoch [10552/20000], Training Loss: 0.0272\n",
            "Epoch [10553/20000], Training Loss: 0.0270\n",
            "Epoch [10554/20000], Training Loss: 0.0287\n",
            "Epoch [10555/20000], Training Loss: 0.0261\n",
            "Epoch [10556/20000], Training Loss: 0.0257\n",
            "Epoch [10557/20000], Training Loss: 0.0262\n",
            "Epoch [10558/20000], Training Loss: 0.0266\n",
            "Epoch [10559/20000], Training Loss: 0.0255\n",
            "Epoch [10560/20000], Training Loss: 0.0252\n",
            "Epoch [10561/20000], Training Loss: 0.0260\n",
            "Epoch [10562/20000], Training Loss: 0.0271\n",
            "Epoch [10563/20000], Training Loss: 0.0284\n",
            "Epoch [10564/20000], Training Loss: 0.0287\n",
            "Epoch [10565/20000], Training Loss: 0.0250\n",
            "Epoch [10566/20000], Training Loss: 0.0251\n",
            "Epoch [10567/20000], Training Loss: 0.0267\n",
            "Epoch [10568/20000], Training Loss: 0.0289\n",
            "Epoch [10569/20000], Training Loss: 0.0266\n",
            "Epoch [10570/20000], Training Loss: 0.0264\n",
            "Epoch [10571/20000], Training Loss: 0.0281\n",
            "Epoch [10572/20000], Training Loss: 0.0264\n",
            "Epoch [10573/20000], Training Loss: 0.0265\n",
            "Epoch [10574/20000], Training Loss: 0.0244\n",
            "Epoch [10575/20000], Training Loss: 0.0278\n",
            "Epoch [10576/20000], Training Loss: 0.0254\n",
            "Epoch [10577/20000], Training Loss: 0.0262\n",
            "Epoch [10578/20000], Training Loss: 0.0258\n",
            "Epoch [10579/20000], Training Loss: 0.0279\n",
            "Epoch [10580/20000], Training Loss: 0.0267\n",
            "Epoch [10581/20000], Training Loss: 0.0259\n",
            "Epoch [10582/20000], Training Loss: 0.0255\n",
            "Epoch [10583/20000], Training Loss: 0.0262\n",
            "Epoch [10584/20000], Training Loss: 0.0252\n",
            "Epoch [10585/20000], Training Loss: 0.0275\n",
            "Epoch [10586/20000], Training Loss: 0.0260\n",
            "Epoch [10587/20000], Training Loss: 0.0295\n",
            "Epoch [10588/20000], Training Loss: 0.0261\n",
            "Epoch [10589/20000], Training Loss: 0.0260\n",
            "Epoch [10590/20000], Training Loss: 0.0273\n",
            "Epoch [10591/20000], Training Loss: 0.0258\n",
            "Epoch [10592/20000], Training Loss: 0.0266\n",
            "Epoch [10593/20000], Training Loss: 0.0258\n",
            "Epoch [10594/20000], Training Loss: 0.0253\n",
            "Epoch [10595/20000], Training Loss: 0.0269\n",
            "Epoch [10596/20000], Training Loss: 0.0260\n",
            "Epoch [10597/20000], Training Loss: 0.0262\n",
            "Epoch [10598/20000], Training Loss: 0.0253\n",
            "Epoch [10599/20000], Training Loss: 0.0261\n",
            "Epoch [10600/20000], Training Loss: 0.0256\n",
            "Epoch [10601/20000], Training Loss: 0.0278\n",
            "Epoch [10602/20000], Training Loss: 0.0284\n",
            "Epoch [10603/20000], Training Loss: 0.0287\n",
            "Epoch [10604/20000], Training Loss: 0.0273\n",
            "Epoch [10605/20000], Training Loss: 0.0248\n",
            "Epoch [10606/20000], Training Loss: 0.0273\n",
            "Epoch [10607/20000], Training Loss: 0.0245\n",
            "Epoch [10608/20000], Training Loss: 0.0284\n",
            "Epoch [10609/20000], Training Loss: 0.0269\n",
            "Epoch [10610/20000], Training Loss: 0.0252\n",
            "Epoch [10611/20000], Training Loss: 0.0253\n",
            "Epoch [10612/20000], Training Loss: 0.0250\n",
            "Epoch [10613/20000], Training Loss: 0.0262\n",
            "Epoch [10614/20000], Training Loss: 0.0280\n",
            "Epoch [10615/20000], Training Loss: 0.0301\n",
            "Epoch [10616/20000], Training Loss: 0.0288\n",
            "Epoch [10617/20000], Training Loss: 0.0255\n",
            "Epoch [10618/20000], Training Loss: 0.0279\n",
            "Epoch [10619/20000], Training Loss: 0.0256\n",
            "Epoch [10620/20000], Training Loss: 0.0270\n",
            "Epoch [10621/20000], Training Loss: 0.0285\n",
            "Epoch [10622/20000], Training Loss: 0.0267\n",
            "Epoch [10623/20000], Training Loss: 0.0273\n",
            "Epoch [10624/20000], Training Loss: 0.0250\n",
            "Epoch [10625/20000], Training Loss: 0.0275\n",
            "Epoch [10626/20000], Training Loss: 0.0262\n",
            "Epoch [10627/20000], Training Loss: 0.0262\n",
            "Epoch [10628/20000], Training Loss: 0.0274\n",
            "Epoch [10629/20000], Training Loss: 0.0269\n",
            "Epoch [10630/20000], Training Loss: 0.0258\n",
            "Epoch [10631/20000], Training Loss: 0.0256\n",
            "Epoch [10632/20000], Training Loss: 0.0278\n",
            "Epoch [10633/20000], Training Loss: 0.0287\n",
            "Epoch [10634/20000], Training Loss: 0.0281\n",
            "Epoch [10635/20000], Training Loss: 0.0251\n",
            "Epoch [10636/20000], Training Loss: 0.0280\n",
            "Epoch [10637/20000], Training Loss: 0.0274\n",
            "Epoch [10638/20000], Training Loss: 0.0294\n",
            "Epoch [10639/20000], Training Loss: 0.0286\n",
            "Epoch [10640/20000], Training Loss: 0.0258\n",
            "Epoch [10641/20000], Training Loss: 0.0256\n",
            "Epoch [10642/20000], Training Loss: 0.0264\n",
            "Epoch [10643/20000], Training Loss: 0.0247\n",
            "Epoch [10644/20000], Training Loss: 0.0261\n",
            "Epoch [10645/20000], Training Loss: 0.0245\n",
            "Epoch [10646/20000], Training Loss: 0.0257\n",
            "Epoch [10647/20000], Training Loss: 0.0262\n",
            "Epoch [10648/20000], Training Loss: 0.0251\n",
            "Epoch [10649/20000], Training Loss: 0.0262\n",
            "Epoch [10650/20000], Training Loss: 0.0264\n",
            "Epoch [10651/20000], Training Loss: 0.0257\n",
            "Epoch [10652/20000], Training Loss: 0.0267\n",
            "Epoch [10653/20000], Training Loss: 0.0272\n",
            "Epoch [10654/20000], Training Loss: 0.0267\n",
            "Epoch [10655/20000], Training Loss: 0.0249\n",
            "Epoch [10656/20000], Training Loss: 0.0265\n",
            "Epoch [10657/20000], Training Loss: 0.0249\n",
            "Epoch [10658/20000], Training Loss: 0.0264\n",
            "Epoch [10659/20000], Training Loss: 0.0259\n",
            "Epoch [10660/20000], Training Loss: 0.0280\n",
            "Epoch [10661/20000], Training Loss: 0.0268\n",
            "Epoch [10662/20000], Training Loss: 0.0250\n",
            "Epoch [10663/20000], Training Loss: 0.0253\n",
            "Epoch [10664/20000], Training Loss: 0.0250\n",
            "Epoch [10665/20000], Training Loss: 0.0275\n",
            "Epoch [10666/20000], Training Loss: 0.0282\n",
            "Epoch [10667/20000], Training Loss: 0.0282\n",
            "Epoch [10668/20000], Training Loss: 0.0246\n",
            "Epoch [10669/20000], Training Loss: 0.0254\n",
            "Epoch [10670/20000], Training Loss: 0.0265\n",
            "Epoch [10671/20000], Training Loss: 0.0236\n",
            "Epoch [10672/20000], Training Loss: 0.0265\n",
            "Epoch [10673/20000], Training Loss: 0.0283\n",
            "Epoch [10674/20000], Training Loss: 0.0269\n",
            "Epoch [10675/20000], Training Loss: 0.0265\n",
            "Epoch [10676/20000], Training Loss: 0.0270\n",
            "Epoch [10677/20000], Training Loss: 0.0275\n",
            "Epoch [10678/20000], Training Loss: 0.0266\n",
            "Epoch [10679/20000], Training Loss: 0.0276\n",
            "Epoch [10680/20000], Training Loss: 0.0272\n",
            "Epoch [10681/20000], Training Loss: 0.0276\n",
            "Epoch [10682/20000], Training Loss: 0.0263\n",
            "Epoch [10683/20000], Training Loss: 0.0283\n",
            "Epoch [10684/20000], Training Loss: 0.0267\n",
            "Epoch [10685/20000], Training Loss: 0.0251\n",
            "Epoch [10686/20000], Training Loss: 0.0245\n",
            "Epoch [10687/20000], Training Loss: 0.0263\n",
            "Epoch [10688/20000], Training Loss: 0.0276\n",
            "Epoch [10689/20000], Training Loss: 0.0269\n",
            "Epoch [10690/20000], Training Loss: 0.0272\n",
            "Epoch [10691/20000], Training Loss: 0.0255\n",
            "Epoch [10692/20000], Training Loss: 0.0257\n",
            "Epoch [10693/20000], Training Loss: 0.0291\n",
            "Epoch [10694/20000], Training Loss: 0.0274\n",
            "Epoch [10695/20000], Training Loss: 0.0274\n",
            "Epoch [10696/20000], Training Loss: 0.0278\n",
            "Epoch [10697/20000], Training Loss: 0.0286\n",
            "Epoch [10698/20000], Training Loss: 0.0251\n",
            "Epoch [10699/20000], Training Loss: 0.0267\n",
            "Epoch [10700/20000], Training Loss: 0.0237\n",
            "Epoch [10701/20000], Training Loss: 0.0264\n",
            "Epoch [10702/20000], Training Loss: 0.0260\n",
            "Epoch [10703/20000], Training Loss: 0.0263\n",
            "Epoch [10704/20000], Training Loss: 0.0249\n",
            "Epoch [10705/20000], Training Loss: 0.0279\n",
            "Epoch [10706/20000], Training Loss: 0.0277\n",
            "Epoch [10707/20000], Training Loss: 0.0268\n",
            "Epoch [10708/20000], Training Loss: 0.0275\n",
            "Epoch [10709/20000], Training Loss: 0.0301\n",
            "Epoch [10710/20000], Training Loss: 0.0278\n",
            "Epoch [10711/20000], Training Loss: 0.0256\n",
            "Epoch [10712/20000], Training Loss: 0.0276\n",
            "Epoch [10713/20000], Training Loss: 0.0269\n",
            "Epoch [10714/20000], Training Loss: 0.0266\n",
            "Epoch [10715/20000], Training Loss: 0.0283\n",
            "Epoch [10716/20000], Training Loss: 0.0270\n",
            "Epoch [10717/20000], Training Loss: 0.0263\n",
            "Epoch [10718/20000], Training Loss: 0.0269\n",
            "Epoch [10719/20000], Training Loss: 0.0279\n",
            "Epoch [10720/20000], Training Loss: 0.0250\n",
            "Epoch [10721/20000], Training Loss: 0.0250\n",
            "Epoch [10722/20000], Training Loss: 0.0271\n",
            "Epoch [10723/20000], Training Loss: 0.0280\n",
            "Epoch [10724/20000], Training Loss: 0.0285\n",
            "Epoch [10725/20000], Training Loss: 0.0276\n",
            "Epoch [10726/20000], Training Loss: 0.0259\n",
            "Epoch [10727/20000], Training Loss: 0.0268\n",
            "Epoch [10728/20000], Training Loss: 0.0256\n",
            "Epoch [10729/20000], Training Loss: 0.0265\n",
            "Epoch [10730/20000], Training Loss: 0.0263\n",
            "Epoch [10731/20000], Training Loss: 0.0246\n",
            "Epoch [10732/20000], Training Loss: 0.0253\n",
            "Epoch [10733/20000], Training Loss: 0.0257\n",
            "Epoch [10734/20000], Training Loss: 0.0254\n",
            "Epoch [10735/20000], Training Loss: 0.0269\n",
            "Epoch [10736/20000], Training Loss: 0.0268\n",
            "Epoch [10737/20000], Training Loss: 0.0260\n",
            "Epoch [10738/20000], Training Loss: 0.0258\n",
            "Epoch [10739/20000], Training Loss: 0.0249\n",
            "Epoch [10740/20000], Training Loss: 0.0265\n",
            "Epoch [10741/20000], Training Loss: 0.0252\n",
            "Epoch [10742/20000], Training Loss: 0.0260\n",
            "Epoch [10743/20000], Training Loss: 0.0267\n",
            "Epoch [10744/20000], Training Loss: 0.0260\n",
            "Epoch [10745/20000], Training Loss: 0.0258\n",
            "Epoch [10746/20000], Training Loss: 0.0255\n",
            "Epoch [10747/20000], Training Loss: 0.0267\n",
            "Epoch [10748/20000], Training Loss: 0.0249\n",
            "Epoch [10749/20000], Training Loss: 0.0240\n",
            "Epoch [10750/20000], Training Loss: 0.0258\n",
            "Epoch [10751/20000], Training Loss: 0.0261\n",
            "Epoch [10752/20000], Training Loss: 0.0257\n",
            "Epoch [10753/20000], Training Loss: 0.0266\n",
            "Epoch [10754/20000], Training Loss: 0.0251\n",
            "Epoch [10755/20000], Training Loss: 0.0269\n",
            "Epoch [10756/20000], Training Loss: 0.0275\n",
            "Epoch [10757/20000], Training Loss: 0.0261\n",
            "Epoch [10758/20000], Training Loss: 0.0280\n",
            "Epoch [10759/20000], Training Loss: 0.0292\n",
            "Epoch [10760/20000], Training Loss: 0.0263\n",
            "Epoch [10761/20000], Training Loss: 0.0253\n",
            "Epoch [10762/20000], Training Loss: 0.0268\n",
            "Epoch [10763/20000], Training Loss: 0.0272\n",
            "Epoch [10764/20000], Training Loss: 0.0265\n",
            "Epoch [10765/20000], Training Loss: 0.0248\n",
            "Epoch [10766/20000], Training Loss: 0.0264\n",
            "Epoch [10767/20000], Training Loss: 0.0237\n",
            "Epoch [10768/20000], Training Loss: 0.0250\n",
            "Epoch [10769/20000], Training Loss: 0.0270\n",
            "Epoch [10770/20000], Training Loss: 0.0248\n",
            "Epoch [10771/20000], Training Loss: 0.0262\n",
            "Epoch [10772/20000], Training Loss: 0.0269\n",
            "Epoch [10773/20000], Training Loss: 0.0255\n",
            "Epoch [10774/20000], Training Loss: 0.0274\n",
            "Epoch [10775/20000], Training Loss: 0.0266\n",
            "Epoch [10776/20000], Training Loss: 0.0271\n",
            "Epoch [10777/20000], Training Loss: 0.0252\n",
            "Epoch [10778/20000], Training Loss: 0.0256\n",
            "Epoch [10779/20000], Training Loss: 0.0255\n",
            "Epoch [10780/20000], Training Loss: 0.0244\n",
            "Epoch [10781/20000], Training Loss: 0.0261\n",
            "Epoch [10782/20000], Training Loss: 0.0277\n",
            "Epoch [10783/20000], Training Loss: 0.0267\n",
            "Epoch [10784/20000], Training Loss: 0.0286\n",
            "Epoch [10785/20000], Training Loss: 0.0245\n",
            "Epoch [10786/20000], Training Loss: 0.0262\n",
            "Epoch [10787/20000], Training Loss: 0.0261\n",
            "Epoch [10788/20000], Training Loss: 0.0279\n",
            "Epoch [10789/20000], Training Loss: 0.0260\n",
            "Epoch [10790/20000], Training Loss: 0.0276\n",
            "Epoch [10791/20000], Training Loss: 0.0270\n",
            "Epoch [10792/20000], Training Loss: 0.0276\n",
            "Epoch [10793/20000], Training Loss: 0.0285\n",
            "Epoch [10794/20000], Training Loss: 0.0286\n",
            "Epoch [10795/20000], Training Loss: 0.0269\n",
            "Epoch [10796/20000], Training Loss: 0.0264\n",
            "Epoch [10797/20000], Training Loss: 0.0259\n",
            "Epoch [10798/20000], Training Loss: 0.0255\n",
            "Epoch [10799/20000], Training Loss: 0.0257\n",
            "Epoch [10800/20000], Training Loss: 0.0283\n",
            "Epoch [10801/20000], Training Loss: 0.0257\n",
            "Epoch [10802/20000], Training Loss: 0.0273\n",
            "Epoch [10803/20000], Training Loss: 0.0280\n",
            "Epoch [10804/20000], Training Loss: 0.0262\n",
            "Epoch [10805/20000], Training Loss: 0.0251\n",
            "Epoch [10806/20000], Training Loss: 0.0255\n",
            "Epoch [10807/20000], Training Loss: 0.0264\n",
            "Epoch [10808/20000], Training Loss: 0.0272\n",
            "Epoch [10809/20000], Training Loss: 0.0281\n",
            "Epoch [10810/20000], Training Loss: 0.0267\n",
            "Epoch [10811/20000], Training Loss: 0.0280\n",
            "Epoch [10812/20000], Training Loss: 0.0267\n",
            "Epoch [10813/20000], Training Loss: 0.0240\n",
            "Epoch [10814/20000], Training Loss: 0.0251\n",
            "Epoch [10815/20000], Training Loss: 0.0269\n",
            "Epoch [10816/20000], Training Loss: 0.0255\n",
            "Epoch [10817/20000], Training Loss: 0.0275\n",
            "Epoch [10818/20000], Training Loss: 0.0289\n",
            "Epoch [10819/20000], Training Loss: 0.0276\n",
            "Epoch [10820/20000], Training Loss: 0.0264\n",
            "Epoch [10821/20000], Training Loss: 0.0260\n",
            "Epoch [10822/20000], Training Loss: 0.0253\n",
            "Epoch [10823/20000], Training Loss: 0.0274\n",
            "Epoch [10824/20000], Training Loss: 0.0257\n",
            "Epoch [10825/20000], Training Loss: 0.0265\n",
            "Epoch [10826/20000], Training Loss: 0.0271\n",
            "Epoch [10827/20000], Training Loss: 0.0279\n",
            "Epoch [10828/20000], Training Loss: 0.0268\n",
            "Epoch [10829/20000], Training Loss: 0.0247\n",
            "Epoch [10830/20000], Training Loss: 0.0258\n",
            "Epoch [10831/20000], Training Loss: 0.0267\n",
            "Epoch [10832/20000], Training Loss: 0.0272\n",
            "Epoch [10833/20000], Training Loss: 0.0268\n",
            "Epoch [10834/20000], Training Loss: 0.0243\n",
            "Epoch [10835/20000], Training Loss: 0.0255\n",
            "Epoch [10836/20000], Training Loss: 0.0255\n",
            "Epoch [10837/20000], Training Loss: 0.0258\n",
            "Epoch [10838/20000], Training Loss: 0.0267\n",
            "Epoch [10839/20000], Training Loss: 0.0274\n",
            "Epoch [10840/20000], Training Loss: 0.0291\n",
            "Epoch [10841/20000], Training Loss: 0.0268\n",
            "Epoch [10842/20000], Training Loss: 0.0250\n",
            "Epoch [10843/20000], Training Loss: 0.0271\n",
            "Epoch [10844/20000], Training Loss: 0.0258\n",
            "Epoch [10845/20000], Training Loss: 0.0252\n",
            "Epoch [10846/20000], Training Loss: 0.0260\n",
            "Epoch [10847/20000], Training Loss: 0.0272\n",
            "Epoch [10848/20000], Training Loss: 0.0253\n",
            "Epoch [10849/20000], Training Loss: 0.0264\n",
            "Epoch [10850/20000], Training Loss: 0.0283\n",
            "Epoch [10851/20000], Training Loss: 0.0269\n",
            "Epoch [10852/20000], Training Loss: 0.0271\n",
            "Epoch [10853/20000], Training Loss: 0.0270\n",
            "Epoch [10854/20000], Training Loss: 0.0267\n",
            "Epoch [10855/20000], Training Loss: 0.0253\n",
            "Epoch [10856/20000], Training Loss: 0.0254\n",
            "Epoch [10857/20000], Training Loss: 0.0244\n",
            "Epoch [10858/20000], Training Loss: 0.0252\n",
            "Epoch [10859/20000], Training Loss: 0.0278\n",
            "Epoch [10860/20000], Training Loss: 0.0291\n",
            "Epoch [10861/20000], Training Loss: 0.0285\n",
            "Epoch [10862/20000], Training Loss: 0.0256\n",
            "Epoch [10863/20000], Training Loss: 0.0264\n",
            "Epoch [10864/20000], Training Loss: 0.0269\n",
            "Epoch [10865/20000], Training Loss: 0.0257\n",
            "Epoch [10866/20000], Training Loss: 0.0271\n",
            "Epoch [10867/20000], Training Loss: 0.0277\n",
            "Epoch [10868/20000], Training Loss: 0.0259\n",
            "Epoch [10869/20000], Training Loss: 0.0257\n",
            "Epoch [10870/20000], Training Loss: 0.0273\n",
            "Epoch [10871/20000], Training Loss: 0.0266\n",
            "Epoch [10872/20000], Training Loss: 0.0258\n",
            "Epoch [10873/20000], Training Loss: 0.0284\n",
            "Epoch [10874/20000], Training Loss: 0.0246\n",
            "Epoch [10875/20000], Training Loss: 0.0272\n",
            "Epoch [10876/20000], Training Loss: 0.0267\n",
            "Epoch [10877/20000], Training Loss: 0.0258\n",
            "Epoch [10878/20000], Training Loss: 0.0254\n",
            "Epoch [10879/20000], Training Loss: 0.0276\n",
            "Epoch [10880/20000], Training Loss: 0.0277\n",
            "Epoch [10881/20000], Training Loss: 0.0259\n",
            "Epoch [10882/20000], Training Loss: 0.0255\n",
            "Epoch [10883/20000], Training Loss: 0.0284\n",
            "Epoch [10884/20000], Training Loss: 0.0241\n",
            "Epoch [10885/20000], Training Loss: 0.0248\n",
            "Epoch [10886/20000], Training Loss: 0.0258\n",
            "Epoch [10887/20000], Training Loss: 0.0254\n",
            "Epoch [10888/20000], Training Loss: 0.0273\n",
            "Epoch [10889/20000], Training Loss: 0.0272\n",
            "Epoch [10890/20000], Training Loss: 0.0287\n",
            "Epoch [10891/20000], Training Loss: 0.0245\n",
            "Epoch [10892/20000], Training Loss: 0.0296\n",
            "Epoch [10893/20000], Training Loss: 0.0258\n",
            "Epoch [10894/20000], Training Loss: 0.0263\n",
            "Epoch [10895/20000], Training Loss: 0.0280\n",
            "Epoch [10896/20000], Training Loss: 0.0253\n",
            "Epoch [10897/20000], Training Loss: 0.0255\n",
            "Epoch [10898/20000], Training Loss: 0.0263\n",
            "Epoch [10899/20000], Training Loss: 0.0258\n",
            "Epoch [10900/20000], Training Loss: 0.0256\n",
            "Epoch [10901/20000], Training Loss: 0.0270\n",
            "Epoch [10902/20000], Training Loss: 0.0266\n",
            "Epoch [10903/20000], Training Loss: 0.0241\n",
            "Epoch [10904/20000], Training Loss: 0.0277\n",
            "Epoch [10905/20000], Training Loss: 0.0274\n",
            "Epoch [10906/20000], Training Loss: 0.0262\n",
            "Epoch [10907/20000], Training Loss: 0.0282\n",
            "Epoch [10908/20000], Training Loss: 0.0259\n",
            "Epoch [10909/20000], Training Loss: 0.0283\n",
            "Epoch [10910/20000], Training Loss: 0.0263\n",
            "Epoch [10911/20000], Training Loss: 0.0268\n",
            "Epoch [10912/20000], Training Loss: 0.0252\n",
            "Epoch [10913/20000], Training Loss: 0.0255\n",
            "Epoch [10914/20000], Training Loss: 0.0258\n",
            "Epoch [10915/20000], Training Loss: 0.0281\n",
            "Epoch [10916/20000], Training Loss: 0.0270\n",
            "Epoch [10917/20000], Training Loss: 0.0262\n",
            "Epoch [10918/20000], Training Loss: 0.0272\n",
            "Epoch [10919/20000], Training Loss: 0.0254\n",
            "Epoch [10920/20000], Training Loss: 0.0277\n",
            "Epoch [10921/20000], Training Loss: 0.0281\n",
            "Epoch [10922/20000], Training Loss: 0.0269\n",
            "Epoch [10923/20000], Training Loss: 0.0268\n",
            "Epoch [10924/20000], Training Loss: 0.0264\n",
            "Epoch [10925/20000], Training Loss: 0.0278\n",
            "Epoch [10926/20000], Training Loss: 0.0263\n",
            "Epoch [10927/20000], Training Loss: 0.0265\n",
            "Epoch [10928/20000], Training Loss: 0.0271\n",
            "Epoch [10929/20000], Training Loss: 0.0252\n",
            "Epoch [10930/20000], Training Loss: 0.0256\n",
            "Epoch [10931/20000], Training Loss: 0.0260\n",
            "Epoch [10932/20000], Training Loss: 0.0258\n",
            "Epoch [10933/20000], Training Loss: 0.0286\n",
            "Epoch [10934/20000], Training Loss: 0.0276\n",
            "Epoch [10935/20000], Training Loss: 0.0264\n",
            "Epoch [10936/20000], Training Loss: 0.0275\n",
            "Epoch [10937/20000], Training Loss: 0.0273\n",
            "Epoch [10938/20000], Training Loss: 0.0278\n",
            "Epoch [10939/20000], Training Loss: 0.0261\n",
            "Epoch [10940/20000], Training Loss: 0.0260\n",
            "Epoch [10941/20000], Training Loss: 0.0286\n",
            "Epoch [10942/20000], Training Loss: 0.0264\n",
            "Epoch [10943/20000], Training Loss: 0.0256\n",
            "Epoch [10944/20000], Training Loss: 0.0280\n",
            "Epoch [10945/20000], Training Loss: 0.0268\n",
            "Epoch [10946/20000], Training Loss: 0.0274\n",
            "Epoch [10947/20000], Training Loss: 0.0271\n",
            "Epoch [10948/20000], Training Loss: 0.0242\n",
            "Epoch [10949/20000], Training Loss: 0.0276\n",
            "Epoch [10950/20000], Training Loss: 0.0256\n",
            "Epoch [10951/20000], Training Loss: 0.0278\n",
            "Epoch [10952/20000], Training Loss: 0.0248\n",
            "Epoch [10953/20000], Training Loss: 0.0272\n",
            "Epoch [10954/20000], Training Loss: 0.0263\n",
            "Epoch [10955/20000], Training Loss: 0.0281\n",
            "Epoch [10956/20000], Training Loss: 0.0247\n",
            "Epoch [10957/20000], Training Loss: 0.0247\n",
            "Epoch [10958/20000], Training Loss: 0.0255\n",
            "Epoch [10959/20000], Training Loss: 0.0265\n",
            "Epoch [10960/20000], Training Loss: 0.0261\n",
            "Epoch [10961/20000], Training Loss: 0.0283\n",
            "Epoch [10962/20000], Training Loss: 0.0247\n",
            "Epoch [10963/20000], Training Loss: 0.0268\n",
            "Epoch [10964/20000], Training Loss: 0.0261\n",
            "Epoch [10965/20000], Training Loss: 0.0269\n",
            "Epoch [10966/20000], Training Loss: 0.0256\n",
            "Epoch [10967/20000], Training Loss: 0.0277\n",
            "Epoch [10968/20000], Training Loss: 0.0256\n",
            "Epoch [10969/20000], Training Loss: 0.0257\n",
            "Epoch [10970/20000], Training Loss: 0.0259\n",
            "Epoch [10971/20000], Training Loss: 0.0269\n",
            "Epoch [10972/20000], Training Loss: 0.0271\n",
            "Epoch [10973/20000], Training Loss: 0.0265\n",
            "Epoch [10974/20000], Training Loss: 0.0253\n",
            "Epoch [10975/20000], Training Loss: 0.0248\n",
            "Epoch [10976/20000], Training Loss: 0.0274\n",
            "Epoch [10977/20000], Training Loss: 0.0275\n",
            "Epoch [10978/20000], Training Loss: 0.0273\n",
            "Epoch [10979/20000], Training Loss: 0.0263\n",
            "Epoch [10980/20000], Training Loss: 0.0247\n",
            "Epoch [10981/20000], Training Loss: 0.0271\n",
            "Epoch [10982/20000], Training Loss: 0.0261\n",
            "Epoch [10983/20000], Training Loss: 0.0268\n",
            "Epoch [10984/20000], Training Loss: 0.0263\n",
            "Epoch [10985/20000], Training Loss: 0.0256\n",
            "Epoch [10986/20000], Training Loss: 0.0263\n",
            "Epoch [10987/20000], Training Loss: 0.0266\n",
            "Epoch [10988/20000], Training Loss: 0.0278\n",
            "Epoch [10989/20000], Training Loss: 0.0259\n",
            "Epoch [10990/20000], Training Loss: 0.0280\n",
            "Epoch [10991/20000], Training Loss: 0.0267\n",
            "Epoch [10992/20000], Training Loss: 0.0245\n",
            "Epoch [10993/20000], Training Loss: 0.0274\n",
            "Epoch [10994/20000], Training Loss: 0.0264\n",
            "Epoch [10995/20000], Training Loss: 0.0257\n",
            "Epoch [10996/20000], Training Loss: 0.0260\n",
            "Epoch [10997/20000], Training Loss: 0.0254\n",
            "Epoch [10998/20000], Training Loss: 0.0261\n",
            "Epoch [10999/20000], Training Loss: 0.0243\n",
            "Epoch [11000/20000], Training Loss: 0.0264\n",
            "Epoch [11001/20000], Training Loss: 0.0261\n",
            "Epoch [11002/20000], Training Loss: 0.0269\n",
            "Epoch [11003/20000], Training Loss: 0.0278\n",
            "Epoch [11004/20000], Training Loss: 0.0265\n",
            "Epoch [11005/20000], Training Loss: 0.0268\n",
            "Epoch [11006/20000], Training Loss: 0.0288\n",
            "Epoch [11007/20000], Training Loss: 0.0273\n",
            "Epoch [11008/20000], Training Loss: 0.0245\n",
            "Epoch [11009/20000], Training Loss: 0.0248\n",
            "Epoch [11010/20000], Training Loss: 0.0274\n",
            "Epoch [11011/20000], Training Loss: 0.0267\n",
            "Epoch [11012/20000], Training Loss: 0.0262\n",
            "Epoch [11013/20000], Training Loss: 0.0277\n",
            "Epoch [11014/20000], Training Loss: 0.0262\n",
            "Epoch [11015/20000], Training Loss: 0.0275\n",
            "Epoch [11016/20000], Training Loss: 0.0248\n",
            "Epoch [11017/20000], Training Loss: 0.0269\n",
            "Epoch [11018/20000], Training Loss: 0.0244\n",
            "Epoch [11019/20000], Training Loss: 0.0282\n",
            "Epoch [11020/20000], Training Loss: 0.0241\n",
            "Epoch [11021/20000], Training Loss: 0.0285\n",
            "Epoch [11022/20000], Training Loss: 0.0242\n",
            "Epoch [11023/20000], Training Loss: 0.0265\n",
            "Epoch [11024/20000], Training Loss: 0.0285\n",
            "Epoch [11025/20000], Training Loss: 0.0269\n",
            "Epoch [11026/20000], Training Loss: 0.0260\n",
            "Epoch [11027/20000], Training Loss: 0.0269\n",
            "Epoch [11028/20000], Training Loss: 0.0276\n",
            "Epoch [11029/20000], Training Loss: 0.0257\n",
            "Epoch [11030/20000], Training Loss: 0.0247\n",
            "Epoch [11031/20000], Training Loss: 0.0252\n",
            "Epoch [11032/20000], Training Loss: 0.0269\n",
            "Epoch [11033/20000], Training Loss: 0.0256\n",
            "Epoch [11034/20000], Training Loss: 0.0256\n",
            "Epoch [11035/20000], Training Loss: 0.0270\n",
            "Epoch [11036/20000], Training Loss: 0.0257\n",
            "Epoch [11037/20000], Training Loss: 0.0287\n",
            "Epoch [11038/20000], Training Loss: 0.0258\n",
            "Epoch [11039/20000], Training Loss: 0.0288\n",
            "Epoch [11040/20000], Training Loss: 0.0255\n",
            "Epoch [11041/20000], Training Loss: 0.0262\n",
            "Epoch [11042/20000], Training Loss: 0.0281\n",
            "Epoch [11043/20000], Training Loss: 0.0242\n",
            "Epoch [11044/20000], Training Loss: 0.0289\n",
            "Epoch [11045/20000], Training Loss: 0.0257\n",
            "Epoch [11046/20000], Training Loss: 0.0271\n",
            "Epoch [11047/20000], Training Loss: 0.0259\n",
            "Epoch [11048/20000], Training Loss: 0.0266\n",
            "Epoch [11049/20000], Training Loss: 0.0258\n",
            "Epoch [11050/20000], Training Loss: 0.0246\n",
            "Epoch [11051/20000], Training Loss: 0.0254\n",
            "Epoch [11052/20000], Training Loss: 0.0268\n",
            "Epoch [11053/20000], Training Loss: 0.0263\n",
            "Epoch [11054/20000], Training Loss: 0.0277\n",
            "Epoch [11055/20000], Training Loss: 0.0256\n",
            "Epoch [11056/20000], Training Loss: 0.0261\n",
            "Epoch [11057/20000], Training Loss: 0.0261\n",
            "Epoch [11058/20000], Training Loss: 0.0248\n",
            "Epoch [11059/20000], Training Loss: 0.0265\n",
            "Epoch [11060/20000], Training Loss: 0.0253\n",
            "Epoch [11061/20000], Training Loss: 0.0282\n",
            "Epoch [11062/20000], Training Loss: 0.0264\n",
            "Epoch [11063/20000], Training Loss: 0.0262\n",
            "Epoch [11064/20000], Training Loss: 0.0262\n",
            "Epoch [11065/20000], Training Loss: 0.0252\n",
            "Epoch [11066/20000], Training Loss: 0.0262\n",
            "Epoch [11067/20000], Training Loss: 0.0291\n",
            "Epoch [11068/20000], Training Loss: 0.0275\n",
            "Epoch [11069/20000], Training Loss: 0.0288\n",
            "Epoch [11070/20000], Training Loss: 0.0257\n",
            "Epoch [11071/20000], Training Loss: 0.0272\n",
            "Epoch [11072/20000], Training Loss: 0.0293\n",
            "Epoch [11073/20000], Training Loss: 0.0274\n",
            "Epoch [11074/20000], Training Loss: 0.0260\n",
            "Epoch [11075/20000], Training Loss: 0.0264\n",
            "Epoch [11076/20000], Training Loss: 0.0278\n",
            "Epoch [11077/20000], Training Loss: 0.0262\n",
            "Epoch [11078/20000], Training Loss: 0.0248\n",
            "Epoch [11079/20000], Training Loss: 0.0282\n",
            "Epoch [11080/20000], Training Loss: 0.0285\n",
            "Epoch [11081/20000], Training Loss: 0.0265\n",
            "Epoch [11082/20000], Training Loss: 0.0261\n",
            "Epoch [11083/20000], Training Loss: 0.0251\n",
            "Epoch [11084/20000], Training Loss: 0.0262\n",
            "Epoch [11085/20000], Training Loss: 0.0288\n",
            "Epoch [11086/20000], Training Loss: 0.0264\n",
            "Epoch [11087/20000], Training Loss: 0.0262\n",
            "Epoch [11088/20000], Training Loss: 0.0265\n",
            "Epoch [11089/20000], Training Loss: 0.0276\n",
            "Epoch [11090/20000], Training Loss: 0.0254\n",
            "Epoch [11091/20000], Training Loss: 0.0268\n",
            "Epoch [11092/20000], Training Loss: 0.0245\n",
            "Epoch [11093/20000], Training Loss: 0.0269\n",
            "Epoch [11094/20000], Training Loss: 0.0283\n",
            "Epoch [11095/20000], Training Loss: 0.0261\n",
            "Epoch [11096/20000], Training Loss: 0.0249\n",
            "Epoch [11097/20000], Training Loss: 0.0265\n",
            "Epoch [11098/20000], Training Loss: 0.0262\n",
            "Epoch [11099/20000], Training Loss: 0.0268\n",
            "Epoch [11100/20000], Training Loss: 0.0273\n",
            "Epoch [11101/20000], Training Loss: 0.0271\n",
            "Epoch [11102/20000], Training Loss: 0.0275\n",
            "Epoch [11103/20000], Training Loss: 0.0265\n",
            "Epoch [11104/20000], Training Loss: 0.0288\n",
            "Epoch [11105/20000], Training Loss: 0.0254\n",
            "Epoch [11106/20000], Training Loss: 0.0265\n",
            "Epoch [11107/20000], Training Loss: 0.0260\n",
            "Epoch [11108/20000], Training Loss: 0.0248\n",
            "Epoch [11109/20000], Training Loss: 0.0258\n",
            "Epoch [11110/20000], Training Loss: 0.0248\n",
            "Epoch [11111/20000], Training Loss: 0.0248\n",
            "Epoch [11112/20000], Training Loss: 0.0278\n",
            "Epoch [11113/20000], Training Loss: 0.0251\n",
            "Epoch [11114/20000], Training Loss: 0.0259\n",
            "Epoch [11115/20000], Training Loss: 0.0246\n",
            "Epoch [11116/20000], Training Loss: 0.0269\n",
            "Epoch [11117/20000], Training Loss: 0.0254\n",
            "Epoch [11118/20000], Training Loss: 0.0262\n",
            "Epoch [11119/20000], Training Loss: 0.0247\n",
            "Epoch [11120/20000], Training Loss: 0.0281\n",
            "Epoch [11121/20000], Training Loss: 0.0255\n",
            "Epoch [11122/20000], Training Loss: 0.0278\n",
            "Epoch [11123/20000], Training Loss: 0.0268\n",
            "Epoch [11124/20000], Training Loss: 0.0261\n",
            "Epoch [11125/20000], Training Loss: 0.0265\n",
            "Epoch [11126/20000], Training Loss: 0.0241\n",
            "Epoch [11127/20000], Training Loss: 0.0273\n",
            "Epoch [11128/20000], Training Loss: 0.0278\n",
            "Epoch [11129/20000], Training Loss: 0.0239\n",
            "Epoch [11130/20000], Training Loss: 0.0282\n",
            "Epoch [11131/20000], Training Loss: 0.0266\n",
            "Epoch [11132/20000], Training Loss: 0.0278\n",
            "Epoch [11133/20000], Training Loss: 0.0280\n",
            "Epoch [11134/20000], Training Loss: 0.0269\n",
            "Epoch [11135/20000], Training Loss: 0.0262\n",
            "Epoch [11136/20000], Training Loss: 0.0278\n",
            "Epoch [11137/20000], Training Loss: 0.0266\n",
            "Epoch [11138/20000], Training Loss: 0.0275\n",
            "Epoch [11139/20000], Training Loss: 0.0264\n",
            "Epoch [11140/20000], Training Loss: 0.0262\n",
            "Epoch [11141/20000], Training Loss: 0.0244\n",
            "Epoch [11142/20000], Training Loss: 0.0266\n",
            "Epoch [11143/20000], Training Loss: 0.0267\n",
            "Epoch [11144/20000], Training Loss: 0.0261\n",
            "Epoch [11145/20000], Training Loss: 0.0249\n",
            "Epoch [11146/20000], Training Loss: 0.0288\n",
            "Epoch [11147/20000], Training Loss: 0.0262\n",
            "Epoch [11148/20000], Training Loss: 0.0289\n",
            "Epoch [11149/20000], Training Loss: 0.0260\n",
            "Epoch [11150/20000], Training Loss: 0.0282\n",
            "Epoch [11151/20000], Training Loss: 0.0241\n",
            "Epoch [11152/20000], Training Loss: 0.0262\n",
            "Epoch [11153/20000], Training Loss: 0.0275\n",
            "Epoch [11154/20000], Training Loss: 0.0258\n",
            "Epoch [11155/20000], Training Loss: 0.0260\n",
            "Epoch [11156/20000], Training Loss: 0.0294\n",
            "Epoch [11157/20000], Training Loss: 0.0262\n",
            "Epoch [11158/20000], Training Loss: 0.0273\n",
            "Epoch [11159/20000], Training Loss: 0.0263\n",
            "Epoch [11160/20000], Training Loss: 0.0268\n",
            "Epoch [11161/20000], Training Loss: 0.0276\n",
            "Epoch [11162/20000], Training Loss: 0.0248\n",
            "Epoch [11163/20000], Training Loss: 0.0279\n",
            "Epoch [11164/20000], Training Loss: 0.0251\n",
            "Epoch [11165/20000], Training Loss: 0.0268\n",
            "Epoch [11166/20000], Training Loss: 0.0272\n",
            "Epoch [11167/20000], Training Loss: 0.0262\n",
            "Epoch [11168/20000], Training Loss: 0.0263\n",
            "Epoch [11169/20000], Training Loss: 0.0281\n",
            "Epoch [11170/20000], Training Loss: 0.0260\n",
            "Epoch [11171/20000], Training Loss: 0.0258\n",
            "Epoch [11172/20000], Training Loss: 0.0263\n",
            "Epoch [11173/20000], Training Loss: 0.0282\n",
            "Epoch [11174/20000], Training Loss: 0.0266\n",
            "Epoch [11175/20000], Training Loss: 0.0247\n",
            "Epoch [11176/20000], Training Loss: 0.0252\n",
            "Epoch [11177/20000], Training Loss: 0.0254\n",
            "Epoch [11178/20000], Training Loss: 0.0273\n",
            "Epoch [11179/20000], Training Loss: 0.0246\n",
            "Epoch [11180/20000], Training Loss: 0.0272\n",
            "Epoch [11181/20000], Training Loss: 0.0266\n",
            "Epoch [11182/20000], Training Loss: 0.0266\n",
            "Epoch [11183/20000], Training Loss: 0.0252\n",
            "Epoch [11184/20000], Training Loss: 0.0263\n",
            "Epoch [11185/20000], Training Loss: 0.0244\n",
            "Epoch [11186/20000], Training Loss: 0.0271\n",
            "Epoch [11187/20000], Training Loss: 0.0266\n",
            "Epoch [11188/20000], Training Loss: 0.0278\n",
            "Epoch [11189/20000], Training Loss: 0.0261\n",
            "Epoch [11190/20000], Training Loss: 0.0245\n",
            "Epoch [11191/20000], Training Loss: 0.0269\n",
            "Epoch [11192/20000], Training Loss: 0.0282\n",
            "Epoch [11193/20000], Training Loss: 0.0272\n",
            "Epoch [11194/20000], Training Loss: 0.0266\n",
            "Epoch [11195/20000], Training Loss: 0.0267\n",
            "Epoch [11196/20000], Training Loss: 0.0266\n",
            "Epoch [11197/20000], Training Loss: 0.0279\n",
            "Epoch [11198/20000], Training Loss: 0.0258\n",
            "Epoch [11199/20000], Training Loss: 0.0260\n",
            "Epoch [11200/20000], Training Loss: 0.0263\n",
            "Epoch [11201/20000], Training Loss: 0.0288\n",
            "Epoch [11202/20000], Training Loss: 0.0251\n",
            "Epoch [11203/20000], Training Loss: 0.0265\n",
            "Epoch [11204/20000], Training Loss: 0.0245\n",
            "Epoch [11205/20000], Training Loss: 0.0273\n",
            "Epoch [11206/20000], Training Loss: 0.0269\n",
            "Epoch [11207/20000], Training Loss: 0.0249\n",
            "Epoch [11208/20000], Training Loss: 0.0252\n",
            "Epoch [11209/20000], Training Loss: 0.0250\n",
            "Epoch [11210/20000], Training Loss: 0.0279\n",
            "Epoch [11211/20000], Training Loss: 0.0254\n",
            "Epoch [11212/20000], Training Loss: 0.0274\n",
            "Epoch [11213/20000], Training Loss: 0.0280\n",
            "Epoch [11214/20000], Training Loss: 0.0260\n",
            "Epoch [11215/20000], Training Loss: 0.0284\n",
            "Epoch [11216/20000], Training Loss: 0.0261\n",
            "Epoch [11217/20000], Training Loss: 0.0260\n",
            "Epoch [11218/20000], Training Loss: 0.0265\n",
            "Epoch [11219/20000], Training Loss: 0.0261\n",
            "Epoch [11220/20000], Training Loss: 0.0244\n",
            "Epoch [11221/20000], Training Loss: 0.0259\n",
            "Epoch [11222/20000], Training Loss: 0.0241\n",
            "Epoch [11223/20000], Training Loss: 0.0261\n",
            "Epoch [11224/20000], Training Loss: 0.0277\n",
            "Epoch [11225/20000], Training Loss: 0.0250\n",
            "Epoch [11226/20000], Training Loss: 0.0271\n",
            "Epoch [11227/20000], Training Loss: 0.0259\n",
            "Epoch [11228/20000], Training Loss: 0.0271\n",
            "Epoch [11229/20000], Training Loss: 0.0271\n",
            "Epoch [11230/20000], Training Loss: 0.0260\n",
            "Epoch [11231/20000], Training Loss: 0.0268\n",
            "Epoch [11232/20000], Training Loss: 0.0262\n",
            "Epoch [11233/20000], Training Loss: 0.0270\n",
            "Epoch [11234/20000], Training Loss: 0.0252\n",
            "Epoch [11235/20000], Training Loss: 0.0270\n",
            "Epoch [11236/20000], Training Loss: 0.0280\n",
            "Epoch [11237/20000], Training Loss: 0.0270\n",
            "Epoch [11238/20000], Training Loss: 0.0271\n",
            "Epoch [11239/20000], Training Loss: 0.0264\n",
            "Epoch [11240/20000], Training Loss: 0.0264\n",
            "Epoch [11241/20000], Training Loss: 0.0279\n",
            "Epoch [11242/20000], Training Loss: 0.0275\n",
            "Epoch [11243/20000], Training Loss: 0.0273\n",
            "Epoch [11244/20000], Training Loss: 0.0259\n",
            "Epoch [11245/20000], Training Loss: 0.0264\n",
            "Epoch [11246/20000], Training Loss: 0.0249\n",
            "Epoch [11247/20000], Training Loss: 0.0275\n",
            "Epoch [11248/20000], Training Loss: 0.0277\n",
            "Epoch [11249/20000], Training Loss: 0.0260\n",
            "Epoch [11250/20000], Training Loss: 0.0284\n",
            "Epoch [11251/20000], Training Loss: 0.0271\n",
            "Epoch [11252/20000], Training Loss: 0.0257\n",
            "Epoch [11253/20000], Training Loss: 0.0268\n",
            "Epoch [11254/20000], Training Loss: 0.0259\n",
            "Epoch [11255/20000], Training Loss: 0.0262\n",
            "Epoch [11256/20000], Training Loss: 0.0270\n",
            "Epoch [11257/20000], Training Loss: 0.0275\n",
            "Epoch [11258/20000], Training Loss: 0.0286\n",
            "Epoch [11259/20000], Training Loss: 0.0275\n",
            "Epoch [11260/20000], Training Loss: 0.0272\n",
            "Epoch [11261/20000], Training Loss: 0.0266\n",
            "Epoch [11262/20000], Training Loss: 0.0262\n",
            "Epoch [11263/20000], Training Loss: 0.0282\n",
            "Epoch [11264/20000], Training Loss: 0.0282\n",
            "Epoch [11265/20000], Training Loss: 0.0284\n",
            "Epoch [11266/20000], Training Loss: 0.0250\n",
            "Epoch [11267/20000], Training Loss: 0.0272\n",
            "Epoch [11268/20000], Training Loss: 0.0273\n",
            "Epoch [11269/20000], Training Loss: 0.0277\n",
            "Epoch [11270/20000], Training Loss: 0.0255\n",
            "Epoch [11271/20000], Training Loss: 0.0269\n",
            "Epoch [11272/20000], Training Loss: 0.0257\n",
            "Epoch [11273/20000], Training Loss: 0.0271\n",
            "Epoch [11274/20000], Training Loss: 0.0264\n",
            "Epoch [11275/20000], Training Loss: 0.0260\n",
            "Epoch [11276/20000], Training Loss: 0.0267\n",
            "Epoch [11277/20000], Training Loss: 0.0262\n",
            "Epoch [11278/20000], Training Loss: 0.0296\n",
            "Epoch [11279/20000], Training Loss: 0.0266\n",
            "Epoch [11280/20000], Training Loss: 0.0272\n",
            "Epoch [11281/20000], Training Loss: 0.0261\n",
            "Epoch [11282/20000], Training Loss: 0.0254\n",
            "Epoch [11283/20000], Training Loss: 0.0257\n",
            "Epoch [11284/20000], Training Loss: 0.0254\n",
            "Epoch [11285/20000], Training Loss: 0.0289\n",
            "Epoch [11286/20000], Training Loss: 0.0260\n",
            "Epoch [11287/20000], Training Loss: 0.0280\n",
            "Epoch [11288/20000], Training Loss: 0.0259\n",
            "Epoch [11289/20000], Training Loss: 0.0248\n",
            "Epoch [11290/20000], Training Loss: 0.0269\n",
            "Epoch [11291/20000], Training Loss: 0.0279\n",
            "Epoch [11292/20000], Training Loss: 0.0275\n",
            "Epoch [11293/20000], Training Loss: 0.0261\n",
            "Epoch [11294/20000], Training Loss: 0.0251\n",
            "Epoch [11295/20000], Training Loss: 0.0274\n",
            "Epoch [11296/20000], Training Loss: 0.0259\n",
            "Epoch [11297/20000], Training Loss: 0.0252\n",
            "Epoch [11298/20000], Training Loss: 0.0255\n",
            "Epoch [11299/20000], Training Loss: 0.0254\n",
            "Epoch [11300/20000], Training Loss: 0.0257\n",
            "Epoch [11301/20000], Training Loss: 0.0271\n",
            "Epoch [11302/20000], Training Loss: 0.0260\n",
            "Epoch [11303/20000], Training Loss: 0.0255\n",
            "Epoch [11304/20000], Training Loss: 0.0271\n",
            "Epoch [11305/20000], Training Loss: 0.0268\n",
            "Epoch [11306/20000], Training Loss: 0.0255\n",
            "Epoch [11307/20000], Training Loss: 0.0261\n",
            "Epoch [11308/20000], Training Loss: 0.0249\n",
            "Epoch [11309/20000], Training Loss: 0.0259\n",
            "Epoch [11310/20000], Training Loss: 0.0279\n",
            "Epoch [11311/20000], Training Loss: 0.0261\n",
            "Epoch [11312/20000], Training Loss: 0.0286\n",
            "Epoch [11313/20000], Training Loss: 0.0272\n",
            "Epoch [11314/20000], Training Loss: 0.0256\n",
            "Epoch [11315/20000], Training Loss: 0.0255\n",
            "Epoch [11316/20000], Training Loss: 0.0263\n",
            "Epoch [11317/20000], Training Loss: 0.0255\n",
            "Epoch [11318/20000], Training Loss: 0.0271\n",
            "Epoch [11319/20000], Training Loss: 0.0273\n",
            "Epoch [11320/20000], Training Loss: 0.0268\n",
            "Epoch [11321/20000], Training Loss: 0.0264\n",
            "Epoch [11322/20000], Training Loss: 0.0260\n",
            "Epoch [11323/20000], Training Loss: 0.0265\n",
            "Epoch [11324/20000], Training Loss: 0.0263\n",
            "Epoch [11325/20000], Training Loss: 0.0286\n",
            "Epoch [11326/20000], Training Loss: 0.0266\n",
            "Epoch [11327/20000], Training Loss: 0.0274\n",
            "Epoch [11328/20000], Training Loss: 0.0251\n",
            "Epoch [11329/20000], Training Loss: 0.0264\n",
            "Epoch [11330/20000], Training Loss: 0.0248\n",
            "Epoch [11331/20000], Training Loss: 0.0257\n",
            "Epoch [11332/20000], Training Loss: 0.0269\n",
            "Epoch [11333/20000], Training Loss: 0.0263\n",
            "Epoch [11334/20000], Training Loss: 0.0284\n",
            "Epoch [11335/20000], Training Loss: 0.0290\n",
            "Epoch [11336/20000], Training Loss: 0.0260\n",
            "Epoch [11337/20000], Training Loss: 0.0258\n",
            "Epoch [11338/20000], Training Loss: 0.0273\n",
            "Epoch [11339/20000], Training Loss: 0.0258\n",
            "Epoch [11340/20000], Training Loss: 0.0250\n",
            "Epoch [11341/20000], Training Loss: 0.0254\n",
            "Epoch [11342/20000], Training Loss: 0.0257\n",
            "Epoch [11343/20000], Training Loss: 0.0252\n",
            "Epoch [11344/20000], Training Loss: 0.0261\n",
            "Epoch [11345/20000], Training Loss: 0.0276\n",
            "Epoch [11346/20000], Training Loss: 0.0279\n",
            "Epoch [11347/20000], Training Loss: 0.0266\n",
            "Epoch [11348/20000], Training Loss: 0.0255\n",
            "Epoch [11349/20000], Training Loss: 0.0253\n",
            "Epoch [11350/20000], Training Loss: 0.0294\n",
            "Epoch [11351/20000], Training Loss: 0.0260\n",
            "Epoch [11352/20000], Training Loss: 0.0255\n",
            "Epoch [11353/20000], Training Loss: 0.0261\n",
            "Epoch [11354/20000], Training Loss: 0.0260\n",
            "Epoch [11355/20000], Training Loss: 0.0245\n",
            "Epoch [11356/20000], Training Loss: 0.0273\n",
            "Epoch [11357/20000], Training Loss: 0.0250\n",
            "Epoch [11358/20000], Training Loss: 0.0260\n",
            "Epoch [11359/20000], Training Loss: 0.0249\n",
            "Epoch [11360/20000], Training Loss: 0.0264\n",
            "Epoch [11361/20000], Training Loss: 0.0270\n",
            "Epoch [11362/20000], Training Loss: 0.0287\n",
            "Epoch [11363/20000], Training Loss: 0.0245\n",
            "Epoch [11364/20000], Training Loss: 0.0256\n",
            "Epoch [11365/20000], Training Loss: 0.0274\n",
            "Epoch [11366/20000], Training Loss: 0.0249\n",
            "Epoch [11367/20000], Training Loss: 0.0250\n",
            "Epoch [11368/20000], Training Loss: 0.0265\n",
            "Epoch [11369/20000], Training Loss: 0.0254\n",
            "Epoch [11370/20000], Training Loss: 0.0258\n",
            "Epoch [11371/20000], Training Loss: 0.0266\n",
            "Epoch [11372/20000], Training Loss: 0.0265\n",
            "Epoch [11373/20000], Training Loss: 0.0261\n",
            "Epoch [11374/20000], Training Loss: 0.0282\n",
            "Epoch [11375/20000], Training Loss: 0.0265\n",
            "Epoch [11376/20000], Training Loss: 0.0265\n",
            "Epoch [11377/20000], Training Loss: 0.0269\n",
            "Epoch [11378/20000], Training Loss: 0.0281\n",
            "Epoch [11379/20000], Training Loss: 0.0246\n",
            "Epoch [11380/20000], Training Loss: 0.0273\n",
            "Epoch [11381/20000], Training Loss: 0.0253\n",
            "Epoch [11382/20000], Training Loss: 0.0249\n",
            "Epoch [11383/20000], Training Loss: 0.0283\n",
            "Epoch [11384/20000], Training Loss: 0.0257\n",
            "Epoch [11385/20000], Training Loss: 0.0243\n",
            "Epoch [11386/20000], Training Loss: 0.0254\n",
            "Epoch [11387/20000], Training Loss: 0.0278\n",
            "Epoch [11388/20000], Training Loss: 0.0262\n",
            "Epoch [11389/20000], Training Loss: 0.0265\n",
            "Epoch [11390/20000], Training Loss: 0.0258\n",
            "Epoch [11391/20000], Training Loss: 0.0263\n",
            "Epoch [11392/20000], Training Loss: 0.0261\n",
            "Epoch [11393/20000], Training Loss: 0.0245\n",
            "Epoch [11394/20000], Training Loss: 0.0282\n",
            "Epoch [11395/20000], Training Loss: 0.0261\n",
            "Epoch [11396/20000], Training Loss: 0.0272\n",
            "Epoch [11397/20000], Training Loss: 0.0265\n",
            "Epoch [11398/20000], Training Loss: 0.0259\n",
            "Epoch [11399/20000], Training Loss: 0.0264\n",
            "Epoch [11400/20000], Training Loss: 0.0251\n",
            "Epoch [11401/20000], Training Loss: 0.0249\n",
            "Epoch [11402/20000], Training Loss: 0.0271\n",
            "Epoch [11403/20000], Training Loss: 0.0254\n",
            "Epoch [11404/20000], Training Loss: 0.0267\n",
            "Epoch [11405/20000], Training Loss: 0.0271\n",
            "Epoch [11406/20000], Training Loss: 0.0277\n",
            "Epoch [11407/20000], Training Loss: 0.0268\n",
            "Epoch [11408/20000], Training Loss: 0.0277\n",
            "Epoch [11409/20000], Training Loss: 0.0260\n",
            "Epoch [11410/20000], Training Loss: 0.0271\n",
            "Epoch [11411/20000], Training Loss: 0.0264\n",
            "Epoch [11412/20000], Training Loss: 0.0267\n",
            "Epoch [11413/20000], Training Loss: 0.0265\n",
            "Epoch [11414/20000], Training Loss: 0.0271\n",
            "Epoch [11415/20000], Training Loss: 0.0280\n",
            "Epoch [11416/20000], Training Loss: 0.0274\n",
            "Epoch [11417/20000], Training Loss: 0.0288\n",
            "Epoch [11418/20000], Training Loss: 0.0240\n",
            "Epoch [11419/20000], Training Loss: 0.0251\n",
            "Epoch [11420/20000], Training Loss: 0.0280\n",
            "Epoch [11421/20000], Training Loss: 0.0249\n",
            "Epoch [11422/20000], Training Loss: 0.0265\n",
            "Epoch [11423/20000], Training Loss: 0.0242\n",
            "Epoch [11424/20000], Training Loss: 0.0275\n",
            "Epoch [11425/20000], Training Loss: 0.0268\n",
            "Epoch [11426/20000], Training Loss: 0.0287\n",
            "Epoch [11427/20000], Training Loss: 0.0264\n",
            "Epoch [11428/20000], Training Loss: 0.0265\n",
            "Epoch [11429/20000], Training Loss: 0.0287\n",
            "Epoch [11430/20000], Training Loss: 0.0250\n",
            "Epoch [11431/20000], Training Loss: 0.0271\n",
            "Epoch [11432/20000], Training Loss: 0.0243\n",
            "Epoch [11433/20000], Training Loss: 0.0279\n",
            "Epoch [11434/20000], Training Loss: 0.0287\n",
            "Epoch [11435/20000], Training Loss: 0.0285\n",
            "Epoch [11436/20000], Training Loss: 0.0253\n",
            "Epoch [11437/20000], Training Loss: 0.0270\n",
            "Epoch [11438/20000], Training Loss: 0.0252\n",
            "Epoch [11439/20000], Training Loss: 0.0248\n",
            "Epoch [11440/20000], Training Loss: 0.0268\n",
            "Epoch [11441/20000], Training Loss: 0.0257\n",
            "Epoch [11442/20000], Training Loss: 0.0272\n",
            "Epoch [11443/20000], Training Loss: 0.0286\n",
            "Epoch [11444/20000], Training Loss: 0.0263\n",
            "Epoch [11445/20000], Training Loss: 0.0274\n",
            "Epoch [11446/20000], Training Loss: 0.0264\n",
            "Epoch [11447/20000], Training Loss: 0.0261\n",
            "Epoch [11448/20000], Training Loss: 0.0251\n",
            "Epoch [11449/20000], Training Loss: 0.0266\n",
            "Epoch [11450/20000], Training Loss: 0.0257\n",
            "Epoch [11451/20000], Training Loss: 0.0278\n",
            "Epoch [11452/20000], Training Loss: 0.0269\n",
            "Epoch [11453/20000], Training Loss: 0.0260\n",
            "Epoch [11454/20000], Training Loss: 0.0242\n",
            "Epoch [11455/20000], Training Loss: 0.0251\n",
            "Epoch [11456/20000], Training Loss: 0.0267\n",
            "Epoch [11457/20000], Training Loss: 0.0262\n",
            "Epoch [11458/20000], Training Loss: 0.0269\n",
            "Epoch [11459/20000], Training Loss: 0.0290\n",
            "Epoch [11460/20000], Training Loss: 0.0274\n",
            "Epoch [11461/20000], Training Loss: 0.0236\n",
            "Epoch [11462/20000], Training Loss: 0.0269\n",
            "Epoch [11463/20000], Training Loss: 0.0267\n",
            "Epoch [11464/20000], Training Loss: 0.0256\n",
            "Epoch [11465/20000], Training Loss: 0.0282\n",
            "Epoch [11466/20000], Training Loss: 0.0290\n",
            "Epoch [11467/20000], Training Loss: 0.0263\n",
            "Epoch [11468/20000], Training Loss: 0.0282\n",
            "Epoch [11469/20000], Training Loss: 0.0270\n",
            "Epoch [11470/20000], Training Loss: 0.0270\n",
            "Epoch [11471/20000], Training Loss: 0.0255\n",
            "Epoch [11472/20000], Training Loss: 0.0270\n",
            "Epoch [11473/20000], Training Loss: 0.0262\n",
            "Epoch [11474/20000], Training Loss: 0.0254\n",
            "Epoch [11475/20000], Training Loss: 0.0267\n",
            "Epoch [11476/20000], Training Loss: 0.0280\n",
            "Epoch [11477/20000], Training Loss: 0.0275\n",
            "Epoch [11478/20000], Training Loss: 0.0255\n",
            "Epoch [11479/20000], Training Loss: 0.0261\n",
            "Epoch [11480/20000], Training Loss: 0.0263\n",
            "Epoch [11481/20000], Training Loss: 0.0249\n",
            "Epoch [11482/20000], Training Loss: 0.0252\n",
            "Epoch [11483/20000], Training Loss: 0.0278\n",
            "Epoch [11484/20000], Training Loss: 0.0257\n",
            "Epoch [11485/20000], Training Loss: 0.0257\n",
            "Epoch [11486/20000], Training Loss: 0.0264\n",
            "Epoch [11487/20000], Training Loss: 0.0285\n",
            "Epoch [11488/20000], Training Loss: 0.0282\n",
            "Epoch [11489/20000], Training Loss: 0.0277\n",
            "Epoch [11490/20000], Training Loss: 0.0281\n",
            "Epoch [11491/20000], Training Loss: 0.0259\n",
            "Epoch [11492/20000], Training Loss: 0.0261\n",
            "Epoch [11493/20000], Training Loss: 0.0294\n",
            "Epoch [11494/20000], Training Loss: 0.0244\n",
            "Epoch [11495/20000], Training Loss: 0.0253\n",
            "Epoch [11496/20000], Training Loss: 0.0258\n",
            "Epoch [11497/20000], Training Loss: 0.0250\n",
            "Epoch [11498/20000], Training Loss: 0.0284\n",
            "Epoch [11499/20000], Training Loss: 0.0262\n",
            "Epoch [11500/20000], Training Loss: 0.0252\n",
            "Epoch [11501/20000], Training Loss: 0.0263\n",
            "Epoch [11502/20000], Training Loss: 0.0252\n",
            "Epoch [11503/20000], Training Loss: 0.0266\n",
            "Epoch [11504/20000], Training Loss: 0.0263\n",
            "Epoch [11505/20000], Training Loss: 0.0279\n",
            "Epoch [11506/20000], Training Loss: 0.0272\n",
            "Epoch [11507/20000], Training Loss: 0.0266\n",
            "Epoch [11508/20000], Training Loss: 0.0262\n",
            "Epoch [11509/20000], Training Loss: 0.0258\n",
            "Epoch [11510/20000], Training Loss: 0.0268\n",
            "Epoch [11511/20000], Training Loss: 0.0259\n",
            "Epoch [11512/20000], Training Loss: 0.0266\n",
            "Epoch [11513/20000], Training Loss: 0.0266\n",
            "Epoch [11514/20000], Training Loss: 0.0283\n",
            "Epoch [11515/20000], Training Loss: 0.0262\n",
            "Epoch [11516/20000], Training Loss: 0.0284\n",
            "Epoch [11517/20000], Training Loss: 0.0260\n",
            "Epoch [11518/20000], Training Loss: 0.0263\n",
            "Epoch [11519/20000], Training Loss: 0.0257\n",
            "Epoch [11520/20000], Training Loss: 0.0281\n",
            "Epoch [11521/20000], Training Loss: 0.0254\n",
            "Epoch [11522/20000], Training Loss: 0.0261\n",
            "Epoch [11523/20000], Training Loss: 0.0274\n",
            "Epoch [11524/20000], Training Loss: 0.0264\n",
            "Epoch [11525/20000], Training Loss: 0.0261\n",
            "Epoch [11526/20000], Training Loss: 0.0252\n",
            "Epoch [11527/20000], Training Loss: 0.0261\n",
            "Epoch [11528/20000], Training Loss: 0.0250\n",
            "Epoch [11529/20000], Training Loss: 0.0253\n",
            "Epoch [11530/20000], Training Loss: 0.0269\n",
            "Epoch [11531/20000], Training Loss: 0.0278\n",
            "Epoch [11532/20000], Training Loss: 0.0241\n",
            "Epoch [11533/20000], Training Loss: 0.0248\n",
            "Epoch [11534/20000], Training Loss: 0.0274\n",
            "Epoch [11535/20000], Training Loss: 0.0287\n",
            "Epoch [11536/20000], Training Loss: 0.0255\n",
            "Epoch [11537/20000], Training Loss: 0.0258\n",
            "Epoch [11538/20000], Training Loss: 0.0284\n",
            "Epoch [11539/20000], Training Loss: 0.0282\n",
            "Epoch [11540/20000], Training Loss: 0.0282\n",
            "Epoch [11541/20000], Training Loss: 0.0289\n",
            "Epoch [11542/20000], Training Loss: 0.0272\n",
            "Epoch [11543/20000], Training Loss: 0.0254\n",
            "Epoch [11544/20000], Training Loss: 0.0253\n",
            "Epoch [11545/20000], Training Loss: 0.0271\n",
            "Epoch [11546/20000], Training Loss: 0.0247\n",
            "Epoch [11547/20000], Training Loss: 0.0279\n",
            "Epoch [11548/20000], Training Loss: 0.0275\n",
            "Epoch [11549/20000], Training Loss: 0.0257\n",
            "Epoch [11550/20000], Training Loss: 0.0257\n",
            "Epoch [11551/20000], Training Loss: 0.0267\n",
            "Epoch [11552/20000], Training Loss: 0.0266\n",
            "Epoch [11553/20000], Training Loss: 0.0277\n",
            "Epoch [11554/20000], Training Loss: 0.0287\n",
            "Epoch [11555/20000], Training Loss: 0.0243\n",
            "Epoch [11556/20000], Training Loss: 0.0275\n",
            "Epoch [11557/20000], Training Loss: 0.0257\n",
            "Epoch [11558/20000], Training Loss: 0.0251\n",
            "Epoch [11559/20000], Training Loss: 0.0255\n",
            "Epoch [11560/20000], Training Loss: 0.0265\n",
            "Epoch [11561/20000], Training Loss: 0.0288\n",
            "Epoch [11562/20000], Training Loss: 0.0276\n",
            "Epoch [11563/20000], Training Loss: 0.0257\n",
            "Epoch [11564/20000], Training Loss: 0.0257\n",
            "Epoch [11565/20000], Training Loss: 0.0287\n",
            "Epoch [11566/20000], Training Loss: 0.0281\n",
            "Epoch [11567/20000], Training Loss: 0.0269\n",
            "Epoch [11568/20000], Training Loss: 0.0256\n",
            "Epoch [11569/20000], Training Loss: 0.0272\n",
            "Epoch [11570/20000], Training Loss: 0.0264\n",
            "Epoch [11571/20000], Training Loss: 0.0268\n",
            "Epoch [11572/20000], Training Loss: 0.0262\n",
            "Epoch [11573/20000], Training Loss: 0.0245\n",
            "Epoch [11574/20000], Training Loss: 0.0270\n",
            "Epoch [11575/20000], Training Loss: 0.0277\n",
            "Epoch [11576/20000], Training Loss: 0.0267\n",
            "Epoch [11577/20000], Training Loss: 0.0246\n",
            "Epoch [11578/20000], Training Loss: 0.0250\n",
            "Epoch [11579/20000], Training Loss: 0.0270\n",
            "Epoch [11580/20000], Training Loss: 0.0263\n",
            "Epoch [11581/20000], Training Loss: 0.0267\n",
            "Epoch [11582/20000], Training Loss: 0.0248\n",
            "Epoch [11583/20000], Training Loss: 0.0275\n",
            "Epoch [11584/20000], Training Loss: 0.0273\n",
            "Epoch [11585/20000], Training Loss: 0.0268\n",
            "Epoch [11586/20000], Training Loss: 0.0293\n",
            "Epoch [11587/20000], Training Loss: 0.0278\n",
            "Epoch [11588/20000], Training Loss: 0.0259\n",
            "Epoch [11589/20000], Training Loss: 0.0270\n",
            "Epoch [11590/20000], Training Loss: 0.0268\n",
            "Epoch [11591/20000], Training Loss: 0.0256\n",
            "Epoch [11592/20000], Training Loss: 0.0270\n",
            "Epoch [11593/20000], Training Loss: 0.0270\n",
            "Epoch [11594/20000], Training Loss: 0.0260\n",
            "Epoch [11595/20000], Training Loss: 0.0261\n",
            "Epoch [11596/20000], Training Loss: 0.0260\n",
            "Epoch [11597/20000], Training Loss: 0.0263\n",
            "Epoch [11598/20000], Training Loss: 0.0276\n",
            "Epoch [11599/20000], Training Loss: 0.0267\n",
            "Epoch [11600/20000], Training Loss: 0.0271\n",
            "Epoch [11601/20000], Training Loss: 0.0254\n",
            "Epoch [11602/20000], Training Loss: 0.0256\n",
            "Epoch [11603/20000], Training Loss: 0.0283\n",
            "Epoch [11604/20000], Training Loss: 0.0259\n",
            "Epoch [11605/20000], Training Loss: 0.0270\n",
            "Epoch [11606/20000], Training Loss: 0.0276\n",
            "Epoch [11607/20000], Training Loss: 0.0255\n",
            "Epoch [11608/20000], Training Loss: 0.0248\n",
            "Epoch [11609/20000], Training Loss: 0.0259\n",
            "Epoch [11610/20000], Training Loss: 0.0263\n",
            "Epoch [11611/20000], Training Loss: 0.0268\n",
            "Epoch [11612/20000], Training Loss: 0.0263\n",
            "Epoch [11613/20000], Training Loss: 0.0280\n",
            "Epoch [11614/20000], Training Loss: 0.0258\n",
            "Epoch [11615/20000], Training Loss: 0.0275\n",
            "Epoch [11616/20000], Training Loss: 0.0257\n",
            "Epoch [11617/20000], Training Loss: 0.0253\n",
            "Epoch [11618/20000], Training Loss: 0.0261\n",
            "Epoch [11619/20000], Training Loss: 0.0290\n",
            "Epoch [11620/20000], Training Loss: 0.0246\n",
            "Epoch [11621/20000], Training Loss: 0.0245\n",
            "Epoch [11622/20000], Training Loss: 0.0262\n",
            "Epoch [11623/20000], Training Loss: 0.0258\n",
            "Epoch [11624/20000], Training Loss: 0.0291\n",
            "Epoch [11625/20000], Training Loss: 0.0271\n",
            "Epoch [11626/20000], Training Loss: 0.0256\n",
            "Epoch [11627/20000], Training Loss: 0.0260\n",
            "Epoch [11628/20000], Training Loss: 0.0258\n",
            "Epoch [11629/20000], Training Loss: 0.0267\n",
            "Epoch [11630/20000], Training Loss: 0.0272\n",
            "Epoch [11631/20000], Training Loss: 0.0242\n",
            "Epoch [11632/20000], Training Loss: 0.0268\n",
            "Epoch [11633/20000], Training Loss: 0.0253\n",
            "Epoch [11634/20000], Training Loss: 0.0238\n",
            "Epoch [11635/20000], Training Loss: 0.0262\n",
            "Epoch [11636/20000], Training Loss: 0.0271\n",
            "Epoch [11637/20000], Training Loss: 0.0281\n",
            "Epoch [11638/20000], Training Loss: 0.0281\n",
            "Epoch [11639/20000], Training Loss: 0.0257\n",
            "Epoch [11640/20000], Training Loss: 0.0252\n",
            "Epoch [11641/20000], Training Loss: 0.0265\n",
            "Epoch [11642/20000], Training Loss: 0.0273\n",
            "Epoch [11643/20000], Training Loss: 0.0260\n",
            "Epoch [11644/20000], Training Loss: 0.0270\n",
            "Epoch [11645/20000], Training Loss: 0.0272\n",
            "Epoch [11646/20000], Training Loss: 0.0271\n",
            "Epoch [11647/20000], Training Loss: 0.0251\n",
            "Epoch [11648/20000], Training Loss: 0.0267\n",
            "Epoch [11649/20000], Training Loss: 0.0268\n",
            "Epoch [11650/20000], Training Loss: 0.0279\n",
            "Epoch [11651/20000], Training Loss: 0.0269\n",
            "Epoch [11652/20000], Training Loss: 0.0287\n",
            "Epoch [11653/20000], Training Loss: 0.0271\n",
            "Epoch [11654/20000], Training Loss: 0.0285\n",
            "Epoch [11655/20000], Training Loss: 0.0269\n",
            "Epoch [11656/20000], Training Loss: 0.0284\n",
            "Epoch [11657/20000], Training Loss: 0.0271\n",
            "Epoch [11658/20000], Training Loss: 0.0260\n",
            "Epoch [11659/20000], Training Loss: 0.0274\n",
            "Epoch [11660/20000], Training Loss: 0.0248\n",
            "Epoch [11661/20000], Training Loss: 0.0252\n",
            "Epoch [11662/20000], Training Loss: 0.0259\n",
            "Epoch [11663/20000], Training Loss: 0.0275\n",
            "Epoch [11664/20000], Training Loss: 0.0271\n",
            "Epoch [11665/20000], Training Loss: 0.0245\n",
            "Epoch [11666/20000], Training Loss: 0.0266\n",
            "Epoch [11667/20000], Training Loss: 0.0255\n",
            "Epoch [11668/20000], Training Loss: 0.0262\n",
            "Epoch [11669/20000], Training Loss: 0.0279\n",
            "Epoch [11670/20000], Training Loss: 0.0256\n",
            "Epoch [11671/20000], Training Loss: 0.0265\n",
            "Epoch [11672/20000], Training Loss: 0.0253\n",
            "Epoch [11673/20000], Training Loss: 0.0271\n",
            "Epoch [11674/20000], Training Loss: 0.0257\n",
            "Epoch [11675/20000], Training Loss: 0.0264\n",
            "Epoch [11676/20000], Training Loss: 0.0271\n",
            "Epoch [11677/20000], Training Loss: 0.0279\n",
            "Epoch [11678/20000], Training Loss: 0.0252\n",
            "Epoch [11679/20000], Training Loss: 0.0263\n",
            "Epoch [11680/20000], Training Loss: 0.0262\n",
            "Epoch [11681/20000], Training Loss: 0.0264\n",
            "Epoch [11682/20000], Training Loss: 0.0268\n",
            "Epoch [11683/20000], Training Loss: 0.0266\n",
            "Epoch [11684/20000], Training Loss: 0.0270\n",
            "Epoch [11685/20000], Training Loss: 0.0280\n",
            "Epoch [11686/20000], Training Loss: 0.0275\n",
            "Epoch [11687/20000], Training Loss: 0.0259\n",
            "Epoch [11688/20000], Training Loss: 0.0271\n",
            "Epoch [11689/20000], Training Loss: 0.0257\n",
            "Epoch [11690/20000], Training Loss: 0.0274\n",
            "Epoch [11691/20000], Training Loss: 0.0238\n",
            "Epoch [11692/20000], Training Loss: 0.0281\n",
            "Epoch [11693/20000], Training Loss: 0.0293\n",
            "Epoch [11694/20000], Training Loss: 0.0280\n",
            "Epoch [11695/20000], Training Loss: 0.0248\n",
            "Epoch [11696/20000], Training Loss: 0.0269\n",
            "Epoch [11697/20000], Training Loss: 0.0259\n",
            "Epoch [11698/20000], Training Loss: 0.0246\n",
            "Epoch [11699/20000], Training Loss: 0.0251\n",
            "Epoch [11700/20000], Training Loss: 0.0268\n",
            "Epoch [11701/20000], Training Loss: 0.0272\n",
            "Epoch [11702/20000], Training Loss: 0.0271\n",
            "Epoch [11703/20000], Training Loss: 0.0259\n",
            "Epoch [11704/20000], Training Loss: 0.0242\n",
            "Epoch [11705/20000], Training Loss: 0.0270\n",
            "Epoch [11706/20000], Training Loss: 0.0260\n",
            "Epoch [11707/20000], Training Loss: 0.0258\n",
            "Epoch [11708/20000], Training Loss: 0.0280\n",
            "Epoch [11709/20000], Training Loss: 0.0281\n",
            "Epoch [11710/20000], Training Loss: 0.0273\n",
            "Epoch [11711/20000], Training Loss: 0.0254\n",
            "Epoch [11712/20000], Training Loss: 0.0269\n",
            "Epoch [11713/20000], Training Loss: 0.0249\n",
            "Epoch [11714/20000], Training Loss: 0.0286\n",
            "Epoch [11715/20000], Training Loss: 0.0273\n",
            "Epoch [11716/20000], Training Loss: 0.0263\n",
            "Epoch [11717/20000], Training Loss: 0.0257\n",
            "Epoch [11718/20000], Training Loss: 0.0262\n",
            "Epoch [11719/20000], Training Loss: 0.0278\n",
            "Epoch [11720/20000], Training Loss: 0.0290\n",
            "Epoch [11721/20000], Training Loss: 0.0275\n",
            "Epoch [11722/20000], Training Loss: 0.0272\n",
            "Epoch [11723/20000], Training Loss: 0.0271\n",
            "Epoch [11724/20000], Training Loss: 0.0259\n",
            "Epoch [11725/20000], Training Loss: 0.0263\n",
            "Epoch [11726/20000], Training Loss: 0.0275\n",
            "Epoch [11727/20000], Training Loss: 0.0240\n",
            "Epoch [11728/20000], Training Loss: 0.0248\n",
            "Epoch [11729/20000], Training Loss: 0.0267\n",
            "Epoch [11730/20000], Training Loss: 0.0275\n",
            "Epoch [11731/20000], Training Loss: 0.0276\n",
            "Epoch [11732/20000], Training Loss: 0.0258\n",
            "Epoch [11733/20000], Training Loss: 0.0283\n",
            "Epoch [11734/20000], Training Loss: 0.0248\n",
            "Epoch [11735/20000], Training Loss: 0.0262\n",
            "Epoch [11736/20000], Training Loss: 0.0271\n",
            "Epoch [11737/20000], Training Loss: 0.0273\n",
            "Epoch [11738/20000], Training Loss: 0.0265\n",
            "Epoch [11739/20000], Training Loss: 0.0269\n",
            "Epoch [11740/20000], Training Loss: 0.0254\n",
            "Epoch [11741/20000], Training Loss: 0.0246\n",
            "Epoch [11742/20000], Training Loss: 0.0255\n",
            "Epoch [11743/20000], Training Loss: 0.0254\n",
            "Epoch [11744/20000], Training Loss: 0.0254\n",
            "Epoch [11745/20000], Training Loss: 0.0253\n",
            "Epoch [11746/20000], Training Loss: 0.0248\n",
            "Epoch [11747/20000], Training Loss: 0.0254\n",
            "Epoch [11748/20000], Training Loss: 0.0265\n",
            "Epoch [11749/20000], Training Loss: 0.0257\n",
            "Epoch [11750/20000], Training Loss: 0.0270\n",
            "Epoch [11751/20000], Training Loss: 0.0251\n",
            "Epoch [11752/20000], Training Loss: 0.0270\n",
            "Epoch [11753/20000], Training Loss: 0.0262\n",
            "Epoch [11754/20000], Training Loss: 0.0272\n",
            "Epoch [11755/20000], Training Loss: 0.0267\n",
            "Epoch [11756/20000], Training Loss: 0.0255\n",
            "Epoch [11757/20000], Training Loss: 0.0287\n",
            "Epoch [11758/20000], Training Loss: 0.0263\n",
            "Epoch [11759/20000], Training Loss: 0.0259\n",
            "Epoch [11760/20000], Training Loss: 0.0262\n",
            "Epoch [11761/20000], Training Loss: 0.0247\n",
            "Epoch [11762/20000], Training Loss: 0.0257\n",
            "Epoch [11763/20000], Training Loss: 0.0275\n",
            "Epoch [11764/20000], Training Loss: 0.0268\n",
            "Epoch [11765/20000], Training Loss: 0.0261\n",
            "Epoch [11766/20000], Training Loss: 0.0276\n",
            "Epoch [11767/20000], Training Loss: 0.0267\n",
            "Epoch [11768/20000], Training Loss: 0.0266\n",
            "Epoch [11769/20000], Training Loss: 0.0283\n",
            "Epoch [11770/20000], Training Loss: 0.0258\n",
            "Epoch [11771/20000], Training Loss: 0.0261\n",
            "Epoch [11772/20000], Training Loss: 0.0270\n",
            "Epoch [11773/20000], Training Loss: 0.0261\n",
            "Epoch [11774/20000], Training Loss: 0.0275\n",
            "Epoch [11775/20000], Training Loss: 0.0271\n",
            "Epoch [11776/20000], Training Loss: 0.0261\n",
            "Epoch [11777/20000], Training Loss: 0.0274\n",
            "Epoch [11778/20000], Training Loss: 0.0267\n",
            "Epoch [11779/20000], Training Loss: 0.0268\n",
            "Epoch [11780/20000], Training Loss: 0.0259\n",
            "Epoch [11781/20000], Training Loss: 0.0259\n",
            "Epoch [11782/20000], Training Loss: 0.0260\n",
            "Epoch [11783/20000], Training Loss: 0.0245\n",
            "Epoch [11784/20000], Training Loss: 0.0267\n",
            "Epoch [11785/20000], Training Loss: 0.0256\n",
            "Epoch [11786/20000], Training Loss: 0.0259\n",
            "Epoch [11787/20000], Training Loss: 0.0251\n",
            "Epoch [11788/20000], Training Loss: 0.0262\n",
            "Epoch [11789/20000], Training Loss: 0.0266\n",
            "Epoch [11790/20000], Training Loss: 0.0256\n",
            "Epoch [11791/20000], Training Loss: 0.0256\n",
            "Epoch [11792/20000], Training Loss: 0.0267\n",
            "Epoch [11793/20000], Training Loss: 0.0265\n",
            "Epoch [11794/20000], Training Loss: 0.0264\n",
            "Epoch [11795/20000], Training Loss: 0.0280\n",
            "Epoch [11796/20000], Training Loss: 0.0265\n",
            "Epoch [11797/20000], Training Loss: 0.0269\n",
            "Epoch [11798/20000], Training Loss: 0.0243\n",
            "Epoch [11799/20000], Training Loss: 0.0258\n",
            "Epoch [11800/20000], Training Loss: 0.0266\n",
            "Epoch [11801/20000], Training Loss: 0.0263\n",
            "Epoch [11802/20000], Training Loss: 0.0253\n",
            "Epoch [11803/20000], Training Loss: 0.0277\n",
            "Epoch [11804/20000], Training Loss: 0.0281\n",
            "Epoch [11805/20000], Training Loss: 0.0270\n",
            "Epoch [11806/20000], Training Loss: 0.0259\n",
            "Epoch [11807/20000], Training Loss: 0.0254\n",
            "Epoch [11808/20000], Training Loss: 0.0261\n",
            "Epoch [11809/20000], Training Loss: 0.0281\n",
            "Epoch [11810/20000], Training Loss: 0.0256\n",
            "Epoch [11811/20000], Training Loss: 0.0252\n",
            "Epoch [11812/20000], Training Loss: 0.0272\n",
            "Epoch [11813/20000], Training Loss: 0.0266\n",
            "Epoch [11814/20000], Training Loss: 0.0251\n",
            "Epoch [11815/20000], Training Loss: 0.0283\n",
            "Epoch [11816/20000], Training Loss: 0.0263\n",
            "Epoch [11817/20000], Training Loss: 0.0275\n",
            "Epoch [11818/20000], Training Loss: 0.0257\n",
            "Epoch [11819/20000], Training Loss: 0.0292\n",
            "Epoch [11820/20000], Training Loss: 0.0273\n",
            "Epoch [11821/20000], Training Loss: 0.0266\n",
            "Epoch [11822/20000], Training Loss: 0.0257\n",
            "Epoch [11823/20000], Training Loss: 0.0268\n",
            "Epoch [11824/20000], Training Loss: 0.0277\n",
            "Epoch [11825/20000], Training Loss: 0.0285\n",
            "Epoch [11826/20000], Training Loss: 0.0271\n",
            "Epoch [11827/20000], Training Loss: 0.0249\n",
            "Epoch [11828/20000], Training Loss: 0.0244\n",
            "Epoch [11829/20000], Training Loss: 0.0276\n",
            "Epoch [11830/20000], Training Loss: 0.0264\n",
            "Epoch [11831/20000], Training Loss: 0.0280\n",
            "Epoch [11832/20000], Training Loss: 0.0251\n",
            "Epoch [11833/20000], Training Loss: 0.0292\n",
            "Epoch [11834/20000], Training Loss: 0.0251\n",
            "Epoch [11835/20000], Training Loss: 0.0247\n",
            "Epoch [11836/20000], Training Loss: 0.0253\n",
            "Epoch [11837/20000], Training Loss: 0.0280\n",
            "Epoch [11838/20000], Training Loss: 0.0274\n",
            "Epoch [11839/20000], Training Loss: 0.0269\n",
            "Epoch [11840/20000], Training Loss: 0.0261\n",
            "Epoch [11841/20000], Training Loss: 0.0273\n",
            "Epoch [11842/20000], Training Loss: 0.0243\n",
            "Epoch [11843/20000], Training Loss: 0.0267\n",
            "Epoch [11844/20000], Training Loss: 0.0275\n",
            "Epoch [11845/20000], Training Loss: 0.0251\n",
            "Epoch [11846/20000], Training Loss: 0.0284\n",
            "Epoch [11847/20000], Training Loss: 0.0260\n",
            "Epoch [11848/20000], Training Loss: 0.0270\n",
            "Epoch [11849/20000], Training Loss: 0.0254\n",
            "Epoch [11850/20000], Training Loss: 0.0254\n",
            "Epoch [11851/20000], Training Loss: 0.0254\n",
            "Epoch [11852/20000], Training Loss: 0.0253\n",
            "Epoch [11853/20000], Training Loss: 0.0258\n",
            "Epoch [11854/20000], Training Loss: 0.0264\n",
            "Epoch [11855/20000], Training Loss: 0.0250\n",
            "Epoch [11856/20000], Training Loss: 0.0256\n",
            "Epoch [11857/20000], Training Loss: 0.0272\n",
            "Epoch [11858/20000], Training Loss: 0.0257\n",
            "Epoch [11859/20000], Training Loss: 0.0261\n",
            "Epoch [11860/20000], Training Loss: 0.0250\n",
            "Epoch [11861/20000], Training Loss: 0.0265\n",
            "Epoch [11862/20000], Training Loss: 0.0253\n",
            "Epoch [11863/20000], Training Loss: 0.0248\n",
            "Epoch [11864/20000], Training Loss: 0.0282\n",
            "Epoch [11865/20000], Training Loss: 0.0273\n",
            "Epoch [11866/20000], Training Loss: 0.0258\n",
            "Epoch [11867/20000], Training Loss: 0.0246\n",
            "Epoch [11868/20000], Training Loss: 0.0259\n",
            "Epoch [11869/20000], Training Loss: 0.0256\n",
            "Epoch [11870/20000], Training Loss: 0.0239\n",
            "Epoch [11871/20000], Training Loss: 0.0260\n",
            "Epoch [11872/20000], Training Loss: 0.0285\n",
            "Epoch [11873/20000], Training Loss: 0.0267\n",
            "Epoch [11874/20000], Training Loss: 0.0285\n",
            "Epoch [11875/20000], Training Loss: 0.0267\n",
            "Epoch [11876/20000], Training Loss: 0.0266\n",
            "Epoch [11877/20000], Training Loss: 0.0269\n",
            "Epoch [11878/20000], Training Loss: 0.0272\n",
            "Epoch [11879/20000], Training Loss: 0.0272\n",
            "Epoch [11880/20000], Training Loss: 0.0257\n",
            "Epoch [11881/20000], Training Loss: 0.0257\n",
            "Epoch [11882/20000], Training Loss: 0.0251\n",
            "Epoch [11883/20000], Training Loss: 0.0258\n",
            "Epoch [11884/20000], Training Loss: 0.0264\n",
            "Epoch [11885/20000], Training Loss: 0.0255\n",
            "Epoch [11886/20000], Training Loss: 0.0254\n",
            "Epoch [11887/20000], Training Loss: 0.0269\n",
            "Epoch [11888/20000], Training Loss: 0.0241\n",
            "Epoch [11889/20000], Training Loss: 0.0274\n",
            "Epoch [11890/20000], Training Loss: 0.0269\n",
            "Epoch [11891/20000], Training Loss: 0.0274\n",
            "Epoch [11892/20000], Training Loss: 0.0276\n",
            "Epoch [11893/20000], Training Loss: 0.0288\n",
            "Epoch [11894/20000], Training Loss: 0.0275\n",
            "Epoch [11895/20000], Training Loss: 0.0246\n",
            "Epoch [11896/20000], Training Loss: 0.0258\n",
            "Epoch [11897/20000], Training Loss: 0.0271\n",
            "Epoch [11898/20000], Training Loss: 0.0268\n",
            "Epoch [11899/20000], Training Loss: 0.0264\n",
            "Epoch [11900/20000], Training Loss: 0.0242\n",
            "Epoch [11901/20000], Training Loss: 0.0255\n",
            "Epoch [11902/20000], Training Loss: 0.0286\n",
            "Epoch [11903/20000], Training Loss: 0.0280\n",
            "Epoch [11904/20000], Training Loss: 0.0280\n",
            "Epoch [11905/20000], Training Loss: 0.0243\n",
            "Epoch [11906/20000], Training Loss: 0.0285\n",
            "Epoch [11907/20000], Training Loss: 0.0249\n",
            "Epoch [11908/20000], Training Loss: 0.0269\n",
            "Epoch [11909/20000], Training Loss: 0.0245\n",
            "Epoch [11910/20000], Training Loss: 0.0261\n",
            "Epoch [11911/20000], Training Loss: 0.0258\n",
            "Epoch [11912/20000], Training Loss: 0.0278\n",
            "Epoch [11913/20000], Training Loss: 0.0288\n",
            "Epoch [11914/20000], Training Loss: 0.0279\n",
            "Epoch [11915/20000], Training Loss: 0.0244\n",
            "Epoch [11916/20000], Training Loss: 0.0271\n",
            "Epoch [11917/20000], Training Loss: 0.0258\n",
            "Epoch [11918/20000], Training Loss: 0.0278\n",
            "Epoch [11919/20000], Training Loss: 0.0245\n",
            "Epoch [11920/20000], Training Loss: 0.0278\n",
            "Epoch [11921/20000], Training Loss: 0.0294\n",
            "Epoch [11922/20000], Training Loss: 0.0271\n",
            "Epoch [11923/20000], Training Loss: 0.0247\n",
            "Epoch [11924/20000], Training Loss: 0.0284\n",
            "Epoch [11925/20000], Training Loss: 0.0245\n",
            "Epoch [11926/20000], Training Loss: 0.0288\n",
            "Epoch [11927/20000], Training Loss: 0.0260\n",
            "Epoch [11928/20000], Training Loss: 0.0273\n",
            "Epoch [11929/20000], Training Loss: 0.0264\n",
            "Epoch [11930/20000], Training Loss: 0.0264\n",
            "Epoch [11931/20000], Training Loss: 0.0248\n",
            "Epoch [11932/20000], Training Loss: 0.0282\n",
            "Epoch [11933/20000], Training Loss: 0.0287\n",
            "Epoch [11934/20000], Training Loss: 0.0270\n",
            "Epoch [11935/20000], Training Loss: 0.0258\n",
            "Epoch [11936/20000], Training Loss: 0.0289\n",
            "Epoch [11937/20000], Training Loss: 0.0278\n",
            "Epoch [11938/20000], Training Loss: 0.0281\n",
            "Epoch [11939/20000], Training Loss: 0.0270\n",
            "Epoch [11940/20000], Training Loss: 0.0267\n",
            "Epoch [11941/20000], Training Loss: 0.0270\n",
            "Epoch [11942/20000], Training Loss: 0.0248\n",
            "Epoch [11943/20000], Training Loss: 0.0278\n",
            "Epoch [11944/20000], Training Loss: 0.0257\n",
            "Epoch [11945/20000], Training Loss: 0.0268\n",
            "Epoch [11946/20000], Training Loss: 0.0256\n",
            "Epoch [11947/20000], Training Loss: 0.0256\n",
            "Epoch [11948/20000], Training Loss: 0.0258\n",
            "Epoch [11949/20000], Training Loss: 0.0272\n",
            "Epoch [11950/20000], Training Loss: 0.0289\n",
            "Epoch [11951/20000], Training Loss: 0.0275\n",
            "Epoch [11952/20000], Training Loss: 0.0268\n",
            "Epoch [11953/20000], Training Loss: 0.0239\n",
            "Epoch [11954/20000], Training Loss: 0.0278\n",
            "Epoch [11955/20000], Training Loss: 0.0277\n",
            "Epoch [11956/20000], Training Loss: 0.0258\n",
            "Epoch [11957/20000], Training Loss: 0.0266\n",
            "Epoch [11958/20000], Training Loss: 0.0265\n",
            "Epoch [11959/20000], Training Loss: 0.0267\n",
            "Epoch [11960/20000], Training Loss: 0.0276\n",
            "Epoch [11961/20000], Training Loss: 0.0258\n",
            "Epoch [11962/20000], Training Loss: 0.0246\n",
            "Epoch [11963/20000], Training Loss: 0.0256\n",
            "Epoch [11964/20000], Training Loss: 0.0276\n",
            "Epoch [11965/20000], Training Loss: 0.0269\n",
            "Epoch [11966/20000], Training Loss: 0.0247\n",
            "Epoch [11967/20000], Training Loss: 0.0264\n",
            "Epoch [11968/20000], Training Loss: 0.0253\n",
            "Epoch [11969/20000], Training Loss: 0.0271\n",
            "Epoch [11970/20000], Training Loss: 0.0247\n",
            "Epoch [11971/20000], Training Loss: 0.0263\n",
            "Epoch [11972/20000], Training Loss: 0.0266\n",
            "Epoch [11973/20000], Training Loss: 0.0271\n",
            "Epoch [11974/20000], Training Loss: 0.0250\n",
            "Epoch [11975/20000], Training Loss: 0.0272\n",
            "Epoch [11976/20000], Training Loss: 0.0281\n",
            "Epoch [11977/20000], Training Loss: 0.0277\n",
            "Epoch [11978/20000], Training Loss: 0.0253\n",
            "Epoch [11979/20000], Training Loss: 0.0273\n",
            "Epoch [11980/20000], Training Loss: 0.0270\n",
            "Epoch [11981/20000], Training Loss: 0.0271\n",
            "Epoch [11982/20000], Training Loss: 0.0261\n",
            "Epoch [11983/20000], Training Loss: 0.0260\n",
            "Epoch [11984/20000], Training Loss: 0.0256\n",
            "Epoch [11985/20000], Training Loss: 0.0250\n",
            "Epoch [11986/20000], Training Loss: 0.0281\n",
            "Epoch [11987/20000], Training Loss: 0.0255\n",
            "Epoch [11988/20000], Training Loss: 0.0279\n",
            "Epoch [11989/20000], Training Loss: 0.0294\n",
            "Epoch [11990/20000], Training Loss: 0.0283\n",
            "Epoch [11991/20000], Training Loss: 0.0269\n",
            "Epoch [11992/20000], Training Loss: 0.0265\n",
            "Epoch [11993/20000], Training Loss: 0.0246\n",
            "Epoch [11994/20000], Training Loss: 0.0245\n",
            "Epoch [11995/20000], Training Loss: 0.0250\n",
            "Epoch [11996/20000], Training Loss: 0.0245\n",
            "Epoch [11997/20000], Training Loss: 0.0281\n",
            "Epoch [11998/20000], Training Loss: 0.0274\n",
            "Epoch [11999/20000], Training Loss: 0.0256\n",
            "Epoch [12000/20000], Training Loss: 0.0261\n",
            "Epoch [12001/20000], Training Loss: 0.0267\n",
            "Epoch [12002/20000], Training Loss: 0.0272\n",
            "Epoch [12003/20000], Training Loss: 0.0267\n",
            "Epoch [12004/20000], Training Loss: 0.0258\n",
            "Epoch [12005/20000], Training Loss: 0.0243\n",
            "Epoch [12006/20000], Training Loss: 0.0255\n",
            "Epoch [12007/20000], Training Loss: 0.0235\n",
            "Epoch [12008/20000], Training Loss: 0.0248\n",
            "Epoch [12009/20000], Training Loss: 0.0274\n",
            "Epoch [12010/20000], Training Loss: 0.0257\n",
            "Epoch [12011/20000], Training Loss: 0.0250\n",
            "Epoch [12012/20000], Training Loss: 0.0266\n",
            "Epoch [12013/20000], Training Loss: 0.0269\n",
            "Epoch [12014/20000], Training Loss: 0.0260\n",
            "Epoch [12015/20000], Training Loss: 0.0260\n",
            "Epoch [12016/20000], Training Loss: 0.0263\n",
            "Epoch [12017/20000], Training Loss: 0.0267\n",
            "Epoch [12018/20000], Training Loss: 0.0270\n",
            "Epoch [12019/20000], Training Loss: 0.0250\n",
            "Epoch [12020/20000], Training Loss: 0.0268\n",
            "Epoch [12021/20000], Training Loss: 0.0282\n",
            "Epoch [12022/20000], Training Loss: 0.0264\n",
            "Epoch [12023/20000], Training Loss: 0.0274\n",
            "Epoch [12024/20000], Training Loss: 0.0261\n",
            "Epoch [12025/20000], Training Loss: 0.0260\n",
            "Epoch [12026/20000], Training Loss: 0.0267\n",
            "Epoch [12027/20000], Training Loss: 0.0254\n",
            "Epoch [12028/20000], Training Loss: 0.0269\n",
            "Epoch [12029/20000], Training Loss: 0.0284\n",
            "Epoch [12030/20000], Training Loss: 0.0252\n",
            "Epoch [12031/20000], Training Loss: 0.0248\n",
            "Epoch [12032/20000], Training Loss: 0.0258\n",
            "Epoch [12033/20000], Training Loss: 0.0249\n",
            "Epoch [12034/20000], Training Loss: 0.0283\n",
            "Epoch [12035/20000], Training Loss: 0.0263\n",
            "Epoch [12036/20000], Training Loss: 0.0255\n",
            "Epoch [12037/20000], Training Loss: 0.0269\n",
            "Epoch [12038/20000], Training Loss: 0.0261\n",
            "Epoch [12039/20000], Training Loss: 0.0258\n",
            "Epoch [12040/20000], Training Loss: 0.0245\n",
            "Epoch [12041/20000], Training Loss: 0.0266\n",
            "Epoch [12042/20000], Training Loss: 0.0261\n",
            "Epoch [12043/20000], Training Loss: 0.0260\n",
            "Epoch [12044/20000], Training Loss: 0.0260\n",
            "Epoch [12045/20000], Training Loss: 0.0245\n",
            "Epoch [12046/20000], Training Loss: 0.0263\n",
            "Epoch [12047/20000], Training Loss: 0.0263\n",
            "Epoch [12048/20000], Training Loss: 0.0272\n",
            "Epoch [12049/20000], Training Loss: 0.0250\n",
            "Epoch [12050/20000], Training Loss: 0.0265\n",
            "Epoch [12051/20000], Training Loss: 0.0286\n",
            "Epoch [12052/20000], Training Loss: 0.0265\n",
            "Epoch [12053/20000], Training Loss: 0.0279\n",
            "Epoch [12054/20000], Training Loss: 0.0248\n",
            "Epoch [12055/20000], Training Loss: 0.0247\n",
            "Epoch [12056/20000], Training Loss: 0.0275\n",
            "Epoch [12057/20000], Training Loss: 0.0264\n",
            "Epoch [12058/20000], Training Loss: 0.0286\n",
            "Epoch [12059/20000], Training Loss: 0.0246\n",
            "Epoch [12060/20000], Training Loss: 0.0279\n",
            "Epoch [12061/20000], Training Loss: 0.0255\n",
            "Epoch [12062/20000], Training Loss: 0.0247\n",
            "Epoch [12063/20000], Training Loss: 0.0264\n",
            "Epoch [12064/20000], Training Loss: 0.0265\n",
            "Epoch [12065/20000], Training Loss: 0.0267\n",
            "Epoch [12066/20000], Training Loss: 0.0282\n",
            "Epoch [12067/20000], Training Loss: 0.0274\n",
            "Epoch [12068/20000], Training Loss: 0.0247\n",
            "Epoch [12069/20000], Training Loss: 0.0268\n",
            "Epoch [12070/20000], Training Loss: 0.0247\n",
            "Epoch [12071/20000], Training Loss: 0.0263\n",
            "Epoch [12072/20000], Training Loss: 0.0264\n",
            "Epoch [12073/20000], Training Loss: 0.0253\n",
            "Epoch [12074/20000], Training Loss: 0.0249\n",
            "Epoch [12075/20000], Training Loss: 0.0257\n",
            "Epoch [12076/20000], Training Loss: 0.0251\n",
            "Epoch [12077/20000], Training Loss: 0.0258\n",
            "Epoch [12078/20000], Training Loss: 0.0251\n",
            "Epoch [12079/20000], Training Loss: 0.0267\n",
            "Epoch [12080/20000], Training Loss: 0.0278\n",
            "Epoch [12081/20000], Training Loss: 0.0269\n",
            "Epoch [12082/20000], Training Loss: 0.0284\n",
            "Epoch [12083/20000], Training Loss: 0.0263\n",
            "Epoch [12084/20000], Training Loss: 0.0273\n",
            "Epoch [12085/20000], Training Loss: 0.0259\n",
            "Epoch [12086/20000], Training Loss: 0.0277\n",
            "Epoch [12087/20000], Training Loss: 0.0263\n",
            "Epoch [12088/20000], Training Loss: 0.0277\n",
            "Epoch [12089/20000], Training Loss: 0.0258\n",
            "Epoch [12090/20000], Training Loss: 0.0281\n",
            "Epoch [12091/20000], Training Loss: 0.0275\n",
            "Epoch [12092/20000], Training Loss: 0.0271\n",
            "Epoch [12093/20000], Training Loss: 0.0267\n",
            "Epoch [12094/20000], Training Loss: 0.0257\n",
            "Epoch [12095/20000], Training Loss: 0.0261\n",
            "Epoch [12096/20000], Training Loss: 0.0271\n",
            "Epoch [12097/20000], Training Loss: 0.0290\n",
            "Epoch [12098/20000], Training Loss: 0.0255\n",
            "Epoch [12099/20000], Training Loss: 0.0252\n",
            "Epoch [12100/20000], Training Loss: 0.0280\n",
            "Epoch [12101/20000], Training Loss: 0.0253\n",
            "Epoch [12102/20000], Training Loss: 0.0254\n",
            "Epoch [12103/20000], Training Loss: 0.0273\n",
            "Epoch [12104/20000], Training Loss: 0.0269\n",
            "Epoch [12105/20000], Training Loss: 0.0271\n",
            "Epoch [12106/20000], Training Loss: 0.0279\n",
            "Epoch [12107/20000], Training Loss: 0.0268\n",
            "Epoch [12108/20000], Training Loss: 0.0282\n",
            "Epoch [12109/20000], Training Loss: 0.0272\n",
            "Epoch [12110/20000], Training Loss: 0.0245\n",
            "Epoch [12111/20000], Training Loss: 0.0256\n",
            "Epoch [12112/20000], Training Loss: 0.0245\n",
            "Epoch [12113/20000], Training Loss: 0.0265\n",
            "Epoch [12114/20000], Training Loss: 0.0271\n",
            "Epoch [12115/20000], Training Loss: 0.0259\n",
            "Epoch [12116/20000], Training Loss: 0.0260\n",
            "Epoch [12117/20000], Training Loss: 0.0262\n",
            "Epoch [12118/20000], Training Loss: 0.0264\n",
            "Epoch [12119/20000], Training Loss: 0.0274\n",
            "Epoch [12120/20000], Training Loss: 0.0266\n",
            "Epoch [12121/20000], Training Loss: 0.0256\n",
            "Epoch [12122/20000], Training Loss: 0.0286\n",
            "Epoch [12123/20000], Training Loss: 0.0252\n",
            "Epoch [12124/20000], Training Loss: 0.0289\n",
            "Epoch [12125/20000], Training Loss: 0.0258\n",
            "Epoch [12126/20000], Training Loss: 0.0275\n",
            "Epoch [12127/20000], Training Loss: 0.0258\n",
            "Epoch [12128/20000], Training Loss: 0.0278\n",
            "Epoch [12129/20000], Training Loss: 0.0253\n",
            "Epoch [12130/20000], Training Loss: 0.0263\n",
            "Epoch [12131/20000], Training Loss: 0.0254\n",
            "Epoch [12132/20000], Training Loss: 0.0254\n",
            "Epoch [12133/20000], Training Loss: 0.0255\n",
            "Epoch [12134/20000], Training Loss: 0.0245\n",
            "Epoch [12135/20000], Training Loss: 0.0252\n",
            "Epoch [12136/20000], Training Loss: 0.0266\n",
            "Epoch [12137/20000], Training Loss: 0.0260\n",
            "Epoch [12138/20000], Training Loss: 0.0252\n",
            "Epoch [12139/20000], Training Loss: 0.0250\n",
            "Epoch [12140/20000], Training Loss: 0.0263\n",
            "Epoch [12141/20000], Training Loss: 0.0260\n",
            "Epoch [12142/20000], Training Loss: 0.0276\n",
            "Epoch [12143/20000], Training Loss: 0.0265\n",
            "Epoch [12144/20000], Training Loss: 0.0253\n",
            "Epoch [12145/20000], Training Loss: 0.0258\n",
            "Epoch [12146/20000], Training Loss: 0.0257\n",
            "Epoch [12147/20000], Training Loss: 0.0276\n",
            "Epoch [12148/20000], Training Loss: 0.0284\n",
            "Epoch [12149/20000], Training Loss: 0.0277\n",
            "Epoch [12150/20000], Training Loss: 0.0266\n",
            "Epoch [12151/20000], Training Loss: 0.0260\n",
            "Epoch [12152/20000], Training Loss: 0.0285\n",
            "Epoch [12153/20000], Training Loss: 0.0271\n",
            "Epoch [12154/20000], Training Loss: 0.0269\n",
            "Epoch [12155/20000], Training Loss: 0.0261\n",
            "Epoch [12156/20000], Training Loss: 0.0270\n",
            "Epoch [12157/20000], Training Loss: 0.0260\n",
            "Epoch [12158/20000], Training Loss: 0.0288\n",
            "Epoch [12159/20000], Training Loss: 0.0259\n",
            "Epoch [12160/20000], Training Loss: 0.0272\n",
            "Epoch [12161/20000], Training Loss: 0.0245\n",
            "Epoch [12162/20000], Training Loss: 0.0250\n",
            "Epoch [12163/20000], Training Loss: 0.0273\n",
            "Epoch [12164/20000], Training Loss: 0.0258\n",
            "Epoch [12165/20000], Training Loss: 0.0262\n",
            "Epoch [12166/20000], Training Loss: 0.0269\n",
            "Epoch [12167/20000], Training Loss: 0.0261\n",
            "Epoch [12168/20000], Training Loss: 0.0259\n",
            "Epoch [12169/20000], Training Loss: 0.0254\n",
            "Epoch [12170/20000], Training Loss: 0.0262\n",
            "Epoch [12171/20000], Training Loss: 0.0276\n",
            "Epoch [12172/20000], Training Loss: 0.0252\n",
            "Epoch [12173/20000], Training Loss: 0.0280\n",
            "Epoch [12174/20000], Training Loss: 0.0274\n",
            "Epoch [12175/20000], Training Loss: 0.0265\n",
            "Epoch [12176/20000], Training Loss: 0.0280\n",
            "Epoch [12177/20000], Training Loss: 0.0270\n",
            "Epoch [12178/20000], Training Loss: 0.0265\n",
            "Epoch [12179/20000], Training Loss: 0.0259\n",
            "Epoch [12180/20000], Training Loss: 0.0262\n",
            "Epoch [12181/20000], Training Loss: 0.0255\n",
            "Epoch [12182/20000], Training Loss: 0.0278\n",
            "Epoch [12183/20000], Training Loss: 0.0280\n",
            "Epoch [12184/20000], Training Loss: 0.0255\n",
            "Epoch [12185/20000], Training Loss: 0.0276\n",
            "Epoch [12186/20000], Training Loss: 0.0264\n",
            "Epoch [12187/20000], Training Loss: 0.0275\n",
            "Epoch [12188/20000], Training Loss: 0.0253\n",
            "Epoch [12189/20000], Training Loss: 0.0261\n",
            "Epoch [12190/20000], Training Loss: 0.0271\n",
            "Epoch [12191/20000], Training Loss: 0.0275\n",
            "Epoch [12192/20000], Training Loss: 0.0251\n",
            "Epoch [12193/20000], Training Loss: 0.0250\n",
            "Epoch [12194/20000], Training Loss: 0.0271\n",
            "Epoch [12195/20000], Training Loss: 0.0241\n",
            "Epoch [12196/20000], Training Loss: 0.0239\n",
            "Epoch [12197/20000], Training Loss: 0.0257\n",
            "Epoch [12198/20000], Training Loss: 0.0265\n",
            "Epoch [12199/20000], Training Loss: 0.0240\n",
            "Epoch [12200/20000], Training Loss: 0.0254\n",
            "Epoch [12201/20000], Training Loss: 0.0265\n",
            "Epoch [12202/20000], Training Loss: 0.0271\n",
            "Epoch [12203/20000], Training Loss: 0.0259\n",
            "Epoch [12204/20000], Training Loss: 0.0286\n",
            "Epoch [12205/20000], Training Loss: 0.0249\n",
            "Epoch [12206/20000], Training Loss: 0.0277\n",
            "Epoch [12207/20000], Training Loss: 0.0266\n",
            "Epoch [12208/20000], Training Loss: 0.0258\n",
            "Epoch [12209/20000], Training Loss: 0.0275\n",
            "Epoch [12210/20000], Training Loss: 0.0261\n",
            "Epoch [12211/20000], Training Loss: 0.0253\n",
            "Epoch [12212/20000], Training Loss: 0.0260\n",
            "Epoch [12213/20000], Training Loss: 0.0254\n",
            "Epoch [12214/20000], Training Loss: 0.0268\n",
            "Epoch [12215/20000], Training Loss: 0.0241\n",
            "Epoch [12216/20000], Training Loss: 0.0258\n",
            "Epoch [12217/20000], Training Loss: 0.0250\n",
            "Epoch [12218/20000], Training Loss: 0.0290\n",
            "Epoch [12219/20000], Training Loss: 0.0269\n",
            "Epoch [12220/20000], Training Loss: 0.0248\n",
            "Epoch [12221/20000], Training Loss: 0.0268\n",
            "Epoch [12222/20000], Training Loss: 0.0248\n",
            "Epoch [12223/20000], Training Loss: 0.0255\n",
            "Epoch [12224/20000], Training Loss: 0.0283\n",
            "Epoch [12225/20000], Training Loss: 0.0284\n",
            "Epoch [12226/20000], Training Loss: 0.0260\n",
            "Epoch [12227/20000], Training Loss: 0.0272\n",
            "Epoch [12228/20000], Training Loss: 0.0244\n",
            "Epoch [12229/20000], Training Loss: 0.0251\n",
            "Epoch [12230/20000], Training Loss: 0.0265\n",
            "Epoch [12231/20000], Training Loss: 0.0279\n",
            "Epoch [12232/20000], Training Loss: 0.0262\n",
            "Epoch [12233/20000], Training Loss: 0.0283\n",
            "Epoch [12234/20000], Training Loss: 0.0285\n",
            "Epoch [12235/20000], Training Loss: 0.0283\n",
            "Epoch [12236/20000], Training Loss: 0.0258\n",
            "Epoch [12237/20000], Training Loss: 0.0251\n",
            "Epoch [12238/20000], Training Loss: 0.0254\n",
            "Epoch [12239/20000], Training Loss: 0.0265\n",
            "Epoch [12240/20000], Training Loss: 0.0253\n",
            "Epoch [12241/20000], Training Loss: 0.0263\n",
            "Epoch [12242/20000], Training Loss: 0.0269\n",
            "Epoch [12243/20000], Training Loss: 0.0276\n",
            "Epoch [12244/20000], Training Loss: 0.0280\n",
            "Epoch [12245/20000], Training Loss: 0.0260\n",
            "Epoch [12246/20000], Training Loss: 0.0239\n",
            "Epoch [12247/20000], Training Loss: 0.0246\n",
            "Epoch [12248/20000], Training Loss: 0.0248\n",
            "Epoch [12249/20000], Training Loss: 0.0290\n",
            "Epoch [12250/20000], Training Loss: 0.0255\n",
            "Epoch [12251/20000], Training Loss: 0.0267\n",
            "Epoch [12252/20000], Training Loss: 0.0268\n",
            "Epoch [12253/20000], Training Loss: 0.0280\n",
            "Epoch [12254/20000], Training Loss: 0.0261\n",
            "Epoch [12255/20000], Training Loss: 0.0261\n",
            "Epoch [12256/20000], Training Loss: 0.0273\n",
            "Epoch [12257/20000], Training Loss: 0.0271\n",
            "Epoch [12258/20000], Training Loss: 0.0263\n",
            "Epoch [12259/20000], Training Loss: 0.0268\n",
            "Epoch [12260/20000], Training Loss: 0.0287\n",
            "Epoch [12261/20000], Training Loss: 0.0262\n",
            "Epoch [12262/20000], Training Loss: 0.0269\n",
            "Epoch [12263/20000], Training Loss: 0.0279\n",
            "Epoch [12264/20000], Training Loss: 0.0274\n",
            "Epoch [12265/20000], Training Loss: 0.0263\n",
            "Epoch [12266/20000], Training Loss: 0.0252\n",
            "Epoch [12267/20000], Training Loss: 0.0254\n",
            "Epoch [12268/20000], Training Loss: 0.0261\n",
            "Epoch [12269/20000], Training Loss: 0.0280\n",
            "Epoch [12270/20000], Training Loss: 0.0271\n",
            "Epoch [12271/20000], Training Loss: 0.0270\n",
            "Epoch [12272/20000], Training Loss: 0.0265\n",
            "Epoch [12273/20000], Training Loss: 0.0287\n",
            "Epoch [12274/20000], Training Loss: 0.0258\n",
            "Epoch [12275/20000], Training Loss: 0.0287\n",
            "Epoch [12276/20000], Training Loss: 0.0261\n",
            "Epoch [12277/20000], Training Loss: 0.0254\n",
            "Epoch [12278/20000], Training Loss: 0.0251\n",
            "Epoch [12279/20000], Training Loss: 0.0288\n",
            "Epoch [12280/20000], Training Loss: 0.0272\n",
            "Epoch [12281/20000], Training Loss: 0.0258\n",
            "Epoch [12282/20000], Training Loss: 0.0244\n",
            "Epoch [12283/20000], Training Loss: 0.0259\n",
            "Epoch [12284/20000], Training Loss: 0.0273\n",
            "Epoch [12285/20000], Training Loss: 0.0247\n",
            "Epoch [12286/20000], Training Loss: 0.0283\n",
            "Epoch [12287/20000], Training Loss: 0.0260\n",
            "Epoch [12288/20000], Training Loss: 0.0269\n",
            "Epoch [12289/20000], Training Loss: 0.0285\n",
            "Epoch [12290/20000], Training Loss: 0.0281\n",
            "Epoch [12291/20000], Training Loss: 0.0257\n",
            "Epoch [12292/20000], Training Loss: 0.0285\n",
            "Epoch [12293/20000], Training Loss: 0.0270\n",
            "Epoch [12294/20000], Training Loss: 0.0248\n",
            "Epoch [12295/20000], Training Loss: 0.0285\n",
            "Epoch [12296/20000], Training Loss: 0.0273\n",
            "Epoch [12297/20000], Training Loss: 0.0269\n",
            "Epoch [12298/20000], Training Loss: 0.0278\n",
            "Epoch [12299/20000], Training Loss: 0.0262\n",
            "Epoch [12300/20000], Training Loss: 0.0283\n",
            "Epoch [12301/20000], Training Loss: 0.0253\n",
            "Epoch [12302/20000], Training Loss: 0.0252\n",
            "Epoch [12303/20000], Training Loss: 0.0250\n",
            "Epoch [12304/20000], Training Loss: 0.0256\n",
            "Epoch [12305/20000], Training Loss: 0.0292\n",
            "Epoch [12306/20000], Training Loss: 0.0267\n",
            "Epoch [12307/20000], Training Loss: 0.0274\n",
            "Epoch [12308/20000], Training Loss: 0.0245\n",
            "Epoch [12309/20000], Training Loss: 0.0279\n",
            "Epoch [12310/20000], Training Loss: 0.0279\n",
            "Epoch [12311/20000], Training Loss: 0.0265\n",
            "Epoch [12312/20000], Training Loss: 0.0273\n",
            "Epoch [12313/20000], Training Loss: 0.0269\n",
            "Epoch [12314/20000], Training Loss: 0.0273\n",
            "Epoch [12315/20000], Training Loss: 0.0272\n",
            "Epoch [12316/20000], Training Loss: 0.0245\n",
            "Epoch [12317/20000], Training Loss: 0.0276\n",
            "Epoch [12318/20000], Training Loss: 0.0251\n",
            "Epoch [12319/20000], Training Loss: 0.0266\n",
            "Epoch [12320/20000], Training Loss: 0.0252\n",
            "Epoch [12321/20000], Training Loss: 0.0274\n",
            "Epoch [12322/20000], Training Loss: 0.0256\n",
            "Epoch [12323/20000], Training Loss: 0.0271\n",
            "Epoch [12324/20000], Training Loss: 0.0273\n",
            "Epoch [12325/20000], Training Loss: 0.0261\n",
            "Epoch [12326/20000], Training Loss: 0.0254\n",
            "Epoch [12327/20000], Training Loss: 0.0258\n",
            "Epoch [12328/20000], Training Loss: 0.0250\n",
            "Epoch [12329/20000], Training Loss: 0.0262\n",
            "Epoch [12330/20000], Training Loss: 0.0276\n",
            "Epoch [12331/20000], Training Loss: 0.0264\n",
            "Epoch [12332/20000], Training Loss: 0.0272\n",
            "Epoch [12333/20000], Training Loss: 0.0253\n",
            "Epoch [12334/20000], Training Loss: 0.0260\n",
            "Epoch [12335/20000], Training Loss: 0.0271\n",
            "Epoch [12336/20000], Training Loss: 0.0282\n",
            "Epoch [12337/20000], Training Loss: 0.0254\n",
            "Epoch [12338/20000], Training Loss: 0.0272\n",
            "Epoch [12339/20000], Training Loss: 0.0247\n",
            "Epoch [12340/20000], Training Loss: 0.0272\n",
            "Epoch [12341/20000], Training Loss: 0.0270\n",
            "Epoch [12342/20000], Training Loss: 0.0266\n",
            "Epoch [12343/20000], Training Loss: 0.0263\n",
            "Epoch [12344/20000], Training Loss: 0.0260\n",
            "Epoch [12345/20000], Training Loss: 0.0257\n",
            "Epoch [12346/20000], Training Loss: 0.0251\n",
            "Epoch [12347/20000], Training Loss: 0.0259\n",
            "Epoch [12348/20000], Training Loss: 0.0282\n",
            "Epoch [12349/20000], Training Loss: 0.0262\n",
            "Epoch [12350/20000], Training Loss: 0.0271\n",
            "Epoch [12351/20000], Training Loss: 0.0259\n",
            "Epoch [12352/20000], Training Loss: 0.0258\n",
            "Epoch [12353/20000], Training Loss: 0.0249\n",
            "Epoch [12354/20000], Training Loss: 0.0268\n",
            "Epoch [12355/20000], Training Loss: 0.0279\n",
            "Epoch [12356/20000], Training Loss: 0.0264\n",
            "Epoch [12357/20000], Training Loss: 0.0262\n",
            "Epoch [12358/20000], Training Loss: 0.0288\n",
            "Epoch [12359/20000], Training Loss: 0.0283\n",
            "Epoch [12360/20000], Training Loss: 0.0266\n",
            "Epoch [12361/20000], Training Loss: 0.0247\n",
            "Epoch [12362/20000], Training Loss: 0.0267\n",
            "Epoch [12363/20000], Training Loss: 0.0265\n",
            "Epoch [12364/20000], Training Loss: 0.0260\n",
            "Epoch [12365/20000], Training Loss: 0.0264\n",
            "Epoch [12366/20000], Training Loss: 0.0266\n",
            "Epoch [12367/20000], Training Loss: 0.0281\n",
            "Epoch [12368/20000], Training Loss: 0.0262\n",
            "Epoch [12369/20000], Training Loss: 0.0283\n",
            "Epoch [12370/20000], Training Loss: 0.0281\n",
            "Epoch [12371/20000], Training Loss: 0.0248\n",
            "Epoch [12372/20000], Training Loss: 0.0253\n",
            "Epoch [12373/20000], Training Loss: 0.0245\n",
            "Epoch [12374/20000], Training Loss: 0.0260\n",
            "Epoch [12375/20000], Training Loss: 0.0291\n",
            "Epoch [12376/20000], Training Loss: 0.0245\n",
            "Epoch [12377/20000], Training Loss: 0.0250\n",
            "Epoch [12378/20000], Training Loss: 0.0250\n",
            "Epoch [12379/20000], Training Loss: 0.0275\n",
            "Epoch [12380/20000], Training Loss: 0.0274\n",
            "Epoch [12381/20000], Training Loss: 0.0261\n",
            "Epoch [12382/20000], Training Loss: 0.0247\n",
            "Epoch [12383/20000], Training Loss: 0.0278\n",
            "Epoch [12384/20000], Training Loss: 0.0263\n",
            "Epoch [12385/20000], Training Loss: 0.0261\n",
            "Epoch [12386/20000], Training Loss: 0.0249\n",
            "Epoch [12387/20000], Training Loss: 0.0271\n",
            "Epoch [12388/20000], Training Loss: 0.0256\n",
            "Epoch [12389/20000], Training Loss: 0.0277\n",
            "Epoch [12390/20000], Training Loss: 0.0265\n",
            "Epoch [12391/20000], Training Loss: 0.0262\n",
            "Epoch [12392/20000], Training Loss: 0.0270\n",
            "Epoch [12393/20000], Training Loss: 0.0233\n",
            "Epoch [12394/20000], Training Loss: 0.0253\n",
            "Epoch [12395/20000], Training Loss: 0.0259\n",
            "Epoch [12396/20000], Training Loss: 0.0277\n",
            "Epoch [12397/20000], Training Loss: 0.0277\n",
            "Epoch [12398/20000], Training Loss: 0.0281\n",
            "Epoch [12399/20000], Training Loss: 0.0239\n",
            "Epoch [12400/20000], Training Loss: 0.0274\n",
            "Epoch [12401/20000], Training Loss: 0.0262\n",
            "Epoch [12402/20000], Training Loss: 0.0277\n",
            "Epoch [12403/20000], Training Loss: 0.0263\n",
            "Epoch [12404/20000], Training Loss: 0.0258\n",
            "Epoch [12405/20000], Training Loss: 0.0249\n",
            "Epoch [12406/20000], Training Loss: 0.0254\n",
            "Epoch [12407/20000], Training Loss: 0.0269\n",
            "Epoch [12408/20000], Training Loss: 0.0272\n",
            "Epoch [12409/20000], Training Loss: 0.0278\n",
            "Epoch [12410/20000], Training Loss: 0.0264\n",
            "Epoch [12411/20000], Training Loss: 0.0272\n",
            "Epoch [12412/20000], Training Loss: 0.0268\n",
            "Epoch [12413/20000], Training Loss: 0.0238\n",
            "Epoch [12414/20000], Training Loss: 0.0271\n",
            "Epoch [12415/20000], Training Loss: 0.0248\n",
            "Epoch [12416/20000], Training Loss: 0.0256\n",
            "Epoch [12417/20000], Training Loss: 0.0266\n",
            "Epoch [12418/20000], Training Loss: 0.0263\n",
            "Epoch [12419/20000], Training Loss: 0.0282\n",
            "Epoch [12420/20000], Training Loss: 0.0288\n",
            "Epoch [12421/20000], Training Loss: 0.0275\n",
            "Epoch [12422/20000], Training Loss: 0.0269\n",
            "Epoch [12423/20000], Training Loss: 0.0264\n",
            "Epoch [12424/20000], Training Loss: 0.0282\n",
            "Epoch [12425/20000], Training Loss: 0.0258\n",
            "Epoch [12426/20000], Training Loss: 0.0258\n",
            "Epoch [12427/20000], Training Loss: 0.0252\n",
            "Epoch [12428/20000], Training Loss: 0.0269\n",
            "Epoch [12429/20000], Training Loss: 0.0245\n",
            "Epoch [12430/20000], Training Loss: 0.0270\n",
            "Epoch [12431/20000], Training Loss: 0.0250\n",
            "Epoch [12432/20000], Training Loss: 0.0263\n",
            "Epoch [12433/20000], Training Loss: 0.0256\n",
            "Epoch [12434/20000], Training Loss: 0.0264\n",
            "Epoch [12435/20000], Training Loss: 0.0262\n",
            "Epoch [12436/20000], Training Loss: 0.0269\n",
            "Epoch [12437/20000], Training Loss: 0.0277\n",
            "Epoch [12438/20000], Training Loss: 0.0248\n",
            "Epoch [12439/20000], Training Loss: 0.0254\n",
            "Epoch [12440/20000], Training Loss: 0.0268\n",
            "Epoch [12441/20000], Training Loss: 0.0253\n",
            "Epoch [12442/20000], Training Loss: 0.0275\n",
            "Epoch [12443/20000], Training Loss: 0.0249\n",
            "Epoch [12444/20000], Training Loss: 0.0256\n",
            "Epoch [12445/20000], Training Loss: 0.0263\n",
            "Epoch [12446/20000], Training Loss: 0.0270\n",
            "Epoch [12447/20000], Training Loss: 0.0279\n",
            "Epoch [12448/20000], Training Loss: 0.0306\n",
            "Epoch [12449/20000], Training Loss: 0.0277\n",
            "Epoch [12450/20000], Training Loss: 0.0272\n",
            "Epoch [12451/20000], Training Loss: 0.0259\n",
            "Epoch [12452/20000], Training Loss: 0.0275\n",
            "Epoch [12453/20000], Training Loss: 0.0256\n",
            "Epoch [12454/20000], Training Loss: 0.0267\n",
            "Epoch [12455/20000], Training Loss: 0.0246\n",
            "Epoch [12456/20000], Training Loss: 0.0289\n",
            "Epoch [12457/20000], Training Loss: 0.0266\n",
            "Epoch [12458/20000], Training Loss: 0.0259\n",
            "Epoch [12459/20000], Training Loss: 0.0253\n",
            "Epoch [12460/20000], Training Loss: 0.0249\n",
            "Epoch [12461/20000], Training Loss: 0.0258\n",
            "Epoch [12462/20000], Training Loss: 0.0259\n",
            "Epoch [12463/20000], Training Loss: 0.0262\n",
            "Epoch [12464/20000], Training Loss: 0.0247\n",
            "Epoch [12465/20000], Training Loss: 0.0273\n",
            "Epoch [12466/20000], Training Loss: 0.0289\n",
            "Epoch [12467/20000], Training Loss: 0.0257\n",
            "Epoch [12468/20000], Training Loss: 0.0274\n",
            "Epoch [12469/20000], Training Loss: 0.0267\n",
            "Epoch [12470/20000], Training Loss: 0.0268\n",
            "Epoch [12471/20000], Training Loss: 0.0275\n",
            "Epoch [12472/20000], Training Loss: 0.0248\n",
            "Epoch [12473/20000], Training Loss: 0.0274\n",
            "Epoch [12474/20000], Training Loss: 0.0256\n",
            "Epoch [12475/20000], Training Loss: 0.0284\n",
            "Epoch [12476/20000], Training Loss: 0.0274\n",
            "Epoch [12477/20000], Training Loss: 0.0260\n",
            "Epoch [12478/20000], Training Loss: 0.0249\n",
            "Epoch [12479/20000], Training Loss: 0.0266\n",
            "Epoch [12480/20000], Training Loss: 0.0267\n",
            "Epoch [12481/20000], Training Loss: 0.0276\n",
            "Epoch [12482/20000], Training Loss: 0.0297\n",
            "Epoch [12483/20000], Training Loss: 0.0253\n",
            "Epoch [12484/20000], Training Loss: 0.0254\n",
            "Epoch [12485/20000], Training Loss: 0.0250\n",
            "Epoch [12486/20000], Training Loss: 0.0244\n",
            "Epoch [12487/20000], Training Loss: 0.0277\n",
            "Epoch [12488/20000], Training Loss: 0.0262\n",
            "Epoch [12489/20000], Training Loss: 0.0261\n",
            "Epoch [12490/20000], Training Loss: 0.0252\n",
            "Epoch [12491/20000], Training Loss: 0.0257\n",
            "Epoch [12492/20000], Training Loss: 0.0276\n",
            "Epoch [12493/20000], Training Loss: 0.0246\n",
            "Epoch [12494/20000], Training Loss: 0.0273\n",
            "Epoch [12495/20000], Training Loss: 0.0277\n",
            "Epoch [12496/20000], Training Loss: 0.0270\n",
            "Epoch [12497/20000], Training Loss: 0.0273\n",
            "Epoch [12498/20000], Training Loss: 0.0262\n",
            "Epoch [12499/20000], Training Loss: 0.0242\n",
            "Epoch [12500/20000], Training Loss: 0.0286\n",
            "Epoch [12501/20000], Training Loss: 0.0273\n",
            "Epoch [12502/20000], Training Loss: 0.0262\n",
            "Epoch [12503/20000], Training Loss: 0.0240\n",
            "Epoch [12504/20000], Training Loss: 0.0266\n",
            "Epoch [12505/20000], Training Loss: 0.0255\n",
            "Epoch [12506/20000], Training Loss: 0.0268\n",
            "Epoch [12507/20000], Training Loss: 0.0254\n",
            "Epoch [12508/20000], Training Loss: 0.0275\n",
            "Epoch [12509/20000], Training Loss: 0.0253\n",
            "Epoch [12510/20000], Training Loss: 0.0263\n",
            "Epoch [12511/20000], Training Loss: 0.0261\n",
            "Epoch [12512/20000], Training Loss: 0.0266\n",
            "Epoch [12513/20000], Training Loss: 0.0259\n",
            "Epoch [12514/20000], Training Loss: 0.0257\n",
            "Epoch [12515/20000], Training Loss: 0.0269\n",
            "Epoch [12516/20000], Training Loss: 0.0269\n",
            "Epoch [12517/20000], Training Loss: 0.0270\n",
            "Epoch [12518/20000], Training Loss: 0.0245\n",
            "Epoch [12519/20000], Training Loss: 0.0269\n",
            "Epoch [12520/20000], Training Loss: 0.0247\n",
            "Epoch [12521/20000], Training Loss: 0.0269\n",
            "Epoch [12522/20000], Training Loss: 0.0269\n",
            "Epoch [12523/20000], Training Loss: 0.0279\n",
            "Epoch [12524/20000], Training Loss: 0.0253\n",
            "Epoch [12525/20000], Training Loss: 0.0285\n",
            "Epoch [12526/20000], Training Loss: 0.0258\n",
            "Epoch [12527/20000], Training Loss: 0.0267\n",
            "Epoch [12528/20000], Training Loss: 0.0278\n",
            "Epoch [12529/20000], Training Loss: 0.0276\n",
            "Epoch [12530/20000], Training Loss: 0.0273\n",
            "Epoch [12531/20000], Training Loss: 0.0261\n",
            "Epoch [12532/20000], Training Loss: 0.0259\n",
            "Epoch [12533/20000], Training Loss: 0.0263\n",
            "Epoch [12534/20000], Training Loss: 0.0249\n",
            "Epoch [12535/20000], Training Loss: 0.0282\n",
            "Epoch [12536/20000], Training Loss: 0.0281\n",
            "Epoch [12537/20000], Training Loss: 0.0271\n",
            "Epoch [12538/20000], Training Loss: 0.0274\n",
            "Epoch [12539/20000], Training Loss: 0.0269\n",
            "Epoch [12540/20000], Training Loss: 0.0250\n",
            "Epoch [12541/20000], Training Loss: 0.0261\n",
            "Epoch [12542/20000], Training Loss: 0.0254\n",
            "Epoch [12543/20000], Training Loss: 0.0261\n",
            "Epoch [12544/20000], Training Loss: 0.0262\n",
            "Epoch [12545/20000], Training Loss: 0.0262\n",
            "Epoch [12546/20000], Training Loss: 0.0266\n",
            "Epoch [12547/20000], Training Loss: 0.0267\n",
            "Epoch [12548/20000], Training Loss: 0.0284\n",
            "Epoch [12549/20000], Training Loss: 0.0262\n",
            "Epoch [12550/20000], Training Loss: 0.0253\n",
            "Epoch [12551/20000], Training Loss: 0.0271\n",
            "Epoch [12552/20000], Training Loss: 0.0261\n",
            "Epoch [12553/20000], Training Loss: 0.0254\n",
            "Epoch [12554/20000], Training Loss: 0.0256\n",
            "Epoch [12555/20000], Training Loss: 0.0269\n",
            "Epoch [12556/20000], Training Loss: 0.0266\n",
            "Epoch [12557/20000], Training Loss: 0.0248\n",
            "Epoch [12558/20000], Training Loss: 0.0254\n",
            "Epoch [12559/20000], Training Loss: 0.0278\n",
            "Epoch [12560/20000], Training Loss: 0.0244\n",
            "Epoch [12561/20000], Training Loss: 0.0253\n",
            "Epoch [12562/20000], Training Loss: 0.0257\n",
            "Epoch [12563/20000], Training Loss: 0.0251\n",
            "Epoch [12564/20000], Training Loss: 0.0276\n",
            "Epoch [12565/20000], Training Loss: 0.0267\n",
            "Epoch [12566/20000], Training Loss: 0.0264\n",
            "Epoch [12567/20000], Training Loss: 0.0265\n",
            "Epoch [12568/20000], Training Loss: 0.0267\n",
            "Epoch [12569/20000], Training Loss: 0.0261\n",
            "Epoch [12570/20000], Training Loss: 0.0266\n",
            "Epoch [12571/20000], Training Loss: 0.0267\n",
            "Epoch [12572/20000], Training Loss: 0.0264\n",
            "Epoch [12573/20000], Training Loss: 0.0252\n",
            "Epoch [12574/20000], Training Loss: 0.0248\n",
            "Epoch [12575/20000], Training Loss: 0.0262\n",
            "Epoch [12576/20000], Training Loss: 0.0276\n",
            "Epoch [12577/20000], Training Loss: 0.0268\n",
            "Epoch [12578/20000], Training Loss: 0.0276\n",
            "Epoch [12579/20000], Training Loss: 0.0270\n",
            "Epoch [12580/20000], Training Loss: 0.0258\n",
            "Epoch [12581/20000], Training Loss: 0.0264\n",
            "Epoch [12582/20000], Training Loss: 0.0274\n",
            "Epoch [12583/20000], Training Loss: 0.0257\n",
            "Epoch [12584/20000], Training Loss: 0.0282\n",
            "Epoch [12585/20000], Training Loss: 0.0242\n",
            "Epoch [12586/20000], Training Loss: 0.0267\n",
            "Epoch [12587/20000], Training Loss: 0.0278\n",
            "Epoch [12588/20000], Training Loss: 0.0256\n",
            "Epoch [12589/20000], Training Loss: 0.0267\n",
            "Epoch [12590/20000], Training Loss: 0.0265\n",
            "Epoch [12591/20000], Training Loss: 0.0244\n",
            "Epoch [12592/20000], Training Loss: 0.0265\n",
            "Epoch [12593/20000], Training Loss: 0.0252\n",
            "Epoch [12594/20000], Training Loss: 0.0275\n",
            "Epoch [12595/20000], Training Loss: 0.0278\n",
            "Epoch [12596/20000], Training Loss: 0.0257\n",
            "Epoch [12597/20000], Training Loss: 0.0244\n",
            "Epoch [12598/20000], Training Loss: 0.0275\n",
            "Epoch [12599/20000], Training Loss: 0.0267\n",
            "Epoch [12600/20000], Training Loss: 0.0253\n",
            "Epoch [12601/20000], Training Loss: 0.0254\n",
            "Epoch [12602/20000], Training Loss: 0.0277\n",
            "Epoch [12603/20000], Training Loss: 0.0273\n",
            "Epoch [12604/20000], Training Loss: 0.0263\n",
            "Epoch [12605/20000], Training Loss: 0.0279\n",
            "Epoch [12606/20000], Training Loss: 0.0244\n",
            "Epoch [12607/20000], Training Loss: 0.0277\n",
            "Epoch [12608/20000], Training Loss: 0.0254\n",
            "Epoch [12609/20000], Training Loss: 0.0252\n",
            "Epoch [12610/20000], Training Loss: 0.0279\n",
            "Epoch [12611/20000], Training Loss: 0.0276\n",
            "Epoch [12612/20000], Training Loss: 0.0261\n",
            "Epoch [12613/20000], Training Loss: 0.0264\n",
            "Epoch [12614/20000], Training Loss: 0.0265\n",
            "Epoch [12615/20000], Training Loss: 0.0259\n",
            "Epoch [12616/20000], Training Loss: 0.0277\n",
            "Epoch [12617/20000], Training Loss: 0.0274\n",
            "Epoch [12618/20000], Training Loss: 0.0278\n",
            "Epoch [12619/20000], Training Loss: 0.0246\n",
            "Epoch [12620/20000], Training Loss: 0.0291\n",
            "Epoch [12621/20000], Training Loss: 0.0272\n",
            "Epoch [12622/20000], Training Loss: 0.0268\n",
            "Epoch [12623/20000], Training Loss: 0.0288\n",
            "Epoch [12624/20000], Training Loss: 0.0286\n",
            "Epoch [12625/20000], Training Loss: 0.0260\n",
            "Epoch [12626/20000], Training Loss: 0.0275\n",
            "Epoch [12627/20000], Training Loss: 0.0254\n",
            "Epoch [12628/20000], Training Loss: 0.0262\n",
            "Epoch [12629/20000], Training Loss: 0.0270\n",
            "Epoch [12630/20000], Training Loss: 0.0273\n",
            "Epoch [12631/20000], Training Loss: 0.0284\n",
            "Epoch [12632/20000], Training Loss: 0.0254\n",
            "Epoch [12633/20000], Training Loss: 0.0253\n",
            "Epoch [12634/20000], Training Loss: 0.0265\n",
            "Epoch [12635/20000], Training Loss: 0.0258\n",
            "Epoch [12636/20000], Training Loss: 0.0256\n",
            "Epoch [12637/20000], Training Loss: 0.0259\n",
            "Epoch [12638/20000], Training Loss: 0.0278\n",
            "Epoch [12639/20000], Training Loss: 0.0283\n",
            "Epoch [12640/20000], Training Loss: 0.0274\n",
            "Epoch [12641/20000], Training Loss: 0.0255\n",
            "Epoch [12642/20000], Training Loss: 0.0273\n",
            "Epoch [12643/20000], Training Loss: 0.0259\n",
            "Epoch [12644/20000], Training Loss: 0.0261\n",
            "Epoch [12645/20000], Training Loss: 0.0277\n",
            "Epoch [12646/20000], Training Loss: 0.0271\n",
            "Epoch [12647/20000], Training Loss: 0.0273\n",
            "Epoch [12648/20000], Training Loss: 0.0266\n",
            "Epoch [12649/20000], Training Loss: 0.0263\n",
            "Epoch [12650/20000], Training Loss: 0.0272\n",
            "Epoch [12651/20000], Training Loss: 0.0237\n",
            "Epoch [12652/20000], Training Loss: 0.0256\n",
            "Epoch [12653/20000], Training Loss: 0.0258\n",
            "Epoch [12654/20000], Training Loss: 0.0262\n",
            "Epoch [12655/20000], Training Loss: 0.0273\n",
            "Epoch [12656/20000], Training Loss: 0.0263\n",
            "Epoch [12657/20000], Training Loss: 0.0258\n",
            "Epoch [12658/20000], Training Loss: 0.0280\n",
            "Epoch [12659/20000], Training Loss: 0.0262\n",
            "Epoch [12660/20000], Training Loss: 0.0259\n",
            "Epoch [12661/20000], Training Loss: 0.0244\n",
            "Epoch [12662/20000], Training Loss: 0.0266\n",
            "Epoch [12663/20000], Training Loss: 0.0283\n",
            "Epoch [12664/20000], Training Loss: 0.0277\n",
            "Epoch [12665/20000], Training Loss: 0.0258\n",
            "Epoch [12666/20000], Training Loss: 0.0273\n",
            "Epoch [12667/20000], Training Loss: 0.0239\n",
            "Epoch [12668/20000], Training Loss: 0.0250\n",
            "Epoch [12669/20000], Training Loss: 0.0240\n",
            "Epoch [12670/20000], Training Loss: 0.0266\n",
            "Epoch [12671/20000], Training Loss: 0.0258\n",
            "Epoch [12672/20000], Training Loss: 0.0268\n",
            "Epoch [12673/20000], Training Loss: 0.0248\n",
            "Epoch [12674/20000], Training Loss: 0.0249\n",
            "Epoch [12675/20000], Training Loss: 0.0271\n",
            "Epoch [12676/20000], Training Loss: 0.0261\n",
            "Epoch [12677/20000], Training Loss: 0.0267\n",
            "Epoch [12678/20000], Training Loss: 0.0272\n",
            "Epoch [12679/20000], Training Loss: 0.0275\n",
            "Epoch [12680/20000], Training Loss: 0.0243\n",
            "Epoch [12681/20000], Training Loss: 0.0256\n",
            "Epoch [12682/20000], Training Loss: 0.0257\n",
            "Epoch [12683/20000], Training Loss: 0.0253\n",
            "Epoch [12684/20000], Training Loss: 0.0265\n",
            "Epoch [12685/20000], Training Loss: 0.0260\n",
            "Epoch [12686/20000], Training Loss: 0.0261\n",
            "Epoch [12687/20000], Training Loss: 0.0256\n",
            "Epoch [12688/20000], Training Loss: 0.0278\n",
            "Epoch [12689/20000], Training Loss: 0.0274\n",
            "Epoch [12690/20000], Training Loss: 0.0261\n",
            "Epoch [12691/20000], Training Loss: 0.0262\n",
            "Epoch [12692/20000], Training Loss: 0.0257\n",
            "Epoch [12693/20000], Training Loss: 0.0264\n",
            "Epoch [12694/20000], Training Loss: 0.0267\n",
            "Epoch [12695/20000], Training Loss: 0.0253\n",
            "Epoch [12696/20000], Training Loss: 0.0261\n",
            "Epoch [12697/20000], Training Loss: 0.0247\n",
            "Epoch [12698/20000], Training Loss: 0.0302\n",
            "Epoch [12699/20000], Training Loss: 0.0278\n",
            "Epoch [12700/20000], Training Loss: 0.0257\n",
            "Epoch [12701/20000], Training Loss: 0.0277\n",
            "Epoch [12702/20000], Training Loss: 0.0264\n",
            "Epoch [12703/20000], Training Loss: 0.0264\n",
            "Epoch [12704/20000], Training Loss: 0.0269\n",
            "Epoch [12705/20000], Training Loss: 0.0249\n",
            "Epoch [12706/20000], Training Loss: 0.0257\n",
            "Epoch [12707/20000], Training Loss: 0.0269\n",
            "Epoch [12708/20000], Training Loss: 0.0248\n",
            "Epoch [12709/20000], Training Loss: 0.0260\n",
            "Epoch [12710/20000], Training Loss: 0.0265\n",
            "Epoch [12711/20000], Training Loss: 0.0275\n",
            "Epoch [12712/20000], Training Loss: 0.0287\n",
            "Epoch [12713/20000], Training Loss: 0.0270\n",
            "Epoch [12714/20000], Training Loss: 0.0275\n",
            "Epoch [12715/20000], Training Loss: 0.0260\n",
            "Epoch [12716/20000], Training Loss: 0.0260\n",
            "Epoch [12717/20000], Training Loss: 0.0249\n",
            "Epoch [12718/20000], Training Loss: 0.0271\n",
            "Epoch [12719/20000], Training Loss: 0.0265\n",
            "Epoch [12720/20000], Training Loss: 0.0265\n",
            "Epoch [12721/20000], Training Loss: 0.0266\n",
            "Epoch [12722/20000], Training Loss: 0.0259\n",
            "Epoch [12723/20000], Training Loss: 0.0273\n",
            "Epoch [12724/20000], Training Loss: 0.0260\n",
            "Epoch [12725/20000], Training Loss: 0.0263\n",
            "Epoch [12726/20000], Training Loss: 0.0279\n",
            "Epoch [12727/20000], Training Loss: 0.0264\n",
            "Epoch [12728/20000], Training Loss: 0.0268\n",
            "Epoch [12729/20000], Training Loss: 0.0286\n",
            "Epoch [12730/20000], Training Loss: 0.0270\n",
            "Epoch [12731/20000], Training Loss: 0.0244\n",
            "Epoch [12732/20000], Training Loss: 0.0252\n",
            "Epoch [12733/20000], Training Loss: 0.0276\n",
            "Epoch [12734/20000], Training Loss: 0.0258\n",
            "Epoch [12735/20000], Training Loss: 0.0244\n",
            "Epoch [12736/20000], Training Loss: 0.0262\n",
            "Epoch [12737/20000], Training Loss: 0.0273\n",
            "Epoch [12738/20000], Training Loss: 0.0256\n",
            "Epoch [12739/20000], Training Loss: 0.0268\n",
            "Epoch [12740/20000], Training Loss: 0.0266\n",
            "Epoch [12741/20000], Training Loss: 0.0262\n",
            "Epoch [12742/20000], Training Loss: 0.0271\n",
            "Epoch [12743/20000], Training Loss: 0.0294\n",
            "Epoch [12744/20000], Training Loss: 0.0256\n",
            "Epoch [12745/20000], Training Loss: 0.0272\n",
            "Epoch [12746/20000], Training Loss: 0.0266\n",
            "Epoch [12747/20000], Training Loss: 0.0266\n",
            "Epoch [12748/20000], Training Loss: 0.0264\n",
            "Epoch [12749/20000], Training Loss: 0.0281\n",
            "Epoch [12750/20000], Training Loss: 0.0270\n",
            "Epoch [12751/20000], Training Loss: 0.0267\n",
            "Epoch [12752/20000], Training Loss: 0.0292\n",
            "Epoch [12753/20000], Training Loss: 0.0287\n",
            "Epoch [12754/20000], Training Loss: 0.0273\n",
            "Epoch [12755/20000], Training Loss: 0.0274\n",
            "Epoch [12756/20000], Training Loss: 0.0260\n",
            "Epoch [12757/20000], Training Loss: 0.0250\n",
            "Epoch [12758/20000], Training Loss: 0.0250\n",
            "Epoch [12759/20000], Training Loss: 0.0283\n",
            "Epoch [12760/20000], Training Loss: 0.0265\n",
            "Epoch [12761/20000], Training Loss: 0.0258\n",
            "Epoch [12762/20000], Training Loss: 0.0269\n",
            "Epoch [12763/20000], Training Loss: 0.0262\n",
            "Epoch [12764/20000], Training Loss: 0.0245\n",
            "Epoch [12765/20000], Training Loss: 0.0263\n",
            "Epoch [12766/20000], Training Loss: 0.0249\n",
            "Epoch [12767/20000], Training Loss: 0.0249\n",
            "Epoch [12768/20000], Training Loss: 0.0250\n",
            "Epoch [12769/20000], Training Loss: 0.0258\n",
            "Epoch [12770/20000], Training Loss: 0.0260\n",
            "Epoch [12771/20000], Training Loss: 0.0275\n",
            "Epoch [12772/20000], Training Loss: 0.0274\n",
            "Epoch [12773/20000], Training Loss: 0.0276\n",
            "Epoch [12774/20000], Training Loss: 0.0251\n",
            "Epoch [12775/20000], Training Loss: 0.0256\n",
            "Epoch [12776/20000], Training Loss: 0.0273\n",
            "Epoch [12777/20000], Training Loss: 0.0251\n",
            "Epoch [12778/20000], Training Loss: 0.0272\n",
            "Epoch [12779/20000], Training Loss: 0.0270\n",
            "Epoch [12780/20000], Training Loss: 0.0279\n",
            "Epoch [12781/20000], Training Loss: 0.0247\n",
            "Epoch [12782/20000], Training Loss: 0.0282\n",
            "Epoch [12783/20000], Training Loss: 0.0262\n",
            "Epoch [12784/20000], Training Loss: 0.0263\n",
            "Epoch [12785/20000], Training Loss: 0.0232\n",
            "Epoch [12786/20000], Training Loss: 0.0281\n",
            "Epoch [12787/20000], Training Loss: 0.0246\n",
            "Epoch [12788/20000], Training Loss: 0.0258\n",
            "Epoch [12789/20000], Training Loss: 0.0267\n",
            "Epoch [12790/20000], Training Loss: 0.0273\n",
            "Epoch [12791/20000], Training Loss: 0.0246\n",
            "Epoch [12792/20000], Training Loss: 0.0261\n",
            "Epoch [12793/20000], Training Loss: 0.0259\n",
            "Epoch [12794/20000], Training Loss: 0.0264\n",
            "Epoch [12795/20000], Training Loss: 0.0275\n",
            "Epoch [12796/20000], Training Loss: 0.0251\n",
            "Epoch [12797/20000], Training Loss: 0.0267\n",
            "Epoch [12798/20000], Training Loss: 0.0251\n",
            "Epoch [12799/20000], Training Loss: 0.0270\n",
            "Epoch [12800/20000], Training Loss: 0.0256\n",
            "Epoch [12801/20000], Training Loss: 0.0267\n",
            "Epoch [12802/20000], Training Loss: 0.0249\n",
            "Epoch [12803/20000], Training Loss: 0.0254\n",
            "Epoch [12804/20000], Training Loss: 0.0247\n",
            "Epoch [12805/20000], Training Loss: 0.0258\n",
            "Epoch [12806/20000], Training Loss: 0.0276\n",
            "Epoch [12807/20000], Training Loss: 0.0261\n",
            "Epoch [12808/20000], Training Loss: 0.0263\n",
            "Epoch [12809/20000], Training Loss: 0.0265\n",
            "Epoch [12810/20000], Training Loss: 0.0259\n",
            "Epoch [12811/20000], Training Loss: 0.0248\n",
            "Epoch [12812/20000], Training Loss: 0.0249\n",
            "Epoch [12813/20000], Training Loss: 0.0257\n",
            "Epoch [12814/20000], Training Loss: 0.0291\n",
            "Epoch [12815/20000], Training Loss: 0.0247\n",
            "Epoch [12816/20000], Training Loss: 0.0262\n",
            "Epoch [12817/20000], Training Loss: 0.0246\n",
            "Epoch [12818/20000], Training Loss: 0.0258\n",
            "Epoch [12819/20000], Training Loss: 0.0272\n",
            "Epoch [12820/20000], Training Loss: 0.0262\n",
            "Epoch [12821/20000], Training Loss: 0.0249\n",
            "Epoch [12822/20000], Training Loss: 0.0247\n",
            "Epoch [12823/20000], Training Loss: 0.0260\n",
            "Epoch [12824/20000], Training Loss: 0.0270\n",
            "Epoch [12825/20000], Training Loss: 0.0263\n",
            "Epoch [12826/20000], Training Loss: 0.0260\n",
            "Epoch [12827/20000], Training Loss: 0.0280\n",
            "Epoch [12828/20000], Training Loss: 0.0274\n",
            "Epoch [12829/20000], Training Loss: 0.0263\n",
            "Epoch [12830/20000], Training Loss: 0.0271\n",
            "Epoch [12831/20000], Training Loss: 0.0262\n",
            "Epoch [12832/20000], Training Loss: 0.0258\n",
            "Epoch [12833/20000], Training Loss: 0.0254\n",
            "Epoch [12834/20000], Training Loss: 0.0261\n",
            "Epoch [12835/20000], Training Loss: 0.0242\n",
            "Epoch [12836/20000], Training Loss: 0.0275\n",
            "Epoch [12837/20000], Training Loss: 0.0275\n",
            "Epoch [12838/20000], Training Loss: 0.0271\n",
            "Epoch [12839/20000], Training Loss: 0.0255\n",
            "Epoch [12840/20000], Training Loss: 0.0279\n",
            "Epoch [12841/20000], Training Loss: 0.0276\n",
            "Epoch [12842/20000], Training Loss: 0.0253\n",
            "Epoch [12843/20000], Training Loss: 0.0260\n",
            "Epoch [12844/20000], Training Loss: 0.0264\n",
            "Epoch [12845/20000], Training Loss: 0.0256\n",
            "Epoch [12846/20000], Training Loss: 0.0260\n",
            "Epoch [12847/20000], Training Loss: 0.0253\n",
            "Epoch [12848/20000], Training Loss: 0.0262\n",
            "Epoch [12849/20000], Training Loss: 0.0254\n",
            "Epoch [12850/20000], Training Loss: 0.0277\n",
            "Epoch [12851/20000], Training Loss: 0.0261\n",
            "Epoch [12852/20000], Training Loss: 0.0273\n",
            "Epoch [12853/20000], Training Loss: 0.0263\n",
            "Epoch [12854/20000], Training Loss: 0.0276\n",
            "Epoch [12855/20000], Training Loss: 0.0272\n",
            "Epoch [12856/20000], Training Loss: 0.0254\n",
            "Epoch [12857/20000], Training Loss: 0.0264\n",
            "Epoch [12858/20000], Training Loss: 0.0253\n",
            "Epoch [12859/20000], Training Loss: 0.0269\n",
            "Epoch [12860/20000], Training Loss: 0.0268\n",
            "Epoch [12861/20000], Training Loss: 0.0254\n",
            "Epoch [12862/20000], Training Loss: 0.0274\n",
            "Epoch [12863/20000], Training Loss: 0.0266\n",
            "Epoch [12864/20000], Training Loss: 0.0266\n",
            "Epoch [12865/20000], Training Loss: 0.0260\n",
            "Epoch [12866/20000], Training Loss: 0.0262\n",
            "Epoch [12867/20000], Training Loss: 0.0257\n",
            "Epoch [12868/20000], Training Loss: 0.0277\n",
            "Epoch [12869/20000], Training Loss: 0.0268\n",
            "Epoch [12870/20000], Training Loss: 0.0276\n",
            "Epoch [12871/20000], Training Loss: 0.0244\n",
            "Epoch [12872/20000], Training Loss: 0.0255\n",
            "Epoch [12873/20000], Training Loss: 0.0277\n",
            "Epoch [12874/20000], Training Loss: 0.0260\n",
            "Epoch [12875/20000], Training Loss: 0.0266\n",
            "Epoch [12876/20000], Training Loss: 0.0255\n",
            "Epoch [12877/20000], Training Loss: 0.0233\n",
            "Epoch [12878/20000], Training Loss: 0.0270\n",
            "Epoch [12879/20000], Training Loss: 0.0269\n",
            "Epoch [12880/20000], Training Loss: 0.0260\n",
            "Epoch [12881/20000], Training Loss: 0.0251\n",
            "Epoch [12882/20000], Training Loss: 0.0247\n",
            "Epoch [12883/20000], Training Loss: 0.0266\n",
            "Epoch [12884/20000], Training Loss: 0.0274\n",
            "Epoch [12885/20000], Training Loss: 0.0274\n",
            "Epoch [12886/20000], Training Loss: 0.0257\n",
            "Epoch [12887/20000], Training Loss: 0.0265\n",
            "Epoch [12888/20000], Training Loss: 0.0273\n",
            "Epoch [12889/20000], Training Loss: 0.0275\n",
            "Epoch [12890/20000], Training Loss: 0.0273\n",
            "Epoch [12891/20000], Training Loss: 0.0254\n",
            "Epoch [12892/20000], Training Loss: 0.0264\n",
            "Epoch [12893/20000], Training Loss: 0.0261\n",
            "Epoch [12894/20000], Training Loss: 0.0251\n",
            "Epoch [12895/20000], Training Loss: 0.0266\n",
            "Epoch [12896/20000], Training Loss: 0.0265\n",
            "Epoch [12897/20000], Training Loss: 0.0279\n",
            "Epoch [12898/20000], Training Loss: 0.0286\n",
            "Epoch [12899/20000], Training Loss: 0.0258\n",
            "Epoch [12900/20000], Training Loss: 0.0263\n",
            "Epoch [12901/20000], Training Loss: 0.0277\n",
            "Epoch [12902/20000], Training Loss: 0.0298\n",
            "Epoch [12903/20000], Training Loss: 0.0251\n",
            "Epoch [12904/20000], Training Loss: 0.0266\n",
            "Epoch [12905/20000], Training Loss: 0.0264\n",
            "Epoch [12906/20000], Training Loss: 0.0268\n",
            "Epoch [12907/20000], Training Loss: 0.0258\n",
            "Epoch [12908/20000], Training Loss: 0.0271\n",
            "Epoch [12909/20000], Training Loss: 0.0264\n",
            "Epoch [12910/20000], Training Loss: 0.0262\n",
            "Epoch [12911/20000], Training Loss: 0.0295\n",
            "Epoch [12912/20000], Training Loss: 0.0253\n",
            "Epoch [12913/20000], Training Loss: 0.0292\n",
            "Epoch [12914/20000], Training Loss: 0.0259\n",
            "Epoch [12915/20000], Training Loss: 0.0247\n",
            "Epoch [12916/20000], Training Loss: 0.0243\n",
            "Epoch [12917/20000], Training Loss: 0.0278\n",
            "Epoch [12918/20000], Training Loss: 0.0253\n",
            "Epoch [12919/20000], Training Loss: 0.0266\n",
            "Epoch [12920/20000], Training Loss: 0.0268\n",
            "Epoch [12921/20000], Training Loss: 0.0271\n",
            "Epoch [12922/20000], Training Loss: 0.0257\n",
            "Epoch [12923/20000], Training Loss: 0.0275\n",
            "Epoch [12924/20000], Training Loss: 0.0242\n",
            "Epoch [12925/20000], Training Loss: 0.0268\n",
            "Epoch [12926/20000], Training Loss: 0.0261\n",
            "Epoch [12927/20000], Training Loss: 0.0263\n",
            "Epoch [12928/20000], Training Loss: 0.0260\n",
            "Epoch [12929/20000], Training Loss: 0.0272\n",
            "Epoch [12930/20000], Training Loss: 0.0275\n",
            "Epoch [12931/20000], Training Loss: 0.0286\n",
            "Epoch [12932/20000], Training Loss: 0.0263\n",
            "Epoch [12933/20000], Training Loss: 0.0256\n",
            "Epoch [12934/20000], Training Loss: 0.0261\n",
            "Epoch [12935/20000], Training Loss: 0.0255\n",
            "Epoch [12936/20000], Training Loss: 0.0272\n",
            "Epoch [12937/20000], Training Loss: 0.0255\n",
            "Epoch [12938/20000], Training Loss: 0.0286\n",
            "Epoch [12939/20000], Training Loss: 0.0263\n",
            "Epoch [12940/20000], Training Loss: 0.0261\n",
            "Epoch [12941/20000], Training Loss: 0.0271\n",
            "Epoch [12942/20000], Training Loss: 0.0265\n",
            "Epoch [12943/20000], Training Loss: 0.0278\n",
            "Epoch [12944/20000], Training Loss: 0.0267\n",
            "Epoch [12945/20000], Training Loss: 0.0274\n",
            "Epoch [12946/20000], Training Loss: 0.0254\n",
            "Epoch [12947/20000], Training Loss: 0.0262\n",
            "Epoch [12948/20000], Training Loss: 0.0265\n",
            "Epoch [12949/20000], Training Loss: 0.0273\n",
            "Epoch [12950/20000], Training Loss: 0.0259\n",
            "Epoch [12951/20000], Training Loss: 0.0266\n",
            "Epoch [12952/20000], Training Loss: 0.0259\n",
            "Epoch [12953/20000], Training Loss: 0.0277\n",
            "Epoch [12954/20000], Training Loss: 0.0263\n",
            "Epoch [12955/20000], Training Loss: 0.0267\n",
            "Epoch [12956/20000], Training Loss: 0.0282\n",
            "Epoch [12957/20000], Training Loss: 0.0244\n",
            "Epoch [12958/20000], Training Loss: 0.0260\n",
            "Epoch [12959/20000], Training Loss: 0.0262\n",
            "Epoch [12960/20000], Training Loss: 0.0261\n",
            "Epoch [12961/20000], Training Loss: 0.0259\n",
            "Epoch [12962/20000], Training Loss: 0.0255\n",
            "Epoch [12963/20000], Training Loss: 0.0257\n",
            "Epoch [12964/20000], Training Loss: 0.0262\n",
            "Epoch [12965/20000], Training Loss: 0.0252\n",
            "Epoch [12966/20000], Training Loss: 0.0272\n",
            "Epoch [12967/20000], Training Loss: 0.0269\n",
            "Epoch [12968/20000], Training Loss: 0.0265\n",
            "Epoch [12969/20000], Training Loss: 0.0250\n",
            "Epoch [12970/20000], Training Loss: 0.0256\n",
            "Epoch [12971/20000], Training Loss: 0.0254\n",
            "Epoch [12972/20000], Training Loss: 0.0256\n",
            "Epoch [12973/20000], Training Loss: 0.0262\n",
            "Epoch [12974/20000], Training Loss: 0.0256\n",
            "Epoch [12975/20000], Training Loss: 0.0279\n",
            "Epoch [12976/20000], Training Loss: 0.0277\n",
            "Epoch [12977/20000], Training Loss: 0.0302\n",
            "Epoch [12978/20000], Training Loss: 0.0267\n",
            "Epoch [12979/20000], Training Loss: 0.0256\n",
            "Epoch [12980/20000], Training Loss: 0.0249\n",
            "Epoch [12981/20000], Training Loss: 0.0254\n",
            "Epoch [12982/20000], Training Loss: 0.0276\n",
            "Epoch [12983/20000], Training Loss: 0.0248\n",
            "Epoch [12984/20000], Training Loss: 0.0270\n",
            "Epoch [12985/20000], Training Loss: 0.0262\n",
            "Epoch [12986/20000], Training Loss: 0.0296\n",
            "Epoch [12987/20000], Training Loss: 0.0257\n",
            "Epoch [12988/20000], Training Loss: 0.0287\n",
            "Epoch [12989/20000], Training Loss: 0.0256\n",
            "Epoch [12990/20000], Training Loss: 0.0260\n",
            "Epoch [12991/20000], Training Loss: 0.0273\n",
            "Epoch [12992/20000], Training Loss: 0.0273\n",
            "Epoch [12993/20000], Training Loss: 0.0253\n",
            "Epoch [12994/20000], Training Loss: 0.0267\n",
            "Epoch [12995/20000], Training Loss: 0.0270\n",
            "Epoch [12996/20000], Training Loss: 0.0253\n",
            "Epoch [12997/20000], Training Loss: 0.0254\n",
            "Epoch [12998/20000], Training Loss: 0.0259\n",
            "Epoch [12999/20000], Training Loss: 0.0279\n",
            "Epoch [13000/20000], Training Loss: 0.0250\n",
            "Epoch [13001/20000], Training Loss: 0.0254\n",
            "Epoch [13002/20000], Training Loss: 0.0250\n",
            "Epoch [13003/20000], Training Loss: 0.0269\n",
            "Epoch [13004/20000], Training Loss: 0.0275\n",
            "Epoch [13005/20000], Training Loss: 0.0263\n",
            "Epoch [13006/20000], Training Loss: 0.0256\n",
            "Epoch [13007/20000], Training Loss: 0.0274\n",
            "Epoch [13008/20000], Training Loss: 0.0248\n",
            "Epoch [13009/20000], Training Loss: 0.0275\n",
            "Epoch [13010/20000], Training Loss: 0.0273\n",
            "Epoch [13011/20000], Training Loss: 0.0256\n",
            "Epoch [13012/20000], Training Loss: 0.0265\n",
            "Epoch [13013/20000], Training Loss: 0.0277\n",
            "Epoch [13014/20000], Training Loss: 0.0273\n",
            "Epoch [13015/20000], Training Loss: 0.0267\n",
            "Epoch [13016/20000], Training Loss: 0.0297\n",
            "Epoch [13017/20000], Training Loss: 0.0259\n",
            "Epoch [13018/20000], Training Loss: 0.0241\n",
            "Epoch [13019/20000], Training Loss: 0.0259\n",
            "Epoch [13020/20000], Training Loss: 0.0266\n",
            "Epoch [13021/20000], Training Loss: 0.0259\n",
            "Epoch [13022/20000], Training Loss: 0.0245\n",
            "Epoch [13023/20000], Training Loss: 0.0279\n",
            "Epoch [13024/20000], Training Loss: 0.0251\n",
            "Epoch [13025/20000], Training Loss: 0.0265\n",
            "Epoch [13026/20000], Training Loss: 0.0255\n",
            "Epoch [13027/20000], Training Loss: 0.0265\n",
            "Epoch [13028/20000], Training Loss: 0.0246\n",
            "Epoch [13029/20000], Training Loss: 0.0275\n",
            "Epoch [13030/20000], Training Loss: 0.0247\n",
            "Epoch [13031/20000], Training Loss: 0.0269\n",
            "Epoch [13032/20000], Training Loss: 0.0247\n",
            "Epoch [13033/20000], Training Loss: 0.0251\n",
            "Epoch [13034/20000], Training Loss: 0.0251\n",
            "Epoch [13035/20000], Training Loss: 0.0250\n",
            "Epoch [13036/20000], Training Loss: 0.0261\n",
            "Epoch [13037/20000], Training Loss: 0.0251\n",
            "Epoch [13038/20000], Training Loss: 0.0278\n",
            "Epoch [13039/20000], Training Loss: 0.0290\n",
            "Epoch [13040/20000], Training Loss: 0.0266\n",
            "Epoch [13041/20000], Training Loss: 0.0272\n",
            "Epoch [13042/20000], Training Loss: 0.0277\n",
            "Epoch [13043/20000], Training Loss: 0.0275\n",
            "Epoch [13044/20000], Training Loss: 0.0294\n",
            "Epoch [13045/20000], Training Loss: 0.0274\n",
            "Epoch [13046/20000], Training Loss: 0.0251\n",
            "Epoch [13047/20000], Training Loss: 0.0243\n",
            "Epoch [13048/20000], Training Loss: 0.0278\n",
            "Epoch [13049/20000], Training Loss: 0.0266\n",
            "Epoch [13050/20000], Training Loss: 0.0268\n",
            "Epoch [13051/20000], Training Loss: 0.0252\n",
            "Epoch [13052/20000], Training Loss: 0.0252\n",
            "Epoch [13053/20000], Training Loss: 0.0272\n",
            "Epoch [13054/20000], Training Loss: 0.0238\n",
            "Epoch [13055/20000], Training Loss: 0.0267\n",
            "Epoch [13056/20000], Training Loss: 0.0285\n",
            "Epoch [13057/20000], Training Loss: 0.0242\n",
            "Epoch [13058/20000], Training Loss: 0.0277\n",
            "Epoch [13059/20000], Training Loss: 0.0247\n",
            "Epoch [13060/20000], Training Loss: 0.0258\n",
            "Epoch [13061/20000], Training Loss: 0.0261\n",
            "Epoch [13062/20000], Training Loss: 0.0249\n",
            "Epoch [13063/20000], Training Loss: 0.0259\n",
            "Epoch [13064/20000], Training Loss: 0.0277\n",
            "Epoch [13065/20000], Training Loss: 0.0275\n",
            "Epoch [13066/20000], Training Loss: 0.0240\n",
            "Epoch [13067/20000], Training Loss: 0.0271\n",
            "Epoch [13068/20000], Training Loss: 0.0270\n",
            "Epoch [13069/20000], Training Loss: 0.0247\n",
            "Epoch [13070/20000], Training Loss: 0.0265\n",
            "Epoch [13071/20000], Training Loss: 0.0275\n",
            "Epoch [13072/20000], Training Loss: 0.0271\n",
            "Epoch [13073/20000], Training Loss: 0.0280\n",
            "Epoch [13074/20000], Training Loss: 0.0275\n",
            "Epoch [13075/20000], Training Loss: 0.0263\n",
            "Epoch [13076/20000], Training Loss: 0.0256\n",
            "Epoch [13077/20000], Training Loss: 0.0270\n",
            "Epoch [13078/20000], Training Loss: 0.0246\n",
            "Epoch [13079/20000], Training Loss: 0.0267\n",
            "Epoch [13080/20000], Training Loss: 0.0268\n",
            "Epoch [13081/20000], Training Loss: 0.0260\n",
            "Epoch [13082/20000], Training Loss: 0.0269\n",
            "Epoch [13083/20000], Training Loss: 0.0261\n",
            "Epoch [13084/20000], Training Loss: 0.0255\n",
            "Epoch [13085/20000], Training Loss: 0.0260\n",
            "Epoch [13086/20000], Training Loss: 0.0269\n",
            "Epoch [13087/20000], Training Loss: 0.0255\n",
            "Epoch [13088/20000], Training Loss: 0.0247\n",
            "Epoch [13089/20000], Training Loss: 0.0242\n",
            "Epoch [13090/20000], Training Loss: 0.0265\n",
            "Epoch [13091/20000], Training Loss: 0.0250\n",
            "Epoch [13092/20000], Training Loss: 0.0253\n",
            "Epoch [13093/20000], Training Loss: 0.0268\n",
            "Epoch [13094/20000], Training Loss: 0.0267\n",
            "Epoch [13095/20000], Training Loss: 0.0266\n",
            "Epoch [13096/20000], Training Loss: 0.0264\n",
            "Epoch [13097/20000], Training Loss: 0.0269\n",
            "Epoch [13098/20000], Training Loss: 0.0259\n",
            "Epoch [13099/20000], Training Loss: 0.0284\n",
            "Epoch [13100/20000], Training Loss: 0.0281\n",
            "Epoch [13101/20000], Training Loss: 0.0276\n",
            "Epoch [13102/20000], Training Loss: 0.0270\n",
            "Epoch [13103/20000], Training Loss: 0.0269\n",
            "Epoch [13104/20000], Training Loss: 0.0267\n",
            "Epoch [13105/20000], Training Loss: 0.0239\n",
            "Epoch [13106/20000], Training Loss: 0.0284\n",
            "Epoch [13107/20000], Training Loss: 0.0257\n",
            "Epoch [13108/20000], Training Loss: 0.0264\n",
            "Epoch [13109/20000], Training Loss: 0.0247\n",
            "Epoch [13110/20000], Training Loss: 0.0276\n",
            "Epoch [13111/20000], Training Loss: 0.0262\n",
            "Epoch [13112/20000], Training Loss: 0.0269\n",
            "Epoch [13113/20000], Training Loss: 0.0277\n",
            "Epoch [13114/20000], Training Loss: 0.0254\n",
            "Epoch [13115/20000], Training Loss: 0.0273\n",
            "Epoch [13116/20000], Training Loss: 0.0274\n",
            "Epoch [13117/20000], Training Loss: 0.0275\n",
            "Epoch [13118/20000], Training Loss: 0.0270\n",
            "Epoch [13119/20000], Training Loss: 0.0266\n",
            "Epoch [13120/20000], Training Loss: 0.0275\n",
            "Epoch [13121/20000], Training Loss: 0.0280\n",
            "Epoch [13122/20000], Training Loss: 0.0250\n",
            "Epoch [13123/20000], Training Loss: 0.0275\n",
            "Epoch [13124/20000], Training Loss: 0.0258\n",
            "Epoch [13125/20000], Training Loss: 0.0258\n",
            "Epoch [13126/20000], Training Loss: 0.0266\n",
            "Epoch [13127/20000], Training Loss: 0.0273\n",
            "Epoch [13128/20000], Training Loss: 0.0298\n",
            "Epoch [13129/20000], Training Loss: 0.0284\n",
            "Epoch [13130/20000], Training Loss: 0.0287\n",
            "Epoch [13131/20000], Training Loss: 0.0263\n",
            "Epoch [13132/20000], Training Loss: 0.0258\n",
            "Epoch [13133/20000], Training Loss: 0.0294\n",
            "Epoch [13134/20000], Training Loss: 0.0257\n",
            "Epoch [13135/20000], Training Loss: 0.0273\n",
            "Epoch [13136/20000], Training Loss: 0.0266\n",
            "Epoch [13137/20000], Training Loss: 0.0247\n",
            "Epoch [13138/20000], Training Loss: 0.0294\n",
            "Epoch [13139/20000], Training Loss: 0.0290\n",
            "Epoch [13140/20000], Training Loss: 0.0263\n",
            "Epoch [13141/20000], Training Loss: 0.0253\n",
            "Epoch [13142/20000], Training Loss: 0.0273\n",
            "Epoch [13143/20000], Training Loss: 0.0266\n",
            "Epoch [13144/20000], Training Loss: 0.0264\n",
            "Epoch [13145/20000], Training Loss: 0.0270\n",
            "Epoch [13146/20000], Training Loss: 0.0269\n",
            "Epoch [13147/20000], Training Loss: 0.0259\n",
            "Epoch [13148/20000], Training Loss: 0.0260\n",
            "Epoch [13149/20000], Training Loss: 0.0271\n",
            "Epoch [13150/20000], Training Loss: 0.0262\n",
            "Epoch [13151/20000], Training Loss: 0.0279\n",
            "Epoch [13152/20000], Training Loss: 0.0248\n",
            "Epoch [13153/20000], Training Loss: 0.0264\n",
            "Epoch [13154/20000], Training Loss: 0.0245\n",
            "Epoch [13155/20000], Training Loss: 0.0266\n",
            "Epoch [13156/20000], Training Loss: 0.0276\n",
            "Epoch [13157/20000], Training Loss: 0.0255\n",
            "Epoch [13158/20000], Training Loss: 0.0278\n",
            "Epoch [13159/20000], Training Loss: 0.0254\n",
            "Epoch [13160/20000], Training Loss: 0.0277\n",
            "Epoch [13161/20000], Training Loss: 0.0268\n",
            "Epoch [13162/20000], Training Loss: 0.0251\n",
            "Epoch [13163/20000], Training Loss: 0.0250\n",
            "Epoch [13164/20000], Training Loss: 0.0245\n",
            "Epoch [13165/20000], Training Loss: 0.0263\n",
            "Epoch [13166/20000], Training Loss: 0.0285\n",
            "Epoch [13167/20000], Training Loss: 0.0262\n",
            "Epoch [13168/20000], Training Loss: 0.0266\n",
            "Epoch [13169/20000], Training Loss: 0.0254\n",
            "Epoch [13170/20000], Training Loss: 0.0270\n",
            "Epoch [13171/20000], Training Loss: 0.0243\n",
            "Epoch [13172/20000], Training Loss: 0.0249\n",
            "Epoch [13173/20000], Training Loss: 0.0257\n",
            "Epoch [13174/20000], Training Loss: 0.0256\n",
            "Epoch [13175/20000], Training Loss: 0.0275\n",
            "Epoch [13176/20000], Training Loss: 0.0269\n",
            "Epoch [13177/20000], Training Loss: 0.0256\n",
            "Epoch [13178/20000], Training Loss: 0.0260\n",
            "Epoch [13179/20000], Training Loss: 0.0265\n",
            "Epoch [13180/20000], Training Loss: 0.0256\n",
            "Epoch [13181/20000], Training Loss: 0.0248\n",
            "Epoch [13182/20000], Training Loss: 0.0267\n",
            "Epoch [13183/20000], Training Loss: 0.0257\n",
            "Epoch [13184/20000], Training Loss: 0.0265\n",
            "Epoch [13185/20000], Training Loss: 0.0269\n",
            "Epoch [13186/20000], Training Loss: 0.0268\n",
            "Epoch [13187/20000], Training Loss: 0.0269\n",
            "Epoch [13188/20000], Training Loss: 0.0280\n",
            "Epoch [13189/20000], Training Loss: 0.0253\n",
            "Epoch [13190/20000], Training Loss: 0.0269\n",
            "Epoch [13191/20000], Training Loss: 0.0284\n",
            "Epoch [13192/20000], Training Loss: 0.0260\n",
            "Epoch [13193/20000], Training Loss: 0.0269\n",
            "Epoch [13194/20000], Training Loss: 0.0266\n",
            "Epoch [13195/20000], Training Loss: 0.0281\n",
            "Epoch [13196/20000], Training Loss: 0.0252\n",
            "Epoch [13197/20000], Training Loss: 0.0274\n",
            "Epoch [13198/20000], Training Loss: 0.0270\n",
            "Epoch [13199/20000], Training Loss: 0.0272\n",
            "Epoch [13200/20000], Training Loss: 0.0280\n",
            "Epoch [13201/20000], Training Loss: 0.0286\n",
            "Epoch [13202/20000], Training Loss: 0.0254\n",
            "Epoch [13203/20000], Training Loss: 0.0250\n",
            "Epoch [13204/20000], Training Loss: 0.0258\n",
            "Epoch [13205/20000], Training Loss: 0.0255\n",
            "Epoch [13206/20000], Training Loss: 0.0275\n",
            "Epoch [13207/20000], Training Loss: 0.0280\n",
            "Epoch [13208/20000], Training Loss: 0.0244\n",
            "Epoch [13209/20000], Training Loss: 0.0277\n",
            "Epoch [13210/20000], Training Loss: 0.0257\n",
            "Epoch [13211/20000], Training Loss: 0.0277\n",
            "Epoch [13212/20000], Training Loss: 0.0237\n",
            "Epoch [13213/20000], Training Loss: 0.0257\n",
            "Epoch [13214/20000], Training Loss: 0.0274\n",
            "Epoch [13215/20000], Training Loss: 0.0248\n",
            "Epoch [13216/20000], Training Loss: 0.0259\n",
            "Epoch [13217/20000], Training Loss: 0.0254\n",
            "Epoch [13218/20000], Training Loss: 0.0289\n",
            "Epoch [13219/20000], Training Loss: 0.0277\n",
            "Epoch [13220/20000], Training Loss: 0.0279\n",
            "Epoch [13221/20000], Training Loss: 0.0267\n",
            "Epoch [13222/20000], Training Loss: 0.0263\n",
            "Epoch [13223/20000], Training Loss: 0.0273\n",
            "Epoch [13224/20000], Training Loss: 0.0266\n",
            "Epoch [13225/20000], Training Loss: 0.0258\n",
            "Epoch [13226/20000], Training Loss: 0.0259\n",
            "Epoch [13227/20000], Training Loss: 0.0269\n",
            "Epoch [13228/20000], Training Loss: 0.0260\n",
            "Epoch [13229/20000], Training Loss: 0.0256\n",
            "Epoch [13230/20000], Training Loss: 0.0273\n",
            "Epoch [13231/20000], Training Loss: 0.0275\n",
            "Epoch [13232/20000], Training Loss: 0.0253\n",
            "Epoch [13233/20000], Training Loss: 0.0269\n",
            "Epoch [13234/20000], Training Loss: 0.0255\n",
            "Epoch [13235/20000], Training Loss: 0.0248\n",
            "Epoch [13236/20000], Training Loss: 0.0249\n",
            "Epoch [13237/20000], Training Loss: 0.0288\n",
            "Epoch [13238/20000], Training Loss: 0.0256\n",
            "Epoch [13239/20000], Training Loss: 0.0272\n",
            "Epoch [13240/20000], Training Loss: 0.0265\n",
            "Epoch [13241/20000], Training Loss: 0.0267\n",
            "Epoch [13242/20000], Training Loss: 0.0274\n",
            "Epoch [13243/20000], Training Loss: 0.0249\n",
            "Epoch [13244/20000], Training Loss: 0.0267\n",
            "Epoch [13245/20000], Training Loss: 0.0279\n",
            "Epoch [13246/20000], Training Loss: 0.0267\n",
            "Epoch [13247/20000], Training Loss: 0.0255\n",
            "Epoch [13248/20000], Training Loss: 0.0277\n",
            "Epoch [13249/20000], Training Loss: 0.0248\n",
            "Epoch [13250/20000], Training Loss: 0.0271\n",
            "Epoch [13251/20000], Training Loss: 0.0278\n",
            "Epoch [13252/20000], Training Loss: 0.0259\n",
            "Epoch [13253/20000], Training Loss: 0.0251\n",
            "Epoch [13254/20000], Training Loss: 0.0279\n",
            "Epoch [13255/20000], Training Loss: 0.0271\n",
            "Epoch [13256/20000], Training Loss: 0.0255\n",
            "Epoch [13257/20000], Training Loss: 0.0285\n",
            "Epoch [13258/20000], Training Loss: 0.0257\n",
            "Epoch [13259/20000], Training Loss: 0.0267\n",
            "Epoch [13260/20000], Training Loss: 0.0271\n",
            "Epoch [13261/20000], Training Loss: 0.0241\n",
            "Epoch [13262/20000], Training Loss: 0.0253\n",
            "Epoch [13263/20000], Training Loss: 0.0251\n",
            "Epoch [13264/20000], Training Loss: 0.0255\n",
            "Epoch [13265/20000], Training Loss: 0.0264\n",
            "Epoch [13266/20000], Training Loss: 0.0275\n",
            "Epoch [13267/20000], Training Loss: 0.0263\n",
            "Epoch [13268/20000], Training Loss: 0.0257\n",
            "Epoch [13269/20000], Training Loss: 0.0282\n",
            "Epoch [13270/20000], Training Loss: 0.0260\n",
            "Epoch [13271/20000], Training Loss: 0.0265\n",
            "Epoch [13272/20000], Training Loss: 0.0250\n",
            "Epoch [13273/20000], Training Loss: 0.0277\n",
            "Epoch [13274/20000], Training Loss: 0.0254\n",
            "Epoch [13275/20000], Training Loss: 0.0262\n",
            "Epoch [13276/20000], Training Loss: 0.0257\n",
            "Epoch [13277/20000], Training Loss: 0.0275\n",
            "Epoch [13278/20000], Training Loss: 0.0253\n",
            "Epoch [13279/20000], Training Loss: 0.0258\n",
            "Epoch [13280/20000], Training Loss: 0.0284\n",
            "Epoch [13281/20000], Training Loss: 0.0260\n",
            "Epoch [13282/20000], Training Loss: 0.0258\n",
            "Epoch [13283/20000], Training Loss: 0.0262\n",
            "Epoch [13284/20000], Training Loss: 0.0278\n",
            "Epoch [13285/20000], Training Loss: 0.0283\n",
            "Epoch [13286/20000], Training Loss: 0.0256\n",
            "Epoch [13287/20000], Training Loss: 0.0252\n",
            "Epoch [13288/20000], Training Loss: 0.0279\n",
            "Epoch [13289/20000], Training Loss: 0.0260\n",
            "Epoch [13290/20000], Training Loss: 0.0269\n",
            "Epoch [13291/20000], Training Loss: 0.0284\n",
            "Epoch [13292/20000], Training Loss: 0.0253\n",
            "Epoch [13293/20000], Training Loss: 0.0258\n",
            "Epoch [13294/20000], Training Loss: 0.0276\n",
            "Epoch [13295/20000], Training Loss: 0.0245\n",
            "Epoch [13296/20000], Training Loss: 0.0267\n",
            "Epoch [13297/20000], Training Loss: 0.0258\n",
            "Epoch [13298/20000], Training Loss: 0.0260\n",
            "Epoch [13299/20000], Training Loss: 0.0274\n",
            "Epoch [13300/20000], Training Loss: 0.0286\n",
            "Epoch [13301/20000], Training Loss: 0.0257\n",
            "Epoch [13302/20000], Training Loss: 0.0257\n",
            "Epoch [13303/20000], Training Loss: 0.0260\n",
            "Epoch [13304/20000], Training Loss: 0.0276\n",
            "Epoch [13305/20000], Training Loss: 0.0284\n",
            "Epoch [13306/20000], Training Loss: 0.0262\n",
            "Epoch [13307/20000], Training Loss: 0.0274\n",
            "Epoch [13308/20000], Training Loss: 0.0256\n",
            "Epoch [13309/20000], Training Loss: 0.0272\n",
            "Epoch [13310/20000], Training Loss: 0.0262\n",
            "Epoch [13311/20000], Training Loss: 0.0257\n",
            "Epoch [13312/20000], Training Loss: 0.0275\n",
            "Epoch [13313/20000], Training Loss: 0.0277\n",
            "Epoch [13314/20000], Training Loss: 0.0266\n",
            "Epoch [13315/20000], Training Loss: 0.0274\n",
            "Epoch [13316/20000], Training Loss: 0.0269\n",
            "Epoch [13317/20000], Training Loss: 0.0275\n",
            "Epoch [13318/20000], Training Loss: 0.0272\n",
            "Epoch [13319/20000], Training Loss: 0.0278\n",
            "Epoch [13320/20000], Training Loss: 0.0261\n",
            "Epoch [13321/20000], Training Loss: 0.0259\n",
            "Epoch [13322/20000], Training Loss: 0.0252\n",
            "Epoch [13323/20000], Training Loss: 0.0258\n",
            "Epoch [13324/20000], Training Loss: 0.0244\n",
            "Epoch [13325/20000], Training Loss: 0.0265\n",
            "Epoch [13326/20000], Training Loss: 0.0282\n",
            "Epoch [13327/20000], Training Loss: 0.0245\n",
            "Epoch [13328/20000], Training Loss: 0.0259\n",
            "Epoch [13329/20000], Training Loss: 0.0246\n",
            "Epoch [13330/20000], Training Loss: 0.0239\n",
            "Epoch [13331/20000], Training Loss: 0.0256\n",
            "Epoch [13332/20000], Training Loss: 0.0247\n",
            "Epoch [13333/20000], Training Loss: 0.0247\n",
            "Epoch [13334/20000], Training Loss: 0.0260\n",
            "Epoch [13335/20000], Training Loss: 0.0272\n",
            "Epoch [13336/20000], Training Loss: 0.0254\n",
            "Epoch [13337/20000], Training Loss: 0.0249\n",
            "Epoch [13338/20000], Training Loss: 0.0280\n",
            "Epoch [13339/20000], Training Loss: 0.0269\n",
            "Epoch [13340/20000], Training Loss: 0.0248\n",
            "Epoch [13341/20000], Training Loss: 0.0257\n",
            "Epoch [13342/20000], Training Loss: 0.0249\n",
            "Epoch [13343/20000], Training Loss: 0.0276\n",
            "Epoch [13344/20000], Training Loss: 0.0266\n",
            "Epoch [13345/20000], Training Loss: 0.0266\n",
            "Epoch [13346/20000], Training Loss: 0.0245\n",
            "Epoch [13347/20000], Training Loss: 0.0269\n",
            "Epoch [13348/20000], Training Loss: 0.0260\n",
            "Epoch [13349/20000], Training Loss: 0.0264\n",
            "Epoch [13350/20000], Training Loss: 0.0252\n",
            "Epoch [13351/20000], Training Loss: 0.0254\n",
            "Epoch [13352/20000], Training Loss: 0.0268\n",
            "Epoch [13353/20000], Training Loss: 0.0275\n",
            "Epoch [13354/20000], Training Loss: 0.0246\n",
            "Epoch [13355/20000], Training Loss: 0.0260\n",
            "Epoch [13356/20000], Training Loss: 0.0262\n",
            "Epoch [13357/20000], Training Loss: 0.0252\n",
            "Epoch [13358/20000], Training Loss: 0.0257\n",
            "Epoch [13359/20000], Training Loss: 0.0248\n",
            "Epoch [13360/20000], Training Loss: 0.0259\n",
            "Epoch [13361/20000], Training Loss: 0.0285\n",
            "Epoch [13362/20000], Training Loss: 0.0256\n",
            "Epoch [13363/20000], Training Loss: 0.0268\n",
            "Epoch [13364/20000], Training Loss: 0.0265\n",
            "Epoch [13365/20000], Training Loss: 0.0265\n",
            "Epoch [13366/20000], Training Loss: 0.0282\n",
            "Epoch [13367/20000], Training Loss: 0.0249\n",
            "Epoch [13368/20000], Training Loss: 0.0257\n",
            "Epoch [13369/20000], Training Loss: 0.0258\n",
            "Epoch [13370/20000], Training Loss: 0.0290\n",
            "Epoch [13371/20000], Training Loss: 0.0282\n",
            "Epoch [13372/20000], Training Loss: 0.0284\n",
            "Epoch [13373/20000], Training Loss: 0.0264\n",
            "Epoch [13374/20000], Training Loss: 0.0269\n",
            "Epoch [13375/20000], Training Loss: 0.0262\n",
            "Epoch [13376/20000], Training Loss: 0.0257\n",
            "Epoch [13377/20000], Training Loss: 0.0267\n",
            "Epoch [13378/20000], Training Loss: 0.0273\n",
            "Epoch [13379/20000], Training Loss: 0.0270\n",
            "Epoch [13380/20000], Training Loss: 0.0281\n",
            "Epoch [13381/20000], Training Loss: 0.0257\n",
            "Epoch [13382/20000], Training Loss: 0.0275\n",
            "Epoch [13383/20000], Training Loss: 0.0238\n",
            "Epoch [13384/20000], Training Loss: 0.0251\n",
            "Epoch [13385/20000], Training Loss: 0.0245\n",
            "Epoch [13386/20000], Training Loss: 0.0260\n",
            "Epoch [13387/20000], Training Loss: 0.0269\n",
            "Epoch [13388/20000], Training Loss: 0.0265\n",
            "Epoch [13389/20000], Training Loss: 0.0256\n",
            "Epoch [13390/20000], Training Loss: 0.0256\n",
            "Epoch [13391/20000], Training Loss: 0.0276\n",
            "Epoch [13392/20000], Training Loss: 0.0257\n",
            "Epoch [13393/20000], Training Loss: 0.0276\n",
            "Epoch [13394/20000], Training Loss: 0.0273\n",
            "Epoch [13395/20000], Training Loss: 0.0258\n",
            "Epoch [13396/20000], Training Loss: 0.0266\n",
            "Epoch [13397/20000], Training Loss: 0.0244\n",
            "Epoch [13398/20000], Training Loss: 0.0278\n",
            "Epoch [13399/20000], Training Loss: 0.0257\n",
            "Epoch [13400/20000], Training Loss: 0.0254\n",
            "Epoch [13401/20000], Training Loss: 0.0258\n",
            "Epoch [13402/20000], Training Loss: 0.0263\n",
            "Epoch [13403/20000], Training Loss: 0.0273\n",
            "Epoch [13404/20000], Training Loss: 0.0274\n",
            "Epoch [13405/20000], Training Loss: 0.0269\n",
            "Epoch [13406/20000], Training Loss: 0.0282\n",
            "Epoch [13407/20000], Training Loss: 0.0273\n",
            "Epoch [13408/20000], Training Loss: 0.0255\n",
            "Epoch [13409/20000], Training Loss: 0.0271\n",
            "Epoch [13410/20000], Training Loss: 0.0286\n",
            "Epoch [13411/20000], Training Loss: 0.0259\n",
            "Epoch [13412/20000], Training Loss: 0.0250\n",
            "Epoch [13413/20000], Training Loss: 0.0253\n",
            "Epoch [13414/20000], Training Loss: 0.0271\n",
            "Epoch [13415/20000], Training Loss: 0.0266\n",
            "Epoch [13416/20000], Training Loss: 0.0248\n",
            "Epoch [13417/20000], Training Loss: 0.0275\n",
            "Epoch [13418/20000], Training Loss: 0.0284\n",
            "Epoch [13419/20000], Training Loss: 0.0281\n",
            "Epoch [13420/20000], Training Loss: 0.0273\n",
            "Epoch [13421/20000], Training Loss: 0.0271\n",
            "Epoch [13422/20000], Training Loss: 0.0261\n",
            "Epoch [13423/20000], Training Loss: 0.0258\n",
            "Epoch [13424/20000], Training Loss: 0.0240\n",
            "Epoch [13425/20000], Training Loss: 0.0253\n",
            "Epoch [13426/20000], Training Loss: 0.0247\n",
            "Epoch [13427/20000], Training Loss: 0.0256\n",
            "Epoch [13428/20000], Training Loss: 0.0269\n",
            "Epoch [13429/20000], Training Loss: 0.0248\n",
            "Epoch [13430/20000], Training Loss: 0.0276\n",
            "Epoch [13431/20000], Training Loss: 0.0292\n",
            "Epoch [13432/20000], Training Loss: 0.0259\n",
            "Epoch [13433/20000], Training Loss: 0.0273\n",
            "Epoch [13434/20000], Training Loss: 0.0251\n",
            "Epoch [13435/20000], Training Loss: 0.0261\n",
            "Epoch [13436/20000], Training Loss: 0.0270\n",
            "Epoch [13437/20000], Training Loss: 0.0274\n",
            "Epoch [13438/20000], Training Loss: 0.0271\n",
            "Epoch [13439/20000], Training Loss: 0.0247\n",
            "Epoch [13440/20000], Training Loss: 0.0269\n",
            "Epoch [13441/20000], Training Loss: 0.0279\n",
            "Epoch [13442/20000], Training Loss: 0.0251\n",
            "Epoch [13443/20000], Training Loss: 0.0261\n",
            "Epoch [13444/20000], Training Loss: 0.0251\n",
            "Epoch [13445/20000], Training Loss: 0.0279\n",
            "Epoch [13446/20000], Training Loss: 0.0274\n",
            "Epoch [13447/20000], Training Loss: 0.0261\n",
            "Epoch [13448/20000], Training Loss: 0.0257\n",
            "Epoch [13449/20000], Training Loss: 0.0268\n",
            "Epoch [13450/20000], Training Loss: 0.0243\n",
            "Epoch [13451/20000], Training Loss: 0.0260\n",
            "Epoch [13452/20000], Training Loss: 0.0247\n",
            "Epoch [13453/20000], Training Loss: 0.0255\n",
            "Epoch [13454/20000], Training Loss: 0.0250\n",
            "Epoch [13455/20000], Training Loss: 0.0257\n",
            "Epoch [13456/20000], Training Loss: 0.0249\n",
            "Epoch [13457/20000], Training Loss: 0.0245\n",
            "Epoch [13458/20000], Training Loss: 0.0267\n",
            "Epoch [13459/20000], Training Loss: 0.0284\n",
            "Epoch [13460/20000], Training Loss: 0.0259\n",
            "Epoch [13461/20000], Training Loss: 0.0239\n",
            "Epoch [13462/20000], Training Loss: 0.0270\n",
            "Epoch [13463/20000], Training Loss: 0.0275\n",
            "Epoch [13464/20000], Training Loss: 0.0262\n",
            "Epoch [13465/20000], Training Loss: 0.0240\n",
            "Epoch [13466/20000], Training Loss: 0.0256\n",
            "Epoch [13467/20000], Training Loss: 0.0264\n",
            "Epoch [13468/20000], Training Loss: 0.0278\n",
            "Epoch [13469/20000], Training Loss: 0.0255\n",
            "Epoch [13470/20000], Training Loss: 0.0275\n",
            "Epoch [13471/20000], Training Loss: 0.0271\n",
            "Epoch [13472/20000], Training Loss: 0.0266\n",
            "Epoch [13473/20000], Training Loss: 0.0233\n",
            "Epoch [13474/20000], Training Loss: 0.0273\n",
            "Epoch [13475/20000], Training Loss: 0.0251\n",
            "Epoch [13476/20000], Training Loss: 0.0265\n",
            "Epoch [13477/20000], Training Loss: 0.0269\n",
            "Epoch [13478/20000], Training Loss: 0.0267\n",
            "Epoch [13479/20000], Training Loss: 0.0257\n",
            "Epoch [13480/20000], Training Loss: 0.0261\n",
            "Epoch [13481/20000], Training Loss: 0.0254\n",
            "Epoch [13482/20000], Training Loss: 0.0268\n",
            "Epoch [13483/20000], Training Loss: 0.0257\n",
            "Epoch [13484/20000], Training Loss: 0.0258\n",
            "Epoch [13485/20000], Training Loss: 0.0262\n",
            "Epoch [13486/20000], Training Loss: 0.0279\n",
            "Epoch [13487/20000], Training Loss: 0.0281\n",
            "Epoch [13488/20000], Training Loss: 0.0277\n",
            "Epoch [13489/20000], Training Loss: 0.0258\n",
            "Epoch [13490/20000], Training Loss: 0.0248\n",
            "Epoch [13491/20000], Training Loss: 0.0264\n",
            "Epoch [13492/20000], Training Loss: 0.0274\n",
            "Epoch [13493/20000], Training Loss: 0.0274\n",
            "Epoch [13494/20000], Training Loss: 0.0264\n",
            "Epoch [13495/20000], Training Loss: 0.0255\n",
            "Epoch [13496/20000], Training Loss: 0.0261\n",
            "Epoch [13497/20000], Training Loss: 0.0270\n",
            "Epoch [13498/20000], Training Loss: 0.0283\n",
            "Epoch [13499/20000], Training Loss: 0.0263\n",
            "Epoch [13500/20000], Training Loss: 0.0256\n",
            "Epoch [13501/20000], Training Loss: 0.0250\n",
            "Epoch [13502/20000], Training Loss: 0.0251\n",
            "Epoch [13503/20000], Training Loss: 0.0284\n",
            "Epoch [13504/20000], Training Loss: 0.0241\n",
            "Epoch [13505/20000], Training Loss: 0.0289\n",
            "Epoch [13506/20000], Training Loss: 0.0277\n",
            "Epoch [13507/20000], Training Loss: 0.0267\n",
            "Epoch [13508/20000], Training Loss: 0.0271\n",
            "Epoch [13509/20000], Training Loss: 0.0275\n",
            "Epoch [13510/20000], Training Loss: 0.0262\n",
            "Epoch [13511/20000], Training Loss: 0.0279\n",
            "Epoch [13512/20000], Training Loss: 0.0283\n",
            "Epoch [13513/20000], Training Loss: 0.0281\n",
            "Epoch [13514/20000], Training Loss: 0.0264\n",
            "Epoch [13515/20000], Training Loss: 0.0251\n",
            "Epoch [13516/20000], Training Loss: 0.0238\n",
            "Epoch [13517/20000], Training Loss: 0.0256\n",
            "Epoch [13518/20000], Training Loss: 0.0254\n",
            "Epoch [13519/20000], Training Loss: 0.0258\n",
            "Epoch [13520/20000], Training Loss: 0.0249\n",
            "Epoch [13521/20000], Training Loss: 0.0258\n",
            "Epoch [13522/20000], Training Loss: 0.0271\n",
            "Epoch [13523/20000], Training Loss: 0.0277\n",
            "Epoch [13524/20000], Training Loss: 0.0247\n",
            "Epoch [13525/20000], Training Loss: 0.0271\n",
            "Epoch [13526/20000], Training Loss: 0.0280\n",
            "Epoch [13527/20000], Training Loss: 0.0276\n",
            "Epoch [13528/20000], Training Loss: 0.0244\n",
            "Epoch [13529/20000], Training Loss: 0.0270\n",
            "Epoch [13530/20000], Training Loss: 0.0274\n",
            "Epoch [13531/20000], Training Loss: 0.0264\n",
            "Epoch [13532/20000], Training Loss: 0.0272\n",
            "Epoch [13533/20000], Training Loss: 0.0269\n",
            "Epoch [13534/20000], Training Loss: 0.0280\n",
            "Epoch [13535/20000], Training Loss: 0.0275\n",
            "Epoch [13536/20000], Training Loss: 0.0246\n",
            "Epoch [13537/20000], Training Loss: 0.0252\n",
            "Epoch [13538/20000], Training Loss: 0.0246\n",
            "Epoch [13539/20000], Training Loss: 0.0282\n",
            "Epoch [13540/20000], Training Loss: 0.0269\n",
            "Epoch [13541/20000], Training Loss: 0.0254\n",
            "Epoch [13542/20000], Training Loss: 0.0261\n",
            "Epoch [13543/20000], Training Loss: 0.0264\n",
            "Epoch [13544/20000], Training Loss: 0.0258\n",
            "Epoch [13545/20000], Training Loss: 0.0274\n",
            "Epoch [13546/20000], Training Loss: 0.0265\n",
            "Epoch [13547/20000], Training Loss: 0.0257\n",
            "Epoch [13548/20000], Training Loss: 0.0270\n",
            "Epoch [13549/20000], Training Loss: 0.0253\n",
            "Epoch [13550/20000], Training Loss: 0.0258\n",
            "Epoch [13551/20000], Training Loss: 0.0256\n",
            "Epoch [13552/20000], Training Loss: 0.0264\n",
            "Epoch [13553/20000], Training Loss: 0.0262\n",
            "Epoch [13554/20000], Training Loss: 0.0262\n",
            "Epoch [13555/20000], Training Loss: 0.0258\n",
            "Epoch [13556/20000], Training Loss: 0.0270\n",
            "Epoch [13557/20000], Training Loss: 0.0281\n",
            "Epoch [13558/20000], Training Loss: 0.0280\n",
            "Epoch [13559/20000], Training Loss: 0.0264\n",
            "Epoch [13560/20000], Training Loss: 0.0263\n",
            "Epoch [13561/20000], Training Loss: 0.0256\n",
            "Epoch [13562/20000], Training Loss: 0.0253\n",
            "Epoch [13563/20000], Training Loss: 0.0253\n",
            "Epoch [13564/20000], Training Loss: 0.0262\n",
            "Epoch [13565/20000], Training Loss: 0.0249\n",
            "Epoch [13566/20000], Training Loss: 0.0289\n",
            "Epoch [13567/20000], Training Loss: 0.0272\n",
            "Epoch [13568/20000], Training Loss: 0.0281\n",
            "Epoch [13569/20000], Training Loss: 0.0245\n",
            "Epoch [13570/20000], Training Loss: 0.0273\n",
            "Epoch [13571/20000], Training Loss: 0.0271\n",
            "Epoch [13572/20000], Training Loss: 0.0281\n",
            "Epoch [13573/20000], Training Loss: 0.0268\n",
            "Epoch [13574/20000], Training Loss: 0.0283\n",
            "Epoch [13575/20000], Training Loss: 0.0267\n",
            "Epoch [13576/20000], Training Loss: 0.0258\n",
            "Epoch [13577/20000], Training Loss: 0.0276\n",
            "Epoch [13578/20000], Training Loss: 0.0261\n",
            "Epoch [13579/20000], Training Loss: 0.0270\n",
            "Epoch [13580/20000], Training Loss: 0.0258\n",
            "Epoch [13581/20000], Training Loss: 0.0264\n",
            "Epoch [13582/20000], Training Loss: 0.0272\n",
            "Epoch [13583/20000], Training Loss: 0.0264\n",
            "Epoch [13584/20000], Training Loss: 0.0253\n",
            "Epoch [13585/20000], Training Loss: 0.0268\n",
            "Epoch [13586/20000], Training Loss: 0.0276\n",
            "Epoch [13587/20000], Training Loss: 0.0271\n",
            "Epoch [13588/20000], Training Loss: 0.0242\n",
            "Epoch [13589/20000], Training Loss: 0.0255\n",
            "Epoch [13590/20000], Training Loss: 0.0249\n",
            "Epoch [13591/20000], Training Loss: 0.0255\n",
            "Epoch [13592/20000], Training Loss: 0.0270\n",
            "Epoch [13593/20000], Training Loss: 0.0239\n",
            "Epoch [13594/20000], Training Loss: 0.0247\n",
            "Epoch [13595/20000], Training Loss: 0.0268\n",
            "Epoch [13596/20000], Training Loss: 0.0246\n",
            "Epoch [13597/20000], Training Loss: 0.0272\n",
            "Epoch [13598/20000], Training Loss: 0.0259\n",
            "Epoch [13599/20000], Training Loss: 0.0283\n",
            "Epoch [13600/20000], Training Loss: 0.0259\n",
            "Epoch [13601/20000], Training Loss: 0.0250\n",
            "Epoch [13602/20000], Training Loss: 0.0252\n",
            "Epoch [13603/20000], Training Loss: 0.0255\n",
            "Epoch [13604/20000], Training Loss: 0.0245\n",
            "Epoch [13605/20000], Training Loss: 0.0287\n",
            "Epoch [13606/20000], Training Loss: 0.0257\n",
            "Epoch [13607/20000], Training Loss: 0.0252\n",
            "Epoch [13608/20000], Training Loss: 0.0255\n",
            "Epoch [13609/20000], Training Loss: 0.0252\n",
            "Epoch [13610/20000], Training Loss: 0.0268\n",
            "Epoch [13611/20000], Training Loss: 0.0248\n",
            "Epoch [13612/20000], Training Loss: 0.0261\n",
            "Epoch [13613/20000], Training Loss: 0.0254\n",
            "Epoch [13614/20000], Training Loss: 0.0267\n",
            "Epoch [13615/20000], Training Loss: 0.0266\n",
            "Epoch [13616/20000], Training Loss: 0.0249\n",
            "Epoch [13617/20000], Training Loss: 0.0268\n",
            "Epoch [13618/20000], Training Loss: 0.0275\n",
            "Epoch [13619/20000], Training Loss: 0.0251\n",
            "Epoch [13620/20000], Training Loss: 0.0248\n",
            "Epoch [13621/20000], Training Loss: 0.0281\n",
            "Epoch [13622/20000], Training Loss: 0.0265\n",
            "Epoch [13623/20000], Training Loss: 0.0260\n",
            "Epoch [13624/20000], Training Loss: 0.0282\n",
            "Epoch [13625/20000], Training Loss: 0.0258\n",
            "Epoch [13626/20000], Training Loss: 0.0270\n",
            "Epoch [13627/20000], Training Loss: 0.0252\n",
            "Epoch [13628/20000], Training Loss: 0.0250\n",
            "Epoch [13629/20000], Training Loss: 0.0260\n",
            "Epoch [13630/20000], Training Loss: 0.0259\n",
            "Epoch [13631/20000], Training Loss: 0.0275\n",
            "Epoch [13632/20000], Training Loss: 0.0270\n",
            "Epoch [13633/20000], Training Loss: 0.0271\n",
            "Epoch [13634/20000], Training Loss: 0.0258\n",
            "Epoch [13635/20000], Training Loss: 0.0275\n",
            "Epoch [13636/20000], Training Loss: 0.0278\n",
            "Epoch [13637/20000], Training Loss: 0.0278\n",
            "Epoch [13638/20000], Training Loss: 0.0249\n",
            "Epoch [13639/20000], Training Loss: 0.0270\n",
            "Epoch [13640/20000], Training Loss: 0.0238\n",
            "Epoch [13641/20000], Training Loss: 0.0260\n",
            "Epoch [13642/20000], Training Loss: 0.0260\n",
            "Epoch [13643/20000], Training Loss: 0.0271\n",
            "Epoch [13644/20000], Training Loss: 0.0263\n",
            "Epoch [13645/20000], Training Loss: 0.0247\n",
            "Epoch [13646/20000], Training Loss: 0.0268\n",
            "Epoch [13647/20000], Training Loss: 0.0288\n",
            "Epoch [13648/20000], Training Loss: 0.0253\n",
            "Epoch [13649/20000], Training Loss: 0.0261\n",
            "Epoch [13650/20000], Training Loss: 0.0277\n",
            "Epoch [13651/20000], Training Loss: 0.0279\n",
            "Epoch [13652/20000], Training Loss: 0.0278\n",
            "Epoch [13653/20000], Training Loss: 0.0272\n",
            "Epoch [13654/20000], Training Loss: 0.0263\n",
            "Epoch [13655/20000], Training Loss: 0.0259\n",
            "Epoch [13656/20000], Training Loss: 0.0261\n",
            "Epoch [13657/20000], Training Loss: 0.0263\n",
            "Epoch [13658/20000], Training Loss: 0.0266\n",
            "Epoch [13659/20000], Training Loss: 0.0264\n",
            "Epoch [13660/20000], Training Loss: 0.0258\n",
            "Epoch [13661/20000], Training Loss: 0.0258\n",
            "Epoch [13662/20000], Training Loss: 0.0264\n",
            "Epoch [13663/20000], Training Loss: 0.0281\n",
            "Epoch [13664/20000], Training Loss: 0.0250\n",
            "Epoch [13665/20000], Training Loss: 0.0278\n",
            "Epoch [13666/20000], Training Loss: 0.0263\n",
            "Epoch [13667/20000], Training Loss: 0.0258\n",
            "Epoch [13668/20000], Training Loss: 0.0261\n",
            "Epoch [13669/20000], Training Loss: 0.0254\n",
            "Epoch [13670/20000], Training Loss: 0.0272\n",
            "Epoch [13671/20000], Training Loss: 0.0252\n",
            "Epoch [13672/20000], Training Loss: 0.0285\n",
            "Epoch [13673/20000], Training Loss: 0.0244\n",
            "Epoch [13674/20000], Training Loss: 0.0249\n",
            "Epoch [13675/20000], Training Loss: 0.0264\n",
            "Epoch [13676/20000], Training Loss: 0.0262\n",
            "Epoch [13677/20000], Training Loss: 0.0275\n",
            "Epoch [13678/20000], Training Loss: 0.0247\n",
            "Epoch [13679/20000], Training Loss: 0.0266\n",
            "Epoch [13680/20000], Training Loss: 0.0266\n",
            "Epoch [13681/20000], Training Loss: 0.0269\n",
            "Epoch [13682/20000], Training Loss: 0.0259\n",
            "Epoch [13683/20000], Training Loss: 0.0269\n",
            "Epoch [13684/20000], Training Loss: 0.0252\n",
            "Epoch [13685/20000], Training Loss: 0.0247\n",
            "Epoch [13686/20000], Training Loss: 0.0267\n",
            "Epoch [13687/20000], Training Loss: 0.0253\n",
            "Epoch [13688/20000], Training Loss: 0.0246\n",
            "Epoch [13689/20000], Training Loss: 0.0279\n",
            "Epoch [13690/20000], Training Loss: 0.0265\n",
            "Epoch [13691/20000], Training Loss: 0.0279\n",
            "Epoch [13692/20000], Training Loss: 0.0259\n",
            "Epoch [13693/20000], Training Loss: 0.0253\n",
            "Epoch [13694/20000], Training Loss: 0.0260\n",
            "Epoch [13695/20000], Training Loss: 0.0261\n",
            "Epoch [13696/20000], Training Loss: 0.0283\n",
            "Epoch [13697/20000], Training Loss: 0.0260\n",
            "Epoch [13698/20000], Training Loss: 0.0256\n",
            "Epoch [13699/20000], Training Loss: 0.0268\n",
            "Epoch [13700/20000], Training Loss: 0.0267\n",
            "Epoch [13701/20000], Training Loss: 0.0261\n",
            "Epoch [13702/20000], Training Loss: 0.0272\n",
            "Epoch [13703/20000], Training Loss: 0.0269\n",
            "Epoch [13704/20000], Training Loss: 0.0275\n",
            "Epoch [13705/20000], Training Loss: 0.0257\n",
            "Epoch [13706/20000], Training Loss: 0.0268\n",
            "Epoch [13707/20000], Training Loss: 0.0251\n",
            "Epoch [13708/20000], Training Loss: 0.0277\n",
            "Epoch [13709/20000], Training Loss: 0.0260\n",
            "Epoch [13710/20000], Training Loss: 0.0251\n",
            "Epoch [13711/20000], Training Loss: 0.0269\n",
            "Epoch [13712/20000], Training Loss: 0.0263\n",
            "Epoch [13713/20000], Training Loss: 0.0262\n",
            "Epoch [13714/20000], Training Loss: 0.0292\n",
            "Epoch [13715/20000], Training Loss: 0.0253\n",
            "Epoch [13716/20000], Training Loss: 0.0266\n",
            "Epoch [13717/20000], Training Loss: 0.0261\n",
            "Epoch [13718/20000], Training Loss: 0.0268\n",
            "Epoch [13719/20000], Training Loss: 0.0271\n",
            "Epoch [13720/20000], Training Loss: 0.0275\n",
            "Epoch [13721/20000], Training Loss: 0.0272\n",
            "Epoch [13722/20000], Training Loss: 0.0266\n",
            "Epoch [13723/20000], Training Loss: 0.0264\n",
            "Epoch [13724/20000], Training Loss: 0.0265\n",
            "Epoch [13725/20000], Training Loss: 0.0260\n",
            "Epoch [13726/20000], Training Loss: 0.0248\n",
            "Epoch [13727/20000], Training Loss: 0.0279\n",
            "Epoch [13728/20000], Training Loss: 0.0268\n",
            "Epoch [13729/20000], Training Loss: 0.0251\n",
            "Epoch [13730/20000], Training Loss: 0.0250\n",
            "Epoch [13731/20000], Training Loss: 0.0282\n",
            "Epoch [13732/20000], Training Loss: 0.0257\n",
            "Epoch [13733/20000], Training Loss: 0.0264\n",
            "Epoch [13734/20000], Training Loss: 0.0258\n",
            "Epoch [13735/20000], Training Loss: 0.0278\n",
            "Epoch [13736/20000], Training Loss: 0.0282\n",
            "Epoch [13737/20000], Training Loss: 0.0278\n",
            "Epoch [13738/20000], Training Loss: 0.0264\n",
            "Epoch [13739/20000], Training Loss: 0.0266\n",
            "Epoch [13740/20000], Training Loss: 0.0274\n",
            "Epoch [13741/20000], Training Loss: 0.0260\n",
            "Epoch [13742/20000], Training Loss: 0.0253\n",
            "Epoch [13743/20000], Training Loss: 0.0270\n",
            "Epoch [13744/20000], Training Loss: 0.0250\n",
            "Epoch [13745/20000], Training Loss: 0.0275\n",
            "Epoch [13746/20000], Training Loss: 0.0247\n",
            "Epoch [13747/20000], Training Loss: 0.0274\n",
            "Epoch [13748/20000], Training Loss: 0.0278\n",
            "Epoch [13749/20000], Training Loss: 0.0266\n",
            "Epoch [13750/20000], Training Loss: 0.0292\n",
            "Epoch [13751/20000], Training Loss: 0.0259\n",
            "Epoch [13752/20000], Training Loss: 0.0291\n",
            "Epoch [13753/20000], Training Loss: 0.0295\n",
            "Epoch [13754/20000], Training Loss: 0.0241\n",
            "Epoch [13755/20000], Training Loss: 0.0271\n",
            "Epoch [13756/20000], Training Loss: 0.0269\n",
            "Epoch [13757/20000], Training Loss: 0.0273\n",
            "Epoch [13758/20000], Training Loss: 0.0269\n",
            "Epoch [13759/20000], Training Loss: 0.0276\n",
            "Epoch [13760/20000], Training Loss: 0.0249\n",
            "Epoch [13761/20000], Training Loss: 0.0284\n",
            "Epoch [13762/20000], Training Loss: 0.0284\n",
            "Epoch [13763/20000], Training Loss: 0.0267\n",
            "Epoch [13764/20000], Training Loss: 0.0250\n",
            "Epoch [13765/20000], Training Loss: 0.0276\n",
            "Epoch [13766/20000], Training Loss: 0.0281\n",
            "Epoch [13767/20000], Training Loss: 0.0251\n",
            "Epoch [13768/20000], Training Loss: 0.0277\n",
            "Epoch [13769/20000], Training Loss: 0.0262\n",
            "Epoch [13770/20000], Training Loss: 0.0276\n",
            "Epoch [13771/20000], Training Loss: 0.0264\n",
            "Epoch [13772/20000], Training Loss: 0.0245\n",
            "Epoch [13773/20000], Training Loss: 0.0237\n",
            "Epoch [13774/20000], Training Loss: 0.0265\n",
            "Epoch [13775/20000], Training Loss: 0.0286\n",
            "Epoch [13776/20000], Training Loss: 0.0268\n",
            "Epoch [13777/20000], Training Loss: 0.0271\n",
            "Epoch [13778/20000], Training Loss: 0.0269\n",
            "Epoch [13779/20000], Training Loss: 0.0270\n",
            "Epoch [13780/20000], Training Loss: 0.0261\n",
            "Epoch [13781/20000], Training Loss: 0.0245\n",
            "Epoch [13782/20000], Training Loss: 0.0272\n",
            "Epoch [13783/20000], Training Loss: 0.0272\n",
            "Epoch [13784/20000], Training Loss: 0.0275\n",
            "Epoch [13785/20000], Training Loss: 0.0277\n",
            "Epoch [13786/20000], Training Loss: 0.0260\n",
            "Epoch [13787/20000], Training Loss: 0.0272\n",
            "Epoch [13788/20000], Training Loss: 0.0283\n",
            "Epoch [13789/20000], Training Loss: 0.0286\n",
            "Epoch [13790/20000], Training Loss: 0.0256\n",
            "Epoch [13791/20000], Training Loss: 0.0271\n",
            "Epoch [13792/20000], Training Loss: 0.0282\n",
            "Epoch [13793/20000], Training Loss: 0.0260\n",
            "Epoch [13794/20000], Training Loss: 0.0260\n",
            "Epoch [13795/20000], Training Loss: 0.0256\n",
            "Epoch [13796/20000], Training Loss: 0.0245\n",
            "Epoch [13797/20000], Training Loss: 0.0286\n",
            "Epoch [13798/20000], Training Loss: 0.0270\n",
            "Epoch [13799/20000], Training Loss: 0.0259\n",
            "Epoch [13800/20000], Training Loss: 0.0269\n",
            "Epoch [13801/20000], Training Loss: 0.0279\n",
            "Epoch [13802/20000], Training Loss: 0.0266\n",
            "Epoch [13803/20000], Training Loss: 0.0268\n",
            "Epoch [13804/20000], Training Loss: 0.0278\n",
            "Epoch [13805/20000], Training Loss: 0.0253\n",
            "Epoch [13806/20000], Training Loss: 0.0264\n",
            "Epoch [13807/20000], Training Loss: 0.0260\n",
            "Epoch [13808/20000], Training Loss: 0.0260\n",
            "Epoch [13809/20000], Training Loss: 0.0270\n",
            "Epoch [13810/20000], Training Loss: 0.0267\n",
            "Epoch [13811/20000], Training Loss: 0.0255\n",
            "Epoch [13812/20000], Training Loss: 0.0278\n",
            "Epoch [13813/20000], Training Loss: 0.0268\n",
            "Epoch [13814/20000], Training Loss: 0.0258\n",
            "Epoch [13815/20000], Training Loss: 0.0287\n",
            "Epoch [13816/20000], Training Loss: 0.0276\n",
            "Epoch [13817/20000], Training Loss: 0.0255\n",
            "Epoch [13818/20000], Training Loss: 0.0279\n",
            "Epoch [13819/20000], Training Loss: 0.0286\n",
            "Epoch [13820/20000], Training Loss: 0.0251\n",
            "Epoch [13821/20000], Training Loss: 0.0272\n",
            "Epoch [13822/20000], Training Loss: 0.0261\n",
            "Epoch [13823/20000], Training Loss: 0.0257\n",
            "Epoch [13824/20000], Training Loss: 0.0255\n",
            "Epoch [13825/20000], Training Loss: 0.0243\n",
            "Epoch [13826/20000], Training Loss: 0.0243\n",
            "Epoch [13827/20000], Training Loss: 0.0255\n",
            "Epoch [13828/20000], Training Loss: 0.0259\n",
            "Epoch [13829/20000], Training Loss: 0.0280\n",
            "Epoch [13830/20000], Training Loss: 0.0280\n",
            "Epoch [13831/20000], Training Loss: 0.0270\n",
            "Epoch [13832/20000], Training Loss: 0.0254\n",
            "Epoch [13833/20000], Training Loss: 0.0265\n",
            "Epoch [13834/20000], Training Loss: 0.0245\n",
            "Epoch [13835/20000], Training Loss: 0.0260\n",
            "Epoch [13836/20000], Training Loss: 0.0261\n",
            "Epoch [13837/20000], Training Loss: 0.0254\n",
            "Epoch [13838/20000], Training Loss: 0.0263\n",
            "Epoch [13839/20000], Training Loss: 0.0244\n",
            "Epoch [13840/20000], Training Loss: 0.0287\n",
            "Epoch [13841/20000], Training Loss: 0.0249\n",
            "Epoch [13842/20000], Training Loss: 0.0263\n",
            "Epoch [13843/20000], Training Loss: 0.0250\n",
            "Epoch [13844/20000], Training Loss: 0.0246\n",
            "Epoch [13845/20000], Training Loss: 0.0268\n",
            "Epoch [13846/20000], Training Loss: 0.0263\n",
            "Epoch [13847/20000], Training Loss: 0.0269\n",
            "Epoch [13848/20000], Training Loss: 0.0284\n",
            "Epoch [13849/20000], Training Loss: 0.0247\n",
            "Epoch [13850/20000], Training Loss: 0.0268\n",
            "Epoch [13851/20000], Training Loss: 0.0250\n",
            "Epoch [13852/20000], Training Loss: 0.0258\n",
            "Epoch [13853/20000], Training Loss: 0.0264\n",
            "Epoch [13854/20000], Training Loss: 0.0267\n",
            "Epoch [13855/20000], Training Loss: 0.0268\n",
            "Epoch [13856/20000], Training Loss: 0.0274\n",
            "Epoch [13857/20000], Training Loss: 0.0247\n",
            "Epoch [13858/20000], Training Loss: 0.0260\n",
            "Epoch [13859/20000], Training Loss: 0.0260\n",
            "Epoch [13860/20000], Training Loss: 0.0259\n",
            "Epoch [13861/20000], Training Loss: 0.0263\n",
            "Epoch [13862/20000], Training Loss: 0.0255\n",
            "Epoch [13863/20000], Training Loss: 0.0277\n",
            "Epoch [13864/20000], Training Loss: 0.0245\n",
            "Epoch [13865/20000], Training Loss: 0.0246\n",
            "Epoch [13866/20000], Training Loss: 0.0283\n",
            "Epoch [13867/20000], Training Loss: 0.0243\n",
            "Epoch [13868/20000], Training Loss: 0.0265\n",
            "Epoch [13869/20000], Training Loss: 0.0269\n",
            "Epoch [13870/20000], Training Loss: 0.0256\n",
            "Epoch [13871/20000], Training Loss: 0.0271\n",
            "Epoch [13872/20000], Training Loss: 0.0279\n",
            "Epoch [13873/20000], Training Loss: 0.0270\n",
            "Epoch [13874/20000], Training Loss: 0.0262\n",
            "Epoch [13875/20000], Training Loss: 0.0258\n",
            "Epoch [13876/20000], Training Loss: 0.0281\n",
            "Epoch [13877/20000], Training Loss: 0.0261\n",
            "Epoch [13878/20000], Training Loss: 0.0256\n",
            "Epoch [13879/20000], Training Loss: 0.0261\n",
            "Epoch [13880/20000], Training Loss: 0.0259\n",
            "Epoch [13881/20000], Training Loss: 0.0259\n",
            "Epoch [13882/20000], Training Loss: 0.0271\n",
            "Epoch [13883/20000], Training Loss: 0.0270\n",
            "Epoch [13884/20000], Training Loss: 0.0256\n",
            "Epoch [13885/20000], Training Loss: 0.0271\n",
            "Epoch [13886/20000], Training Loss: 0.0278\n",
            "Epoch [13887/20000], Training Loss: 0.0242\n",
            "Epoch [13888/20000], Training Loss: 0.0263\n",
            "Epoch [13889/20000], Training Loss: 0.0260\n",
            "Epoch [13890/20000], Training Loss: 0.0274\n",
            "Epoch [13891/20000], Training Loss: 0.0274\n",
            "Epoch [13892/20000], Training Loss: 0.0275\n",
            "Epoch [13893/20000], Training Loss: 0.0252\n",
            "Epoch [13894/20000], Training Loss: 0.0249\n",
            "Epoch [13895/20000], Training Loss: 0.0275\n",
            "Epoch [13896/20000], Training Loss: 0.0259\n",
            "Epoch [13897/20000], Training Loss: 0.0283\n",
            "Epoch [13898/20000], Training Loss: 0.0267\n",
            "Epoch [13899/20000], Training Loss: 0.0266\n",
            "Epoch [13900/20000], Training Loss: 0.0277\n",
            "Epoch [13901/20000], Training Loss: 0.0265\n",
            "Epoch [13902/20000], Training Loss: 0.0265\n",
            "Epoch [13903/20000], Training Loss: 0.0269\n",
            "Epoch [13904/20000], Training Loss: 0.0249\n",
            "Epoch [13905/20000], Training Loss: 0.0262\n",
            "Epoch [13906/20000], Training Loss: 0.0271\n",
            "Epoch [13907/20000], Training Loss: 0.0266\n",
            "Epoch [13908/20000], Training Loss: 0.0250\n",
            "Epoch [13909/20000], Training Loss: 0.0267\n",
            "Epoch [13910/20000], Training Loss: 0.0257\n",
            "Epoch [13911/20000], Training Loss: 0.0282\n",
            "Epoch [13912/20000], Training Loss: 0.0256\n",
            "Epoch [13913/20000], Training Loss: 0.0266\n",
            "Epoch [13914/20000], Training Loss: 0.0245\n",
            "Epoch [13915/20000], Training Loss: 0.0280\n",
            "Epoch [13916/20000], Training Loss: 0.0270\n",
            "Epoch [13917/20000], Training Loss: 0.0265\n",
            "Epoch [13918/20000], Training Loss: 0.0266\n",
            "Epoch [13919/20000], Training Loss: 0.0253\n",
            "Epoch [13920/20000], Training Loss: 0.0258\n",
            "Epoch [13921/20000], Training Loss: 0.0260\n",
            "Epoch [13922/20000], Training Loss: 0.0269\n",
            "Epoch [13923/20000], Training Loss: 0.0271\n",
            "Epoch [13924/20000], Training Loss: 0.0268\n",
            "Epoch [13925/20000], Training Loss: 0.0257\n",
            "Epoch [13926/20000], Training Loss: 0.0275\n",
            "Epoch [13927/20000], Training Loss: 0.0244\n",
            "Epoch [13928/20000], Training Loss: 0.0260\n",
            "Epoch [13929/20000], Training Loss: 0.0276\n",
            "Epoch [13930/20000], Training Loss: 0.0262\n",
            "Epoch [13931/20000], Training Loss: 0.0252\n",
            "Epoch [13932/20000], Training Loss: 0.0271\n",
            "Epoch [13933/20000], Training Loss: 0.0265\n",
            "Epoch [13934/20000], Training Loss: 0.0263\n",
            "Epoch [13935/20000], Training Loss: 0.0267\n",
            "Epoch [13936/20000], Training Loss: 0.0256\n",
            "Epoch [13937/20000], Training Loss: 0.0249\n",
            "Epoch [13938/20000], Training Loss: 0.0265\n",
            "Epoch [13939/20000], Training Loss: 0.0273\n",
            "Epoch [13940/20000], Training Loss: 0.0262\n",
            "Epoch [13941/20000], Training Loss: 0.0253\n",
            "Epoch [13942/20000], Training Loss: 0.0253\n",
            "Epoch [13943/20000], Training Loss: 0.0261\n",
            "Epoch [13944/20000], Training Loss: 0.0257\n",
            "Epoch [13945/20000], Training Loss: 0.0265\n",
            "Epoch [13946/20000], Training Loss: 0.0290\n",
            "Epoch [13947/20000], Training Loss: 0.0273\n",
            "Epoch [13948/20000], Training Loss: 0.0289\n",
            "Epoch [13949/20000], Training Loss: 0.0265\n",
            "Epoch [13950/20000], Training Loss: 0.0274\n",
            "Epoch [13951/20000], Training Loss: 0.0247\n",
            "Epoch [13952/20000], Training Loss: 0.0247\n",
            "Epoch [13953/20000], Training Loss: 0.0284\n",
            "Epoch [13954/20000], Training Loss: 0.0272\n",
            "Epoch [13955/20000], Training Loss: 0.0257\n",
            "Epoch [13956/20000], Training Loss: 0.0245\n",
            "Epoch [13957/20000], Training Loss: 0.0254\n",
            "Epoch [13958/20000], Training Loss: 0.0289\n",
            "Epoch [13959/20000], Training Loss: 0.0287\n",
            "Epoch [13960/20000], Training Loss: 0.0261\n",
            "Epoch [13961/20000], Training Loss: 0.0268\n",
            "Epoch [13962/20000], Training Loss: 0.0246\n",
            "Epoch [13963/20000], Training Loss: 0.0264\n",
            "Epoch [13964/20000], Training Loss: 0.0268\n",
            "Epoch [13965/20000], Training Loss: 0.0279\n",
            "Epoch [13966/20000], Training Loss: 0.0272\n",
            "Epoch [13967/20000], Training Loss: 0.0244\n",
            "Epoch [13968/20000], Training Loss: 0.0263\n",
            "Epoch [13969/20000], Training Loss: 0.0268\n",
            "Epoch [13970/20000], Training Loss: 0.0244\n",
            "Epoch [13971/20000], Training Loss: 0.0253\n",
            "Epoch [13972/20000], Training Loss: 0.0258\n",
            "Epoch [13973/20000], Training Loss: 0.0263\n",
            "Epoch [13974/20000], Training Loss: 0.0265\n",
            "Epoch [13975/20000], Training Loss: 0.0271\n",
            "Epoch [13976/20000], Training Loss: 0.0268\n",
            "Epoch [13977/20000], Training Loss: 0.0278\n",
            "Epoch [13978/20000], Training Loss: 0.0259\n",
            "Epoch [13979/20000], Training Loss: 0.0276\n",
            "Epoch [13980/20000], Training Loss: 0.0269\n",
            "Epoch [13981/20000], Training Loss: 0.0266\n",
            "Epoch [13982/20000], Training Loss: 0.0275\n",
            "Epoch [13983/20000], Training Loss: 0.0273\n",
            "Epoch [13984/20000], Training Loss: 0.0261\n",
            "Epoch [13985/20000], Training Loss: 0.0286\n",
            "Epoch [13986/20000], Training Loss: 0.0250\n",
            "Epoch [13987/20000], Training Loss: 0.0297\n",
            "Epoch [13988/20000], Training Loss: 0.0251\n",
            "Epoch [13989/20000], Training Loss: 0.0250\n",
            "Epoch [13990/20000], Training Loss: 0.0281\n",
            "Epoch [13991/20000], Training Loss: 0.0251\n",
            "Epoch [13992/20000], Training Loss: 0.0257\n",
            "Epoch [13993/20000], Training Loss: 0.0252\n",
            "Epoch [13994/20000], Training Loss: 0.0259\n",
            "Epoch [13995/20000], Training Loss: 0.0264\n",
            "Epoch [13996/20000], Training Loss: 0.0262\n",
            "Epoch [13997/20000], Training Loss: 0.0264\n",
            "Epoch [13998/20000], Training Loss: 0.0265\n",
            "Epoch [13999/20000], Training Loss: 0.0252\n",
            "Epoch [14000/20000], Training Loss: 0.0271\n",
            "Epoch [14001/20000], Training Loss: 0.0289\n",
            "Epoch [14002/20000], Training Loss: 0.0272\n",
            "Epoch [14003/20000], Training Loss: 0.0292\n",
            "Epoch [14004/20000], Training Loss: 0.0276\n",
            "Epoch [14005/20000], Training Loss: 0.0281\n",
            "Epoch [14006/20000], Training Loss: 0.0253\n",
            "Epoch [14007/20000], Training Loss: 0.0276\n",
            "Epoch [14008/20000], Training Loss: 0.0256\n",
            "Epoch [14009/20000], Training Loss: 0.0269\n",
            "Epoch [14010/20000], Training Loss: 0.0274\n",
            "Epoch [14011/20000], Training Loss: 0.0261\n",
            "Epoch [14012/20000], Training Loss: 0.0292\n",
            "Epoch [14013/20000], Training Loss: 0.0261\n",
            "Epoch [14014/20000], Training Loss: 0.0269\n",
            "Epoch [14015/20000], Training Loss: 0.0259\n",
            "Epoch [14016/20000], Training Loss: 0.0248\n",
            "Epoch [14017/20000], Training Loss: 0.0267\n",
            "Epoch [14018/20000], Training Loss: 0.0248\n",
            "Epoch [14019/20000], Training Loss: 0.0252\n",
            "Epoch [14020/20000], Training Loss: 0.0255\n",
            "Epoch [14021/20000], Training Loss: 0.0266\n",
            "Epoch [14022/20000], Training Loss: 0.0266\n",
            "Epoch [14023/20000], Training Loss: 0.0277\n",
            "Epoch [14024/20000], Training Loss: 0.0252\n",
            "Epoch [14025/20000], Training Loss: 0.0254\n",
            "Epoch [14026/20000], Training Loss: 0.0266\n",
            "Epoch [14027/20000], Training Loss: 0.0269\n",
            "Epoch [14028/20000], Training Loss: 0.0255\n",
            "Epoch [14029/20000], Training Loss: 0.0262\n",
            "Epoch [14030/20000], Training Loss: 0.0278\n",
            "Epoch [14031/20000], Training Loss: 0.0268\n",
            "Epoch [14032/20000], Training Loss: 0.0247\n",
            "Epoch [14033/20000], Training Loss: 0.0252\n",
            "Epoch [14034/20000], Training Loss: 0.0286\n",
            "Epoch [14035/20000], Training Loss: 0.0257\n",
            "Epoch [14036/20000], Training Loss: 0.0263\n",
            "Epoch [14037/20000], Training Loss: 0.0274\n",
            "Epoch [14038/20000], Training Loss: 0.0264\n",
            "Epoch [14039/20000], Training Loss: 0.0252\n",
            "Epoch [14040/20000], Training Loss: 0.0261\n",
            "Epoch [14041/20000], Training Loss: 0.0260\n",
            "Epoch [14042/20000], Training Loss: 0.0277\n",
            "Epoch [14043/20000], Training Loss: 0.0257\n",
            "Epoch [14044/20000], Training Loss: 0.0259\n",
            "Epoch [14045/20000], Training Loss: 0.0288\n",
            "Epoch [14046/20000], Training Loss: 0.0270\n",
            "Epoch [14047/20000], Training Loss: 0.0252\n",
            "Epoch [14048/20000], Training Loss: 0.0257\n",
            "Epoch [14049/20000], Training Loss: 0.0259\n",
            "Epoch [14050/20000], Training Loss: 0.0252\n",
            "Epoch [14051/20000], Training Loss: 0.0271\n",
            "Epoch [14052/20000], Training Loss: 0.0251\n",
            "Epoch [14053/20000], Training Loss: 0.0272\n",
            "Epoch [14054/20000], Training Loss: 0.0256\n",
            "Epoch [14055/20000], Training Loss: 0.0247\n",
            "Epoch [14056/20000], Training Loss: 0.0266\n",
            "Epoch [14057/20000], Training Loss: 0.0284\n",
            "Epoch [14058/20000], Training Loss: 0.0268\n",
            "Epoch [14059/20000], Training Loss: 0.0270\n",
            "Epoch [14060/20000], Training Loss: 0.0259\n",
            "Epoch [14061/20000], Training Loss: 0.0258\n",
            "Epoch [14062/20000], Training Loss: 0.0267\n",
            "Epoch [14063/20000], Training Loss: 0.0273\n",
            "Epoch [14064/20000], Training Loss: 0.0253\n",
            "Epoch [14065/20000], Training Loss: 0.0263\n",
            "Epoch [14066/20000], Training Loss: 0.0283\n",
            "Epoch [14067/20000], Training Loss: 0.0287\n",
            "Epoch [14068/20000], Training Loss: 0.0265\n",
            "Epoch [14069/20000], Training Loss: 0.0262\n",
            "Epoch [14070/20000], Training Loss: 0.0290\n",
            "Epoch [14071/20000], Training Loss: 0.0259\n",
            "Epoch [14072/20000], Training Loss: 0.0263\n",
            "Epoch [14073/20000], Training Loss: 0.0259\n",
            "Epoch [14074/20000], Training Loss: 0.0255\n",
            "Epoch [14075/20000], Training Loss: 0.0271\n",
            "Epoch [14076/20000], Training Loss: 0.0266\n",
            "Epoch [14077/20000], Training Loss: 0.0270\n",
            "Epoch [14078/20000], Training Loss: 0.0276\n",
            "Epoch [14079/20000], Training Loss: 0.0266\n",
            "Epoch [14080/20000], Training Loss: 0.0238\n",
            "Epoch [14081/20000], Training Loss: 0.0243\n",
            "Epoch [14082/20000], Training Loss: 0.0278\n",
            "Epoch [14083/20000], Training Loss: 0.0250\n",
            "Epoch [14084/20000], Training Loss: 0.0268\n",
            "Epoch [14085/20000], Training Loss: 0.0261\n",
            "Epoch [14086/20000], Training Loss: 0.0257\n",
            "Epoch [14087/20000], Training Loss: 0.0259\n",
            "Epoch [14088/20000], Training Loss: 0.0268\n",
            "Epoch [14089/20000], Training Loss: 0.0253\n",
            "Epoch [14090/20000], Training Loss: 0.0255\n",
            "Epoch [14091/20000], Training Loss: 0.0282\n",
            "Epoch [14092/20000], Training Loss: 0.0253\n",
            "Epoch [14093/20000], Training Loss: 0.0252\n",
            "Epoch [14094/20000], Training Loss: 0.0261\n",
            "Epoch [14095/20000], Training Loss: 0.0262\n",
            "Epoch [14096/20000], Training Loss: 0.0258\n",
            "Epoch [14097/20000], Training Loss: 0.0236\n",
            "Epoch [14098/20000], Training Loss: 0.0270\n",
            "Epoch [14099/20000], Training Loss: 0.0267\n",
            "Epoch [14100/20000], Training Loss: 0.0292\n",
            "Epoch [14101/20000], Training Loss: 0.0248\n",
            "Epoch [14102/20000], Training Loss: 0.0270\n",
            "Epoch [14103/20000], Training Loss: 0.0273\n",
            "Epoch [14104/20000], Training Loss: 0.0268\n",
            "Epoch [14105/20000], Training Loss: 0.0264\n",
            "Epoch [14106/20000], Training Loss: 0.0265\n",
            "Epoch [14107/20000], Training Loss: 0.0254\n",
            "Epoch [14108/20000], Training Loss: 0.0284\n",
            "Epoch [14109/20000], Training Loss: 0.0288\n",
            "Epoch [14110/20000], Training Loss: 0.0267\n",
            "Epoch [14111/20000], Training Loss: 0.0279\n",
            "Epoch [14112/20000], Training Loss: 0.0265\n",
            "Epoch [14113/20000], Training Loss: 0.0271\n",
            "Epoch [14114/20000], Training Loss: 0.0265\n",
            "Epoch [14115/20000], Training Loss: 0.0252\n",
            "Epoch [14116/20000], Training Loss: 0.0259\n",
            "Epoch [14117/20000], Training Loss: 0.0270\n",
            "Epoch [14118/20000], Training Loss: 0.0252\n",
            "Epoch [14119/20000], Training Loss: 0.0250\n",
            "Epoch [14120/20000], Training Loss: 0.0266\n",
            "Epoch [14121/20000], Training Loss: 0.0266\n",
            "Epoch [14122/20000], Training Loss: 0.0248\n",
            "Epoch [14123/20000], Training Loss: 0.0273\n",
            "Epoch [14124/20000], Training Loss: 0.0252\n",
            "Epoch [14125/20000], Training Loss: 0.0289\n",
            "Epoch [14126/20000], Training Loss: 0.0278\n",
            "Epoch [14127/20000], Training Loss: 0.0253\n",
            "Epoch [14128/20000], Training Loss: 0.0241\n",
            "Epoch [14129/20000], Training Loss: 0.0278\n",
            "Epoch [14130/20000], Training Loss: 0.0251\n",
            "Epoch [14131/20000], Training Loss: 0.0266\n",
            "Epoch [14132/20000], Training Loss: 0.0245\n",
            "Epoch [14133/20000], Training Loss: 0.0258\n",
            "Epoch [14134/20000], Training Loss: 0.0256\n",
            "Epoch [14135/20000], Training Loss: 0.0264\n",
            "Epoch [14136/20000], Training Loss: 0.0254\n",
            "Epoch [14137/20000], Training Loss: 0.0266\n",
            "Epoch [14138/20000], Training Loss: 0.0262\n",
            "Epoch [14139/20000], Training Loss: 0.0284\n",
            "Epoch [14140/20000], Training Loss: 0.0256\n",
            "Epoch [14141/20000], Training Loss: 0.0260\n",
            "Epoch [14142/20000], Training Loss: 0.0245\n",
            "Epoch [14143/20000], Training Loss: 0.0291\n",
            "Epoch [14144/20000], Training Loss: 0.0252\n",
            "Epoch [14145/20000], Training Loss: 0.0261\n",
            "Epoch [14146/20000], Training Loss: 0.0282\n",
            "Epoch [14147/20000], Training Loss: 0.0278\n",
            "Epoch [14148/20000], Training Loss: 0.0250\n",
            "Epoch [14149/20000], Training Loss: 0.0257\n",
            "Epoch [14150/20000], Training Loss: 0.0256\n",
            "Epoch [14151/20000], Training Loss: 0.0258\n",
            "Epoch [14152/20000], Training Loss: 0.0262\n",
            "Epoch [14153/20000], Training Loss: 0.0256\n",
            "Epoch [14154/20000], Training Loss: 0.0252\n",
            "Epoch [14155/20000], Training Loss: 0.0251\n",
            "Epoch [14156/20000], Training Loss: 0.0268\n",
            "Epoch [14157/20000], Training Loss: 0.0254\n",
            "Epoch [14158/20000], Training Loss: 0.0255\n",
            "Epoch [14159/20000], Training Loss: 0.0252\n",
            "Epoch [14160/20000], Training Loss: 0.0272\n",
            "Epoch [14161/20000], Training Loss: 0.0265\n",
            "Epoch [14162/20000], Training Loss: 0.0267\n",
            "Epoch [14163/20000], Training Loss: 0.0253\n",
            "Epoch [14164/20000], Training Loss: 0.0250\n",
            "Epoch [14165/20000], Training Loss: 0.0279\n",
            "Epoch [14166/20000], Training Loss: 0.0256\n",
            "Epoch [14167/20000], Training Loss: 0.0264\n",
            "Epoch [14168/20000], Training Loss: 0.0282\n",
            "Epoch [14169/20000], Training Loss: 0.0274\n",
            "Epoch [14170/20000], Training Loss: 0.0268\n",
            "Epoch [14171/20000], Training Loss: 0.0255\n",
            "Epoch [14172/20000], Training Loss: 0.0256\n",
            "Epoch [14173/20000], Training Loss: 0.0274\n",
            "Epoch [14174/20000], Training Loss: 0.0272\n",
            "Epoch [14175/20000], Training Loss: 0.0278\n",
            "Epoch [14176/20000], Training Loss: 0.0282\n",
            "Epoch [14177/20000], Training Loss: 0.0278\n",
            "Epoch [14178/20000], Training Loss: 0.0247\n",
            "Epoch [14179/20000], Training Loss: 0.0255\n",
            "Epoch [14180/20000], Training Loss: 0.0254\n",
            "Epoch [14181/20000], Training Loss: 0.0259\n",
            "Epoch [14182/20000], Training Loss: 0.0259\n",
            "Epoch [14183/20000], Training Loss: 0.0271\n",
            "Epoch [14184/20000], Training Loss: 0.0270\n",
            "Epoch [14185/20000], Training Loss: 0.0291\n",
            "Epoch [14186/20000], Training Loss: 0.0268\n",
            "Epoch [14187/20000], Training Loss: 0.0263\n",
            "Epoch [14188/20000], Training Loss: 0.0253\n",
            "Epoch [14189/20000], Training Loss: 0.0262\n",
            "Epoch [14190/20000], Training Loss: 0.0275\n",
            "Epoch [14191/20000], Training Loss: 0.0258\n",
            "Epoch [14192/20000], Training Loss: 0.0281\n",
            "Epoch [14193/20000], Training Loss: 0.0284\n",
            "Epoch [14194/20000], Training Loss: 0.0264\n",
            "Epoch [14195/20000], Training Loss: 0.0271\n",
            "Epoch [14196/20000], Training Loss: 0.0253\n",
            "Epoch [14197/20000], Training Loss: 0.0265\n",
            "Epoch [14198/20000], Training Loss: 0.0263\n",
            "Epoch [14199/20000], Training Loss: 0.0268\n",
            "Epoch [14200/20000], Training Loss: 0.0250\n",
            "Epoch [14201/20000], Training Loss: 0.0270\n",
            "Epoch [14202/20000], Training Loss: 0.0278\n",
            "Epoch [14203/20000], Training Loss: 0.0266\n",
            "Epoch [14204/20000], Training Loss: 0.0291\n",
            "Epoch [14205/20000], Training Loss: 0.0276\n",
            "Epoch [14206/20000], Training Loss: 0.0278\n",
            "Epoch [14207/20000], Training Loss: 0.0264\n",
            "Epoch [14208/20000], Training Loss: 0.0260\n",
            "Epoch [14209/20000], Training Loss: 0.0276\n",
            "Epoch [14210/20000], Training Loss: 0.0250\n",
            "Epoch [14211/20000], Training Loss: 0.0276\n",
            "Epoch [14212/20000], Training Loss: 0.0259\n",
            "Epoch [14213/20000], Training Loss: 0.0266\n",
            "Epoch [14214/20000], Training Loss: 0.0264\n",
            "Epoch [14215/20000], Training Loss: 0.0256\n",
            "Epoch [14216/20000], Training Loss: 0.0247\n",
            "Epoch [14217/20000], Training Loss: 0.0247\n",
            "Epoch [14218/20000], Training Loss: 0.0248\n",
            "Epoch [14219/20000], Training Loss: 0.0272\n",
            "Epoch [14220/20000], Training Loss: 0.0289\n",
            "Epoch [14221/20000], Training Loss: 0.0259\n",
            "Epoch [14222/20000], Training Loss: 0.0275\n",
            "Epoch [14223/20000], Training Loss: 0.0250\n",
            "Epoch [14224/20000], Training Loss: 0.0275\n",
            "Epoch [14225/20000], Training Loss: 0.0254\n",
            "Epoch [14226/20000], Training Loss: 0.0263\n",
            "Epoch [14227/20000], Training Loss: 0.0271\n",
            "Epoch [14228/20000], Training Loss: 0.0287\n",
            "Epoch [14229/20000], Training Loss: 0.0280\n",
            "Epoch [14230/20000], Training Loss: 0.0267\n",
            "Epoch [14231/20000], Training Loss: 0.0259\n",
            "Epoch [14232/20000], Training Loss: 0.0254\n",
            "Epoch [14233/20000], Training Loss: 0.0283\n",
            "Epoch [14234/20000], Training Loss: 0.0234\n",
            "Epoch [14235/20000], Training Loss: 0.0269\n",
            "Epoch [14236/20000], Training Loss: 0.0267\n",
            "Epoch [14237/20000], Training Loss: 0.0261\n",
            "Epoch [14238/20000], Training Loss: 0.0280\n",
            "Epoch [14239/20000], Training Loss: 0.0252\n",
            "Epoch [14240/20000], Training Loss: 0.0253\n",
            "Epoch [14241/20000], Training Loss: 0.0265\n",
            "Epoch [14242/20000], Training Loss: 0.0266\n",
            "Epoch [14243/20000], Training Loss: 0.0266\n",
            "Epoch [14244/20000], Training Loss: 0.0267\n",
            "Epoch [14245/20000], Training Loss: 0.0254\n",
            "Epoch [14246/20000], Training Loss: 0.0268\n",
            "Epoch [14247/20000], Training Loss: 0.0255\n",
            "Epoch [14248/20000], Training Loss: 0.0279\n",
            "Epoch [14249/20000], Training Loss: 0.0244\n",
            "Epoch [14250/20000], Training Loss: 0.0271\n",
            "Epoch [14251/20000], Training Loss: 0.0242\n",
            "Epoch [14252/20000], Training Loss: 0.0282\n",
            "Epoch [14253/20000], Training Loss: 0.0262\n",
            "Epoch [14254/20000], Training Loss: 0.0260\n",
            "Epoch [14255/20000], Training Loss: 0.0263\n",
            "Epoch [14256/20000], Training Loss: 0.0255\n",
            "Epoch [14257/20000], Training Loss: 0.0270\n",
            "Epoch [14258/20000], Training Loss: 0.0273\n",
            "Epoch [14259/20000], Training Loss: 0.0260\n",
            "Epoch [14260/20000], Training Loss: 0.0272\n",
            "Epoch [14261/20000], Training Loss: 0.0248\n",
            "Epoch [14262/20000], Training Loss: 0.0279\n",
            "Epoch [14263/20000], Training Loss: 0.0251\n",
            "Epoch [14264/20000], Training Loss: 0.0268\n",
            "Epoch [14265/20000], Training Loss: 0.0272\n",
            "Epoch [14266/20000], Training Loss: 0.0284\n",
            "Epoch [14267/20000], Training Loss: 0.0270\n",
            "Epoch [14268/20000], Training Loss: 0.0270\n",
            "Epoch [14269/20000], Training Loss: 0.0268\n",
            "Epoch [14270/20000], Training Loss: 0.0272\n",
            "Epoch [14271/20000], Training Loss: 0.0242\n",
            "Epoch [14272/20000], Training Loss: 0.0284\n",
            "Epoch [14273/20000], Training Loss: 0.0264\n",
            "Epoch [14274/20000], Training Loss: 0.0276\n",
            "Epoch [14275/20000], Training Loss: 0.0252\n",
            "Epoch [14276/20000], Training Loss: 0.0274\n",
            "Epoch [14277/20000], Training Loss: 0.0276\n",
            "Epoch [14278/20000], Training Loss: 0.0279\n",
            "Epoch [14279/20000], Training Loss: 0.0268\n",
            "Epoch [14280/20000], Training Loss: 0.0275\n",
            "Epoch [14281/20000], Training Loss: 0.0276\n",
            "Epoch [14282/20000], Training Loss: 0.0270\n",
            "Epoch [14283/20000], Training Loss: 0.0257\n",
            "Epoch [14284/20000], Training Loss: 0.0270\n",
            "Epoch [14285/20000], Training Loss: 0.0260\n",
            "Epoch [14286/20000], Training Loss: 0.0264\n",
            "Epoch [14287/20000], Training Loss: 0.0263\n",
            "Epoch [14288/20000], Training Loss: 0.0245\n",
            "Epoch [14289/20000], Training Loss: 0.0260\n",
            "Epoch [14290/20000], Training Loss: 0.0279\n",
            "Epoch [14291/20000], Training Loss: 0.0254\n",
            "Epoch [14292/20000], Training Loss: 0.0250\n",
            "Epoch [14293/20000], Training Loss: 0.0248\n",
            "Epoch [14294/20000], Training Loss: 0.0256\n",
            "Epoch [14295/20000], Training Loss: 0.0257\n",
            "Epoch [14296/20000], Training Loss: 0.0284\n",
            "Epoch [14297/20000], Training Loss: 0.0265\n",
            "Epoch [14298/20000], Training Loss: 0.0252\n",
            "Epoch [14299/20000], Training Loss: 0.0267\n",
            "Epoch [14300/20000], Training Loss: 0.0265\n",
            "Epoch [14301/20000], Training Loss: 0.0261\n",
            "Epoch [14302/20000], Training Loss: 0.0282\n",
            "Epoch [14303/20000], Training Loss: 0.0262\n",
            "Epoch [14304/20000], Training Loss: 0.0281\n",
            "Epoch [14305/20000], Training Loss: 0.0255\n",
            "Epoch [14306/20000], Training Loss: 0.0275\n",
            "Epoch [14307/20000], Training Loss: 0.0281\n",
            "Epoch [14308/20000], Training Loss: 0.0249\n",
            "Epoch [14309/20000], Training Loss: 0.0280\n",
            "Epoch [14310/20000], Training Loss: 0.0257\n",
            "Epoch [14311/20000], Training Loss: 0.0265\n",
            "Epoch [14312/20000], Training Loss: 0.0254\n",
            "Epoch [14313/20000], Training Loss: 0.0250\n",
            "Epoch [14314/20000], Training Loss: 0.0283\n",
            "Epoch [14315/20000], Training Loss: 0.0263\n",
            "Epoch [14316/20000], Training Loss: 0.0244\n",
            "Epoch [14317/20000], Training Loss: 0.0273\n",
            "Epoch [14318/20000], Training Loss: 0.0272\n",
            "Epoch [14319/20000], Training Loss: 0.0268\n",
            "Epoch [14320/20000], Training Loss: 0.0272\n",
            "Epoch [14321/20000], Training Loss: 0.0273\n",
            "Epoch [14322/20000], Training Loss: 0.0269\n",
            "Epoch [14323/20000], Training Loss: 0.0283\n",
            "Epoch [14324/20000], Training Loss: 0.0274\n",
            "Epoch [14325/20000], Training Loss: 0.0265\n",
            "Epoch [14326/20000], Training Loss: 0.0261\n",
            "Epoch [14327/20000], Training Loss: 0.0270\n",
            "Epoch [14328/20000], Training Loss: 0.0263\n",
            "Epoch [14329/20000], Training Loss: 0.0273\n",
            "Epoch [14330/20000], Training Loss: 0.0277\n",
            "Epoch [14331/20000], Training Loss: 0.0251\n",
            "Epoch [14332/20000], Training Loss: 0.0258\n",
            "Epoch [14333/20000], Training Loss: 0.0277\n",
            "Epoch [14334/20000], Training Loss: 0.0247\n",
            "Epoch [14335/20000], Training Loss: 0.0249\n",
            "Epoch [14336/20000], Training Loss: 0.0283\n",
            "Epoch [14337/20000], Training Loss: 0.0258\n",
            "Epoch [14338/20000], Training Loss: 0.0256\n",
            "Epoch [14339/20000], Training Loss: 0.0266\n",
            "Epoch [14340/20000], Training Loss: 0.0255\n",
            "Epoch [14341/20000], Training Loss: 0.0276\n",
            "Epoch [14342/20000], Training Loss: 0.0265\n",
            "Epoch [14343/20000], Training Loss: 0.0257\n",
            "Epoch [14344/20000], Training Loss: 0.0268\n",
            "Epoch [14345/20000], Training Loss: 0.0271\n",
            "Epoch [14346/20000], Training Loss: 0.0273\n",
            "Epoch [14347/20000], Training Loss: 0.0265\n",
            "Epoch [14348/20000], Training Loss: 0.0265\n",
            "Epoch [14349/20000], Training Loss: 0.0277\n",
            "Epoch [14350/20000], Training Loss: 0.0264\n",
            "Epoch [14351/20000], Training Loss: 0.0241\n",
            "Epoch [14352/20000], Training Loss: 0.0279\n",
            "Epoch [14353/20000], Training Loss: 0.0246\n",
            "Epoch [14354/20000], Training Loss: 0.0262\n",
            "Epoch [14355/20000], Training Loss: 0.0258\n",
            "Epoch [14356/20000], Training Loss: 0.0282\n",
            "Epoch [14357/20000], Training Loss: 0.0280\n",
            "Epoch [14358/20000], Training Loss: 0.0279\n",
            "Epoch [14359/20000], Training Loss: 0.0271\n",
            "Epoch [14360/20000], Training Loss: 0.0265\n",
            "Epoch [14361/20000], Training Loss: 0.0253\n",
            "Epoch [14362/20000], Training Loss: 0.0267\n",
            "Epoch [14363/20000], Training Loss: 0.0246\n",
            "Epoch [14364/20000], Training Loss: 0.0260\n",
            "Epoch [14365/20000], Training Loss: 0.0267\n",
            "Epoch [14366/20000], Training Loss: 0.0259\n",
            "Epoch [14367/20000], Training Loss: 0.0276\n",
            "Epoch [14368/20000], Training Loss: 0.0265\n",
            "Epoch [14369/20000], Training Loss: 0.0264\n",
            "Epoch [14370/20000], Training Loss: 0.0258\n",
            "Epoch [14371/20000], Training Loss: 0.0262\n",
            "Epoch [14372/20000], Training Loss: 0.0268\n",
            "Epoch [14373/20000], Training Loss: 0.0247\n",
            "Epoch [14374/20000], Training Loss: 0.0263\n",
            "Epoch [14375/20000], Training Loss: 0.0282\n",
            "Epoch [14376/20000], Training Loss: 0.0268\n",
            "Epoch [14377/20000], Training Loss: 0.0281\n",
            "Epoch [14378/20000], Training Loss: 0.0267\n",
            "Epoch [14379/20000], Training Loss: 0.0272\n",
            "Epoch [14380/20000], Training Loss: 0.0277\n",
            "Epoch [14381/20000], Training Loss: 0.0261\n",
            "Epoch [14382/20000], Training Loss: 0.0258\n",
            "Epoch [14383/20000], Training Loss: 0.0257\n",
            "Epoch [14384/20000], Training Loss: 0.0263\n",
            "Epoch [14385/20000], Training Loss: 0.0271\n",
            "Epoch [14386/20000], Training Loss: 0.0247\n",
            "Epoch [14387/20000], Training Loss: 0.0268\n",
            "Epoch [14388/20000], Training Loss: 0.0269\n",
            "Epoch [14389/20000], Training Loss: 0.0257\n",
            "Epoch [14390/20000], Training Loss: 0.0255\n",
            "Epoch [14391/20000], Training Loss: 0.0262\n",
            "Epoch [14392/20000], Training Loss: 0.0242\n",
            "Epoch [14393/20000], Training Loss: 0.0270\n",
            "Epoch [14394/20000], Training Loss: 0.0288\n",
            "Epoch [14395/20000], Training Loss: 0.0276\n",
            "Epoch [14396/20000], Training Loss: 0.0288\n",
            "Epoch [14397/20000], Training Loss: 0.0266\n",
            "Epoch [14398/20000], Training Loss: 0.0259\n",
            "Epoch [14399/20000], Training Loss: 0.0257\n",
            "Epoch [14400/20000], Training Loss: 0.0264\n",
            "Epoch [14401/20000], Training Loss: 0.0268\n",
            "Epoch [14402/20000], Training Loss: 0.0281\n",
            "Epoch [14403/20000], Training Loss: 0.0255\n",
            "Epoch [14404/20000], Training Loss: 0.0268\n",
            "Epoch [14405/20000], Training Loss: 0.0257\n",
            "Epoch [14406/20000], Training Loss: 0.0271\n",
            "Epoch [14407/20000], Training Loss: 0.0257\n",
            "Epoch [14408/20000], Training Loss: 0.0273\n",
            "Epoch [14409/20000], Training Loss: 0.0264\n",
            "Epoch [14410/20000], Training Loss: 0.0256\n",
            "Epoch [14411/20000], Training Loss: 0.0263\n",
            "Epoch [14412/20000], Training Loss: 0.0250\n",
            "Epoch [14413/20000], Training Loss: 0.0246\n",
            "Epoch [14414/20000], Training Loss: 0.0250\n",
            "Epoch [14415/20000], Training Loss: 0.0273\n",
            "Epoch [14416/20000], Training Loss: 0.0247\n",
            "Epoch [14417/20000], Training Loss: 0.0268\n",
            "Epoch [14418/20000], Training Loss: 0.0246\n",
            "Epoch [14419/20000], Training Loss: 0.0268\n",
            "Epoch [14420/20000], Training Loss: 0.0248\n",
            "Epoch [14421/20000], Training Loss: 0.0258\n",
            "Epoch [14422/20000], Training Loss: 0.0268\n",
            "Epoch [14423/20000], Training Loss: 0.0246\n",
            "Epoch [14424/20000], Training Loss: 0.0257\n",
            "Epoch [14425/20000], Training Loss: 0.0266\n",
            "Epoch [14426/20000], Training Loss: 0.0272\n",
            "Epoch [14427/20000], Training Loss: 0.0259\n",
            "Epoch [14428/20000], Training Loss: 0.0287\n",
            "Epoch [14429/20000], Training Loss: 0.0264\n",
            "Epoch [14430/20000], Training Loss: 0.0269\n",
            "Epoch [14431/20000], Training Loss: 0.0273\n",
            "Epoch [14432/20000], Training Loss: 0.0260\n",
            "Epoch [14433/20000], Training Loss: 0.0288\n",
            "Epoch [14434/20000], Training Loss: 0.0260\n",
            "Epoch [14435/20000], Training Loss: 0.0254\n",
            "Epoch [14436/20000], Training Loss: 0.0273\n",
            "Epoch [14437/20000], Training Loss: 0.0247\n",
            "Epoch [14438/20000], Training Loss: 0.0249\n",
            "Epoch [14439/20000], Training Loss: 0.0263\n",
            "Epoch [14440/20000], Training Loss: 0.0267\n",
            "Epoch [14441/20000], Training Loss: 0.0270\n",
            "Epoch [14442/20000], Training Loss: 0.0259\n",
            "Epoch [14443/20000], Training Loss: 0.0248\n",
            "Epoch [14444/20000], Training Loss: 0.0242\n",
            "Epoch [14445/20000], Training Loss: 0.0269\n",
            "Epoch [14446/20000], Training Loss: 0.0269\n",
            "Epoch [14447/20000], Training Loss: 0.0264\n",
            "Epoch [14448/20000], Training Loss: 0.0273\n",
            "Epoch [14449/20000], Training Loss: 0.0267\n",
            "Epoch [14450/20000], Training Loss: 0.0269\n",
            "Epoch [14451/20000], Training Loss: 0.0263\n",
            "Epoch [14452/20000], Training Loss: 0.0271\n",
            "Epoch [14453/20000], Training Loss: 0.0263\n",
            "Epoch [14454/20000], Training Loss: 0.0270\n",
            "Epoch [14455/20000], Training Loss: 0.0238\n",
            "Epoch [14456/20000], Training Loss: 0.0284\n",
            "Epoch [14457/20000], Training Loss: 0.0272\n",
            "Epoch [14458/20000], Training Loss: 0.0267\n",
            "Epoch [14459/20000], Training Loss: 0.0265\n",
            "Epoch [14460/20000], Training Loss: 0.0270\n",
            "Epoch [14461/20000], Training Loss: 0.0279\n",
            "Epoch [14462/20000], Training Loss: 0.0281\n",
            "Epoch [14463/20000], Training Loss: 0.0270\n",
            "Epoch [14464/20000], Training Loss: 0.0261\n",
            "Epoch [14465/20000], Training Loss: 0.0265\n",
            "Epoch [14466/20000], Training Loss: 0.0267\n",
            "Epoch [14467/20000], Training Loss: 0.0289\n",
            "Epoch [14468/20000], Training Loss: 0.0270\n",
            "Epoch [14469/20000], Training Loss: 0.0261\n",
            "Epoch [14470/20000], Training Loss: 0.0272\n",
            "Epoch [14471/20000], Training Loss: 0.0258\n",
            "Epoch [14472/20000], Training Loss: 0.0272\n",
            "Epoch [14473/20000], Training Loss: 0.0274\n",
            "Epoch [14474/20000], Training Loss: 0.0261\n",
            "Epoch [14475/20000], Training Loss: 0.0273\n",
            "Epoch [14476/20000], Training Loss: 0.0259\n",
            "Epoch [14477/20000], Training Loss: 0.0255\n",
            "Epoch [14478/20000], Training Loss: 0.0290\n",
            "Epoch [14479/20000], Training Loss: 0.0254\n",
            "Epoch [14480/20000], Training Loss: 0.0266\n",
            "Epoch [14481/20000], Training Loss: 0.0285\n",
            "Epoch [14482/20000], Training Loss: 0.0252\n",
            "Epoch [14483/20000], Training Loss: 0.0275\n",
            "Epoch [14484/20000], Training Loss: 0.0246\n",
            "Epoch [14485/20000], Training Loss: 0.0258\n",
            "Epoch [14486/20000], Training Loss: 0.0268\n",
            "Epoch [14487/20000], Training Loss: 0.0249\n",
            "Epoch [14488/20000], Training Loss: 0.0255\n",
            "Epoch [14489/20000], Training Loss: 0.0260\n",
            "Epoch [14490/20000], Training Loss: 0.0259\n",
            "Epoch [14491/20000], Training Loss: 0.0253\n",
            "Epoch [14492/20000], Training Loss: 0.0250\n",
            "Epoch [14493/20000], Training Loss: 0.0291\n",
            "Epoch [14494/20000], Training Loss: 0.0258\n",
            "Epoch [14495/20000], Training Loss: 0.0268\n",
            "Epoch [14496/20000], Training Loss: 0.0286\n",
            "Epoch [14497/20000], Training Loss: 0.0268\n",
            "Epoch [14498/20000], Training Loss: 0.0282\n",
            "Epoch [14499/20000], Training Loss: 0.0242\n",
            "Epoch [14500/20000], Training Loss: 0.0245\n",
            "Epoch [14501/20000], Training Loss: 0.0255\n",
            "Epoch [14502/20000], Training Loss: 0.0265\n",
            "Epoch [14503/20000], Training Loss: 0.0288\n",
            "Epoch [14504/20000], Training Loss: 0.0264\n",
            "Epoch [14505/20000], Training Loss: 0.0269\n",
            "Epoch [14506/20000], Training Loss: 0.0262\n",
            "Epoch [14507/20000], Training Loss: 0.0247\n",
            "Epoch [14508/20000], Training Loss: 0.0272\n",
            "Epoch [14509/20000], Training Loss: 0.0253\n",
            "Epoch [14510/20000], Training Loss: 0.0252\n",
            "Epoch [14511/20000], Training Loss: 0.0255\n",
            "Epoch [14512/20000], Training Loss: 0.0285\n",
            "Epoch [14513/20000], Training Loss: 0.0262\n",
            "Epoch [14514/20000], Training Loss: 0.0272\n",
            "Epoch [14515/20000], Training Loss: 0.0267\n",
            "Epoch [14516/20000], Training Loss: 0.0258\n",
            "Epoch [14517/20000], Training Loss: 0.0264\n",
            "Epoch [14518/20000], Training Loss: 0.0271\n",
            "Epoch [14519/20000], Training Loss: 0.0257\n",
            "Epoch [14520/20000], Training Loss: 0.0250\n",
            "Epoch [14521/20000], Training Loss: 0.0269\n",
            "Epoch [14522/20000], Training Loss: 0.0271\n",
            "Epoch [14523/20000], Training Loss: 0.0279\n",
            "Epoch [14524/20000], Training Loss: 0.0274\n",
            "Epoch [14525/20000], Training Loss: 0.0268\n",
            "Epoch [14526/20000], Training Loss: 0.0293\n",
            "Epoch [14527/20000], Training Loss: 0.0249\n",
            "Epoch [14528/20000], Training Loss: 0.0270\n",
            "Epoch [14529/20000], Training Loss: 0.0268\n",
            "Epoch [14530/20000], Training Loss: 0.0257\n",
            "Epoch [14531/20000], Training Loss: 0.0282\n",
            "Epoch [14532/20000], Training Loss: 0.0267\n",
            "Epoch [14533/20000], Training Loss: 0.0247\n",
            "Epoch [14534/20000], Training Loss: 0.0270\n",
            "Epoch [14535/20000], Training Loss: 0.0260\n",
            "Epoch [14536/20000], Training Loss: 0.0274\n",
            "Epoch [14537/20000], Training Loss: 0.0275\n",
            "Epoch [14538/20000], Training Loss: 0.0268\n",
            "Epoch [14539/20000], Training Loss: 0.0237\n",
            "Epoch [14540/20000], Training Loss: 0.0254\n",
            "Epoch [14541/20000], Training Loss: 0.0252\n",
            "Epoch [14542/20000], Training Loss: 0.0250\n",
            "Epoch [14543/20000], Training Loss: 0.0256\n",
            "Epoch [14544/20000], Training Loss: 0.0283\n",
            "Epoch [14545/20000], Training Loss: 0.0242\n",
            "Epoch [14546/20000], Training Loss: 0.0265\n",
            "Epoch [14547/20000], Training Loss: 0.0260\n",
            "Epoch [14548/20000], Training Loss: 0.0249\n",
            "Epoch [14549/20000], Training Loss: 0.0272\n",
            "Epoch [14550/20000], Training Loss: 0.0268\n",
            "Epoch [14551/20000], Training Loss: 0.0258\n",
            "Epoch [14552/20000], Training Loss: 0.0272\n",
            "Epoch [14553/20000], Training Loss: 0.0253\n",
            "Epoch [14554/20000], Training Loss: 0.0271\n",
            "Epoch [14555/20000], Training Loss: 0.0267\n",
            "Epoch [14556/20000], Training Loss: 0.0273\n",
            "Epoch [14557/20000], Training Loss: 0.0254\n",
            "Epoch [14558/20000], Training Loss: 0.0249\n",
            "Epoch [14559/20000], Training Loss: 0.0244\n",
            "Epoch [14560/20000], Training Loss: 0.0274\n",
            "Epoch [14561/20000], Training Loss: 0.0263\n",
            "Epoch [14562/20000], Training Loss: 0.0257\n",
            "Epoch [14563/20000], Training Loss: 0.0239\n",
            "Epoch [14564/20000], Training Loss: 0.0247\n",
            "Epoch [14565/20000], Training Loss: 0.0262\n",
            "Epoch [14566/20000], Training Loss: 0.0259\n",
            "Epoch [14567/20000], Training Loss: 0.0259\n",
            "Epoch [14568/20000], Training Loss: 0.0253\n",
            "Epoch [14569/20000], Training Loss: 0.0249\n",
            "Epoch [14570/20000], Training Loss: 0.0271\n",
            "Epoch [14571/20000], Training Loss: 0.0285\n",
            "Epoch [14572/20000], Training Loss: 0.0264\n",
            "Epoch [14573/20000], Training Loss: 0.0292\n",
            "Epoch [14574/20000], Training Loss: 0.0278\n",
            "Epoch [14575/20000], Training Loss: 0.0249\n",
            "Epoch [14576/20000], Training Loss: 0.0250\n",
            "Epoch [14577/20000], Training Loss: 0.0267\n",
            "Epoch [14578/20000], Training Loss: 0.0276\n",
            "Epoch [14579/20000], Training Loss: 0.0267\n",
            "Epoch [14580/20000], Training Loss: 0.0258\n",
            "Epoch [14581/20000], Training Loss: 0.0257\n",
            "Epoch [14582/20000], Training Loss: 0.0262\n",
            "Epoch [14583/20000], Training Loss: 0.0268\n",
            "Epoch [14584/20000], Training Loss: 0.0264\n",
            "Epoch [14585/20000], Training Loss: 0.0269\n",
            "Epoch [14586/20000], Training Loss: 0.0269\n",
            "Epoch [14587/20000], Training Loss: 0.0262\n",
            "Epoch [14588/20000], Training Loss: 0.0253\n",
            "Epoch [14589/20000], Training Loss: 0.0271\n",
            "Epoch [14590/20000], Training Loss: 0.0266\n",
            "Epoch [14591/20000], Training Loss: 0.0247\n",
            "Epoch [14592/20000], Training Loss: 0.0279\n",
            "Epoch [14593/20000], Training Loss: 0.0269\n",
            "Epoch [14594/20000], Training Loss: 0.0275\n",
            "Epoch [14595/20000], Training Loss: 0.0255\n",
            "Epoch [14596/20000], Training Loss: 0.0276\n",
            "Epoch [14597/20000], Training Loss: 0.0256\n",
            "Epoch [14598/20000], Training Loss: 0.0260\n",
            "Epoch [14599/20000], Training Loss: 0.0243\n",
            "Epoch [14600/20000], Training Loss: 0.0256\n",
            "Epoch [14601/20000], Training Loss: 0.0262\n",
            "Epoch [14602/20000], Training Loss: 0.0264\n",
            "Epoch [14603/20000], Training Loss: 0.0270\n",
            "Epoch [14604/20000], Training Loss: 0.0253\n",
            "Epoch [14605/20000], Training Loss: 0.0262\n",
            "Epoch [14606/20000], Training Loss: 0.0272\n",
            "Epoch [14607/20000], Training Loss: 0.0252\n",
            "Epoch [14608/20000], Training Loss: 0.0262\n",
            "Epoch [14609/20000], Training Loss: 0.0271\n",
            "Epoch [14610/20000], Training Loss: 0.0258\n",
            "Epoch [14611/20000], Training Loss: 0.0248\n",
            "Epoch [14612/20000], Training Loss: 0.0261\n",
            "Epoch [14613/20000], Training Loss: 0.0252\n",
            "Epoch [14614/20000], Training Loss: 0.0294\n",
            "Epoch [14615/20000], Training Loss: 0.0273\n",
            "Epoch [14616/20000], Training Loss: 0.0259\n",
            "Epoch [14617/20000], Training Loss: 0.0280\n",
            "Epoch [14618/20000], Training Loss: 0.0263\n",
            "Epoch [14619/20000], Training Loss: 0.0288\n",
            "Epoch [14620/20000], Training Loss: 0.0269\n",
            "Epoch [14621/20000], Training Loss: 0.0270\n",
            "Epoch [14622/20000], Training Loss: 0.0259\n",
            "Epoch [14623/20000], Training Loss: 0.0286\n",
            "Epoch [14624/20000], Training Loss: 0.0263\n",
            "Epoch [14625/20000], Training Loss: 0.0252\n",
            "Epoch [14626/20000], Training Loss: 0.0267\n",
            "Epoch [14627/20000], Training Loss: 0.0276\n",
            "Epoch [14628/20000], Training Loss: 0.0256\n",
            "Epoch [14629/20000], Training Loss: 0.0271\n",
            "Epoch [14630/20000], Training Loss: 0.0280\n",
            "Epoch [14631/20000], Training Loss: 0.0269\n",
            "Epoch [14632/20000], Training Loss: 0.0263\n",
            "Epoch [14633/20000], Training Loss: 0.0262\n",
            "Epoch [14634/20000], Training Loss: 0.0276\n",
            "Epoch [14635/20000], Training Loss: 0.0246\n",
            "Epoch [14636/20000], Training Loss: 0.0261\n",
            "Epoch [14637/20000], Training Loss: 0.0260\n",
            "Epoch [14638/20000], Training Loss: 0.0278\n",
            "Epoch [14639/20000], Training Loss: 0.0278\n",
            "Epoch [14640/20000], Training Loss: 0.0262\n",
            "Epoch [14641/20000], Training Loss: 0.0243\n",
            "Epoch [14642/20000], Training Loss: 0.0269\n",
            "Epoch [14643/20000], Training Loss: 0.0257\n",
            "Epoch [14644/20000], Training Loss: 0.0278\n",
            "Epoch [14645/20000], Training Loss: 0.0272\n",
            "Epoch [14646/20000], Training Loss: 0.0262\n",
            "Epoch [14647/20000], Training Loss: 0.0271\n",
            "Epoch [14648/20000], Training Loss: 0.0253\n",
            "Epoch [14649/20000], Training Loss: 0.0258\n",
            "Epoch [14650/20000], Training Loss: 0.0262\n",
            "Epoch [14651/20000], Training Loss: 0.0273\n",
            "Epoch [14652/20000], Training Loss: 0.0254\n",
            "Epoch [14653/20000], Training Loss: 0.0250\n",
            "Epoch [14654/20000], Training Loss: 0.0264\n",
            "Epoch [14655/20000], Training Loss: 0.0277\n",
            "Epoch [14656/20000], Training Loss: 0.0260\n",
            "Epoch [14657/20000], Training Loss: 0.0259\n",
            "Epoch [14658/20000], Training Loss: 0.0271\n",
            "Epoch [14659/20000], Training Loss: 0.0265\n",
            "Epoch [14660/20000], Training Loss: 0.0260\n",
            "Epoch [14661/20000], Training Loss: 0.0273\n",
            "Epoch [14662/20000], Training Loss: 0.0275\n",
            "Epoch [14663/20000], Training Loss: 0.0274\n",
            "Epoch [14664/20000], Training Loss: 0.0269\n",
            "Epoch [14665/20000], Training Loss: 0.0271\n",
            "Epoch [14666/20000], Training Loss: 0.0259\n",
            "Epoch [14667/20000], Training Loss: 0.0256\n",
            "Epoch [14668/20000], Training Loss: 0.0285\n",
            "Epoch [14669/20000], Training Loss: 0.0273\n",
            "Epoch [14670/20000], Training Loss: 0.0255\n",
            "Epoch [14671/20000], Training Loss: 0.0270\n",
            "Epoch [14672/20000], Training Loss: 0.0261\n",
            "Epoch [14673/20000], Training Loss: 0.0250\n",
            "Epoch [14674/20000], Training Loss: 0.0256\n",
            "Epoch [14675/20000], Training Loss: 0.0270\n",
            "Epoch [14676/20000], Training Loss: 0.0248\n",
            "Epoch [14677/20000], Training Loss: 0.0264\n",
            "Epoch [14678/20000], Training Loss: 0.0253\n",
            "Epoch [14679/20000], Training Loss: 0.0262\n",
            "Epoch [14680/20000], Training Loss: 0.0278\n",
            "Epoch [14681/20000], Training Loss: 0.0258\n",
            "Epoch [14682/20000], Training Loss: 0.0259\n",
            "Epoch [14683/20000], Training Loss: 0.0270\n",
            "Epoch [14684/20000], Training Loss: 0.0274\n",
            "Epoch [14685/20000], Training Loss: 0.0282\n",
            "Epoch [14686/20000], Training Loss: 0.0261\n",
            "Epoch [14687/20000], Training Loss: 0.0274\n",
            "Epoch [14688/20000], Training Loss: 0.0274\n",
            "Epoch [14689/20000], Training Loss: 0.0263\n",
            "Epoch [14690/20000], Training Loss: 0.0248\n",
            "Epoch [14691/20000], Training Loss: 0.0255\n",
            "Epoch [14692/20000], Training Loss: 0.0259\n",
            "Epoch [14693/20000], Training Loss: 0.0280\n",
            "Epoch [14694/20000], Training Loss: 0.0263\n",
            "Epoch [14695/20000], Training Loss: 0.0278\n",
            "Epoch [14696/20000], Training Loss: 0.0242\n",
            "Epoch [14697/20000], Training Loss: 0.0262\n",
            "Epoch [14698/20000], Training Loss: 0.0263\n",
            "Epoch [14699/20000], Training Loss: 0.0247\n",
            "Epoch [14700/20000], Training Loss: 0.0264\n",
            "Epoch [14701/20000], Training Loss: 0.0274\n",
            "Epoch [14702/20000], Training Loss: 0.0273\n",
            "Epoch [14703/20000], Training Loss: 0.0273\n",
            "Epoch [14704/20000], Training Loss: 0.0284\n",
            "Epoch [14705/20000], Training Loss: 0.0252\n",
            "Epoch [14706/20000], Training Loss: 0.0278\n",
            "Epoch [14707/20000], Training Loss: 0.0276\n",
            "Epoch [14708/20000], Training Loss: 0.0283\n",
            "Epoch [14709/20000], Training Loss: 0.0259\n",
            "Epoch [14710/20000], Training Loss: 0.0277\n",
            "Epoch [14711/20000], Training Loss: 0.0262\n",
            "Epoch [14712/20000], Training Loss: 0.0275\n",
            "Epoch [14713/20000], Training Loss: 0.0283\n",
            "Epoch [14714/20000], Training Loss: 0.0277\n",
            "Epoch [14715/20000], Training Loss: 0.0278\n",
            "Epoch [14716/20000], Training Loss: 0.0282\n",
            "Epoch [14717/20000], Training Loss: 0.0243\n",
            "Epoch [14718/20000], Training Loss: 0.0290\n",
            "Epoch [14719/20000], Training Loss: 0.0274\n",
            "Epoch [14720/20000], Training Loss: 0.0263\n",
            "Epoch [14721/20000], Training Loss: 0.0270\n",
            "Epoch [14722/20000], Training Loss: 0.0260\n",
            "Epoch [14723/20000], Training Loss: 0.0278\n",
            "Epoch [14724/20000], Training Loss: 0.0274\n",
            "Epoch [14725/20000], Training Loss: 0.0255\n",
            "Epoch [14726/20000], Training Loss: 0.0276\n",
            "Epoch [14727/20000], Training Loss: 0.0280\n",
            "Epoch [14728/20000], Training Loss: 0.0260\n",
            "Epoch [14729/20000], Training Loss: 0.0262\n",
            "Epoch [14730/20000], Training Loss: 0.0283\n",
            "Epoch [14731/20000], Training Loss: 0.0270\n",
            "Epoch [14732/20000], Training Loss: 0.0275\n",
            "Epoch [14733/20000], Training Loss: 0.0262\n",
            "Epoch [14734/20000], Training Loss: 0.0249\n",
            "Epoch [14735/20000], Training Loss: 0.0265\n",
            "Epoch [14736/20000], Training Loss: 0.0264\n",
            "Epoch [14737/20000], Training Loss: 0.0264\n",
            "Epoch [14738/20000], Training Loss: 0.0276\n",
            "Epoch [14739/20000], Training Loss: 0.0261\n",
            "Epoch [14740/20000], Training Loss: 0.0242\n",
            "Epoch [14741/20000], Training Loss: 0.0252\n",
            "Epoch [14742/20000], Training Loss: 0.0269\n",
            "Epoch [14743/20000], Training Loss: 0.0270\n",
            "Epoch [14744/20000], Training Loss: 0.0252\n",
            "Epoch [14745/20000], Training Loss: 0.0270\n",
            "Epoch [14746/20000], Training Loss: 0.0272\n",
            "Epoch [14747/20000], Training Loss: 0.0280\n",
            "Epoch [14748/20000], Training Loss: 0.0284\n",
            "Epoch [14749/20000], Training Loss: 0.0274\n",
            "Epoch [14750/20000], Training Loss: 0.0281\n",
            "Epoch [14751/20000], Training Loss: 0.0287\n",
            "Epoch [14752/20000], Training Loss: 0.0268\n",
            "Epoch [14753/20000], Training Loss: 0.0280\n",
            "Epoch [14754/20000], Training Loss: 0.0259\n",
            "Epoch [14755/20000], Training Loss: 0.0280\n",
            "Epoch [14756/20000], Training Loss: 0.0261\n",
            "Epoch [14757/20000], Training Loss: 0.0268\n",
            "Epoch [14758/20000], Training Loss: 0.0261\n",
            "Epoch [14759/20000], Training Loss: 0.0265\n",
            "Epoch [14760/20000], Training Loss: 0.0242\n",
            "Epoch [14761/20000], Training Loss: 0.0252\n",
            "Epoch [14762/20000], Training Loss: 0.0247\n",
            "Epoch [14763/20000], Training Loss: 0.0268\n",
            "Epoch [14764/20000], Training Loss: 0.0269\n",
            "Epoch [14765/20000], Training Loss: 0.0283\n",
            "Epoch [14766/20000], Training Loss: 0.0262\n",
            "Epoch [14767/20000], Training Loss: 0.0269\n",
            "Epoch [14768/20000], Training Loss: 0.0283\n",
            "Epoch [14769/20000], Training Loss: 0.0283\n",
            "Epoch [14770/20000], Training Loss: 0.0258\n",
            "Epoch [14771/20000], Training Loss: 0.0266\n",
            "Epoch [14772/20000], Training Loss: 0.0248\n",
            "Epoch [14773/20000], Training Loss: 0.0263\n",
            "Epoch [14774/20000], Training Loss: 0.0258\n",
            "Epoch [14775/20000], Training Loss: 0.0258\n",
            "Epoch [14776/20000], Training Loss: 0.0251\n",
            "Epoch [14777/20000], Training Loss: 0.0265\n",
            "Epoch [14778/20000], Training Loss: 0.0261\n",
            "Epoch [14779/20000], Training Loss: 0.0257\n",
            "Epoch [14780/20000], Training Loss: 0.0265\n",
            "Epoch [14781/20000], Training Loss: 0.0266\n",
            "Epoch [14782/20000], Training Loss: 0.0266\n",
            "Epoch [14783/20000], Training Loss: 0.0263\n",
            "Epoch [14784/20000], Training Loss: 0.0257\n",
            "Epoch [14785/20000], Training Loss: 0.0272\n",
            "Epoch [14786/20000], Training Loss: 0.0268\n",
            "Epoch [14787/20000], Training Loss: 0.0255\n",
            "Epoch [14788/20000], Training Loss: 0.0241\n",
            "Epoch [14789/20000], Training Loss: 0.0263\n",
            "Epoch [14790/20000], Training Loss: 0.0269\n",
            "Epoch [14791/20000], Training Loss: 0.0269\n",
            "Epoch [14792/20000], Training Loss: 0.0267\n",
            "Epoch [14793/20000], Training Loss: 0.0285\n",
            "Epoch [14794/20000], Training Loss: 0.0269\n",
            "Epoch [14795/20000], Training Loss: 0.0265\n",
            "Epoch [14796/20000], Training Loss: 0.0257\n",
            "Epoch [14797/20000], Training Loss: 0.0255\n",
            "Epoch [14798/20000], Training Loss: 0.0276\n",
            "Epoch [14799/20000], Training Loss: 0.0279\n",
            "Epoch [14800/20000], Training Loss: 0.0259\n",
            "Epoch [14801/20000], Training Loss: 0.0288\n",
            "Epoch [14802/20000], Training Loss: 0.0256\n",
            "Epoch [14803/20000], Training Loss: 0.0251\n",
            "Epoch [14804/20000], Training Loss: 0.0249\n",
            "Epoch [14805/20000], Training Loss: 0.0254\n",
            "Epoch [14806/20000], Training Loss: 0.0256\n",
            "Epoch [14807/20000], Training Loss: 0.0282\n",
            "Epoch [14808/20000], Training Loss: 0.0262\n",
            "Epoch [14809/20000], Training Loss: 0.0279\n",
            "Epoch [14810/20000], Training Loss: 0.0264\n",
            "Epoch [14811/20000], Training Loss: 0.0267\n",
            "Epoch [14812/20000], Training Loss: 0.0258\n",
            "Epoch [14813/20000], Training Loss: 0.0273\n",
            "Epoch [14814/20000], Training Loss: 0.0275\n",
            "Epoch [14815/20000], Training Loss: 0.0259\n",
            "Epoch [14816/20000], Training Loss: 0.0260\n",
            "Epoch [14817/20000], Training Loss: 0.0254\n",
            "Epoch [14818/20000], Training Loss: 0.0273\n",
            "Epoch [14819/20000], Training Loss: 0.0261\n",
            "Epoch [14820/20000], Training Loss: 0.0282\n",
            "Epoch [14821/20000], Training Loss: 0.0278\n",
            "Epoch [14822/20000], Training Loss: 0.0248\n",
            "Epoch [14823/20000], Training Loss: 0.0265\n",
            "Epoch [14824/20000], Training Loss: 0.0265\n",
            "Epoch [14825/20000], Training Loss: 0.0251\n",
            "Epoch [14826/20000], Training Loss: 0.0258\n",
            "Epoch [14827/20000], Training Loss: 0.0286\n",
            "Epoch [14828/20000], Training Loss: 0.0246\n",
            "Epoch [14829/20000], Training Loss: 0.0272\n",
            "Epoch [14830/20000], Training Loss: 0.0268\n",
            "Epoch [14831/20000], Training Loss: 0.0271\n",
            "Epoch [14832/20000], Training Loss: 0.0280\n",
            "Epoch [14833/20000], Training Loss: 0.0258\n",
            "Epoch [14834/20000], Training Loss: 0.0259\n",
            "Epoch [14835/20000], Training Loss: 0.0259\n",
            "Epoch [14836/20000], Training Loss: 0.0254\n",
            "Epoch [14837/20000], Training Loss: 0.0273\n",
            "Epoch [14838/20000], Training Loss: 0.0265\n",
            "Epoch [14839/20000], Training Loss: 0.0267\n",
            "Epoch [14840/20000], Training Loss: 0.0269\n",
            "Epoch [14841/20000], Training Loss: 0.0247\n",
            "Epoch [14842/20000], Training Loss: 0.0253\n",
            "Epoch [14843/20000], Training Loss: 0.0264\n",
            "Epoch [14844/20000], Training Loss: 0.0256\n",
            "Epoch [14845/20000], Training Loss: 0.0244\n",
            "Epoch [14846/20000], Training Loss: 0.0274\n",
            "Epoch [14847/20000], Training Loss: 0.0284\n",
            "Epoch [14848/20000], Training Loss: 0.0244\n",
            "Epoch [14849/20000], Training Loss: 0.0253\n",
            "Epoch [14850/20000], Training Loss: 0.0257\n",
            "Epoch [14851/20000], Training Loss: 0.0264\n",
            "Epoch [14852/20000], Training Loss: 0.0245\n",
            "Epoch [14853/20000], Training Loss: 0.0256\n",
            "Epoch [14854/20000], Training Loss: 0.0264\n",
            "Epoch [14855/20000], Training Loss: 0.0267\n",
            "Epoch [14856/20000], Training Loss: 0.0250\n",
            "Epoch [14857/20000], Training Loss: 0.0274\n",
            "Epoch [14858/20000], Training Loss: 0.0279\n",
            "Epoch [14859/20000], Training Loss: 0.0253\n",
            "Epoch [14860/20000], Training Loss: 0.0249\n",
            "Epoch [14861/20000], Training Loss: 0.0258\n",
            "Epoch [14862/20000], Training Loss: 0.0254\n",
            "Epoch [14863/20000], Training Loss: 0.0257\n",
            "Epoch [14864/20000], Training Loss: 0.0267\n",
            "Epoch [14865/20000], Training Loss: 0.0276\n",
            "Epoch [14866/20000], Training Loss: 0.0286\n",
            "Epoch [14867/20000], Training Loss: 0.0272\n",
            "Epoch [14868/20000], Training Loss: 0.0277\n",
            "Epoch [14869/20000], Training Loss: 0.0266\n",
            "Epoch [14870/20000], Training Loss: 0.0250\n",
            "Epoch [14871/20000], Training Loss: 0.0282\n",
            "Epoch [14872/20000], Training Loss: 0.0265\n",
            "Epoch [14873/20000], Training Loss: 0.0264\n",
            "Epoch [14874/20000], Training Loss: 0.0256\n",
            "Epoch [14875/20000], Training Loss: 0.0254\n",
            "Epoch [14876/20000], Training Loss: 0.0285\n",
            "Epoch [14877/20000], Training Loss: 0.0258\n",
            "Epoch [14878/20000], Training Loss: 0.0264\n",
            "Epoch [14879/20000], Training Loss: 0.0259\n",
            "Epoch [14880/20000], Training Loss: 0.0276\n",
            "Epoch [14881/20000], Training Loss: 0.0264\n",
            "Epoch [14882/20000], Training Loss: 0.0270\n",
            "Epoch [14883/20000], Training Loss: 0.0271\n",
            "Epoch [14884/20000], Training Loss: 0.0269\n",
            "Epoch [14885/20000], Training Loss: 0.0273\n",
            "Epoch [14886/20000], Training Loss: 0.0279\n",
            "Epoch [14887/20000], Training Loss: 0.0263\n",
            "Epoch [14888/20000], Training Loss: 0.0270\n",
            "Epoch [14889/20000], Training Loss: 0.0269\n",
            "Epoch [14890/20000], Training Loss: 0.0255\n",
            "Epoch [14891/20000], Training Loss: 0.0274\n",
            "Epoch [14892/20000], Training Loss: 0.0268\n",
            "Epoch [14893/20000], Training Loss: 0.0248\n",
            "Epoch [14894/20000], Training Loss: 0.0287\n",
            "Epoch [14895/20000], Training Loss: 0.0267\n",
            "Epoch [14896/20000], Training Loss: 0.0250\n",
            "Epoch [14897/20000], Training Loss: 0.0258\n",
            "Epoch [14898/20000], Training Loss: 0.0269\n",
            "Epoch [14899/20000], Training Loss: 0.0273\n",
            "Epoch [14900/20000], Training Loss: 0.0273\n",
            "Epoch [14901/20000], Training Loss: 0.0275\n",
            "Epoch [14902/20000], Training Loss: 0.0261\n",
            "Epoch [14903/20000], Training Loss: 0.0256\n",
            "Epoch [14904/20000], Training Loss: 0.0254\n",
            "Epoch [14905/20000], Training Loss: 0.0260\n",
            "Epoch [14906/20000], Training Loss: 0.0265\n",
            "Epoch [14907/20000], Training Loss: 0.0273\n",
            "Epoch [14908/20000], Training Loss: 0.0290\n",
            "Epoch [14909/20000], Training Loss: 0.0259\n",
            "Epoch [14910/20000], Training Loss: 0.0251\n",
            "Epoch [14911/20000], Training Loss: 0.0269\n",
            "Epoch [14912/20000], Training Loss: 0.0285\n",
            "Epoch [14913/20000], Training Loss: 0.0265\n",
            "Epoch [14914/20000], Training Loss: 0.0285\n",
            "Epoch [14915/20000], Training Loss: 0.0253\n",
            "Epoch [14916/20000], Training Loss: 0.0263\n",
            "Epoch [14917/20000], Training Loss: 0.0270\n",
            "Epoch [14918/20000], Training Loss: 0.0275\n",
            "Epoch [14919/20000], Training Loss: 0.0252\n",
            "Epoch [14920/20000], Training Loss: 0.0284\n",
            "Epoch [14921/20000], Training Loss: 0.0274\n",
            "Epoch [14922/20000], Training Loss: 0.0276\n",
            "Epoch [14923/20000], Training Loss: 0.0248\n",
            "Epoch [14924/20000], Training Loss: 0.0259\n",
            "Epoch [14925/20000], Training Loss: 0.0249\n",
            "Epoch [14926/20000], Training Loss: 0.0275\n",
            "Epoch [14927/20000], Training Loss: 0.0271\n",
            "Epoch [14928/20000], Training Loss: 0.0256\n",
            "Epoch [14929/20000], Training Loss: 0.0265\n",
            "Epoch [14930/20000], Training Loss: 0.0250\n",
            "Epoch [14931/20000], Training Loss: 0.0271\n",
            "Epoch [14932/20000], Training Loss: 0.0256\n",
            "Epoch [14933/20000], Training Loss: 0.0256\n",
            "Epoch [14934/20000], Training Loss: 0.0272\n",
            "Epoch [14935/20000], Training Loss: 0.0261\n",
            "Epoch [14936/20000], Training Loss: 0.0250\n",
            "Epoch [14937/20000], Training Loss: 0.0262\n",
            "Epoch [14938/20000], Training Loss: 0.0245\n",
            "Epoch [14939/20000], Training Loss: 0.0249\n",
            "Epoch [14940/20000], Training Loss: 0.0280\n",
            "Epoch [14941/20000], Training Loss: 0.0275\n",
            "Epoch [14942/20000], Training Loss: 0.0266\n",
            "Epoch [14943/20000], Training Loss: 0.0284\n",
            "Epoch [14944/20000], Training Loss: 0.0253\n",
            "Epoch [14945/20000], Training Loss: 0.0264\n",
            "Epoch [14946/20000], Training Loss: 0.0278\n",
            "Epoch [14947/20000], Training Loss: 0.0270\n",
            "Epoch [14948/20000], Training Loss: 0.0277\n",
            "Epoch [14949/20000], Training Loss: 0.0263\n",
            "Epoch [14950/20000], Training Loss: 0.0251\n",
            "Epoch [14951/20000], Training Loss: 0.0266\n",
            "Epoch [14952/20000], Training Loss: 0.0256\n",
            "Epoch [14953/20000], Training Loss: 0.0259\n",
            "Epoch [14954/20000], Training Loss: 0.0243\n",
            "Epoch [14955/20000], Training Loss: 0.0265\n",
            "Epoch [14956/20000], Training Loss: 0.0244\n",
            "Epoch [14957/20000], Training Loss: 0.0245\n",
            "Epoch [14958/20000], Training Loss: 0.0252\n",
            "Epoch [14959/20000], Training Loss: 0.0266\n",
            "Epoch [14960/20000], Training Loss: 0.0256\n",
            "Epoch [14961/20000], Training Loss: 0.0282\n",
            "Epoch [14962/20000], Training Loss: 0.0265\n",
            "Epoch [14963/20000], Training Loss: 0.0252\n",
            "Epoch [14964/20000], Training Loss: 0.0249\n",
            "Epoch [14965/20000], Training Loss: 0.0271\n",
            "Epoch [14966/20000], Training Loss: 0.0268\n",
            "Epoch [14967/20000], Training Loss: 0.0290\n",
            "Epoch [14968/20000], Training Loss: 0.0255\n",
            "Epoch [14969/20000], Training Loss: 0.0270\n",
            "Epoch [14970/20000], Training Loss: 0.0254\n",
            "Epoch [14971/20000], Training Loss: 0.0277\n",
            "Epoch [14972/20000], Training Loss: 0.0268\n",
            "Epoch [14973/20000], Training Loss: 0.0242\n",
            "Epoch [14974/20000], Training Loss: 0.0280\n",
            "Epoch [14975/20000], Training Loss: 0.0261\n",
            "Epoch [14976/20000], Training Loss: 0.0236\n",
            "Epoch [14977/20000], Training Loss: 0.0280\n",
            "Epoch [14978/20000], Training Loss: 0.0250\n",
            "Epoch [14979/20000], Training Loss: 0.0247\n",
            "Epoch [14980/20000], Training Loss: 0.0278\n",
            "Epoch [14981/20000], Training Loss: 0.0264\n",
            "Epoch [14982/20000], Training Loss: 0.0280\n",
            "Epoch [14983/20000], Training Loss: 0.0275\n",
            "Epoch [14984/20000], Training Loss: 0.0250\n",
            "Epoch [14985/20000], Training Loss: 0.0260\n",
            "Epoch [14986/20000], Training Loss: 0.0245\n",
            "Epoch [14987/20000], Training Loss: 0.0284\n",
            "Epoch [14988/20000], Training Loss: 0.0271\n",
            "Epoch [14989/20000], Training Loss: 0.0249\n",
            "Epoch [14990/20000], Training Loss: 0.0265\n",
            "Epoch [14991/20000], Training Loss: 0.0276\n",
            "Epoch [14992/20000], Training Loss: 0.0253\n",
            "Epoch [14993/20000], Training Loss: 0.0249\n",
            "Epoch [14994/20000], Training Loss: 0.0261\n",
            "Epoch [14995/20000], Training Loss: 0.0249\n",
            "Epoch [14996/20000], Training Loss: 0.0252\n",
            "Epoch [14997/20000], Training Loss: 0.0265\n",
            "Epoch [14998/20000], Training Loss: 0.0260\n",
            "Epoch [14999/20000], Training Loss: 0.0277\n",
            "Epoch [15000/20000], Training Loss: 0.0271\n",
            "Epoch [15001/20000], Training Loss: 0.0269\n",
            "Epoch [15002/20000], Training Loss: 0.0249\n",
            "Epoch [15003/20000], Training Loss: 0.0252\n",
            "Epoch [15004/20000], Training Loss: 0.0280\n",
            "Epoch [15005/20000], Training Loss: 0.0281\n",
            "Epoch [15006/20000], Training Loss: 0.0265\n",
            "Epoch [15007/20000], Training Loss: 0.0268\n",
            "Epoch [15008/20000], Training Loss: 0.0265\n",
            "Epoch [15009/20000], Training Loss: 0.0267\n",
            "Epoch [15010/20000], Training Loss: 0.0262\n",
            "Epoch [15011/20000], Training Loss: 0.0267\n",
            "Epoch [15012/20000], Training Loss: 0.0262\n",
            "Epoch [15013/20000], Training Loss: 0.0259\n",
            "Epoch [15014/20000], Training Loss: 0.0266\n",
            "Epoch [15015/20000], Training Loss: 0.0265\n",
            "Epoch [15016/20000], Training Loss: 0.0247\n",
            "Epoch [15017/20000], Training Loss: 0.0262\n",
            "Epoch [15018/20000], Training Loss: 0.0269\n",
            "Epoch [15019/20000], Training Loss: 0.0297\n",
            "Epoch [15020/20000], Training Loss: 0.0257\n",
            "Epoch [15021/20000], Training Loss: 0.0257\n",
            "Epoch [15022/20000], Training Loss: 0.0287\n",
            "Epoch [15023/20000], Training Loss: 0.0262\n",
            "Epoch [15024/20000], Training Loss: 0.0241\n",
            "Epoch [15025/20000], Training Loss: 0.0252\n",
            "Epoch [15026/20000], Training Loss: 0.0264\n",
            "Epoch [15027/20000], Training Loss: 0.0272\n",
            "Epoch [15028/20000], Training Loss: 0.0253\n",
            "Epoch [15029/20000], Training Loss: 0.0272\n",
            "Epoch [15030/20000], Training Loss: 0.0269\n",
            "Epoch [15031/20000], Training Loss: 0.0264\n",
            "Epoch [15032/20000], Training Loss: 0.0261\n",
            "Epoch [15033/20000], Training Loss: 0.0266\n",
            "Epoch [15034/20000], Training Loss: 0.0247\n",
            "Epoch [15035/20000], Training Loss: 0.0270\n",
            "Epoch [15036/20000], Training Loss: 0.0261\n",
            "Epoch [15037/20000], Training Loss: 0.0274\n",
            "Epoch [15038/20000], Training Loss: 0.0275\n",
            "Epoch [15039/20000], Training Loss: 0.0258\n",
            "Epoch [15040/20000], Training Loss: 0.0252\n",
            "Epoch [15041/20000], Training Loss: 0.0268\n",
            "Epoch [15042/20000], Training Loss: 0.0271\n",
            "Epoch [15043/20000], Training Loss: 0.0258\n",
            "Epoch [15044/20000], Training Loss: 0.0249\n",
            "Epoch [15045/20000], Training Loss: 0.0261\n",
            "Epoch [15046/20000], Training Loss: 0.0278\n",
            "Epoch [15047/20000], Training Loss: 0.0250\n",
            "Epoch [15048/20000], Training Loss: 0.0274\n",
            "Epoch [15049/20000], Training Loss: 0.0261\n",
            "Epoch [15050/20000], Training Loss: 0.0264\n",
            "Epoch [15051/20000], Training Loss: 0.0245\n",
            "Epoch [15052/20000], Training Loss: 0.0267\n",
            "Epoch [15053/20000], Training Loss: 0.0246\n",
            "Epoch [15054/20000], Training Loss: 0.0267\n",
            "Epoch [15055/20000], Training Loss: 0.0282\n",
            "Epoch [15056/20000], Training Loss: 0.0264\n",
            "Epoch [15057/20000], Training Loss: 0.0273\n",
            "Epoch [15058/20000], Training Loss: 0.0265\n",
            "Epoch [15059/20000], Training Loss: 0.0252\n",
            "Epoch [15060/20000], Training Loss: 0.0269\n",
            "Epoch [15061/20000], Training Loss: 0.0269\n",
            "Epoch [15062/20000], Training Loss: 0.0269\n",
            "Epoch [15063/20000], Training Loss: 0.0264\n",
            "Epoch [15064/20000], Training Loss: 0.0264\n",
            "Epoch [15065/20000], Training Loss: 0.0269\n",
            "Epoch [15066/20000], Training Loss: 0.0271\n",
            "Epoch [15067/20000], Training Loss: 0.0273\n",
            "Epoch [15068/20000], Training Loss: 0.0259\n",
            "Epoch [15069/20000], Training Loss: 0.0268\n",
            "Epoch [15070/20000], Training Loss: 0.0284\n",
            "Epoch [15071/20000], Training Loss: 0.0261\n",
            "Epoch [15072/20000], Training Loss: 0.0259\n",
            "Epoch [15073/20000], Training Loss: 0.0259\n",
            "Epoch [15074/20000], Training Loss: 0.0274\n",
            "Epoch [15075/20000], Training Loss: 0.0290\n",
            "Epoch [15076/20000], Training Loss: 0.0266\n",
            "Epoch [15077/20000], Training Loss: 0.0250\n",
            "Epoch [15078/20000], Training Loss: 0.0278\n",
            "Epoch [15079/20000], Training Loss: 0.0261\n",
            "Epoch [15080/20000], Training Loss: 0.0279\n",
            "Epoch [15081/20000], Training Loss: 0.0251\n",
            "Epoch [15082/20000], Training Loss: 0.0264\n",
            "Epoch [15083/20000], Training Loss: 0.0264\n",
            "Epoch [15084/20000], Training Loss: 0.0257\n",
            "Epoch [15085/20000], Training Loss: 0.0264\n",
            "Epoch [15086/20000], Training Loss: 0.0259\n",
            "Epoch [15087/20000], Training Loss: 0.0251\n",
            "Epoch [15088/20000], Training Loss: 0.0257\n",
            "Epoch [15089/20000], Training Loss: 0.0252\n",
            "Epoch [15090/20000], Training Loss: 0.0281\n",
            "Epoch [15091/20000], Training Loss: 0.0267\n",
            "Epoch [15092/20000], Training Loss: 0.0264\n",
            "Epoch [15093/20000], Training Loss: 0.0250\n",
            "Epoch [15094/20000], Training Loss: 0.0257\n",
            "Epoch [15095/20000], Training Loss: 0.0267\n",
            "Epoch [15096/20000], Training Loss: 0.0274\n",
            "Epoch [15097/20000], Training Loss: 0.0284\n",
            "Epoch [15098/20000], Training Loss: 0.0284\n",
            "Epoch [15099/20000], Training Loss: 0.0257\n",
            "Epoch [15100/20000], Training Loss: 0.0253\n",
            "Epoch [15101/20000], Training Loss: 0.0271\n",
            "Epoch [15102/20000], Training Loss: 0.0257\n",
            "Epoch [15103/20000], Training Loss: 0.0250\n",
            "Epoch [15104/20000], Training Loss: 0.0272\n",
            "Epoch [15105/20000], Training Loss: 0.0263\n",
            "Epoch [15106/20000], Training Loss: 0.0268\n",
            "Epoch [15107/20000], Training Loss: 0.0258\n",
            "Epoch [15108/20000], Training Loss: 0.0272\n",
            "Epoch [15109/20000], Training Loss: 0.0264\n",
            "Epoch [15110/20000], Training Loss: 0.0276\n",
            "Epoch [15111/20000], Training Loss: 0.0257\n",
            "Epoch [15112/20000], Training Loss: 0.0268\n",
            "Epoch [15113/20000], Training Loss: 0.0267\n",
            "Epoch [15114/20000], Training Loss: 0.0264\n",
            "Epoch [15115/20000], Training Loss: 0.0277\n",
            "Epoch [15116/20000], Training Loss: 0.0266\n",
            "Epoch [15117/20000], Training Loss: 0.0259\n",
            "Epoch [15118/20000], Training Loss: 0.0253\n",
            "Epoch [15119/20000], Training Loss: 0.0283\n",
            "Epoch [15120/20000], Training Loss: 0.0276\n",
            "Epoch [15121/20000], Training Loss: 0.0256\n",
            "Epoch [15122/20000], Training Loss: 0.0278\n",
            "Epoch [15123/20000], Training Loss: 0.0251\n",
            "Epoch [15124/20000], Training Loss: 0.0257\n",
            "Epoch [15125/20000], Training Loss: 0.0244\n",
            "Epoch [15126/20000], Training Loss: 0.0277\n",
            "Epoch [15127/20000], Training Loss: 0.0273\n",
            "Epoch [15128/20000], Training Loss: 0.0273\n",
            "Epoch [15129/20000], Training Loss: 0.0267\n",
            "Epoch [15130/20000], Training Loss: 0.0253\n",
            "Epoch [15131/20000], Training Loss: 0.0263\n",
            "Epoch [15132/20000], Training Loss: 0.0267\n",
            "Epoch [15133/20000], Training Loss: 0.0242\n",
            "Epoch [15134/20000], Training Loss: 0.0274\n",
            "Epoch [15135/20000], Training Loss: 0.0242\n",
            "Epoch [15136/20000], Training Loss: 0.0265\n",
            "Epoch [15137/20000], Training Loss: 0.0250\n",
            "Epoch [15138/20000], Training Loss: 0.0272\n",
            "Epoch [15139/20000], Training Loss: 0.0272\n",
            "Epoch [15140/20000], Training Loss: 0.0265\n",
            "Epoch [15141/20000], Training Loss: 0.0258\n",
            "Epoch [15142/20000], Training Loss: 0.0255\n",
            "Epoch [15143/20000], Training Loss: 0.0284\n",
            "Epoch [15144/20000], Training Loss: 0.0258\n",
            "Epoch [15145/20000], Training Loss: 0.0250\n",
            "Epoch [15146/20000], Training Loss: 0.0278\n",
            "Epoch [15147/20000], Training Loss: 0.0285\n",
            "Epoch [15148/20000], Training Loss: 0.0255\n",
            "Epoch [15149/20000], Training Loss: 0.0243\n",
            "Epoch [15150/20000], Training Loss: 0.0253\n",
            "Epoch [15151/20000], Training Loss: 0.0262\n",
            "Epoch [15152/20000], Training Loss: 0.0254\n",
            "Epoch [15153/20000], Training Loss: 0.0242\n",
            "Epoch [15154/20000], Training Loss: 0.0267\n",
            "Epoch [15155/20000], Training Loss: 0.0250\n",
            "Epoch [15156/20000], Training Loss: 0.0268\n",
            "Epoch [15157/20000], Training Loss: 0.0273\n",
            "Epoch [15158/20000], Training Loss: 0.0266\n",
            "Epoch [15159/20000], Training Loss: 0.0253\n",
            "Epoch [15160/20000], Training Loss: 0.0256\n",
            "Epoch [15161/20000], Training Loss: 0.0255\n",
            "Epoch [15162/20000], Training Loss: 0.0269\n",
            "Epoch [15163/20000], Training Loss: 0.0261\n",
            "Epoch [15164/20000], Training Loss: 0.0284\n",
            "Epoch [15165/20000], Training Loss: 0.0263\n",
            "Epoch [15166/20000], Training Loss: 0.0276\n",
            "Epoch [15167/20000], Training Loss: 0.0240\n",
            "Epoch [15168/20000], Training Loss: 0.0261\n",
            "Epoch [15169/20000], Training Loss: 0.0262\n",
            "Epoch [15170/20000], Training Loss: 0.0285\n",
            "Epoch [15171/20000], Training Loss: 0.0253\n",
            "Epoch [15172/20000], Training Loss: 0.0246\n",
            "Epoch [15173/20000], Training Loss: 0.0272\n",
            "Epoch [15174/20000], Training Loss: 0.0245\n",
            "Epoch [15175/20000], Training Loss: 0.0262\n",
            "Epoch [15176/20000], Training Loss: 0.0267\n",
            "Epoch [15177/20000], Training Loss: 0.0286\n",
            "Epoch [15178/20000], Training Loss: 0.0256\n",
            "Epoch [15179/20000], Training Loss: 0.0259\n",
            "Epoch [15180/20000], Training Loss: 0.0257\n",
            "Epoch [15181/20000], Training Loss: 0.0258\n",
            "Epoch [15182/20000], Training Loss: 0.0264\n",
            "Epoch [15183/20000], Training Loss: 0.0255\n",
            "Epoch [15184/20000], Training Loss: 0.0269\n",
            "Epoch [15185/20000], Training Loss: 0.0259\n",
            "Epoch [15186/20000], Training Loss: 0.0272\n",
            "Epoch [15187/20000], Training Loss: 0.0243\n",
            "Epoch [15188/20000], Training Loss: 0.0268\n",
            "Epoch [15189/20000], Training Loss: 0.0269\n",
            "Epoch [15190/20000], Training Loss: 0.0270\n",
            "Epoch [15191/20000], Training Loss: 0.0264\n",
            "Epoch [15192/20000], Training Loss: 0.0263\n",
            "Epoch [15193/20000], Training Loss: 0.0288\n",
            "Epoch [15194/20000], Training Loss: 0.0294\n",
            "Epoch [15195/20000], Training Loss: 0.0266\n",
            "Epoch [15196/20000], Training Loss: 0.0274\n",
            "Epoch [15197/20000], Training Loss: 0.0263\n",
            "Epoch [15198/20000], Training Loss: 0.0256\n",
            "Epoch [15199/20000], Training Loss: 0.0264\n",
            "Epoch [15200/20000], Training Loss: 0.0261\n",
            "Epoch [15201/20000], Training Loss: 0.0254\n",
            "Epoch [15202/20000], Training Loss: 0.0260\n",
            "Epoch [15203/20000], Training Loss: 0.0271\n",
            "Epoch [15204/20000], Training Loss: 0.0283\n",
            "Epoch [15205/20000], Training Loss: 0.0283\n",
            "Epoch [15206/20000], Training Loss: 0.0277\n",
            "Epoch [15207/20000], Training Loss: 0.0256\n",
            "Epoch [15208/20000], Training Loss: 0.0253\n",
            "Epoch [15209/20000], Training Loss: 0.0274\n",
            "Epoch [15210/20000], Training Loss: 0.0277\n",
            "Epoch [15211/20000], Training Loss: 0.0257\n",
            "Epoch [15212/20000], Training Loss: 0.0256\n",
            "Epoch [15213/20000], Training Loss: 0.0256\n",
            "Epoch [15214/20000], Training Loss: 0.0255\n",
            "Epoch [15215/20000], Training Loss: 0.0271\n",
            "Epoch [15216/20000], Training Loss: 0.0254\n",
            "Epoch [15217/20000], Training Loss: 0.0266\n",
            "Epoch [15218/20000], Training Loss: 0.0279\n",
            "Epoch [15219/20000], Training Loss: 0.0274\n",
            "Epoch [15220/20000], Training Loss: 0.0281\n",
            "Epoch [15221/20000], Training Loss: 0.0265\n",
            "Epoch [15222/20000], Training Loss: 0.0259\n",
            "Epoch [15223/20000], Training Loss: 0.0269\n",
            "Epoch [15224/20000], Training Loss: 0.0258\n",
            "Epoch [15225/20000], Training Loss: 0.0264\n",
            "Epoch [15226/20000], Training Loss: 0.0268\n",
            "Epoch [15227/20000], Training Loss: 0.0253\n",
            "Epoch [15228/20000], Training Loss: 0.0255\n",
            "Epoch [15229/20000], Training Loss: 0.0274\n",
            "Epoch [15230/20000], Training Loss: 0.0259\n",
            "Epoch [15231/20000], Training Loss: 0.0281\n",
            "Epoch [15232/20000], Training Loss: 0.0259\n",
            "Epoch [15233/20000], Training Loss: 0.0288\n",
            "Epoch [15234/20000], Training Loss: 0.0237\n",
            "Epoch [15235/20000], Training Loss: 0.0267\n",
            "Epoch [15236/20000], Training Loss: 0.0284\n",
            "Epoch [15237/20000], Training Loss: 0.0304\n",
            "Epoch [15238/20000], Training Loss: 0.0275\n",
            "Epoch [15239/20000], Training Loss: 0.0269\n",
            "Epoch [15240/20000], Training Loss: 0.0245\n",
            "Epoch [15241/20000], Training Loss: 0.0271\n",
            "Epoch [15242/20000], Training Loss: 0.0282\n",
            "Epoch [15243/20000], Training Loss: 0.0283\n",
            "Epoch [15244/20000], Training Loss: 0.0245\n",
            "Epoch [15245/20000], Training Loss: 0.0256\n",
            "Epoch [15246/20000], Training Loss: 0.0278\n",
            "Epoch [15247/20000], Training Loss: 0.0268\n",
            "Epoch [15248/20000], Training Loss: 0.0266\n",
            "Epoch [15249/20000], Training Loss: 0.0252\n",
            "Epoch [15250/20000], Training Loss: 0.0288\n",
            "Epoch [15251/20000], Training Loss: 0.0269\n",
            "Epoch [15252/20000], Training Loss: 0.0276\n",
            "Epoch [15253/20000], Training Loss: 0.0261\n",
            "Epoch [15254/20000], Training Loss: 0.0264\n",
            "Epoch [15255/20000], Training Loss: 0.0277\n",
            "Epoch [15256/20000], Training Loss: 0.0256\n",
            "Epoch [15257/20000], Training Loss: 0.0249\n",
            "Epoch [15258/20000], Training Loss: 0.0251\n",
            "Epoch [15259/20000], Training Loss: 0.0281\n",
            "Epoch [15260/20000], Training Loss: 0.0284\n",
            "Epoch [15261/20000], Training Loss: 0.0249\n",
            "Epoch [15262/20000], Training Loss: 0.0255\n",
            "Epoch [15263/20000], Training Loss: 0.0258\n",
            "Epoch [15264/20000], Training Loss: 0.0261\n",
            "Epoch [15265/20000], Training Loss: 0.0260\n",
            "Epoch [15266/20000], Training Loss: 0.0264\n",
            "Epoch [15267/20000], Training Loss: 0.0277\n",
            "Epoch [15268/20000], Training Loss: 0.0289\n",
            "Epoch [15269/20000], Training Loss: 0.0249\n",
            "Epoch [15270/20000], Training Loss: 0.0275\n",
            "Epoch [15271/20000], Training Loss: 0.0244\n",
            "Epoch [15272/20000], Training Loss: 0.0277\n",
            "Epoch [15273/20000], Training Loss: 0.0255\n",
            "Epoch [15274/20000], Training Loss: 0.0262\n",
            "Epoch [15275/20000], Training Loss: 0.0271\n",
            "Epoch [15276/20000], Training Loss: 0.0271\n",
            "Epoch [15277/20000], Training Loss: 0.0255\n",
            "Epoch [15278/20000], Training Loss: 0.0277\n",
            "Epoch [15279/20000], Training Loss: 0.0272\n",
            "Epoch [15280/20000], Training Loss: 0.0257\n",
            "Epoch [15281/20000], Training Loss: 0.0267\n",
            "Epoch [15282/20000], Training Loss: 0.0246\n",
            "Epoch [15283/20000], Training Loss: 0.0267\n",
            "Epoch [15284/20000], Training Loss: 0.0261\n",
            "Epoch [15285/20000], Training Loss: 0.0254\n",
            "Epoch [15286/20000], Training Loss: 0.0266\n",
            "Epoch [15287/20000], Training Loss: 0.0253\n",
            "Epoch [15288/20000], Training Loss: 0.0266\n",
            "Epoch [15289/20000], Training Loss: 0.0244\n",
            "Epoch [15290/20000], Training Loss: 0.0249\n",
            "Epoch [15291/20000], Training Loss: 0.0259\n",
            "Epoch [15292/20000], Training Loss: 0.0271\n",
            "Epoch [15293/20000], Training Loss: 0.0289\n",
            "Epoch [15294/20000], Training Loss: 0.0263\n",
            "Epoch [15295/20000], Training Loss: 0.0259\n",
            "Epoch [15296/20000], Training Loss: 0.0273\n",
            "Epoch [15297/20000], Training Loss: 0.0253\n",
            "Epoch [15298/20000], Training Loss: 0.0248\n",
            "Epoch [15299/20000], Training Loss: 0.0259\n",
            "Epoch [15300/20000], Training Loss: 0.0264\n",
            "Epoch [15301/20000], Training Loss: 0.0284\n",
            "Epoch [15302/20000], Training Loss: 0.0269\n",
            "Epoch [15303/20000], Training Loss: 0.0240\n",
            "Epoch [15304/20000], Training Loss: 0.0281\n",
            "Epoch [15305/20000], Training Loss: 0.0274\n",
            "Epoch [15306/20000], Training Loss: 0.0285\n",
            "Epoch [15307/20000], Training Loss: 0.0258\n",
            "Epoch [15308/20000], Training Loss: 0.0277\n",
            "Epoch [15309/20000], Training Loss: 0.0254\n",
            "Epoch [15310/20000], Training Loss: 0.0265\n",
            "Epoch [15311/20000], Training Loss: 0.0261\n",
            "Epoch [15312/20000], Training Loss: 0.0234\n",
            "Epoch [15313/20000], Training Loss: 0.0262\n",
            "Epoch [15314/20000], Training Loss: 0.0251\n",
            "Epoch [15315/20000], Training Loss: 0.0267\n",
            "Epoch [15316/20000], Training Loss: 0.0267\n",
            "Epoch [15317/20000], Training Loss: 0.0249\n",
            "Epoch [15318/20000], Training Loss: 0.0267\n",
            "Epoch [15319/20000], Training Loss: 0.0261\n",
            "Epoch [15320/20000], Training Loss: 0.0293\n",
            "Epoch [15321/20000], Training Loss: 0.0257\n",
            "Epoch [15322/20000], Training Loss: 0.0254\n",
            "Epoch [15323/20000], Training Loss: 0.0261\n",
            "Epoch [15324/20000], Training Loss: 0.0240\n",
            "Epoch [15325/20000], Training Loss: 0.0258\n",
            "Epoch [15326/20000], Training Loss: 0.0263\n",
            "Epoch [15327/20000], Training Loss: 0.0286\n",
            "Epoch [15328/20000], Training Loss: 0.0247\n",
            "Epoch [15329/20000], Training Loss: 0.0256\n",
            "Epoch [15330/20000], Training Loss: 0.0253\n",
            "Epoch [15331/20000], Training Loss: 0.0273\n",
            "Epoch [15332/20000], Training Loss: 0.0265\n",
            "Epoch [15333/20000], Training Loss: 0.0277\n",
            "Epoch [15334/20000], Training Loss: 0.0273\n",
            "Epoch [15335/20000], Training Loss: 0.0260\n",
            "Epoch [15336/20000], Training Loss: 0.0282\n",
            "Epoch [15337/20000], Training Loss: 0.0284\n",
            "Epoch [15338/20000], Training Loss: 0.0262\n",
            "Epoch [15339/20000], Training Loss: 0.0266\n",
            "Epoch [15340/20000], Training Loss: 0.0252\n",
            "Epoch [15341/20000], Training Loss: 0.0250\n",
            "Epoch [15342/20000], Training Loss: 0.0261\n",
            "Epoch [15343/20000], Training Loss: 0.0243\n",
            "Epoch [15344/20000], Training Loss: 0.0269\n",
            "Epoch [15345/20000], Training Loss: 0.0271\n",
            "Epoch [15346/20000], Training Loss: 0.0266\n",
            "Epoch [15347/20000], Training Loss: 0.0265\n",
            "Epoch [15348/20000], Training Loss: 0.0266\n",
            "Epoch [15349/20000], Training Loss: 0.0272\n",
            "Epoch [15350/20000], Training Loss: 0.0272\n",
            "Epoch [15351/20000], Training Loss: 0.0257\n",
            "Epoch [15352/20000], Training Loss: 0.0247\n",
            "Epoch [15353/20000], Training Loss: 0.0260\n",
            "Epoch [15354/20000], Training Loss: 0.0247\n",
            "Epoch [15355/20000], Training Loss: 0.0260\n",
            "Epoch [15356/20000], Training Loss: 0.0283\n",
            "Epoch [15357/20000], Training Loss: 0.0261\n",
            "Epoch [15358/20000], Training Loss: 0.0266\n",
            "Epoch [15359/20000], Training Loss: 0.0268\n",
            "Epoch [15360/20000], Training Loss: 0.0252\n",
            "Epoch [15361/20000], Training Loss: 0.0274\n",
            "Epoch [15362/20000], Training Loss: 0.0272\n",
            "Epoch [15363/20000], Training Loss: 0.0239\n",
            "Epoch [15364/20000], Training Loss: 0.0259\n",
            "Epoch [15365/20000], Training Loss: 0.0256\n",
            "Epoch [15366/20000], Training Loss: 0.0261\n",
            "Epoch [15367/20000], Training Loss: 0.0260\n",
            "Epoch [15368/20000], Training Loss: 0.0257\n",
            "Epoch [15369/20000], Training Loss: 0.0255\n",
            "Epoch [15370/20000], Training Loss: 0.0275\n",
            "Epoch [15371/20000], Training Loss: 0.0266\n",
            "Epoch [15372/20000], Training Loss: 0.0278\n",
            "Epoch [15373/20000], Training Loss: 0.0250\n",
            "Epoch [15374/20000], Training Loss: 0.0257\n",
            "Epoch [15375/20000], Training Loss: 0.0270\n",
            "Epoch [15376/20000], Training Loss: 0.0259\n",
            "Epoch [15377/20000], Training Loss: 0.0281\n",
            "Epoch [15378/20000], Training Loss: 0.0251\n",
            "Epoch [15379/20000], Training Loss: 0.0261\n",
            "Epoch [15380/20000], Training Loss: 0.0264\n",
            "Epoch [15381/20000], Training Loss: 0.0258\n",
            "Epoch [15382/20000], Training Loss: 0.0288\n",
            "Epoch [15383/20000], Training Loss: 0.0286\n",
            "Epoch [15384/20000], Training Loss: 0.0246\n",
            "Epoch [15385/20000], Training Loss: 0.0272\n",
            "Epoch [15386/20000], Training Loss: 0.0275\n",
            "Epoch [15387/20000], Training Loss: 0.0279\n",
            "Epoch [15388/20000], Training Loss: 0.0278\n",
            "Epoch [15389/20000], Training Loss: 0.0275\n",
            "Epoch [15390/20000], Training Loss: 0.0261\n",
            "Epoch [15391/20000], Training Loss: 0.0273\n",
            "Epoch [15392/20000], Training Loss: 0.0260\n",
            "Epoch [15393/20000], Training Loss: 0.0250\n",
            "Epoch [15394/20000], Training Loss: 0.0257\n",
            "Epoch [15395/20000], Training Loss: 0.0273\n",
            "Epoch [15396/20000], Training Loss: 0.0255\n",
            "Epoch [15397/20000], Training Loss: 0.0260\n",
            "Epoch [15398/20000], Training Loss: 0.0278\n",
            "Epoch [15399/20000], Training Loss: 0.0275\n",
            "Epoch [15400/20000], Training Loss: 0.0286\n",
            "Epoch [15401/20000], Training Loss: 0.0254\n",
            "Epoch [15402/20000], Training Loss: 0.0257\n",
            "Epoch [15403/20000], Training Loss: 0.0262\n",
            "Epoch [15404/20000], Training Loss: 0.0263\n",
            "Epoch [15405/20000], Training Loss: 0.0248\n",
            "Epoch [15406/20000], Training Loss: 0.0252\n",
            "Epoch [15407/20000], Training Loss: 0.0252\n",
            "Epoch [15408/20000], Training Loss: 0.0264\n",
            "Epoch [15409/20000], Training Loss: 0.0249\n",
            "Epoch [15410/20000], Training Loss: 0.0261\n",
            "Epoch [15411/20000], Training Loss: 0.0271\n",
            "Epoch [15412/20000], Training Loss: 0.0268\n",
            "Epoch [15413/20000], Training Loss: 0.0273\n",
            "Epoch [15414/20000], Training Loss: 0.0263\n",
            "Epoch [15415/20000], Training Loss: 0.0268\n",
            "Epoch [15416/20000], Training Loss: 0.0263\n",
            "Epoch [15417/20000], Training Loss: 0.0264\n",
            "Epoch [15418/20000], Training Loss: 0.0275\n",
            "Epoch [15419/20000], Training Loss: 0.0274\n",
            "Epoch [15420/20000], Training Loss: 0.0276\n",
            "Epoch [15421/20000], Training Loss: 0.0239\n",
            "Epoch [15422/20000], Training Loss: 0.0274\n",
            "Epoch [15423/20000], Training Loss: 0.0264\n",
            "Epoch [15424/20000], Training Loss: 0.0274\n",
            "Epoch [15425/20000], Training Loss: 0.0261\n",
            "Epoch [15426/20000], Training Loss: 0.0283\n",
            "Epoch [15427/20000], Training Loss: 0.0276\n",
            "Epoch [15428/20000], Training Loss: 0.0268\n",
            "Epoch [15429/20000], Training Loss: 0.0268\n",
            "Epoch [15430/20000], Training Loss: 0.0266\n",
            "Epoch [15431/20000], Training Loss: 0.0256\n",
            "Epoch [15432/20000], Training Loss: 0.0255\n",
            "Epoch [15433/20000], Training Loss: 0.0279\n",
            "Epoch [15434/20000], Training Loss: 0.0277\n",
            "Epoch [15435/20000], Training Loss: 0.0252\n",
            "Epoch [15436/20000], Training Loss: 0.0261\n",
            "Epoch [15437/20000], Training Loss: 0.0245\n",
            "Epoch [15438/20000], Training Loss: 0.0258\n",
            "Epoch [15439/20000], Training Loss: 0.0292\n",
            "Epoch [15440/20000], Training Loss: 0.0272\n",
            "Epoch [15441/20000], Training Loss: 0.0279\n",
            "Epoch [15442/20000], Training Loss: 0.0259\n",
            "Epoch [15443/20000], Training Loss: 0.0260\n",
            "Epoch [15444/20000], Training Loss: 0.0256\n",
            "Epoch [15445/20000], Training Loss: 0.0265\n",
            "Epoch [15446/20000], Training Loss: 0.0294\n",
            "Epoch [15447/20000], Training Loss: 0.0268\n",
            "Epoch [15448/20000], Training Loss: 0.0268\n",
            "Epoch [15449/20000], Training Loss: 0.0262\n",
            "Epoch [15450/20000], Training Loss: 0.0258\n",
            "Epoch [15451/20000], Training Loss: 0.0272\n",
            "Epoch [15452/20000], Training Loss: 0.0270\n",
            "Epoch [15453/20000], Training Loss: 0.0247\n",
            "Epoch [15454/20000], Training Loss: 0.0263\n",
            "Epoch [15455/20000], Training Loss: 0.0270\n",
            "Epoch [15456/20000], Training Loss: 0.0248\n",
            "Epoch [15457/20000], Training Loss: 0.0271\n",
            "Epoch [15458/20000], Training Loss: 0.0268\n",
            "Epoch [15459/20000], Training Loss: 0.0276\n",
            "Epoch [15460/20000], Training Loss: 0.0263\n",
            "Epoch [15461/20000], Training Loss: 0.0255\n",
            "Epoch [15462/20000], Training Loss: 0.0264\n",
            "Epoch [15463/20000], Training Loss: 0.0260\n",
            "Epoch [15464/20000], Training Loss: 0.0276\n",
            "Epoch [15465/20000], Training Loss: 0.0256\n",
            "Epoch [15466/20000], Training Loss: 0.0264\n",
            "Epoch [15467/20000], Training Loss: 0.0261\n",
            "Epoch [15468/20000], Training Loss: 0.0259\n",
            "Epoch [15469/20000], Training Loss: 0.0249\n",
            "Epoch [15470/20000], Training Loss: 0.0261\n",
            "Epoch [15471/20000], Training Loss: 0.0267\n",
            "Epoch [15472/20000], Training Loss: 0.0264\n",
            "Epoch [15473/20000], Training Loss: 0.0273\n",
            "Epoch [15474/20000], Training Loss: 0.0259\n",
            "Epoch [15475/20000], Training Loss: 0.0241\n",
            "Epoch [15476/20000], Training Loss: 0.0277\n",
            "Epoch [15477/20000], Training Loss: 0.0250\n",
            "Epoch [15478/20000], Training Loss: 0.0279\n",
            "Epoch [15479/20000], Training Loss: 0.0239\n",
            "Epoch [15480/20000], Training Loss: 0.0276\n",
            "Epoch [15481/20000], Training Loss: 0.0259\n",
            "Epoch [15482/20000], Training Loss: 0.0269\n",
            "Epoch [15483/20000], Training Loss: 0.0258\n",
            "Epoch [15484/20000], Training Loss: 0.0263\n",
            "Epoch [15485/20000], Training Loss: 0.0250\n",
            "Epoch [15486/20000], Training Loss: 0.0265\n",
            "Epoch [15487/20000], Training Loss: 0.0265\n",
            "Epoch [15488/20000], Training Loss: 0.0259\n",
            "Epoch [15489/20000], Training Loss: 0.0273\n",
            "Epoch [15490/20000], Training Loss: 0.0237\n",
            "Epoch [15491/20000], Training Loss: 0.0278\n",
            "Epoch [15492/20000], Training Loss: 0.0264\n",
            "Epoch [15493/20000], Training Loss: 0.0255\n",
            "Epoch [15494/20000], Training Loss: 0.0265\n",
            "Epoch [15495/20000], Training Loss: 0.0273\n",
            "Epoch [15496/20000], Training Loss: 0.0257\n",
            "Epoch [15497/20000], Training Loss: 0.0256\n",
            "Epoch [15498/20000], Training Loss: 0.0261\n",
            "Epoch [15499/20000], Training Loss: 0.0255\n",
            "Epoch [15500/20000], Training Loss: 0.0269\n",
            "Epoch [15501/20000], Training Loss: 0.0251\n",
            "Epoch [15502/20000], Training Loss: 0.0261\n",
            "Epoch [15503/20000], Training Loss: 0.0279\n",
            "Epoch [15504/20000], Training Loss: 0.0273\n",
            "Epoch [15505/20000], Training Loss: 0.0239\n",
            "Epoch [15506/20000], Training Loss: 0.0274\n",
            "Epoch [15507/20000], Training Loss: 0.0268\n",
            "Epoch [15508/20000], Training Loss: 0.0243\n",
            "Epoch [15509/20000], Training Loss: 0.0266\n",
            "Epoch [15510/20000], Training Loss: 0.0256\n",
            "Epoch [15511/20000], Training Loss: 0.0253\n",
            "Epoch [15512/20000], Training Loss: 0.0288\n",
            "Epoch [15513/20000], Training Loss: 0.0252\n",
            "Epoch [15514/20000], Training Loss: 0.0268\n",
            "Epoch [15515/20000], Training Loss: 0.0270\n",
            "Epoch [15516/20000], Training Loss: 0.0266\n",
            "Epoch [15517/20000], Training Loss: 0.0290\n",
            "Epoch [15518/20000], Training Loss: 0.0268\n",
            "Epoch [15519/20000], Training Loss: 0.0272\n",
            "Epoch [15520/20000], Training Loss: 0.0266\n",
            "Epoch [15521/20000], Training Loss: 0.0278\n",
            "Epoch [15522/20000], Training Loss: 0.0248\n",
            "Epoch [15523/20000], Training Loss: 0.0268\n",
            "Epoch [15524/20000], Training Loss: 0.0252\n",
            "Epoch [15525/20000], Training Loss: 0.0244\n",
            "Epoch [15526/20000], Training Loss: 0.0245\n",
            "Epoch [15527/20000], Training Loss: 0.0259\n",
            "Epoch [15528/20000], Training Loss: 0.0263\n",
            "Epoch [15529/20000], Training Loss: 0.0252\n",
            "Epoch [15530/20000], Training Loss: 0.0277\n",
            "Epoch [15531/20000], Training Loss: 0.0267\n",
            "Epoch [15532/20000], Training Loss: 0.0271\n",
            "Epoch [15533/20000], Training Loss: 0.0269\n",
            "Epoch [15534/20000], Training Loss: 0.0272\n",
            "Epoch [15535/20000], Training Loss: 0.0253\n",
            "Epoch [15536/20000], Training Loss: 0.0271\n",
            "Epoch [15537/20000], Training Loss: 0.0266\n",
            "Epoch [15538/20000], Training Loss: 0.0253\n",
            "Epoch [15539/20000], Training Loss: 0.0258\n",
            "Epoch [15540/20000], Training Loss: 0.0250\n",
            "Epoch [15541/20000], Training Loss: 0.0268\n",
            "Epoch [15542/20000], Training Loss: 0.0263\n",
            "Epoch [15543/20000], Training Loss: 0.0250\n",
            "Epoch [15544/20000], Training Loss: 0.0257\n",
            "Epoch [15545/20000], Training Loss: 0.0269\n",
            "Epoch [15546/20000], Training Loss: 0.0267\n",
            "Epoch [15547/20000], Training Loss: 0.0270\n",
            "Epoch [15548/20000], Training Loss: 0.0250\n",
            "Epoch [15549/20000], Training Loss: 0.0262\n",
            "Epoch [15550/20000], Training Loss: 0.0272\n",
            "Epoch [15551/20000], Training Loss: 0.0254\n",
            "Epoch [15552/20000], Training Loss: 0.0263\n",
            "Epoch [15553/20000], Training Loss: 0.0281\n",
            "Epoch [15554/20000], Training Loss: 0.0263\n",
            "Epoch [15555/20000], Training Loss: 0.0269\n",
            "Epoch [15556/20000], Training Loss: 0.0249\n",
            "Epoch [15557/20000], Training Loss: 0.0250\n",
            "Epoch [15558/20000], Training Loss: 0.0259\n",
            "Epoch [15559/20000], Training Loss: 0.0258\n",
            "Epoch [15560/20000], Training Loss: 0.0263\n",
            "Epoch [15561/20000], Training Loss: 0.0275\n",
            "Epoch [15562/20000], Training Loss: 0.0255\n",
            "Epoch [15563/20000], Training Loss: 0.0295\n",
            "Epoch [15564/20000], Training Loss: 0.0273\n",
            "Epoch [15565/20000], Training Loss: 0.0257\n",
            "Epoch [15566/20000], Training Loss: 0.0266\n",
            "Epoch [15567/20000], Training Loss: 0.0246\n",
            "Epoch [15568/20000], Training Loss: 0.0252\n",
            "Epoch [15569/20000], Training Loss: 0.0274\n",
            "Epoch [15570/20000], Training Loss: 0.0271\n",
            "Epoch [15571/20000], Training Loss: 0.0251\n",
            "Epoch [15572/20000], Training Loss: 0.0265\n",
            "Epoch [15573/20000], Training Loss: 0.0265\n",
            "Epoch [15574/20000], Training Loss: 0.0245\n",
            "Epoch [15575/20000], Training Loss: 0.0277\n",
            "Epoch [15576/20000], Training Loss: 0.0280\n",
            "Epoch [15577/20000], Training Loss: 0.0275\n",
            "Epoch [15578/20000], Training Loss: 0.0257\n",
            "Epoch [15579/20000], Training Loss: 0.0272\n",
            "Epoch [15580/20000], Training Loss: 0.0269\n",
            "Epoch [15581/20000], Training Loss: 0.0248\n",
            "Epoch [15582/20000], Training Loss: 0.0269\n",
            "Epoch [15583/20000], Training Loss: 0.0259\n",
            "Epoch [15584/20000], Training Loss: 0.0262\n",
            "Epoch [15585/20000], Training Loss: 0.0275\n",
            "Epoch [15586/20000], Training Loss: 0.0258\n",
            "Epoch [15587/20000], Training Loss: 0.0265\n",
            "Epoch [15588/20000], Training Loss: 0.0245\n",
            "Epoch [15589/20000], Training Loss: 0.0264\n",
            "Epoch [15590/20000], Training Loss: 0.0271\n",
            "Epoch [15591/20000], Training Loss: 0.0261\n",
            "Epoch [15592/20000], Training Loss: 0.0247\n",
            "Epoch [15593/20000], Training Loss: 0.0261\n",
            "Epoch [15594/20000], Training Loss: 0.0249\n",
            "Epoch [15595/20000], Training Loss: 0.0247\n",
            "Epoch [15596/20000], Training Loss: 0.0250\n",
            "Epoch [15597/20000], Training Loss: 0.0257\n",
            "Epoch [15598/20000], Training Loss: 0.0265\n",
            "Epoch [15599/20000], Training Loss: 0.0256\n",
            "Epoch [15600/20000], Training Loss: 0.0264\n",
            "Epoch [15601/20000], Training Loss: 0.0283\n",
            "Epoch [15602/20000], Training Loss: 0.0256\n",
            "Epoch [15603/20000], Training Loss: 0.0267\n",
            "Epoch [15604/20000], Training Loss: 0.0272\n",
            "Epoch [15605/20000], Training Loss: 0.0252\n",
            "Epoch [15606/20000], Training Loss: 0.0264\n",
            "Epoch [15607/20000], Training Loss: 0.0258\n",
            "Epoch [15608/20000], Training Loss: 0.0258\n",
            "Epoch [15609/20000], Training Loss: 0.0243\n",
            "Epoch [15610/20000], Training Loss: 0.0274\n",
            "Epoch [15611/20000], Training Loss: 0.0265\n",
            "Epoch [15612/20000], Training Loss: 0.0271\n",
            "Epoch [15613/20000], Training Loss: 0.0247\n",
            "Epoch [15614/20000], Training Loss: 0.0273\n",
            "Epoch [15615/20000], Training Loss: 0.0276\n",
            "Epoch [15616/20000], Training Loss: 0.0252\n",
            "Epoch [15617/20000], Training Loss: 0.0261\n",
            "Epoch [15618/20000], Training Loss: 0.0258\n",
            "Epoch [15619/20000], Training Loss: 0.0272\n",
            "Epoch [15620/20000], Training Loss: 0.0272\n",
            "Epoch [15621/20000], Training Loss: 0.0260\n",
            "Epoch [15622/20000], Training Loss: 0.0271\n",
            "Epoch [15623/20000], Training Loss: 0.0248\n",
            "Epoch [15624/20000], Training Loss: 0.0240\n",
            "Epoch [15625/20000], Training Loss: 0.0249\n",
            "Epoch [15626/20000], Training Loss: 0.0271\n",
            "Epoch [15627/20000], Training Loss: 0.0277\n",
            "Epoch [15628/20000], Training Loss: 0.0259\n",
            "Epoch [15629/20000], Training Loss: 0.0253\n",
            "Epoch [15630/20000], Training Loss: 0.0267\n",
            "Epoch [15631/20000], Training Loss: 0.0292\n",
            "Epoch [15632/20000], Training Loss: 0.0265\n",
            "Epoch [15633/20000], Training Loss: 0.0257\n",
            "Epoch [15634/20000], Training Loss: 0.0261\n",
            "Epoch [15635/20000], Training Loss: 0.0267\n",
            "Epoch [15636/20000], Training Loss: 0.0260\n",
            "Epoch [15637/20000], Training Loss: 0.0256\n",
            "Epoch [15638/20000], Training Loss: 0.0274\n",
            "Epoch [15639/20000], Training Loss: 0.0271\n",
            "Epoch [15640/20000], Training Loss: 0.0270\n",
            "Epoch [15641/20000], Training Loss: 0.0290\n",
            "Epoch [15642/20000], Training Loss: 0.0252\n",
            "Epoch [15643/20000], Training Loss: 0.0263\n",
            "Epoch [15644/20000], Training Loss: 0.0270\n",
            "Epoch [15645/20000], Training Loss: 0.0273\n",
            "Epoch [15646/20000], Training Loss: 0.0271\n",
            "Epoch [15647/20000], Training Loss: 0.0270\n",
            "Epoch [15648/20000], Training Loss: 0.0276\n",
            "Epoch [15649/20000], Training Loss: 0.0253\n",
            "Epoch [15650/20000], Training Loss: 0.0253\n",
            "Epoch [15651/20000], Training Loss: 0.0281\n",
            "Epoch [15652/20000], Training Loss: 0.0272\n",
            "Epoch [15653/20000], Training Loss: 0.0256\n",
            "Epoch [15654/20000], Training Loss: 0.0272\n",
            "Epoch [15655/20000], Training Loss: 0.0281\n",
            "Epoch [15656/20000], Training Loss: 0.0277\n",
            "Epoch [15657/20000], Training Loss: 0.0261\n",
            "Epoch [15658/20000], Training Loss: 0.0264\n",
            "Epoch [15659/20000], Training Loss: 0.0262\n",
            "Epoch [15660/20000], Training Loss: 0.0274\n",
            "Epoch [15661/20000], Training Loss: 0.0253\n",
            "Epoch [15662/20000], Training Loss: 0.0257\n",
            "Epoch [15663/20000], Training Loss: 0.0259\n",
            "Epoch [15664/20000], Training Loss: 0.0259\n",
            "Epoch [15665/20000], Training Loss: 0.0260\n",
            "Epoch [15666/20000], Training Loss: 0.0263\n",
            "Epoch [15667/20000], Training Loss: 0.0259\n",
            "Epoch [15668/20000], Training Loss: 0.0263\n",
            "Epoch [15669/20000], Training Loss: 0.0263\n",
            "Epoch [15670/20000], Training Loss: 0.0257\n",
            "Epoch [15671/20000], Training Loss: 0.0267\n",
            "Epoch [15672/20000], Training Loss: 0.0269\n",
            "Epoch [15673/20000], Training Loss: 0.0262\n",
            "Epoch [15674/20000], Training Loss: 0.0264\n",
            "Epoch [15675/20000], Training Loss: 0.0281\n",
            "Epoch [15676/20000], Training Loss: 0.0269\n",
            "Epoch [15677/20000], Training Loss: 0.0277\n",
            "Epoch [15678/20000], Training Loss: 0.0254\n",
            "Epoch [15679/20000], Training Loss: 0.0245\n",
            "Epoch [15680/20000], Training Loss: 0.0247\n",
            "Epoch [15681/20000], Training Loss: 0.0266\n",
            "Epoch [15682/20000], Training Loss: 0.0256\n",
            "Epoch [15683/20000], Training Loss: 0.0247\n",
            "Epoch [15684/20000], Training Loss: 0.0273\n",
            "Epoch [15685/20000], Training Loss: 0.0258\n",
            "Epoch [15686/20000], Training Loss: 0.0274\n",
            "Epoch [15687/20000], Training Loss: 0.0279\n",
            "Epoch [15688/20000], Training Loss: 0.0270\n",
            "Epoch [15689/20000], Training Loss: 0.0274\n",
            "Epoch [15690/20000], Training Loss: 0.0288\n",
            "Epoch [15691/20000], Training Loss: 0.0280\n",
            "Epoch [15692/20000], Training Loss: 0.0268\n",
            "Epoch [15693/20000], Training Loss: 0.0264\n",
            "Epoch [15694/20000], Training Loss: 0.0270\n",
            "Epoch [15695/20000], Training Loss: 0.0266\n",
            "Epoch [15696/20000], Training Loss: 0.0262\n",
            "Epoch [15697/20000], Training Loss: 0.0250\n",
            "Epoch [15698/20000], Training Loss: 0.0273\n",
            "Epoch [15699/20000], Training Loss: 0.0240\n",
            "Epoch [15700/20000], Training Loss: 0.0241\n",
            "Epoch [15701/20000], Training Loss: 0.0269\n",
            "Epoch [15702/20000], Training Loss: 0.0271\n",
            "Epoch [15703/20000], Training Loss: 0.0287\n",
            "Epoch [15704/20000], Training Loss: 0.0252\n",
            "Epoch [15705/20000], Training Loss: 0.0283\n",
            "Epoch [15706/20000], Training Loss: 0.0276\n",
            "Epoch [15707/20000], Training Loss: 0.0249\n",
            "Epoch [15708/20000], Training Loss: 0.0265\n",
            "Epoch [15709/20000], Training Loss: 0.0281\n",
            "Epoch [15710/20000], Training Loss: 0.0276\n",
            "Epoch [15711/20000], Training Loss: 0.0261\n",
            "Epoch [15712/20000], Training Loss: 0.0275\n",
            "Epoch [15713/20000], Training Loss: 0.0251\n",
            "Epoch [15714/20000], Training Loss: 0.0262\n",
            "Epoch [15715/20000], Training Loss: 0.0281\n",
            "Epoch [15716/20000], Training Loss: 0.0259\n",
            "Epoch [15717/20000], Training Loss: 0.0273\n",
            "Epoch [15718/20000], Training Loss: 0.0272\n",
            "Epoch [15719/20000], Training Loss: 0.0267\n",
            "Epoch [15720/20000], Training Loss: 0.0270\n",
            "Epoch [15721/20000], Training Loss: 0.0264\n",
            "Epoch [15722/20000], Training Loss: 0.0256\n",
            "Epoch [15723/20000], Training Loss: 0.0288\n",
            "Epoch [15724/20000], Training Loss: 0.0290\n",
            "Epoch [15725/20000], Training Loss: 0.0257\n",
            "Epoch [15726/20000], Training Loss: 0.0254\n",
            "Epoch [15727/20000], Training Loss: 0.0250\n",
            "Epoch [15728/20000], Training Loss: 0.0249\n",
            "Epoch [15729/20000], Training Loss: 0.0258\n",
            "Epoch [15730/20000], Training Loss: 0.0267\n",
            "Epoch [15731/20000], Training Loss: 0.0251\n",
            "Epoch [15732/20000], Training Loss: 0.0255\n",
            "Epoch [15733/20000], Training Loss: 0.0262\n",
            "Epoch [15734/20000], Training Loss: 0.0275\n",
            "Epoch [15735/20000], Training Loss: 0.0256\n",
            "Epoch [15736/20000], Training Loss: 0.0258\n",
            "Epoch [15737/20000], Training Loss: 0.0256\n",
            "Epoch [15738/20000], Training Loss: 0.0266\n",
            "Epoch [15739/20000], Training Loss: 0.0250\n",
            "Epoch [15740/20000], Training Loss: 0.0254\n",
            "Epoch [15741/20000], Training Loss: 0.0267\n",
            "Epoch [15742/20000], Training Loss: 0.0249\n",
            "Epoch [15743/20000], Training Loss: 0.0254\n",
            "Epoch [15744/20000], Training Loss: 0.0254\n",
            "Epoch [15745/20000], Training Loss: 0.0253\n",
            "Epoch [15746/20000], Training Loss: 0.0280\n",
            "Epoch [15747/20000], Training Loss: 0.0259\n",
            "Epoch [15748/20000], Training Loss: 0.0277\n",
            "Epoch [15749/20000], Training Loss: 0.0273\n",
            "Epoch [15750/20000], Training Loss: 0.0254\n",
            "Epoch [15751/20000], Training Loss: 0.0263\n",
            "Epoch [15752/20000], Training Loss: 0.0246\n",
            "Epoch [15753/20000], Training Loss: 0.0258\n",
            "Epoch [15754/20000], Training Loss: 0.0258\n",
            "Epoch [15755/20000], Training Loss: 0.0274\n",
            "Epoch [15756/20000], Training Loss: 0.0281\n",
            "Epoch [15757/20000], Training Loss: 0.0270\n",
            "Epoch [15758/20000], Training Loss: 0.0251\n",
            "Epoch [15759/20000], Training Loss: 0.0277\n",
            "Epoch [15760/20000], Training Loss: 0.0254\n",
            "Epoch [15761/20000], Training Loss: 0.0266\n",
            "Epoch [15762/20000], Training Loss: 0.0277\n",
            "Epoch [15763/20000], Training Loss: 0.0250\n",
            "Epoch [15764/20000], Training Loss: 0.0282\n",
            "Epoch [15765/20000], Training Loss: 0.0267\n",
            "Epoch [15766/20000], Training Loss: 0.0260\n",
            "Epoch [15767/20000], Training Loss: 0.0268\n",
            "Epoch [15768/20000], Training Loss: 0.0281\n",
            "Epoch [15769/20000], Training Loss: 0.0263\n",
            "Epoch [15770/20000], Training Loss: 0.0243\n",
            "Epoch [15771/20000], Training Loss: 0.0280\n",
            "Epoch [15772/20000], Training Loss: 0.0259\n",
            "Epoch [15773/20000], Training Loss: 0.0269\n",
            "Epoch [15774/20000], Training Loss: 0.0266\n",
            "Epoch [15775/20000], Training Loss: 0.0272\n",
            "Epoch [15776/20000], Training Loss: 0.0270\n",
            "Epoch [15777/20000], Training Loss: 0.0251\n",
            "Epoch [15778/20000], Training Loss: 0.0265\n",
            "Epoch [15779/20000], Training Loss: 0.0280\n",
            "Epoch [15780/20000], Training Loss: 0.0260\n",
            "Epoch [15781/20000], Training Loss: 0.0253\n",
            "Epoch [15782/20000], Training Loss: 0.0253\n",
            "Epoch [15783/20000], Training Loss: 0.0258\n",
            "Epoch [15784/20000], Training Loss: 0.0261\n",
            "Epoch [15785/20000], Training Loss: 0.0274\n",
            "Epoch [15786/20000], Training Loss: 0.0252\n",
            "Epoch [15787/20000], Training Loss: 0.0263\n",
            "Epoch [15788/20000], Training Loss: 0.0274\n",
            "Epoch [15789/20000], Training Loss: 0.0261\n",
            "Epoch [15790/20000], Training Loss: 0.0269\n",
            "Epoch [15791/20000], Training Loss: 0.0275\n",
            "Epoch [15792/20000], Training Loss: 0.0265\n",
            "Epoch [15793/20000], Training Loss: 0.0269\n",
            "Epoch [15794/20000], Training Loss: 0.0250\n",
            "Epoch [15795/20000], Training Loss: 0.0278\n",
            "Epoch [15796/20000], Training Loss: 0.0253\n",
            "Epoch [15797/20000], Training Loss: 0.0263\n",
            "Epoch [15798/20000], Training Loss: 0.0252\n",
            "Epoch [15799/20000], Training Loss: 0.0260\n",
            "Epoch [15800/20000], Training Loss: 0.0277\n",
            "Epoch [15801/20000], Training Loss: 0.0268\n",
            "Epoch [15802/20000], Training Loss: 0.0262\n",
            "Epoch [15803/20000], Training Loss: 0.0273\n",
            "Epoch [15804/20000], Training Loss: 0.0241\n",
            "Epoch [15805/20000], Training Loss: 0.0277\n",
            "Epoch [15806/20000], Training Loss: 0.0252\n",
            "Epoch [15807/20000], Training Loss: 0.0271\n",
            "Epoch [15808/20000], Training Loss: 0.0262\n",
            "Epoch [15809/20000], Training Loss: 0.0272\n",
            "Epoch [15810/20000], Training Loss: 0.0267\n",
            "Epoch [15811/20000], Training Loss: 0.0254\n",
            "Epoch [15812/20000], Training Loss: 0.0273\n",
            "Epoch [15813/20000], Training Loss: 0.0251\n",
            "Epoch [15814/20000], Training Loss: 0.0260\n",
            "Epoch [15815/20000], Training Loss: 0.0263\n",
            "Epoch [15816/20000], Training Loss: 0.0253\n",
            "Epoch [15817/20000], Training Loss: 0.0275\n",
            "Epoch [15818/20000], Training Loss: 0.0264\n",
            "Epoch [15819/20000], Training Loss: 0.0260\n",
            "Epoch [15820/20000], Training Loss: 0.0277\n",
            "Epoch [15821/20000], Training Loss: 0.0252\n",
            "Epoch [15822/20000], Training Loss: 0.0280\n",
            "Epoch [15823/20000], Training Loss: 0.0269\n",
            "Epoch [15824/20000], Training Loss: 0.0257\n",
            "Epoch [15825/20000], Training Loss: 0.0245\n",
            "Epoch [15826/20000], Training Loss: 0.0258\n",
            "Epoch [15827/20000], Training Loss: 0.0268\n",
            "Epoch [15828/20000], Training Loss: 0.0243\n",
            "Epoch [15829/20000], Training Loss: 0.0277\n",
            "Epoch [15830/20000], Training Loss: 0.0269\n",
            "Epoch [15831/20000], Training Loss: 0.0259\n",
            "Epoch [15832/20000], Training Loss: 0.0263\n",
            "Epoch [15833/20000], Training Loss: 0.0264\n",
            "Epoch [15834/20000], Training Loss: 0.0262\n",
            "Epoch [15835/20000], Training Loss: 0.0274\n",
            "Epoch [15836/20000], Training Loss: 0.0261\n",
            "Epoch [15837/20000], Training Loss: 0.0259\n",
            "Epoch [15838/20000], Training Loss: 0.0267\n",
            "Epoch [15839/20000], Training Loss: 0.0276\n",
            "Epoch [15840/20000], Training Loss: 0.0260\n",
            "Epoch [15841/20000], Training Loss: 0.0249\n",
            "Epoch [15842/20000], Training Loss: 0.0277\n",
            "Epoch [15843/20000], Training Loss: 0.0268\n",
            "Epoch [15844/20000], Training Loss: 0.0270\n",
            "Epoch [15845/20000], Training Loss: 0.0260\n",
            "Epoch [15846/20000], Training Loss: 0.0247\n",
            "Epoch [15847/20000], Training Loss: 0.0258\n",
            "Epoch [15848/20000], Training Loss: 0.0279\n",
            "Epoch [15849/20000], Training Loss: 0.0274\n",
            "Epoch [15850/20000], Training Loss: 0.0271\n",
            "Epoch [15851/20000], Training Loss: 0.0262\n",
            "Epoch [15852/20000], Training Loss: 0.0263\n",
            "Epoch [15853/20000], Training Loss: 0.0261\n",
            "Epoch [15854/20000], Training Loss: 0.0279\n",
            "Epoch [15855/20000], Training Loss: 0.0257\n",
            "Epoch [15856/20000], Training Loss: 0.0265\n",
            "Epoch [15857/20000], Training Loss: 0.0275\n",
            "Epoch [15858/20000], Training Loss: 0.0250\n",
            "Epoch [15859/20000], Training Loss: 0.0239\n",
            "Epoch [15860/20000], Training Loss: 0.0257\n",
            "Epoch [15861/20000], Training Loss: 0.0250\n",
            "Epoch [15862/20000], Training Loss: 0.0276\n",
            "Epoch [15863/20000], Training Loss: 0.0280\n",
            "Epoch [15864/20000], Training Loss: 0.0259\n",
            "Epoch [15865/20000], Training Loss: 0.0255\n",
            "Epoch [15866/20000], Training Loss: 0.0260\n",
            "Epoch [15867/20000], Training Loss: 0.0274\n",
            "Epoch [15868/20000], Training Loss: 0.0283\n",
            "Epoch [15869/20000], Training Loss: 0.0260\n",
            "Epoch [15870/20000], Training Loss: 0.0270\n",
            "Epoch [15871/20000], Training Loss: 0.0282\n",
            "Epoch [15872/20000], Training Loss: 0.0256\n",
            "Epoch [15873/20000], Training Loss: 0.0280\n",
            "Epoch [15874/20000], Training Loss: 0.0275\n",
            "Epoch [15875/20000], Training Loss: 0.0250\n",
            "Epoch [15876/20000], Training Loss: 0.0274\n",
            "Epoch [15877/20000], Training Loss: 0.0248\n",
            "Epoch [15878/20000], Training Loss: 0.0294\n",
            "Epoch [15879/20000], Training Loss: 0.0265\n",
            "Epoch [15880/20000], Training Loss: 0.0288\n",
            "Epoch [15881/20000], Training Loss: 0.0268\n",
            "Epoch [15882/20000], Training Loss: 0.0268\n",
            "Epoch [15883/20000], Training Loss: 0.0260\n",
            "Epoch [15884/20000], Training Loss: 0.0285\n",
            "Epoch [15885/20000], Training Loss: 0.0267\n",
            "Epoch [15886/20000], Training Loss: 0.0267\n",
            "Epoch [15887/20000], Training Loss: 0.0246\n",
            "Epoch [15888/20000], Training Loss: 0.0271\n",
            "Epoch [15889/20000], Training Loss: 0.0264\n",
            "Epoch [15890/20000], Training Loss: 0.0275\n",
            "Epoch [15891/20000], Training Loss: 0.0254\n",
            "Epoch [15892/20000], Training Loss: 0.0267\n",
            "Epoch [15893/20000], Training Loss: 0.0267\n",
            "Epoch [15894/20000], Training Loss: 0.0271\n",
            "Epoch [15895/20000], Training Loss: 0.0254\n",
            "Epoch [15896/20000], Training Loss: 0.0248\n",
            "Epoch [15897/20000], Training Loss: 0.0273\n",
            "Epoch [15898/20000], Training Loss: 0.0262\n",
            "Epoch [15899/20000], Training Loss: 0.0252\n",
            "Epoch [15900/20000], Training Loss: 0.0282\n",
            "Epoch [15901/20000], Training Loss: 0.0246\n",
            "Epoch [15902/20000], Training Loss: 0.0256\n",
            "Epoch [15903/20000], Training Loss: 0.0272\n",
            "Epoch [15904/20000], Training Loss: 0.0268\n",
            "Epoch [15905/20000], Training Loss: 0.0256\n",
            "Epoch [15906/20000], Training Loss: 0.0295\n",
            "Epoch [15907/20000], Training Loss: 0.0276\n",
            "Epoch [15908/20000], Training Loss: 0.0283\n",
            "Epoch [15909/20000], Training Loss: 0.0270\n",
            "Epoch [15910/20000], Training Loss: 0.0263\n",
            "Epoch [15911/20000], Training Loss: 0.0252\n",
            "Epoch [15912/20000], Training Loss: 0.0246\n",
            "Epoch [15913/20000], Training Loss: 0.0272\n",
            "Epoch [15914/20000], Training Loss: 0.0257\n",
            "Epoch [15915/20000], Training Loss: 0.0264\n",
            "Epoch [15916/20000], Training Loss: 0.0279\n",
            "Epoch [15917/20000], Training Loss: 0.0277\n",
            "Epoch [15918/20000], Training Loss: 0.0246\n",
            "Epoch [15919/20000], Training Loss: 0.0259\n",
            "Epoch [15920/20000], Training Loss: 0.0256\n",
            "Epoch [15921/20000], Training Loss: 0.0264\n",
            "Epoch [15922/20000], Training Loss: 0.0257\n",
            "Epoch [15923/20000], Training Loss: 0.0277\n",
            "Epoch [15924/20000], Training Loss: 0.0267\n",
            "Epoch [15925/20000], Training Loss: 0.0249\n",
            "Epoch [15926/20000], Training Loss: 0.0253\n",
            "Epoch [15927/20000], Training Loss: 0.0261\n",
            "Epoch [15928/20000], Training Loss: 0.0266\n",
            "Epoch [15929/20000], Training Loss: 0.0273\n",
            "Epoch [15930/20000], Training Loss: 0.0274\n",
            "Epoch [15931/20000], Training Loss: 0.0250\n",
            "Epoch [15932/20000], Training Loss: 0.0266\n",
            "Epoch [15933/20000], Training Loss: 0.0268\n",
            "Epoch [15934/20000], Training Loss: 0.0270\n",
            "Epoch [15935/20000], Training Loss: 0.0279\n",
            "Epoch [15936/20000], Training Loss: 0.0282\n",
            "Epoch [15937/20000], Training Loss: 0.0257\n",
            "Epoch [15938/20000], Training Loss: 0.0261\n",
            "Epoch [15939/20000], Training Loss: 0.0256\n",
            "Epoch [15940/20000], Training Loss: 0.0238\n",
            "Epoch [15941/20000], Training Loss: 0.0245\n",
            "Epoch [15942/20000], Training Loss: 0.0247\n",
            "Epoch [15943/20000], Training Loss: 0.0261\n",
            "Epoch [15944/20000], Training Loss: 0.0262\n",
            "Epoch [15945/20000], Training Loss: 0.0255\n",
            "Epoch [15946/20000], Training Loss: 0.0258\n",
            "Epoch [15947/20000], Training Loss: 0.0266\n",
            "Epoch [15948/20000], Training Loss: 0.0256\n",
            "Epoch [15949/20000], Training Loss: 0.0259\n",
            "Epoch [15950/20000], Training Loss: 0.0275\n",
            "Epoch [15951/20000], Training Loss: 0.0252\n",
            "Epoch [15952/20000], Training Loss: 0.0266\n",
            "Epoch [15953/20000], Training Loss: 0.0258\n",
            "Epoch [15954/20000], Training Loss: 0.0260\n",
            "Epoch [15955/20000], Training Loss: 0.0253\n",
            "Epoch [15956/20000], Training Loss: 0.0254\n",
            "Epoch [15957/20000], Training Loss: 0.0267\n",
            "Epoch [15958/20000], Training Loss: 0.0251\n",
            "Epoch [15959/20000], Training Loss: 0.0247\n",
            "Epoch [15960/20000], Training Loss: 0.0252\n",
            "Epoch [15961/20000], Training Loss: 0.0248\n",
            "Epoch [15962/20000], Training Loss: 0.0265\n",
            "Epoch [15963/20000], Training Loss: 0.0240\n",
            "Epoch [15964/20000], Training Loss: 0.0262\n",
            "Epoch [15965/20000], Training Loss: 0.0259\n",
            "Epoch [15966/20000], Training Loss: 0.0277\n",
            "Epoch [15967/20000], Training Loss: 0.0260\n",
            "Epoch [15968/20000], Training Loss: 0.0249\n",
            "Epoch [15969/20000], Training Loss: 0.0262\n",
            "Epoch [15970/20000], Training Loss: 0.0267\n",
            "Epoch [15971/20000], Training Loss: 0.0251\n",
            "Epoch [15972/20000], Training Loss: 0.0273\n",
            "Epoch [15973/20000], Training Loss: 0.0256\n",
            "Epoch [15974/20000], Training Loss: 0.0263\n",
            "Epoch [15975/20000], Training Loss: 0.0270\n",
            "Epoch [15976/20000], Training Loss: 0.0276\n",
            "Epoch [15977/20000], Training Loss: 0.0283\n",
            "Epoch [15978/20000], Training Loss: 0.0256\n",
            "Epoch [15979/20000], Training Loss: 0.0264\n",
            "Epoch [15980/20000], Training Loss: 0.0262\n",
            "Epoch [15981/20000], Training Loss: 0.0255\n",
            "Epoch [15982/20000], Training Loss: 0.0272\n",
            "Epoch [15983/20000], Training Loss: 0.0277\n",
            "Epoch [15984/20000], Training Loss: 0.0241\n",
            "Epoch [15985/20000], Training Loss: 0.0261\n",
            "Epoch [15986/20000], Training Loss: 0.0291\n",
            "Epoch [15987/20000], Training Loss: 0.0275\n",
            "Epoch [15988/20000], Training Loss: 0.0270\n",
            "Epoch [15989/20000], Training Loss: 0.0257\n",
            "Epoch [15990/20000], Training Loss: 0.0261\n",
            "Epoch [15991/20000], Training Loss: 0.0256\n",
            "Epoch [15992/20000], Training Loss: 0.0255\n",
            "Epoch [15993/20000], Training Loss: 0.0266\n",
            "Epoch [15994/20000], Training Loss: 0.0267\n",
            "Epoch [15995/20000], Training Loss: 0.0282\n",
            "Epoch [15996/20000], Training Loss: 0.0273\n",
            "Epoch [15997/20000], Training Loss: 0.0259\n",
            "Epoch [15998/20000], Training Loss: 0.0258\n",
            "Epoch [15999/20000], Training Loss: 0.0250\n",
            "Epoch [16000/20000], Training Loss: 0.0256\n",
            "Epoch [16001/20000], Training Loss: 0.0267\n",
            "Epoch [16002/20000], Training Loss: 0.0269\n",
            "Epoch [16003/20000], Training Loss: 0.0258\n",
            "Epoch [16004/20000], Training Loss: 0.0275\n",
            "Epoch [16005/20000], Training Loss: 0.0270\n",
            "Epoch [16006/20000], Training Loss: 0.0279\n",
            "Epoch [16007/20000], Training Loss: 0.0280\n",
            "Epoch [16008/20000], Training Loss: 0.0272\n",
            "Epoch [16009/20000], Training Loss: 0.0265\n",
            "Epoch [16010/20000], Training Loss: 0.0265\n",
            "Epoch [16011/20000], Training Loss: 0.0290\n",
            "Epoch [16012/20000], Training Loss: 0.0254\n",
            "Epoch [16013/20000], Training Loss: 0.0276\n",
            "Epoch [16014/20000], Training Loss: 0.0269\n",
            "Epoch [16015/20000], Training Loss: 0.0274\n",
            "Epoch [16016/20000], Training Loss: 0.0253\n",
            "Epoch [16017/20000], Training Loss: 0.0278\n",
            "Epoch [16018/20000], Training Loss: 0.0278\n",
            "Epoch [16019/20000], Training Loss: 0.0251\n",
            "Epoch [16020/20000], Training Loss: 0.0278\n",
            "Epoch [16021/20000], Training Loss: 0.0271\n",
            "Epoch [16022/20000], Training Loss: 0.0272\n",
            "Epoch [16023/20000], Training Loss: 0.0266\n",
            "Epoch [16024/20000], Training Loss: 0.0259\n",
            "Epoch [16025/20000], Training Loss: 0.0261\n",
            "Epoch [16026/20000], Training Loss: 0.0243\n",
            "Epoch [16027/20000], Training Loss: 0.0237\n",
            "Epoch [16028/20000], Training Loss: 0.0279\n",
            "Epoch [16029/20000], Training Loss: 0.0271\n",
            "Epoch [16030/20000], Training Loss: 0.0289\n",
            "Epoch [16031/20000], Training Loss: 0.0265\n",
            "Epoch [16032/20000], Training Loss: 0.0254\n",
            "Epoch [16033/20000], Training Loss: 0.0267\n",
            "Epoch [16034/20000], Training Loss: 0.0264\n",
            "Epoch [16035/20000], Training Loss: 0.0258\n",
            "Epoch [16036/20000], Training Loss: 0.0256\n",
            "Epoch [16037/20000], Training Loss: 0.0257\n",
            "Epoch [16038/20000], Training Loss: 0.0264\n",
            "Epoch [16039/20000], Training Loss: 0.0274\n",
            "Epoch [16040/20000], Training Loss: 0.0278\n",
            "Epoch [16041/20000], Training Loss: 0.0252\n",
            "Epoch [16042/20000], Training Loss: 0.0258\n",
            "Epoch [16043/20000], Training Loss: 0.0266\n",
            "Epoch [16044/20000], Training Loss: 0.0263\n",
            "Epoch [16045/20000], Training Loss: 0.0273\n",
            "Epoch [16046/20000], Training Loss: 0.0267\n",
            "Epoch [16047/20000], Training Loss: 0.0259\n",
            "Epoch [16048/20000], Training Loss: 0.0285\n",
            "Epoch [16049/20000], Training Loss: 0.0260\n",
            "Epoch [16050/20000], Training Loss: 0.0266\n",
            "Epoch [16051/20000], Training Loss: 0.0292\n",
            "Epoch [16052/20000], Training Loss: 0.0267\n",
            "Epoch [16053/20000], Training Loss: 0.0248\n",
            "Epoch [16054/20000], Training Loss: 0.0251\n",
            "Epoch [16055/20000], Training Loss: 0.0253\n",
            "Epoch [16056/20000], Training Loss: 0.0273\n",
            "Epoch [16057/20000], Training Loss: 0.0252\n",
            "Epoch [16058/20000], Training Loss: 0.0261\n",
            "Epoch [16059/20000], Training Loss: 0.0258\n",
            "Epoch [16060/20000], Training Loss: 0.0278\n",
            "Epoch [16061/20000], Training Loss: 0.0279\n",
            "Epoch [16062/20000], Training Loss: 0.0275\n",
            "Epoch [16063/20000], Training Loss: 0.0277\n",
            "Epoch [16064/20000], Training Loss: 0.0245\n",
            "Epoch [16065/20000], Training Loss: 0.0268\n",
            "Epoch [16066/20000], Training Loss: 0.0271\n",
            "Epoch [16067/20000], Training Loss: 0.0285\n",
            "Epoch [16068/20000], Training Loss: 0.0270\n",
            "Epoch [16069/20000], Training Loss: 0.0249\n",
            "Epoch [16070/20000], Training Loss: 0.0270\n",
            "Epoch [16071/20000], Training Loss: 0.0259\n",
            "Epoch [16072/20000], Training Loss: 0.0260\n",
            "Epoch [16073/20000], Training Loss: 0.0247\n",
            "Epoch [16074/20000], Training Loss: 0.0249\n",
            "Epoch [16075/20000], Training Loss: 0.0261\n",
            "Epoch [16076/20000], Training Loss: 0.0260\n",
            "Epoch [16077/20000], Training Loss: 0.0270\n",
            "Epoch [16078/20000], Training Loss: 0.0268\n",
            "Epoch [16079/20000], Training Loss: 0.0269\n",
            "Epoch [16080/20000], Training Loss: 0.0248\n",
            "Epoch [16081/20000], Training Loss: 0.0280\n",
            "Epoch [16082/20000], Training Loss: 0.0266\n",
            "Epoch [16083/20000], Training Loss: 0.0260\n",
            "Epoch [16084/20000], Training Loss: 0.0271\n",
            "Epoch [16085/20000], Training Loss: 0.0273\n",
            "Epoch [16086/20000], Training Loss: 0.0287\n",
            "Epoch [16087/20000], Training Loss: 0.0260\n",
            "Epoch [16088/20000], Training Loss: 0.0243\n",
            "Epoch [16089/20000], Training Loss: 0.0269\n",
            "Epoch [16090/20000], Training Loss: 0.0264\n",
            "Epoch [16091/20000], Training Loss: 0.0262\n",
            "Epoch [16092/20000], Training Loss: 0.0260\n",
            "Epoch [16093/20000], Training Loss: 0.0281\n",
            "Epoch [16094/20000], Training Loss: 0.0259\n",
            "Epoch [16095/20000], Training Loss: 0.0258\n",
            "Epoch [16096/20000], Training Loss: 0.0253\n",
            "Epoch [16097/20000], Training Loss: 0.0256\n",
            "Epoch [16098/20000], Training Loss: 0.0248\n",
            "Epoch [16099/20000], Training Loss: 0.0265\n",
            "Epoch [16100/20000], Training Loss: 0.0255\n",
            "Epoch [16101/20000], Training Loss: 0.0267\n",
            "Epoch [16102/20000], Training Loss: 0.0270\n",
            "Epoch [16103/20000], Training Loss: 0.0266\n",
            "Epoch [16104/20000], Training Loss: 0.0254\n",
            "Epoch [16105/20000], Training Loss: 0.0269\n",
            "Epoch [16106/20000], Training Loss: 0.0274\n",
            "Epoch [16107/20000], Training Loss: 0.0275\n",
            "Epoch [16108/20000], Training Loss: 0.0246\n",
            "Epoch [16109/20000], Training Loss: 0.0250\n",
            "Epoch [16110/20000], Training Loss: 0.0269\n",
            "Epoch [16111/20000], Training Loss: 0.0270\n",
            "Epoch [16112/20000], Training Loss: 0.0269\n",
            "Epoch [16113/20000], Training Loss: 0.0265\n",
            "Epoch [16114/20000], Training Loss: 0.0277\n",
            "Epoch [16115/20000], Training Loss: 0.0270\n",
            "Epoch [16116/20000], Training Loss: 0.0249\n",
            "Epoch [16117/20000], Training Loss: 0.0257\n",
            "Epoch [16118/20000], Training Loss: 0.0267\n",
            "Epoch [16119/20000], Training Loss: 0.0272\n",
            "Epoch [16120/20000], Training Loss: 0.0255\n",
            "Epoch [16121/20000], Training Loss: 0.0270\n",
            "Epoch [16122/20000], Training Loss: 0.0274\n",
            "Epoch [16123/20000], Training Loss: 0.0263\n",
            "Epoch [16124/20000], Training Loss: 0.0268\n",
            "Epoch [16125/20000], Training Loss: 0.0260\n",
            "Epoch [16126/20000], Training Loss: 0.0246\n",
            "Epoch [16127/20000], Training Loss: 0.0253\n",
            "Epoch [16128/20000], Training Loss: 0.0266\n",
            "Epoch [16129/20000], Training Loss: 0.0255\n",
            "Epoch [16130/20000], Training Loss: 0.0249\n",
            "Epoch [16131/20000], Training Loss: 0.0270\n",
            "Epoch [16132/20000], Training Loss: 0.0255\n",
            "Epoch [16133/20000], Training Loss: 0.0244\n",
            "Epoch [16134/20000], Training Loss: 0.0278\n",
            "Epoch [16135/20000], Training Loss: 0.0252\n",
            "Epoch [16136/20000], Training Loss: 0.0247\n",
            "Epoch [16137/20000], Training Loss: 0.0248\n",
            "Epoch [16138/20000], Training Loss: 0.0279\n",
            "Epoch [16139/20000], Training Loss: 0.0272\n",
            "Epoch [16140/20000], Training Loss: 0.0286\n",
            "Epoch [16141/20000], Training Loss: 0.0269\n",
            "Epoch [16142/20000], Training Loss: 0.0253\n",
            "Epoch [16143/20000], Training Loss: 0.0270\n",
            "Epoch [16144/20000], Training Loss: 0.0268\n",
            "Epoch [16145/20000], Training Loss: 0.0270\n",
            "Epoch [16146/20000], Training Loss: 0.0270\n",
            "Epoch [16147/20000], Training Loss: 0.0277\n",
            "Epoch [16148/20000], Training Loss: 0.0264\n",
            "Epoch [16149/20000], Training Loss: 0.0280\n",
            "Epoch [16150/20000], Training Loss: 0.0282\n",
            "Epoch [16151/20000], Training Loss: 0.0255\n",
            "Epoch [16152/20000], Training Loss: 0.0259\n",
            "Epoch [16153/20000], Training Loss: 0.0276\n",
            "Epoch [16154/20000], Training Loss: 0.0244\n",
            "Epoch [16155/20000], Training Loss: 0.0266\n",
            "Epoch [16156/20000], Training Loss: 0.0265\n",
            "Epoch [16157/20000], Training Loss: 0.0247\n",
            "Epoch [16158/20000], Training Loss: 0.0274\n",
            "Epoch [16159/20000], Training Loss: 0.0277\n",
            "Epoch [16160/20000], Training Loss: 0.0270\n",
            "Epoch [16161/20000], Training Loss: 0.0265\n",
            "Epoch [16162/20000], Training Loss: 0.0250\n",
            "Epoch [16163/20000], Training Loss: 0.0272\n",
            "Epoch [16164/20000], Training Loss: 0.0277\n",
            "Epoch [16165/20000], Training Loss: 0.0253\n",
            "Epoch [16166/20000], Training Loss: 0.0279\n",
            "Epoch [16167/20000], Training Loss: 0.0277\n",
            "Epoch [16168/20000], Training Loss: 0.0280\n",
            "Epoch [16169/20000], Training Loss: 0.0262\n",
            "Epoch [16170/20000], Training Loss: 0.0274\n",
            "Epoch [16171/20000], Training Loss: 0.0295\n",
            "Epoch [16172/20000], Training Loss: 0.0250\n",
            "Epoch [16173/20000], Training Loss: 0.0248\n",
            "Epoch [16174/20000], Training Loss: 0.0259\n",
            "Epoch [16175/20000], Training Loss: 0.0272\n",
            "Epoch [16176/20000], Training Loss: 0.0257\n",
            "Epoch [16177/20000], Training Loss: 0.0270\n",
            "Epoch [16178/20000], Training Loss: 0.0284\n",
            "Epoch [16179/20000], Training Loss: 0.0264\n",
            "Epoch [16180/20000], Training Loss: 0.0271\n",
            "Epoch [16181/20000], Training Loss: 0.0251\n",
            "Epoch [16182/20000], Training Loss: 0.0263\n",
            "Epoch [16183/20000], Training Loss: 0.0252\n",
            "Epoch [16184/20000], Training Loss: 0.0253\n",
            "Epoch [16185/20000], Training Loss: 0.0267\n",
            "Epoch [16186/20000], Training Loss: 0.0254\n",
            "Epoch [16187/20000], Training Loss: 0.0256\n",
            "Epoch [16188/20000], Training Loss: 0.0258\n",
            "Epoch [16189/20000], Training Loss: 0.0248\n",
            "Epoch [16190/20000], Training Loss: 0.0272\n",
            "Epoch [16191/20000], Training Loss: 0.0283\n",
            "Epoch [16192/20000], Training Loss: 0.0246\n",
            "Epoch [16193/20000], Training Loss: 0.0253\n",
            "Epoch [16194/20000], Training Loss: 0.0278\n",
            "Epoch [16195/20000], Training Loss: 0.0285\n",
            "Epoch [16196/20000], Training Loss: 0.0261\n",
            "Epoch [16197/20000], Training Loss: 0.0288\n",
            "Epoch [16198/20000], Training Loss: 0.0260\n",
            "Epoch [16199/20000], Training Loss: 0.0263\n",
            "Epoch [16200/20000], Training Loss: 0.0263\n",
            "Epoch [16201/20000], Training Loss: 0.0246\n",
            "Epoch [16202/20000], Training Loss: 0.0284\n",
            "Epoch [16203/20000], Training Loss: 0.0258\n",
            "Epoch [16204/20000], Training Loss: 0.0252\n",
            "Epoch [16205/20000], Training Loss: 0.0254\n",
            "Epoch [16206/20000], Training Loss: 0.0251\n",
            "Epoch [16207/20000], Training Loss: 0.0288\n",
            "Epoch [16208/20000], Training Loss: 0.0264\n",
            "Epoch [16209/20000], Training Loss: 0.0251\n",
            "Epoch [16210/20000], Training Loss: 0.0247\n",
            "Epoch [16211/20000], Training Loss: 0.0288\n",
            "Epoch [16212/20000], Training Loss: 0.0255\n",
            "Epoch [16213/20000], Training Loss: 0.0277\n",
            "Epoch [16214/20000], Training Loss: 0.0258\n",
            "Epoch [16215/20000], Training Loss: 0.0242\n",
            "Epoch [16216/20000], Training Loss: 0.0264\n",
            "Epoch [16217/20000], Training Loss: 0.0278\n",
            "Epoch [16218/20000], Training Loss: 0.0261\n",
            "Epoch [16219/20000], Training Loss: 0.0263\n",
            "Epoch [16220/20000], Training Loss: 0.0258\n",
            "Epoch [16221/20000], Training Loss: 0.0275\n",
            "Epoch [16222/20000], Training Loss: 0.0243\n",
            "Epoch [16223/20000], Training Loss: 0.0275\n",
            "Epoch [16224/20000], Training Loss: 0.0272\n",
            "Epoch [16225/20000], Training Loss: 0.0263\n",
            "Epoch [16226/20000], Training Loss: 0.0251\n",
            "Epoch [16227/20000], Training Loss: 0.0273\n",
            "Epoch [16228/20000], Training Loss: 0.0273\n",
            "Epoch [16229/20000], Training Loss: 0.0264\n",
            "Epoch [16230/20000], Training Loss: 0.0262\n",
            "Epoch [16231/20000], Training Loss: 0.0280\n",
            "Epoch [16232/20000], Training Loss: 0.0252\n",
            "Epoch [16233/20000], Training Loss: 0.0253\n",
            "Epoch [16234/20000], Training Loss: 0.0266\n",
            "Epoch [16235/20000], Training Loss: 0.0268\n",
            "Epoch [16236/20000], Training Loss: 0.0276\n",
            "Epoch [16237/20000], Training Loss: 0.0272\n",
            "Epoch [16238/20000], Training Loss: 0.0270\n",
            "Epoch [16239/20000], Training Loss: 0.0263\n",
            "Epoch [16240/20000], Training Loss: 0.0254\n",
            "Epoch [16241/20000], Training Loss: 0.0267\n",
            "Epoch [16242/20000], Training Loss: 0.0274\n",
            "Epoch [16243/20000], Training Loss: 0.0279\n",
            "Epoch [16244/20000], Training Loss: 0.0277\n",
            "Epoch [16245/20000], Training Loss: 0.0278\n",
            "Epoch [16246/20000], Training Loss: 0.0275\n",
            "Epoch [16247/20000], Training Loss: 0.0248\n",
            "Epoch [16248/20000], Training Loss: 0.0269\n",
            "Epoch [16249/20000], Training Loss: 0.0267\n",
            "Epoch [16250/20000], Training Loss: 0.0255\n",
            "Epoch [16251/20000], Training Loss: 0.0264\n",
            "Epoch [16252/20000], Training Loss: 0.0274\n",
            "Epoch [16253/20000], Training Loss: 0.0249\n",
            "Epoch [16254/20000], Training Loss: 0.0294\n",
            "Epoch [16255/20000], Training Loss: 0.0253\n",
            "Epoch [16256/20000], Training Loss: 0.0257\n",
            "Epoch [16257/20000], Training Loss: 0.0257\n",
            "Epoch [16258/20000], Training Loss: 0.0281\n",
            "Epoch [16259/20000], Training Loss: 0.0260\n",
            "Epoch [16260/20000], Training Loss: 0.0282\n",
            "Epoch [16261/20000], Training Loss: 0.0285\n",
            "Epoch [16262/20000], Training Loss: 0.0282\n",
            "Epoch [16263/20000], Training Loss: 0.0259\n",
            "Epoch [16264/20000], Training Loss: 0.0261\n",
            "Epoch [16265/20000], Training Loss: 0.0263\n",
            "Epoch [16266/20000], Training Loss: 0.0272\n",
            "Epoch [16267/20000], Training Loss: 0.0249\n",
            "Epoch [16268/20000], Training Loss: 0.0264\n",
            "Epoch [16269/20000], Training Loss: 0.0265\n",
            "Epoch [16270/20000], Training Loss: 0.0261\n",
            "Epoch [16271/20000], Training Loss: 0.0270\n",
            "Epoch [16272/20000], Training Loss: 0.0253\n",
            "Epoch [16273/20000], Training Loss: 0.0259\n",
            "Epoch [16274/20000], Training Loss: 0.0269\n",
            "Epoch [16275/20000], Training Loss: 0.0261\n",
            "Epoch [16276/20000], Training Loss: 0.0246\n",
            "Epoch [16277/20000], Training Loss: 0.0242\n",
            "Epoch [16278/20000], Training Loss: 0.0270\n",
            "Epoch [16279/20000], Training Loss: 0.0243\n",
            "Epoch [16280/20000], Training Loss: 0.0249\n",
            "Epoch [16281/20000], Training Loss: 0.0265\n",
            "Epoch [16282/20000], Training Loss: 0.0249\n",
            "Epoch [16283/20000], Training Loss: 0.0249\n",
            "Epoch [16284/20000], Training Loss: 0.0270\n",
            "Epoch [16285/20000], Training Loss: 0.0257\n",
            "Epoch [16286/20000], Training Loss: 0.0256\n",
            "Epoch [16287/20000], Training Loss: 0.0257\n",
            "Epoch [16288/20000], Training Loss: 0.0254\n",
            "Epoch [16289/20000], Training Loss: 0.0264\n",
            "Epoch [16290/20000], Training Loss: 0.0268\n",
            "Epoch [16291/20000], Training Loss: 0.0257\n",
            "Epoch [16292/20000], Training Loss: 0.0259\n",
            "Epoch [16293/20000], Training Loss: 0.0270\n",
            "Epoch [16294/20000], Training Loss: 0.0257\n",
            "Epoch [16295/20000], Training Loss: 0.0281\n",
            "Epoch [16296/20000], Training Loss: 0.0282\n",
            "Epoch [16297/20000], Training Loss: 0.0252\n",
            "Epoch [16298/20000], Training Loss: 0.0276\n",
            "Epoch [16299/20000], Training Loss: 0.0280\n",
            "Epoch [16300/20000], Training Loss: 0.0266\n",
            "Epoch [16301/20000], Training Loss: 0.0263\n",
            "Epoch [16302/20000], Training Loss: 0.0249\n",
            "Epoch [16303/20000], Training Loss: 0.0261\n",
            "Epoch [16304/20000], Training Loss: 0.0270\n",
            "Epoch [16305/20000], Training Loss: 0.0259\n",
            "Epoch [16306/20000], Training Loss: 0.0261\n",
            "Epoch [16307/20000], Training Loss: 0.0285\n",
            "Epoch [16308/20000], Training Loss: 0.0267\n",
            "Epoch [16309/20000], Training Loss: 0.0269\n",
            "Epoch [16310/20000], Training Loss: 0.0280\n",
            "Epoch [16311/20000], Training Loss: 0.0257\n",
            "Epoch [16312/20000], Training Loss: 0.0270\n",
            "Epoch [16313/20000], Training Loss: 0.0274\n",
            "Epoch [16314/20000], Training Loss: 0.0272\n",
            "Epoch [16315/20000], Training Loss: 0.0241\n",
            "Epoch [16316/20000], Training Loss: 0.0274\n",
            "Epoch [16317/20000], Training Loss: 0.0281\n",
            "Epoch [16318/20000], Training Loss: 0.0274\n",
            "Epoch [16319/20000], Training Loss: 0.0252\n",
            "Epoch [16320/20000], Training Loss: 0.0264\n",
            "Epoch [16321/20000], Training Loss: 0.0267\n",
            "Epoch [16322/20000], Training Loss: 0.0268\n",
            "Epoch [16323/20000], Training Loss: 0.0279\n",
            "Epoch [16324/20000], Training Loss: 0.0287\n",
            "Epoch [16325/20000], Training Loss: 0.0260\n",
            "Epoch [16326/20000], Training Loss: 0.0286\n",
            "Epoch [16327/20000], Training Loss: 0.0267\n",
            "Epoch [16328/20000], Training Loss: 0.0270\n",
            "Epoch [16329/20000], Training Loss: 0.0263\n",
            "Epoch [16330/20000], Training Loss: 0.0281\n",
            "Epoch [16331/20000], Training Loss: 0.0252\n",
            "Epoch [16332/20000], Training Loss: 0.0270\n",
            "Epoch [16333/20000], Training Loss: 0.0263\n",
            "Epoch [16334/20000], Training Loss: 0.0240\n",
            "Epoch [16335/20000], Training Loss: 0.0263\n",
            "Epoch [16336/20000], Training Loss: 0.0272\n",
            "Epoch [16337/20000], Training Loss: 0.0276\n",
            "Epoch [16338/20000], Training Loss: 0.0245\n",
            "Epoch [16339/20000], Training Loss: 0.0256\n",
            "Epoch [16340/20000], Training Loss: 0.0246\n",
            "Epoch [16341/20000], Training Loss: 0.0261\n",
            "Epoch [16342/20000], Training Loss: 0.0263\n",
            "Epoch [16343/20000], Training Loss: 0.0258\n",
            "Epoch [16344/20000], Training Loss: 0.0258\n",
            "Epoch [16345/20000], Training Loss: 0.0274\n",
            "Epoch [16346/20000], Training Loss: 0.0270\n",
            "Epoch [16347/20000], Training Loss: 0.0255\n",
            "Epoch [16348/20000], Training Loss: 0.0257\n",
            "Epoch [16349/20000], Training Loss: 0.0266\n",
            "Epoch [16350/20000], Training Loss: 0.0261\n",
            "Epoch [16351/20000], Training Loss: 0.0272\n",
            "Epoch [16352/20000], Training Loss: 0.0271\n",
            "Epoch [16353/20000], Training Loss: 0.0264\n",
            "Epoch [16354/20000], Training Loss: 0.0256\n",
            "Epoch [16355/20000], Training Loss: 0.0265\n",
            "Epoch [16356/20000], Training Loss: 0.0253\n",
            "Epoch [16357/20000], Training Loss: 0.0275\n",
            "Epoch [16358/20000], Training Loss: 0.0252\n",
            "Epoch [16359/20000], Training Loss: 0.0267\n",
            "Epoch [16360/20000], Training Loss: 0.0278\n",
            "Epoch [16361/20000], Training Loss: 0.0281\n",
            "Epoch [16362/20000], Training Loss: 0.0257\n",
            "Epoch [16363/20000], Training Loss: 0.0255\n",
            "Epoch [16364/20000], Training Loss: 0.0271\n",
            "Epoch [16365/20000], Training Loss: 0.0284\n",
            "Epoch [16366/20000], Training Loss: 0.0250\n",
            "Epoch [16367/20000], Training Loss: 0.0279\n",
            "Epoch [16368/20000], Training Loss: 0.0279\n",
            "Epoch [16369/20000], Training Loss: 0.0262\n",
            "Epoch [16370/20000], Training Loss: 0.0261\n",
            "Epoch [16371/20000], Training Loss: 0.0264\n",
            "Epoch [16372/20000], Training Loss: 0.0278\n",
            "Epoch [16373/20000], Training Loss: 0.0272\n",
            "Epoch [16374/20000], Training Loss: 0.0267\n",
            "Epoch [16375/20000], Training Loss: 0.0266\n",
            "Epoch [16376/20000], Training Loss: 0.0291\n",
            "Epoch [16377/20000], Training Loss: 0.0275\n",
            "Epoch [16378/20000], Training Loss: 0.0277\n",
            "Epoch [16379/20000], Training Loss: 0.0246\n",
            "Epoch [16380/20000], Training Loss: 0.0265\n",
            "Epoch [16381/20000], Training Loss: 0.0250\n",
            "Epoch [16382/20000], Training Loss: 0.0272\n",
            "Epoch [16383/20000], Training Loss: 0.0276\n",
            "Epoch [16384/20000], Training Loss: 0.0252\n",
            "Epoch [16385/20000], Training Loss: 0.0287\n",
            "Epoch [16386/20000], Training Loss: 0.0260\n",
            "Epoch [16387/20000], Training Loss: 0.0275\n",
            "Epoch [16388/20000], Training Loss: 0.0257\n",
            "Epoch [16389/20000], Training Loss: 0.0269\n",
            "Epoch [16390/20000], Training Loss: 0.0269\n",
            "Epoch [16391/20000], Training Loss: 0.0265\n",
            "Epoch [16392/20000], Training Loss: 0.0261\n",
            "Epoch [16393/20000], Training Loss: 0.0281\n",
            "Epoch [16394/20000], Training Loss: 0.0275\n",
            "Epoch [16395/20000], Training Loss: 0.0283\n",
            "Epoch [16396/20000], Training Loss: 0.0257\n",
            "Epoch [16397/20000], Training Loss: 0.0266\n",
            "Epoch [16398/20000], Training Loss: 0.0282\n",
            "Epoch [16399/20000], Training Loss: 0.0255\n",
            "Epoch [16400/20000], Training Loss: 0.0253\n",
            "Epoch [16401/20000], Training Loss: 0.0288\n",
            "Epoch [16402/20000], Training Loss: 0.0250\n",
            "Epoch [16403/20000], Training Loss: 0.0266\n",
            "Epoch [16404/20000], Training Loss: 0.0263\n",
            "Epoch [16405/20000], Training Loss: 0.0266\n",
            "Epoch [16406/20000], Training Loss: 0.0261\n",
            "Epoch [16407/20000], Training Loss: 0.0251\n",
            "Epoch [16408/20000], Training Loss: 0.0266\n",
            "Epoch [16409/20000], Training Loss: 0.0270\n",
            "Epoch [16410/20000], Training Loss: 0.0246\n",
            "Epoch [16411/20000], Training Loss: 0.0270\n",
            "Epoch [16412/20000], Training Loss: 0.0253\n",
            "Epoch [16413/20000], Training Loss: 0.0266\n",
            "Epoch [16414/20000], Training Loss: 0.0289\n",
            "Epoch [16415/20000], Training Loss: 0.0271\n",
            "Epoch [16416/20000], Training Loss: 0.0269\n",
            "Epoch [16417/20000], Training Loss: 0.0261\n",
            "Epoch [16418/20000], Training Loss: 0.0261\n",
            "Epoch [16419/20000], Training Loss: 0.0249\n",
            "Epoch [16420/20000], Training Loss: 0.0256\n",
            "Epoch [16421/20000], Training Loss: 0.0251\n",
            "Epoch [16422/20000], Training Loss: 0.0287\n",
            "Epoch [16423/20000], Training Loss: 0.0274\n",
            "Epoch [16424/20000], Training Loss: 0.0255\n",
            "Epoch [16425/20000], Training Loss: 0.0265\n",
            "Epoch [16426/20000], Training Loss: 0.0248\n",
            "Epoch [16427/20000], Training Loss: 0.0258\n",
            "Epoch [16428/20000], Training Loss: 0.0281\n",
            "Epoch [16429/20000], Training Loss: 0.0286\n",
            "Epoch [16430/20000], Training Loss: 0.0254\n",
            "Epoch [16431/20000], Training Loss: 0.0272\n",
            "Epoch [16432/20000], Training Loss: 0.0278\n",
            "Epoch [16433/20000], Training Loss: 0.0273\n",
            "Epoch [16434/20000], Training Loss: 0.0265\n",
            "Epoch [16435/20000], Training Loss: 0.0273\n",
            "Epoch [16436/20000], Training Loss: 0.0247\n",
            "Epoch [16437/20000], Training Loss: 0.0272\n",
            "Epoch [16438/20000], Training Loss: 0.0280\n",
            "Epoch [16439/20000], Training Loss: 0.0264\n",
            "Epoch [16440/20000], Training Loss: 0.0264\n",
            "Epoch [16441/20000], Training Loss: 0.0253\n",
            "Epoch [16442/20000], Training Loss: 0.0258\n",
            "Epoch [16443/20000], Training Loss: 0.0259\n",
            "Epoch [16444/20000], Training Loss: 0.0271\n",
            "Epoch [16445/20000], Training Loss: 0.0264\n",
            "Epoch [16446/20000], Training Loss: 0.0255\n",
            "Epoch [16447/20000], Training Loss: 0.0261\n",
            "Epoch [16448/20000], Training Loss: 0.0257\n",
            "Epoch [16449/20000], Training Loss: 0.0267\n",
            "Epoch [16450/20000], Training Loss: 0.0276\n",
            "Epoch [16451/20000], Training Loss: 0.0257\n",
            "Epoch [16452/20000], Training Loss: 0.0253\n",
            "Epoch [16453/20000], Training Loss: 0.0275\n",
            "Epoch [16454/20000], Training Loss: 0.0273\n",
            "Epoch [16455/20000], Training Loss: 0.0268\n",
            "Epoch [16456/20000], Training Loss: 0.0259\n",
            "Epoch [16457/20000], Training Loss: 0.0259\n",
            "Epoch [16458/20000], Training Loss: 0.0251\n",
            "Epoch [16459/20000], Training Loss: 0.0264\n",
            "Epoch [16460/20000], Training Loss: 0.0274\n",
            "Epoch [16461/20000], Training Loss: 0.0251\n",
            "Epoch [16462/20000], Training Loss: 0.0266\n",
            "Epoch [16463/20000], Training Loss: 0.0278\n",
            "Epoch [16464/20000], Training Loss: 0.0263\n",
            "Epoch [16465/20000], Training Loss: 0.0263\n",
            "Epoch [16466/20000], Training Loss: 0.0266\n",
            "Epoch [16467/20000], Training Loss: 0.0266\n",
            "Epoch [16468/20000], Training Loss: 0.0283\n",
            "Epoch [16469/20000], Training Loss: 0.0259\n",
            "Epoch [16470/20000], Training Loss: 0.0242\n",
            "Epoch [16471/20000], Training Loss: 0.0250\n",
            "Epoch [16472/20000], Training Loss: 0.0253\n",
            "Epoch [16473/20000], Training Loss: 0.0261\n",
            "Epoch [16474/20000], Training Loss: 0.0255\n",
            "Epoch [16475/20000], Training Loss: 0.0265\n",
            "Epoch [16476/20000], Training Loss: 0.0263\n",
            "Epoch [16477/20000], Training Loss: 0.0264\n",
            "Epoch [16478/20000], Training Loss: 0.0262\n",
            "Epoch [16479/20000], Training Loss: 0.0269\n",
            "Epoch [16480/20000], Training Loss: 0.0257\n",
            "Epoch [16481/20000], Training Loss: 0.0253\n",
            "Epoch [16482/20000], Training Loss: 0.0273\n",
            "Epoch [16483/20000], Training Loss: 0.0279\n",
            "Epoch [16484/20000], Training Loss: 0.0261\n",
            "Epoch [16485/20000], Training Loss: 0.0267\n",
            "Epoch [16486/20000], Training Loss: 0.0246\n",
            "Epoch [16487/20000], Training Loss: 0.0249\n",
            "Epoch [16488/20000], Training Loss: 0.0257\n",
            "Epoch [16489/20000], Training Loss: 0.0280\n",
            "Epoch [16490/20000], Training Loss: 0.0262\n",
            "Epoch [16491/20000], Training Loss: 0.0276\n",
            "Epoch [16492/20000], Training Loss: 0.0269\n",
            "Epoch [16493/20000], Training Loss: 0.0275\n",
            "Epoch [16494/20000], Training Loss: 0.0275\n",
            "Epoch [16495/20000], Training Loss: 0.0277\n",
            "Epoch [16496/20000], Training Loss: 0.0253\n",
            "Epoch [16497/20000], Training Loss: 0.0278\n",
            "Epoch [16498/20000], Training Loss: 0.0256\n",
            "Epoch [16499/20000], Training Loss: 0.0249\n",
            "Epoch [16500/20000], Training Loss: 0.0290\n",
            "Epoch [16501/20000], Training Loss: 0.0257\n",
            "Epoch [16502/20000], Training Loss: 0.0245\n",
            "Epoch [16503/20000], Training Loss: 0.0264\n",
            "Epoch [16504/20000], Training Loss: 0.0270\n",
            "Epoch [16505/20000], Training Loss: 0.0257\n",
            "Epoch [16506/20000], Training Loss: 0.0241\n",
            "Epoch [16507/20000], Training Loss: 0.0262\n",
            "Epoch [16508/20000], Training Loss: 0.0263\n",
            "Epoch [16509/20000], Training Loss: 0.0248\n",
            "Epoch [16510/20000], Training Loss: 0.0278\n",
            "Epoch [16511/20000], Training Loss: 0.0286\n",
            "Epoch [16512/20000], Training Loss: 0.0261\n",
            "Epoch [16513/20000], Training Loss: 0.0251\n",
            "Epoch [16514/20000], Training Loss: 0.0277\n",
            "Epoch [16515/20000], Training Loss: 0.0264\n",
            "Epoch [16516/20000], Training Loss: 0.0283\n",
            "Epoch [16517/20000], Training Loss: 0.0286\n",
            "Epoch [16518/20000], Training Loss: 0.0288\n",
            "Epoch [16519/20000], Training Loss: 0.0241\n",
            "Epoch [16520/20000], Training Loss: 0.0278\n",
            "Epoch [16521/20000], Training Loss: 0.0258\n",
            "Epoch [16522/20000], Training Loss: 0.0255\n",
            "Epoch [16523/20000], Training Loss: 0.0280\n",
            "Epoch [16524/20000], Training Loss: 0.0261\n",
            "Epoch [16525/20000], Training Loss: 0.0260\n",
            "Epoch [16526/20000], Training Loss: 0.0254\n",
            "Epoch [16527/20000], Training Loss: 0.0287\n",
            "Epoch [16528/20000], Training Loss: 0.0268\n",
            "Epoch [16529/20000], Training Loss: 0.0255\n",
            "Epoch [16530/20000], Training Loss: 0.0261\n",
            "Epoch [16531/20000], Training Loss: 0.0268\n",
            "Epoch [16532/20000], Training Loss: 0.0272\n",
            "Epoch [16533/20000], Training Loss: 0.0254\n",
            "Epoch [16534/20000], Training Loss: 0.0268\n",
            "Epoch [16535/20000], Training Loss: 0.0264\n",
            "Epoch [16536/20000], Training Loss: 0.0260\n",
            "Epoch [16537/20000], Training Loss: 0.0265\n",
            "Epoch [16538/20000], Training Loss: 0.0261\n",
            "Epoch [16539/20000], Training Loss: 0.0274\n",
            "Epoch [16540/20000], Training Loss: 0.0265\n",
            "Epoch [16541/20000], Training Loss: 0.0272\n",
            "Epoch [16542/20000], Training Loss: 0.0276\n",
            "Epoch [16543/20000], Training Loss: 0.0258\n",
            "Epoch [16544/20000], Training Loss: 0.0287\n",
            "Epoch [16545/20000], Training Loss: 0.0260\n",
            "Epoch [16546/20000], Training Loss: 0.0242\n",
            "Epoch [16547/20000], Training Loss: 0.0262\n",
            "Epoch [16548/20000], Training Loss: 0.0267\n",
            "Epoch [16549/20000], Training Loss: 0.0260\n",
            "Epoch [16550/20000], Training Loss: 0.0282\n",
            "Epoch [16551/20000], Training Loss: 0.0257\n",
            "Epoch [16552/20000], Training Loss: 0.0281\n",
            "Epoch [16553/20000], Training Loss: 0.0254\n",
            "Epoch [16554/20000], Training Loss: 0.0249\n",
            "Epoch [16555/20000], Training Loss: 0.0250\n",
            "Epoch [16556/20000], Training Loss: 0.0260\n",
            "Epoch [16557/20000], Training Loss: 0.0261\n",
            "Epoch [16558/20000], Training Loss: 0.0262\n",
            "Epoch [16559/20000], Training Loss: 0.0278\n",
            "Epoch [16560/20000], Training Loss: 0.0265\n",
            "Epoch [16561/20000], Training Loss: 0.0241\n",
            "Epoch [16562/20000], Training Loss: 0.0253\n",
            "Epoch [16563/20000], Training Loss: 0.0276\n",
            "Epoch [16564/20000], Training Loss: 0.0250\n",
            "Epoch [16565/20000], Training Loss: 0.0257\n",
            "Epoch [16566/20000], Training Loss: 0.0253\n",
            "Epoch [16567/20000], Training Loss: 0.0255\n",
            "Epoch [16568/20000], Training Loss: 0.0260\n",
            "Epoch [16569/20000], Training Loss: 0.0258\n",
            "Epoch [16570/20000], Training Loss: 0.0259\n",
            "Epoch [16571/20000], Training Loss: 0.0263\n",
            "Epoch [16572/20000], Training Loss: 0.0266\n",
            "Epoch [16573/20000], Training Loss: 0.0290\n",
            "Epoch [16574/20000], Training Loss: 0.0257\n",
            "Epoch [16575/20000], Training Loss: 0.0279\n",
            "Epoch [16576/20000], Training Loss: 0.0281\n",
            "Epoch [16577/20000], Training Loss: 0.0275\n",
            "Epoch [16578/20000], Training Loss: 0.0273\n",
            "Epoch [16579/20000], Training Loss: 0.0253\n",
            "Epoch [16580/20000], Training Loss: 0.0257\n",
            "Epoch [16581/20000], Training Loss: 0.0274\n",
            "Epoch [16582/20000], Training Loss: 0.0282\n",
            "Epoch [16583/20000], Training Loss: 0.0249\n",
            "Epoch [16584/20000], Training Loss: 0.0258\n",
            "Epoch [16585/20000], Training Loss: 0.0263\n",
            "Epoch [16586/20000], Training Loss: 0.0278\n",
            "Epoch [16587/20000], Training Loss: 0.0262\n",
            "Epoch [16588/20000], Training Loss: 0.0252\n",
            "Epoch [16589/20000], Training Loss: 0.0274\n",
            "Epoch [16590/20000], Training Loss: 0.0247\n",
            "Epoch [16591/20000], Training Loss: 0.0285\n",
            "Epoch [16592/20000], Training Loss: 0.0257\n",
            "Epoch [16593/20000], Training Loss: 0.0244\n",
            "Epoch [16594/20000], Training Loss: 0.0270\n",
            "Epoch [16595/20000], Training Loss: 0.0262\n",
            "Epoch [16596/20000], Training Loss: 0.0266\n",
            "Epoch [16597/20000], Training Loss: 0.0261\n",
            "Epoch [16598/20000], Training Loss: 0.0265\n",
            "Epoch [16599/20000], Training Loss: 0.0254\n",
            "Epoch [16600/20000], Training Loss: 0.0255\n",
            "Epoch [16601/20000], Training Loss: 0.0275\n",
            "Epoch [16602/20000], Training Loss: 0.0265\n",
            "Epoch [16603/20000], Training Loss: 0.0249\n",
            "Epoch [16604/20000], Training Loss: 0.0264\n",
            "Epoch [16605/20000], Training Loss: 0.0262\n",
            "Epoch [16606/20000], Training Loss: 0.0288\n",
            "Epoch [16607/20000], Training Loss: 0.0252\n",
            "Epoch [16608/20000], Training Loss: 0.0261\n",
            "Epoch [16609/20000], Training Loss: 0.0250\n",
            "Epoch [16610/20000], Training Loss: 0.0255\n",
            "Epoch [16611/20000], Training Loss: 0.0288\n",
            "Epoch [16612/20000], Training Loss: 0.0261\n",
            "Epoch [16613/20000], Training Loss: 0.0282\n",
            "Epoch [16614/20000], Training Loss: 0.0275\n",
            "Epoch [16615/20000], Training Loss: 0.0275\n",
            "Epoch [16616/20000], Training Loss: 0.0279\n",
            "Epoch [16617/20000], Training Loss: 0.0272\n",
            "Epoch [16618/20000], Training Loss: 0.0279\n",
            "Epoch [16619/20000], Training Loss: 0.0272\n",
            "Epoch [16620/20000], Training Loss: 0.0259\n",
            "Epoch [16621/20000], Training Loss: 0.0278\n",
            "Epoch [16622/20000], Training Loss: 0.0263\n",
            "Epoch [16623/20000], Training Loss: 0.0256\n",
            "Epoch [16624/20000], Training Loss: 0.0248\n",
            "Epoch [16625/20000], Training Loss: 0.0293\n",
            "Epoch [16626/20000], Training Loss: 0.0262\n",
            "Epoch [16627/20000], Training Loss: 0.0241\n",
            "Epoch [16628/20000], Training Loss: 0.0255\n",
            "Epoch [16629/20000], Training Loss: 0.0251\n",
            "Epoch [16630/20000], Training Loss: 0.0270\n",
            "Epoch [16631/20000], Training Loss: 0.0247\n",
            "Epoch [16632/20000], Training Loss: 0.0258\n",
            "Epoch [16633/20000], Training Loss: 0.0263\n",
            "Epoch [16634/20000], Training Loss: 0.0277\n",
            "Epoch [16635/20000], Training Loss: 0.0273\n",
            "Epoch [16636/20000], Training Loss: 0.0260\n",
            "Epoch [16637/20000], Training Loss: 0.0258\n",
            "Epoch [16638/20000], Training Loss: 0.0254\n",
            "Epoch [16639/20000], Training Loss: 0.0236\n",
            "Epoch [16640/20000], Training Loss: 0.0251\n",
            "Epoch [16641/20000], Training Loss: 0.0246\n",
            "Epoch [16642/20000], Training Loss: 0.0276\n",
            "Epoch [16643/20000], Training Loss: 0.0272\n",
            "Epoch [16644/20000], Training Loss: 0.0269\n",
            "Epoch [16645/20000], Training Loss: 0.0265\n",
            "Epoch [16646/20000], Training Loss: 0.0253\n",
            "Epoch [16647/20000], Training Loss: 0.0262\n",
            "Epoch [16648/20000], Training Loss: 0.0252\n",
            "Epoch [16649/20000], Training Loss: 0.0242\n",
            "Epoch [16650/20000], Training Loss: 0.0274\n",
            "Epoch [16651/20000], Training Loss: 0.0242\n",
            "Epoch [16652/20000], Training Loss: 0.0265\n",
            "Epoch [16653/20000], Training Loss: 0.0273\n",
            "Epoch [16654/20000], Training Loss: 0.0252\n",
            "Epoch [16655/20000], Training Loss: 0.0271\n",
            "Epoch [16656/20000], Training Loss: 0.0256\n",
            "Epoch [16657/20000], Training Loss: 0.0263\n",
            "Epoch [16658/20000], Training Loss: 0.0264\n",
            "Epoch [16659/20000], Training Loss: 0.0261\n",
            "Epoch [16660/20000], Training Loss: 0.0289\n",
            "Epoch [16661/20000], Training Loss: 0.0245\n",
            "Epoch [16662/20000], Training Loss: 0.0252\n",
            "Epoch [16663/20000], Training Loss: 0.0295\n",
            "Epoch [16664/20000], Training Loss: 0.0258\n",
            "Epoch [16665/20000], Training Loss: 0.0256\n",
            "Epoch [16666/20000], Training Loss: 0.0267\n",
            "Epoch [16667/20000], Training Loss: 0.0285\n",
            "Epoch [16668/20000], Training Loss: 0.0276\n",
            "Epoch [16669/20000], Training Loss: 0.0244\n",
            "Epoch [16670/20000], Training Loss: 0.0285\n",
            "Epoch [16671/20000], Training Loss: 0.0253\n",
            "Epoch [16672/20000], Training Loss: 0.0253\n",
            "Epoch [16673/20000], Training Loss: 0.0251\n",
            "Epoch [16674/20000], Training Loss: 0.0257\n",
            "Epoch [16675/20000], Training Loss: 0.0271\n",
            "Epoch [16676/20000], Training Loss: 0.0263\n",
            "Epoch [16677/20000], Training Loss: 0.0259\n",
            "Epoch [16678/20000], Training Loss: 0.0264\n",
            "Epoch [16679/20000], Training Loss: 0.0264\n",
            "Epoch [16680/20000], Training Loss: 0.0252\n",
            "Epoch [16681/20000], Training Loss: 0.0282\n",
            "Epoch [16682/20000], Training Loss: 0.0260\n",
            "Epoch [16683/20000], Training Loss: 0.0264\n",
            "Epoch [16684/20000], Training Loss: 0.0266\n",
            "Epoch [16685/20000], Training Loss: 0.0277\n",
            "Epoch [16686/20000], Training Loss: 0.0259\n",
            "Epoch [16687/20000], Training Loss: 0.0286\n",
            "Epoch [16688/20000], Training Loss: 0.0259\n",
            "Epoch [16689/20000], Training Loss: 0.0250\n",
            "Epoch [16690/20000], Training Loss: 0.0258\n",
            "Epoch [16691/20000], Training Loss: 0.0281\n",
            "Epoch [16692/20000], Training Loss: 0.0259\n",
            "Epoch [16693/20000], Training Loss: 0.0265\n",
            "Epoch [16694/20000], Training Loss: 0.0271\n",
            "Epoch [16695/20000], Training Loss: 0.0248\n",
            "Epoch [16696/20000], Training Loss: 0.0276\n",
            "Epoch [16697/20000], Training Loss: 0.0254\n",
            "Epoch [16698/20000], Training Loss: 0.0266\n",
            "Epoch [16699/20000], Training Loss: 0.0279\n",
            "Epoch [16700/20000], Training Loss: 0.0253\n",
            "Epoch [16701/20000], Training Loss: 0.0255\n",
            "Epoch [16702/20000], Training Loss: 0.0269\n",
            "Epoch [16703/20000], Training Loss: 0.0272\n",
            "Epoch [16704/20000], Training Loss: 0.0284\n",
            "Epoch [16705/20000], Training Loss: 0.0262\n",
            "Epoch [16706/20000], Training Loss: 0.0268\n",
            "Epoch [16707/20000], Training Loss: 0.0266\n",
            "Epoch [16708/20000], Training Loss: 0.0244\n",
            "Epoch [16709/20000], Training Loss: 0.0246\n",
            "Epoch [16710/20000], Training Loss: 0.0292\n",
            "Epoch [16711/20000], Training Loss: 0.0254\n",
            "Epoch [16712/20000], Training Loss: 0.0270\n",
            "Epoch [16713/20000], Training Loss: 0.0243\n",
            "Epoch [16714/20000], Training Loss: 0.0255\n",
            "Epoch [16715/20000], Training Loss: 0.0291\n",
            "Epoch [16716/20000], Training Loss: 0.0263\n",
            "Epoch [16717/20000], Training Loss: 0.0274\n",
            "Epoch [16718/20000], Training Loss: 0.0258\n",
            "Epoch [16719/20000], Training Loss: 0.0265\n",
            "Epoch [16720/20000], Training Loss: 0.0271\n",
            "Epoch [16721/20000], Training Loss: 0.0253\n",
            "Epoch [16722/20000], Training Loss: 0.0285\n",
            "Epoch [16723/20000], Training Loss: 0.0265\n",
            "Epoch [16724/20000], Training Loss: 0.0252\n",
            "Epoch [16725/20000], Training Loss: 0.0273\n",
            "Epoch [16726/20000], Training Loss: 0.0283\n",
            "Epoch [16727/20000], Training Loss: 0.0279\n",
            "Epoch [16728/20000], Training Loss: 0.0242\n",
            "Epoch [16729/20000], Training Loss: 0.0260\n",
            "Epoch [16730/20000], Training Loss: 0.0260\n",
            "Epoch [16731/20000], Training Loss: 0.0262\n",
            "Epoch [16732/20000], Training Loss: 0.0258\n",
            "Epoch [16733/20000], Training Loss: 0.0268\n",
            "Epoch [16734/20000], Training Loss: 0.0268\n",
            "Epoch [16735/20000], Training Loss: 0.0280\n",
            "Epoch [16736/20000], Training Loss: 0.0275\n",
            "Epoch [16737/20000], Training Loss: 0.0258\n",
            "Epoch [16738/20000], Training Loss: 0.0277\n",
            "Epoch [16739/20000], Training Loss: 0.0249\n",
            "Epoch [16740/20000], Training Loss: 0.0267\n",
            "Epoch [16741/20000], Training Loss: 0.0239\n",
            "Epoch [16742/20000], Training Loss: 0.0260\n",
            "Epoch [16743/20000], Training Loss: 0.0277\n",
            "Epoch [16744/20000], Training Loss: 0.0253\n",
            "Epoch [16745/20000], Training Loss: 0.0251\n",
            "Epoch [16746/20000], Training Loss: 0.0251\n",
            "Epoch [16747/20000], Training Loss: 0.0283\n",
            "Epoch [16748/20000], Training Loss: 0.0253\n",
            "Epoch [16749/20000], Training Loss: 0.0272\n",
            "Epoch [16750/20000], Training Loss: 0.0257\n",
            "Epoch [16751/20000], Training Loss: 0.0271\n",
            "Epoch [16752/20000], Training Loss: 0.0247\n",
            "Epoch [16753/20000], Training Loss: 0.0270\n",
            "Epoch [16754/20000], Training Loss: 0.0259\n",
            "Epoch [16755/20000], Training Loss: 0.0257\n",
            "Epoch [16756/20000], Training Loss: 0.0271\n",
            "Epoch [16757/20000], Training Loss: 0.0268\n",
            "Epoch [16758/20000], Training Loss: 0.0271\n",
            "Epoch [16759/20000], Training Loss: 0.0272\n",
            "Epoch [16760/20000], Training Loss: 0.0269\n",
            "Epoch [16761/20000], Training Loss: 0.0261\n",
            "Epoch [16762/20000], Training Loss: 0.0273\n",
            "Epoch [16763/20000], Training Loss: 0.0250\n",
            "Epoch [16764/20000], Training Loss: 0.0274\n",
            "Epoch [16765/20000], Training Loss: 0.0256\n",
            "Epoch [16766/20000], Training Loss: 0.0258\n",
            "Epoch [16767/20000], Training Loss: 0.0268\n",
            "Epoch [16768/20000], Training Loss: 0.0260\n",
            "Epoch [16769/20000], Training Loss: 0.0274\n",
            "Epoch [16770/20000], Training Loss: 0.0262\n",
            "Epoch [16771/20000], Training Loss: 0.0248\n",
            "Epoch [16772/20000], Training Loss: 0.0283\n",
            "Epoch [16773/20000], Training Loss: 0.0289\n",
            "Epoch [16774/20000], Training Loss: 0.0261\n",
            "Epoch [16775/20000], Training Loss: 0.0279\n",
            "Epoch [16776/20000], Training Loss: 0.0246\n",
            "Epoch [16777/20000], Training Loss: 0.0266\n",
            "Epoch [16778/20000], Training Loss: 0.0266\n",
            "Epoch [16779/20000], Training Loss: 0.0284\n",
            "Epoch [16780/20000], Training Loss: 0.0257\n",
            "Epoch [16781/20000], Training Loss: 0.0269\n",
            "Epoch [16782/20000], Training Loss: 0.0264\n",
            "Epoch [16783/20000], Training Loss: 0.0274\n",
            "Epoch [16784/20000], Training Loss: 0.0258\n",
            "Epoch [16785/20000], Training Loss: 0.0273\n",
            "Epoch [16786/20000], Training Loss: 0.0258\n",
            "Epoch [16787/20000], Training Loss: 0.0258\n",
            "Epoch [16788/20000], Training Loss: 0.0250\n",
            "Epoch [16789/20000], Training Loss: 0.0261\n",
            "Epoch [16790/20000], Training Loss: 0.0270\n",
            "Epoch [16791/20000], Training Loss: 0.0262\n",
            "Epoch [16792/20000], Training Loss: 0.0248\n",
            "Epoch [16793/20000], Training Loss: 0.0264\n",
            "Epoch [16794/20000], Training Loss: 0.0257\n",
            "Epoch [16795/20000], Training Loss: 0.0265\n",
            "Epoch [16796/20000], Training Loss: 0.0254\n",
            "Epoch [16797/20000], Training Loss: 0.0254\n",
            "Epoch [16798/20000], Training Loss: 0.0247\n",
            "Epoch [16799/20000], Training Loss: 0.0291\n",
            "Epoch [16800/20000], Training Loss: 0.0252\n",
            "Epoch [16801/20000], Training Loss: 0.0250\n",
            "Epoch [16802/20000], Training Loss: 0.0270\n",
            "Epoch [16803/20000], Training Loss: 0.0278\n",
            "Epoch [16804/20000], Training Loss: 0.0236\n",
            "Epoch [16805/20000], Training Loss: 0.0245\n",
            "Epoch [16806/20000], Training Loss: 0.0258\n",
            "Epoch [16807/20000], Training Loss: 0.0241\n",
            "Epoch [16808/20000], Training Loss: 0.0259\n",
            "Epoch [16809/20000], Training Loss: 0.0261\n",
            "Epoch [16810/20000], Training Loss: 0.0258\n",
            "Epoch [16811/20000], Training Loss: 0.0261\n",
            "Epoch [16812/20000], Training Loss: 0.0241\n",
            "Epoch [16813/20000], Training Loss: 0.0268\n",
            "Epoch [16814/20000], Training Loss: 0.0277\n",
            "Epoch [16815/20000], Training Loss: 0.0244\n",
            "Epoch [16816/20000], Training Loss: 0.0243\n",
            "Epoch [16817/20000], Training Loss: 0.0242\n",
            "Epoch [16818/20000], Training Loss: 0.0278\n",
            "Epoch [16819/20000], Training Loss: 0.0273\n",
            "Epoch [16820/20000], Training Loss: 0.0255\n",
            "Epoch [16821/20000], Training Loss: 0.0268\n",
            "Epoch [16822/20000], Training Loss: 0.0264\n",
            "Epoch [16823/20000], Training Loss: 0.0259\n",
            "Epoch [16824/20000], Training Loss: 0.0247\n",
            "Epoch [16825/20000], Training Loss: 0.0278\n",
            "Epoch [16826/20000], Training Loss: 0.0272\n",
            "Epoch [16827/20000], Training Loss: 0.0273\n",
            "Epoch [16828/20000], Training Loss: 0.0263\n",
            "Epoch [16829/20000], Training Loss: 0.0256\n",
            "Epoch [16830/20000], Training Loss: 0.0275\n",
            "Epoch [16831/20000], Training Loss: 0.0271\n",
            "Epoch [16832/20000], Training Loss: 0.0248\n",
            "Epoch [16833/20000], Training Loss: 0.0242\n",
            "Epoch [16834/20000], Training Loss: 0.0287\n",
            "Epoch [16835/20000], Training Loss: 0.0271\n",
            "Epoch [16836/20000], Training Loss: 0.0265\n",
            "Epoch [16837/20000], Training Loss: 0.0265\n",
            "Epoch [16838/20000], Training Loss: 0.0264\n",
            "Epoch [16839/20000], Training Loss: 0.0275\n",
            "Epoch [16840/20000], Training Loss: 0.0257\n",
            "Epoch [16841/20000], Training Loss: 0.0271\n",
            "Epoch [16842/20000], Training Loss: 0.0254\n",
            "Epoch [16843/20000], Training Loss: 0.0249\n",
            "Epoch [16844/20000], Training Loss: 0.0266\n",
            "Epoch [16845/20000], Training Loss: 0.0255\n",
            "Epoch [16846/20000], Training Loss: 0.0277\n",
            "Epoch [16847/20000], Training Loss: 0.0264\n",
            "Epoch [16848/20000], Training Loss: 0.0271\n",
            "Epoch [16849/20000], Training Loss: 0.0275\n",
            "Epoch [16850/20000], Training Loss: 0.0257\n",
            "Epoch [16851/20000], Training Loss: 0.0262\n",
            "Epoch [16852/20000], Training Loss: 0.0273\n",
            "Epoch [16853/20000], Training Loss: 0.0241\n",
            "Epoch [16854/20000], Training Loss: 0.0258\n",
            "Epoch [16855/20000], Training Loss: 0.0266\n",
            "Epoch [16856/20000], Training Loss: 0.0266\n",
            "Epoch [16857/20000], Training Loss: 0.0244\n",
            "Epoch [16858/20000], Training Loss: 0.0285\n",
            "Epoch [16859/20000], Training Loss: 0.0262\n",
            "Epoch [16860/20000], Training Loss: 0.0267\n",
            "Epoch [16861/20000], Training Loss: 0.0275\n",
            "Epoch [16862/20000], Training Loss: 0.0282\n",
            "Epoch [16863/20000], Training Loss: 0.0251\n",
            "Epoch [16864/20000], Training Loss: 0.0273\n",
            "Epoch [16865/20000], Training Loss: 0.0276\n",
            "Epoch [16866/20000], Training Loss: 0.0275\n",
            "Epoch [16867/20000], Training Loss: 0.0254\n",
            "Epoch [16868/20000], Training Loss: 0.0273\n",
            "Epoch [16869/20000], Training Loss: 0.0245\n",
            "Epoch [16870/20000], Training Loss: 0.0258\n",
            "Epoch [16871/20000], Training Loss: 0.0246\n",
            "Epoch [16872/20000], Training Loss: 0.0243\n",
            "Epoch [16873/20000], Training Loss: 0.0279\n",
            "Epoch [16874/20000], Training Loss: 0.0255\n",
            "Epoch [16875/20000], Training Loss: 0.0257\n",
            "Epoch [16876/20000], Training Loss: 0.0270\n",
            "Epoch [16877/20000], Training Loss: 0.0270\n",
            "Epoch [16878/20000], Training Loss: 0.0267\n",
            "Epoch [16879/20000], Training Loss: 0.0260\n",
            "Epoch [16880/20000], Training Loss: 0.0263\n",
            "Epoch [16881/20000], Training Loss: 0.0276\n",
            "Epoch [16882/20000], Training Loss: 0.0282\n",
            "Epoch [16883/20000], Training Loss: 0.0287\n",
            "Epoch [16884/20000], Training Loss: 0.0270\n",
            "Epoch [16885/20000], Training Loss: 0.0268\n",
            "Epoch [16886/20000], Training Loss: 0.0269\n",
            "Epoch [16887/20000], Training Loss: 0.0280\n",
            "Epoch [16888/20000], Training Loss: 0.0262\n",
            "Epoch [16889/20000], Training Loss: 0.0271\n",
            "Epoch [16890/20000], Training Loss: 0.0256\n",
            "Epoch [16891/20000], Training Loss: 0.0263\n",
            "Epoch [16892/20000], Training Loss: 0.0248\n",
            "Epoch [16893/20000], Training Loss: 0.0257\n",
            "Epoch [16894/20000], Training Loss: 0.0283\n",
            "Epoch [16895/20000], Training Loss: 0.0269\n",
            "Epoch [16896/20000], Training Loss: 0.0256\n",
            "Epoch [16897/20000], Training Loss: 0.0253\n",
            "Epoch [16898/20000], Training Loss: 0.0269\n",
            "Epoch [16899/20000], Training Loss: 0.0247\n",
            "Epoch [16900/20000], Training Loss: 0.0261\n",
            "Epoch [16901/20000], Training Loss: 0.0269\n",
            "Epoch [16902/20000], Training Loss: 0.0280\n",
            "Epoch [16903/20000], Training Loss: 0.0258\n",
            "Epoch [16904/20000], Training Loss: 0.0233\n",
            "Epoch [16905/20000], Training Loss: 0.0286\n",
            "Epoch [16906/20000], Training Loss: 0.0276\n",
            "Epoch [16907/20000], Training Loss: 0.0267\n",
            "Epoch [16908/20000], Training Loss: 0.0252\n",
            "Epoch [16909/20000], Training Loss: 0.0284\n",
            "Epoch [16910/20000], Training Loss: 0.0267\n",
            "Epoch [16911/20000], Training Loss: 0.0250\n",
            "Epoch [16912/20000], Training Loss: 0.0275\n",
            "Epoch [16913/20000], Training Loss: 0.0250\n",
            "Epoch [16914/20000], Training Loss: 0.0259\n",
            "Epoch [16915/20000], Training Loss: 0.0267\n",
            "Epoch [16916/20000], Training Loss: 0.0259\n",
            "Epoch [16917/20000], Training Loss: 0.0283\n",
            "Epoch [16918/20000], Training Loss: 0.0276\n",
            "Epoch [16919/20000], Training Loss: 0.0270\n",
            "Epoch [16920/20000], Training Loss: 0.0248\n",
            "Epoch [16921/20000], Training Loss: 0.0275\n",
            "Epoch [16922/20000], Training Loss: 0.0271\n",
            "Epoch [16923/20000], Training Loss: 0.0264\n",
            "Epoch [16924/20000], Training Loss: 0.0265\n",
            "Epoch [16925/20000], Training Loss: 0.0273\n",
            "Epoch [16926/20000], Training Loss: 0.0250\n",
            "Epoch [16927/20000], Training Loss: 0.0258\n",
            "Epoch [16928/20000], Training Loss: 0.0249\n",
            "Epoch [16929/20000], Training Loss: 0.0269\n",
            "Epoch [16930/20000], Training Loss: 0.0247\n",
            "Epoch [16931/20000], Training Loss: 0.0251\n",
            "Epoch [16932/20000], Training Loss: 0.0281\n",
            "Epoch [16933/20000], Training Loss: 0.0247\n",
            "Epoch [16934/20000], Training Loss: 0.0246\n",
            "Epoch [16935/20000], Training Loss: 0.0252\n",
            "Epoch [16936/20000], Training Loss: 0.0248\n",
            "Epoch [16937/20000], Training Loss: 0.0249\n",
            "Epoch [16938/20000], Training Loss: 0.0269\n",
            "Epoch [16939/20000], Training Loss: 0.0267\n",
            "Epoch [16940/20000], Training Loss: 0.0243\n",
            "Epoch [16941/20000], Training Loss: 0.0267\n",
            "Epoch [16942/20000], Training Loss: 0.0266\n",
            "Epoch [16943/20000], Training Loss: 0.0275\n",
            "Epoch [16944/20000], Training Loss: 0.0263\n",
            "Epoch [16945/20000], Training Loss: 0.0280\n",
            "Epoch [16946/20000], Training Loss: 0.0262\n",
            "Epoch [16947/20000], Training Loss: 0.0258\n",
            "Epoch [16948/20000], Training Loss: 0.0270\n",
            "Epoch [16949/20000], Training Loss: 0.0245\n",
            "Epoch [16950/20000], Training Loss: 0.0254\n",
            "Epoch [16951/20000], Training Loss: 0.0257\n",
            "Epoch [16952/20000], Training Loss: 0.0248\n",
            "Epoch [16953/20000], Training Loss: 0.0283\n",
            "Epoch [16954/20000], Training Loss: 0.0270\n",
            "Epoch [16955/20000], Training Loss: 0.0254\n",
            "Epoch [16956/20000], Training Loss: 0.0255\n",
            "Epoch [16957/20000], Training Loss: 0.0261\n",
            "Epoch [16958/20000], Training Loss: 0.0247\n",
            "Epoch [16959/20000], Training Loss: 0.0249\n",
            "Epoch [16960/20000], Training Loss: 0.0270\n",
            "Epoch [16961/20000], Training Loss: 0.0287\n",
            "Epoch [16962/20000], Training Loss: 0.0273\n",
            "Epoch [16963/20000], Training Loss: 0.0277\n",
            "Epoch [16964/20000], Training Loss: 0.0266\n",
            "Epoch [16965/20000], Training Loss: 0.0269\n",
            "Epoch [16966/20000], Training Loss: 0.0269\n",
            "Epoch [16967/20000], Training Loss: 0.0267\n",
            "Epoch [16968/20000], Training Loss: 0.0270\n",
            "Epoch [16969/20000], Training Loss: 0.0252\n",
            "Epoch [16970/20000], Training Loss: 0.0264\n",
            "Epoch [16971/20000], Training Loss: 0.0276\n",
            "Epoch [16972/20000], Training Loss: 0.0271\n",
            "Epoch [16973/20000], Training Loss: 0.0251\n",
            "Epoch [16974/20000], Training Loss: 0.0273\n",
            "Epoch [16975/20000], Training Loss: 0.0269\n",
            "Epoch [16976/20000], Training Loss: 0.0263\n",
            "Epoch [16977/20000], Training Loss: 0.0252\n",
            "Epoch [16978/20000], Training Loss: 0.0245\n",
            "Epoch [16979/20000], Training Loss: 0.0256\n",
            "Epoch [16980/20000], Training Loss: 0.0280\n",
            "Epoch [16981/20000], Training Loss: 0.0274\n",
            "Epoch [16982/20000], Training Loss: 0.0265\n",
            "Epoch [16983/20000], Training Loss: 0.0268\n",
            "Epoch [16984/20000], Training Loss: 0.0264\n",
            "Epoch [16985/20000], Training Loss: 0.0263\n",
            "Epoch [16986/20000], Training Loss: 0.0262\n",
            "Epoch [16987/20000], Training Loss: 0.0259\n",
            "Epoch [16988/20000], Training Loss: 0.0269\n",
            "Epoch [16989/20000], Training Loss: 0.0253\n",
            "Epoch [16990/20000], Training Loss: 0.0257\n",
            "Epoch [16991/20000], Training Loss: 0.0267\n",
            "Epoch [16992/20000], Training Loss: 0.0286\n",
            "Epoch [16993/20000], Training Loss: 0.0263\n",
            "Epoch [16994/20000], Training Loss: 0.0266\n",
            "Epoch [16995/20000], Training Loss: 0.0244\n",
            "Epoch [16996/20000], Training Loss: 0.0248\n",
            "Epoch [16997/20000], Training Loss: 0.0265\n",
            "Epoch [16998/20000], Training Loss: 0.0248\n",
            "Epoch [16999/20000], Training Loss: 0.0273\n",
            "Epoch [17000/20000], Training Loss: 0.0259\n",
            "Epoch [17001/20000], Training Loss: 0.0268\n",
            "Epoch [17002/20000], Training Loss: 0.0250\n",
            "Epoch [17003/20000], Training Loss: 0.0276\n",
            "Epoch [17004/20000], Training Loss: 0.0271\n",
            "Epoch [17005/20000], Training Loss: 0.0262\n",
            "Epoch [17006/20000], Training Loss: 0.0280\n",
            "Epoch [17007/20000], Training Loss: 0.0259\n",
            "Epoch [17008/20000], Training Loss: 0.0259\n",
            "Epoch [17009/20000], Training Loss: 0.0253\n",
            "Epoch [17010/20000], Training Loss: 0.0267\n",
            "Epoch [17011/20000], Training Loss: 0.0248\n",
            "Epoch [17012/20000], Training Loss: 0.0256\n",
            "Epoch [17013/20000], Training Loss: 0.0255\n",
            "Epoch [17014/20000], Training Loss: 0.0260\n",
            "Epoch [17015/20000], Training Loss: 0.0274\n",
            "Epoch [17016/20000], Training Loss: 0.0268\n",
            "Epoch [17017/20000], Training Loss: 0.0263\n",
            "Epoch [17018/20000], Training Loss: 0.0267\n",
            "Epoch [17019/20000], Training Loss: 0.0274\n",
            "Epoch [17020/20000], Training Loss: 0.0266\n",
            "Epoch [17021/20000], Training Loss: 0.0274\n",
            "Epoch [17022/20000], Training Loss: 0.0253\n",
            "Epoch [17023/20000], Training Loss: 0.0247\n",
            "Epoch [17024/20000], Training Loss: 0.0257\n",
            "Epoch [17025/20000], Training Loss: 0.0273\n",
            "Epoch [17026/20000], Training Loss: 0.0265\n",
            "Epoch [17027/20000], Training Loss: 0.0292\n",
            "Epoch [17028/20000], Training Loss: 0.0257\n",
            "Epoch [17029/20000], Training Loss: 0.0277\n",
            "Epoch [17030/20000], Training Loss: 0.0268\n",
            "Epoch [17031/20000], Training Loss: 0.0261\n",
            "Epoch [17032/20000], Training Loss: 0.0250\n",
            "Epoch [17033/20000], Training Loss: 0.0272\n",
            "Epoch [17034/20000], Training Loss: 0.0273\n",
            "Epoch [17035/20000], Training Loss: 0.0254\n",
            "Epoch [17036/20000], Training Loss: 0.0266\n",
            "Epoch [17037/20000], Training Loss: 0.0259\n",
            "Epoch [17038/20000], Training Loss: 0.0263\n",
            "Epoch [17039/20000], Training Loss: 0.0252\n",
            "Epoch [17040/20000], Training Loss: 0.0258\n",
            "Epoch [17041/20000], Training Loss: 0.0276\n",
            "Epoch [17042/20000], Training Loss: 0.0269\n",
            "Epoch [17043/20000], Training Loss: 0.0265\n",
            "Epoch [17044/20000], Training Loss: 0.0279\n",
            "Epoch [17045/20000], Training Loss: 0.0265\n",
            "Epoch [17046/20000], Training Loss: 0.0266\n",
            "Epoch [17047/20000], Training Loss: 0.0255\n",
            "Epoch [17048/20000], Training Loss: 0.0251\n",
            "Epoch [17049/20000], Training Loss: 0.0266\n",
            "Epoch [17050/20000], Training Loss: 0.0286\n",
            "Epoch [17051/20000], Training Loss: 0.0252\n",
            "Epoch [17052/20000], Training Loss: 0.0261\n",
            "Epoch [17053/20000], Training Loss: 0.0253\n",
            "Epoch [17054/20000], Training Loss: 0.0273\n",
            "Epoch [17055/20000], Training Loss: 0.0287\n",
            "Epoch [17056/20000], Training Loss: 0.0277\n",
            "Epoch [17057/20000], Training Loss: 0.0275\n",
            "Epoch [17058/20000], Training Loss: 0.0267\n",
            "Epoch [17059/20000], Training Loss: 0.0253\n",
            "Epoch [17060/20000], Training Loss: 0.0258\n",
            "Epoch [17061/20000], Training Loss: 0.0249\n",
            "Epoch [17062/20000], Training Loss: 0.0266\n",
            "Epoch [17063/20000], Training Loss: 0.0277\n",
            "Epoch [17064/20000], Training Loss: 0.0288\n",
            "Epoch [17065/20000], Training Loss: 0.0260\n",
            "Epoch [17066/20000], Training Loss: 0.0258\n",
            "Epoch [17067/20000], Training Loss: 0.0257\n",
            "Epoch [17068/20000], Training Loss: 0.0274\n",
            "Epoch [17069/20000], Training Loss: 0.0263\n",
            "Epoch [17070/20000], Training Loss: 0.0271\n",
            "Epoch [17071/20000], Training Loss: 0.0262\n",
            "Epoch [17072/20000], Training Loss: 0.0267\n",
            "Epoch [17073/20000], Training Loss: 0.0262\n",
            "Epoch [17074/20000], Training Loss: 0.0269\n",
            "Epoch [17075/20000], Training Loss: 0.0254\n",
            "Epoch [17076/20000], Training Loss: 0.0260\n",
            "Epoch [17077/20000], Training Loss: 0.0252\n",
            "Epoch [17078/20000], Training Loss: 0.0260\n",
            "Epoch [17079/20000], Training Loss: 0.0267\n",
            "Epoch [17080/20000], Training Loss: 0.0275\n",
            "Epoch [17081/20000], Training Loss: 0.0254\n",
            "Epoch [17082/20000], Training Loss: 0.0271\n",
            "Epoch [17083/20000], Training Loss: 0.0270\n",
            "Epoch [17084/20000], Training Loss: 0.0260\n",
            "Epoch [17085/20000], Training Loss: 0.0286\n",
            "Epoch [17086/20000], Training Loss: 0.0256\n",
            "Epoch [17087/20000], Training Loss: 0.0274\n",
            "Epoch [17088/20000], Training Loss: 0.0280\n",
            "Epoch [17089/20000], Training Loss: 0.0251\n",
            "Epoch [17090/20000], Training Loss: 0.0267\n",
            "Epoch [17091/20000], Training Loss: 0.0277\n",
            "Epoch [17092/20000], Training Loss: 0.0260\n",
            "Epoch [17093/20000], Training Loss: 0.0245\n",
            "Epoch [17094/20000], Training Loss: 0.0259\n",
            "Epoch [17095/20000], Training Loss: 0.0268\n",
            "Epoch [17096/20000], Training Loss: 0.0261\n",
            "Epoch [17097/20000], Training Loss: 0.0274\n",
            "Epoch [17098/20000], Training Loss: 0.0251\n",
            "Epoch [17099/20000], Training Loss: 0.0270\n",
            "Epoch [17100/20000], Training Loss: 0.0282\n",
            "Epoch [17101/20000], Training Loss: 0.0257\n",
            "Epoch [17102/20000], Training Loss: 0.0258\n",
            "Epoch [17103/20000], Training Loss: 0.0288\n",
            "Epoch [17104/20000], Training Loss: 0.0286\n",
            "Epoch [17105/20000], Training Loss: 0.0252\n",
            "Epoch [17106/20000], Training Loss: 0.0282\n",
            "Epoch [17107/20000], Training Loss: 0.0245\n",
            "Epoch [17108/20000], Training Loss: 0.0255\n",
            "Epoch [17109/20000], Training Loss: 0.0265\n",
            "Epoch [17110/20000], Training Loss: 0.0242\n",
            "Epoch [17111/20000], Training Loss: 0.0270\n",
            "Epoch [17112/20000], Training Loss: 0.0272\n",
            "Epoch [17113/20000], Training Loss: 0.0249\n",
            "Epoch [17114/20000], Training Loss: 0.0272\n",
            "Epoch [17115/20000], Training Loss: 0.0286\n",
            "Epoch [17116/20000], Training Loss: 0.0259\n",
            "Epoch [17117/20000], Training Loss: 0.0280\n",
            "Epoch [17118/20000], Training Loss: 0.0249\n",
            "Epoch [17119/20000], Training Loss: 0.0269\n",
            "Epoch [17120/20000], Training Loss: 0.0255\n",
            "Epoch [17121/20000], Training Loss: 0.0273\n",
            "Epoch [17122/20000], Training Loss: 0.0270\n",
            "Epoch [17123/20000], Training Loss: 0.0266\n",
            "Epoch [17124/20000], Training Loss: 0.0249\n",
            "Epoch [17125/20000], Training Loss: 0.0240\n",
            "Epoch [17126/20000], Training Loss: 0.0276\n",
            "Epoch [17127/20000], Training Loss: 0.0275\n",
            "Epoch [17128/20000], Training Loss: 0.0267\n",
            "Epoch [17129/20000], Training Loss: 0.0269\n",
            "Epoch [17130/20000], Training Loss: 0.0236\n",
            "Epoch [17131/20000], Training Loss: 0.0244\n",
            "Epoch [17132/20000], Training Loss: 0.0269\n",
            "Epoch [17133/20000], Training Loss: 0.0248\n",
            "Epoch [17134/20000], Training Loss: 0.0278\n",
            "Epoch [17135/20000], Training Loss: 0.0249\n",
            "Epoch [17136/20000], Training Loss: 0.0240\n",
            "Epoch [17137/20000], Training Loss: 0.0276\n",
            "Epoch [17138/20000], Training Loss: 0.0260\n",
            "Epoch [17139/20000], Training Loss: 0.0268\n",
            "Epoch [17140/20000], Training Loss: 0.0258\n",
            "Epoch [17141/20000], Training Loss: 0.0258\n",
            "Epoch [17142/20000], Training Loss: 0.0272\n",
            "Epoch [17143/20000], Training Loss: 0.0254\n",
            "Epoch [17144/20000], Training Loss: 0.0262\n",
            "Epoch [17145/20000], Training Loss: 0.0256\n",
            "Epoch [17146/20000], Training Loss: 0.0261\n",
            "Epoch [17147/20000], Training Loss: 0.0247\n",
            "Epoch [17148/20000], Training Loss: 0.0255\n",
            "Epoch [17149/20000], Training Loss: 0.0249\n",
            "Epoch [17150/20000], Training Loss: 0.0262\n",
            "Epoch [17151/20000], Training Loss: 0.0263\n",
            "Epoch [17152/20000], Training Loss: 0.0252\n",
            "Epoch [17153/20000], Training Loss: 0.0240\n",
            "Epoch [17154/20000], Training Loss: 0.0296\n",
            "Epoch [17155/20000], Training Loss: 0.0260\n",
            "Epoch [17156/20000], Training Loss: 0.0266\n",
            "Epoch [17157/20000], Training Loss: 0.0246\n",
            "Epoch [17158/20000], Training Loss: 0.0242\n",
            "Epoch [17159/20000], Training Loss: 0.0243\n",
            "Epoch [17160/20000], Training Loss: 0.0250\n",
            "Epoch [17161/20000], Training Loss: 0.0256\n",
            "Epoch [17162/20000], Training Loss: 0.0258\n",
            "Epoch [17163/20000], Training Loss: 0.0272\n",
            "Epoch [17164/20000], Training Loss: 0.0256\n",
            "Epoch [17165/20000], Training Loss: 0.0253\n",
            "Epoch [17166/20000], Training Loss: 0.0270\n",
            "Epoch [17167/20000], Training Loss: 0.0276\n",
            "Epoch [17168/20000], Training Loss: 0.0252\n",
            "Epoch [17169/20000], Training Loss: 0.0260\n",
            "Epoch [17170/20000], Training Loss: 0.0270\n",
            "Epoch [17171/20000], Training Loss: 0.0271\n",
            "Epoch [17172/20000], Training Loss: 0.0257\n",
            "Epoch [17173/20000], Training Loss: 0.0263\n",
            "Epoch [17174/20000], Training Loss: 0.0238\n",
            "Epoch [17175/20000], Training Loss: 0.0292\n",
            "Epoch [17176/20000], Training Loss: 0.0274\n",
            "Epoch [17177/20000], Training Loss: 0.0261\n",
            "Epoch [17178/20000], Training Loss: 0.0268\n",
            "Epoch [17179/20000], Training Loss: 0.0276\n",
            "Epoch [17180/20000], Training Loss: 0.0272\n",
            "Epoch [17181/20000], Training Loss: 0.0256\n",
            "Epoch [17182/20000], Training Loss: 0.0301\n",
            "Epoch [17183/20000], Training Loss: 0.0275\n",
            "Epoch [17184/20000], Training Loss: 0.0247\n",
            "Epoch [17185/20000], Training Loss: 0.0261\n",
            "Epoch [17186/20000], Training Loss: 0.0259\n",
            "Epoch [17187/20000], Training Loss: 0.0262\n",
            "Epoch [17188/20000], Training Loss: 0.0268\n",
            "Epoch [17189/20000], Training Loss: 0.0273\n",
            "Epoch [17190/20000], Training Loss: 0.0258\n",
            "Epoch [17191/20000], Training Loss: 0.0253\n",
            "Epoch [17192/20000], Training Loss: 0.0280\n",
            "Epoch [17193/20000], Training Loss: 0.0273\n",
            "Epoch [17194/20000], Training Loss: 0.0252\n",
            "Epoch [17195/20000], Training Loss: 0.0245\n",
            "Epoch [17196/20000], Training Loss: 0.0282\n",
            "Epoch [17197/20000], Training Loss: 0.0254\n",
            "Epoch [17198/20000], Training Loss: 0.0265\n",
            "Epoch [17199/20000], Training Loss: 0.0277\n",
            "Epoch [17200/20000], Training Loss: 0.0253\n",
            "Epoch [17201/20000], Training Loss: 0.0252\n",
            "Epoch [17202/20000], Training Loss: 0.0265\n",
            "Epoch [17203/20000], Training Loss: 0.0251\n",
            "Epoch [17204/20000], Training Loss: 0.0278\n",
            "Epoch [17205/20000], Training Loss: 0.0267\n",
            "Epoch [17206/20000], Training Loss: 0.0261\n",
            "Epoch [17207/20000], Training Loss: 0.0261\n",
            "Epoch [17208/20000], Training Loss: 0.0272\n",
            "Epoch [17209/20000], Training Loss: 0.0277\n",
            "Epoch [17210/20000], Training Loss: 0.0274\n",
            "Epoch [17211/20000], Training Loss: 0.0274\n",
            "Epoch [17212/20000], Training Loss: 0.0285\n",
            "Epoch [17213/20000], Training Loss: 0.0247\n",
            "Epoch [17214/20000], Training Loss: 0.0260\n",
            "Epoch [17215/20000], Training Loss: 0.0277\n",
            "Epoch [17216/20000], Training Loss: 0.0252\n",
            "Epoch [17217/20000], Training Loss: 0.0256\n",
            "Epoch [17218/20000], Training Loss: 0.0265\n",
            "Epoch [17219/20000], Training Loss: 0.0254\n",
            "Epoch [17220/20000], Training Loss: 0.0281\n",
            "Epoch [17221/20000], Training Loss: 0.0291\n",
            "Epoch [17222/20000], Training Loss: 0.0268\n",
            "Epoch [17223/20000], Training Loss: 0.0266\n",
            "Epoch [17224/20000], Training Loss: 0.0280\n",
            "Epoch [17225/20000], Training Loss: 0.0268\n",
            "Epoch [17226/20000], Training Loss: 0.0263\n",
            "Epoch [17227/20000], Training Loss: 0.0285\n",
            "Epoch [17228/20000], Training Loss: 0.0267\n",
            "Epoch [17229/20000], Training Loss: 0.0253\n",
            "Epoch [17230/20000], Training Loss: 0.0258\n",
            "Epoch [17231/20000], Training Loss: 0.0258\n",
            "Epoch [17232/20000], Training Loss: 0.0252\n",
            "Epoch [17233/20000], Training Loss: 0.0275\n",
            "Epoch [17234/20000], Training Loss: 0.0264\n",
            "Epoch [17235/20000], Training Loss: 0.0280\n",
            "Epoch [17236/20000], Training Loss: 0.0263\n",
            "Epoch [17237/20000], Training Loss: 0.0250\n",
            "Epoch [17238/20000], Training Loss: 0.0274\n",
            "Epoch [17239/20000], Training Loss: 0.0254\n",
            "Epoch [17240/20000], Training Loss: 0.0258\n",
            "Epoch [17241/20000], Training Loss: 0.0268\n",
            "Epoch [17242/20000], Training Loss: 0.0260\n",
            "Epoch [17243/20000], Training Loss: 0.0262\n",
            "Epoch [17244/20000], Training Loss: 0.0263\n",
            "Epoch [17245/20000], Training Loss: 0.0274\n",
            "Epoch [17246/20000], Training Loss: 0.0262\n",
            "Epoch [17247/20000], Training Loss: 0.0275\n",
            "Epoch [17248/20000], Training Loss: 0.0281\n",
            "Epoch [17249/20000], Training Loss: 0.0269\n",
            "Epoch [17250/20000], Training Loss: 0.0251\n",
            "Epoch [17251/20000], Training Loss: 0.0290\n",
            "Epoch [17252/20000], Training Loss: 0.0276\n",
            "Epoch [17253/20000], Training Loss: 0.0257\n",
            "Epoch [17254/20000], Training Loss: 0.0267\n",
            "Epoch [17255/20000], Training Loss: 0.0268\n",
            "Epoch [17256/20000], Training Loss: 0.0290\n",
            "Epoch [17257/20000], Training Loss: 0.0270\n",
            "Epoch [17258/20000], Training Loss: 0.0250\n",
            "Epoch [17259/20000], Training Loss: 0.0255\n",
            "Epoch [17260/20000], Training Loss: 0.0273\n",
            "Epoch [17261/20000], Training Loss: 0.0255\n",
            "Epoch [17262/20000], Training Loss: 0.0258\n",
            "Epoch [17263/20000], Training Loss: 0.0247\n",
            "Epoch [17264/20000], Training Loss: 0.0254\n",
            "Epoch [17265/20000], Training Loss: 0.0244\n",
            "Epoch [17266/20000], Training Loss: 0.0260\n",
            "Epoch [17267/20000], Training Loss: 0.0286\n",
            "Epoch [17268/20000], Training Loss: 0.0274\n",
            "Epoch [17269/20000], Training Loss: 0.0263\n",
            "Epoch [17270/20000], Training Loss: 0.0256\n",
            "Epoch [17271/20000], Training Loss: 0.0256\n",
            "Epoch [17272/20000], Training Loss: 0.0285\n",
            "Epoch [17273/20000], Training Loss: 0.0256\n",
            "Epoch [17274/20000], Training Loss: 0.0255\n",
            "Epoch [17275/20000], Training Loss: 0.0269\n",
            "Epoch [17276/20000], Training Loss: 0.0277\n",
            "Epoch [17277/20000], Training Loss: 0.0246\n",
            "Epoch [17278/20000], Training Loss: 0.0255\n",
            "Epoch [17279/20000], Training Loss: 0.0267\n",
            "Epoch [17280/20000], Training Loss: 0.0285\n",
            "Epoch [17281/20000], Training Loss: 0.0250\n",
            "Epoch [17282/20000], Training Loss: 0.0282\n",
            "Epoch [17283/20000], Training Loss: 0.0253\n",
            "Epoch [17284/20000], Training Loss: 0.0276\n",
            "Epoch [17285/20000], Training Loss: 0.0263\n",
            "Epoch [17286/20000], Training Loss: 0.0264\n",
            "Epoch [17287/20000], Training Loss: 0.0244\n",
            "Epoch [17288/20000], Training Loss: 0.0284\n",
            "Epoch [17289/20000], Training Loss: 0.0272\n",
            "Epoch [17290/20000], Training Loss: 0.0267\n",
            "Epoch [17291/20000], Training Loss: 0.0250\n",
            "Epoch [17292/20000], Training Loss: 0.0279\n",
            "Epoch [17293/20000], Training Loss: 0.0271\n",
            "Epoch [17294/20000], Training Loss: 0.0255\n",
            "Epoch [17295/20000], Training Loss: 0.0290\n",
            "Epoch [17296/20000], Training Loss: 0.0268\n",
            "Epoch [17297/20000], Training Loss: 0.0248\n",
            "Epoch [17298/20000], Training Loss: 0.0268\n",
            "Epoch [17299/20000], Training Loss: 0.0248\n",
            "Epoch [17300/20000], Training Loss: 0.0253\n",
            "Epoch [17301/20000], Training Loss: 0.0269\n",
            "Epoch [17302/20000], Training Loss: 0.0253\n",
            "Epoch [17303/20000], Training Loss: 0.0264\n",
            "Epoch [17304/20000], Training Loss: 0.0259\n",
            "Epoch [17305/20000], Training Loss: 0.0255\n",
            "Epoch [17306/20000], Training Loss: 0.0244\n",
            "Epoch [17307/20000], Training Loss: 0.0261\n",
            "Epoch [17308/20000], Training Loss: 0.0274\n",
            "Epoch [17309/20000], Training Loss: 0.0270\n",
            "Epoch [17310/20000], Training Loss: 0.0262\n",
            "Epoch [17311/20000], Training Loss: 0.0262\n",
            "Epoch [17312/20000], Training Loss: 0.0245\n",
            "Epoch [17313/20000], Training Loss: 0.0245\n",
            "Epoch [17314/20000], Training Loss: 0.0278\n",
            "Epoch [17315/20000], Training Loss: 0.0281\n",
            "Epoch [17316/20000], Training Loss: 0.0258\n",
            "Epoch [17317/20000], Training Loss: 0.0269\n",
            "Epoch [17318/20000], Training Loss: 0.0255\n",
            "Epoch [17319/20000], Training Loss: 0.0275\n",
            "Epoch [17320/20000], Training Loss: 0.0250\n",
            "Epoch [17321/20000], Training Loss: 0.0277\n",
            "Epoch [17322/20000], Training Loss: 0.0258\n",
            "Epoch [17323/20000], Training Loss: 0.0262\n",
            "Epoch [17324/20000], Training Loss: 0.0255\n",
            "Epoch [17325/20000], Training Loss: 0.0249\n",
            "Epoch [17326/20000], Training Loss: 0.0265\n",
            "Epoch [17327/20000], Training Loss: 0.0264\n",
            "Epoch [17328/20000], Training Loss: 0.0247\n",
            "Epoch [17329/20000], Training Loss: 0.0267\n",
            "Epoch [17330/20000], Training Loss: 0.0265\n",
            "Epoch [17331/20000], Training Loss: 0.0276\n",
            "Epoch [17332/20000], Training Loss: 0.0259\n",
            "Epoch [17333/20000], Training Loss: 0.0261\n",
            "Epoch [17334/20000], Training Loss: 0.0286\n",
            "Epoch [17335/20000], Training Loss: 0.0275\n",
            "Epoch [17336/20000], Training Loss: 0.0264\n",
            "Epoch [17337/20000], Training Loss: 0.0264\n",
            "Epoch [17338/20000], Training Loss: 0.0243\n",
            "Epoch [17339/20000], Training Loss: 0.0246\n",
            "Epoch [17340/20000], Training Loss: 0.0244\n",
            "Epoch [17341/20000], Training Loss: 0.0276\n",
            "Epoch [17342/20000], Training Loss: 0.0260\n",
            "Epoch [17343/20000], Training Loss: 0.0265\n",
            "Epoch [17344/20000], Training Loss: 0.0261\n",
            "Epoch [17345/20000], Training Loss: 0.0258\n",
            "Epoch [17346/20000], Training Loss: 0.0261\n",
            "Epoch [17347/20000], Training Loss: 0.0253\n",
            "Epoch [17348/20000], Training Loss: 0.0267\n",
            "Epoch [17349/20000], Training Loss: 0.0278\n",
            "Epoch [17350/20000], Training Loss: 0.0279\n",
            "Epoch [17351/20000], Training Loss: 0.0251\n",
            "Epoch [17352/20000], Training Loss: 0.0254\n",
            "Epoch [17353/20000], Training Loss: 0.0261\n",
            "Epoch [17354/20000], Training Loss: 0.0272\n",
            "Epoch [17355/20000], Training Loss: 0.0271\n",
            "Epoch [17356/20000], Training Loss: 0.0258\n",
            "Epoch [17357/20000], Training Loss: 0.0252\n",
            "Epoch [17358/20000], Training Loss: 0.0255\n",
            "Epoch [17359/20000], Training Loss: 0.0272\n",
            "Epoch [17360/20000], Training Loss: 0.0275\n",
            "Epoch [17361/20000], Training Loss: 0.0257\n",
            "Epoch [17362/20000], Training Loss: 0.0248\n",
            "Epoch [17363/20000], Training Loss: 0.0266\n",
            "Epoch [17364/20000], Training Loss: 0.0260\n",
            "Epoch [17365/20000], Training Loss: 0.0239\n",
            "Epoch [17366/20000], Training Loss: 0.0246\n",
            "Epoch [17367/20000], Training Loss: 0.0257\n",
            "Epoch [17368/20000], Training Loss: 0.0263\n",
            "Epoch [17369/20000], Training Loss: 0.0269\n",
            "Epoch [17370/20000], Training Loss: 0.0269\n",
            "Epoch [17371/20000], Training Loss: 0.0271\n",
            "Epoch [17372/20000], Training Loss: 0.0265\n",
            "Epoch [17373/20000], Training Loss: 0.0264\n",
            "Epoch [17374/20000], Training Loss: 0.0266\n",
            "Epoch [17375/20000], Training Loss: 0.0289\n",
            "Epoch [17376/20000], Training Loss: 0.0269\n",
            "Epoch [17377/20000], Training Loss: 0.0252\n",
            "Epoch [17378/20000], Training Loss: 0.0260\n",
            "Epoch [17379/20000], Training Loss: 0.0285\n",
            "Epoch [17380/20000], Training Loss: 0.0262\n",
            "Epoch [17381/20000], Training Loss: 0.0282\n",
            "Epoch [17382/20000], Training Loss: 0.0271\n",
            "Epoch [17383/20000], Training Loss: 0.0283\n",
            "Epoch [17384/20000], Training Loss: 0.0257\n",
            "Epoch [17385/20000], Training Loss: 0.0283\n",
            "Epoch [17386/20000], Training Loss: 0.0260\n",
            "Epoch [17387/20000], Training Loss: 0.0262\n",
            "Epoch [17388/20000], Training Loss: 0.0276\n",
            "Epoch [17389/20000], Training Loss: 0.0277\n",
            "Epoch [17390/20000], Training Loss: 0.0265\n",
            "Epoch [17391/20000], Training Loss: 0.0252\n",
            "Epoch [17392/20000], Training Loss: 0.0265\n",
            "Epoch [17393/20000], Training Loss: 0.0264\n",
            "Epoch [17394/20000], Training Loss: 0.0291\n",
            "Epoch [17395/20000], Training Loss: 0.0281\n",
            "Epoch [17396/20000], Training Loss: 0.0255\n",
            "Epoch [17397/20000], Training Loss: 0.0243\n",
            "Epoch [17398/20000], Training Loss: 0.0264\n",
            "Epoch [17399/20000], Training Loss: 0.0252\n",
            "Epoch [17400/20000], Training Loss: 0.0248\n",
            "Epoch [17401/20000], Training Loss: 0.0280\n",
            "Epoch [17402/20000], Training Loss: 0.0259\n",
            "Epoch [17403/20000], Training Loss: 0.0258\n",
            "Epoch [17404/20000], Training Loss: 0.0264\n",
            "Epoch [17405/20000], Training Loss: 0.0254\n",
            "Epoch [17406/20000], Training Loss: 0.0278\n",
            "Epoch [17407/20000], Training Loss: 0.0260\n",
            "Epoch [17408/20000], Training Loss: 0.0274\n",
            "Epoch [17409/20000], Training Loss: 0.0285\n",
            "Epoch [17410/20000], Training Loss: 0.0272\n",
            "Epoch [17411/20000], Training Loss: 0.0265\n",
            "Epoch [17412/20000], Training Loss: 0.0257\n",
            "Epoch [17413/20000], Training Loss: 0.0252\n",
            "Epoch [17414/20000], Training Loss: 0.0252\n",
            "Epoch [17415/20000], Training Loss: 0.0285\n",
            "Epoch [17416/20000], Training Loss: 0.0238\n",
            "Epoch [17417/20000], Training Loss: 0.0275\n",
            "Epoch [17418/20000], Training Loss: 0.0259\n",
            "Epoch [17419/20000], Training Loss: 0.0249\n",
            "Epoch [17420/20000], Training Loss: 0.0263\n",
            "Epoch [17421/20000], Training Loss: 0.0268\n",
            "Epoch [17422/20000], Training Loss: 0.0260\n",
            "Epoch [17423/20000], Training Loss: 0.0250\n",
            "Epoch [17424/20000], Training Loss: 0.0256\n",
            "Epoch [17425/20000], Training Loss: 0.0273\n",
            "Epoch [17426/20000], Training Loss: 0.0263\n",
            "Epoch [17427/20000], Training Loss: 0.0238\n",
            "Epoch [17428/20000], Training Loss: 0.0252\n",
            "Epoch [17429/20000], Training Loss: 0.0259\n",
            "Epoch [17430/20000], Training Loss: 0.0262\n",
            "Epoch [17431/20000], Training Loss: 0.0244\n",
            "Epoch [17432/20000], Training Loss: 0.0264\n",
            "Epoch [17433/20000], Training Loss: 0.0269\n",
            "Epoch [17434/20000], Training Loss: 0.0264\n",
            "Epoch [17435/20000], Training Loss: 0.0243\n",
            "Epoch [17436/20000], Training Loss: 0.0270\n",
            "Epoch [17437/20000], Training Loss: 0.0253\n",
            "Epoch [17438/20000], Training Loss: 0.0263\n",
            "Epoch [17439/20000], Training Loss: 0.0273\n",
            "Epoch [17440/20000], Training Loss: 0.0263\n",
            "Epoch [17441/20000], Training Loss: 0.0275\n",
            "Epoch [17442/20000], Training Loss: 0.0238\n",
            "Epoch [17443/20000], Training Loss: 0.0242\n",
            "Epoch [17444/20000], Training Loss: 0.0253\n",
            "Epoch [17445/20000], Training Loss: 0.0264\n",
            "Epoch [17446/20000], Training Loss: 0.0254\n",
            "Epoch [17447/20000], Training Loss: 0.0276\n",
            "Epoch [17448/20000], Training Loss: 0.0275\n",
            "Epoch [17449/20000], Training Loss: 0.0274\n",
            "Epoch [17450/20000], Training Loss: 0.0246\n",
            "Epoch [17451/20000], Training Loss: 0.0274\n",
            "Epoch [17452/20000], Training Loss: 0.0270\n",
            "Epoch [17453/20000], Training Loss: 0.0274\n",
            "Epoch [17454/20000], Training Loss: 0.0242\n",
            "Epoch [17455/20000], Training Loss: 0.0269\n",
            "Epoch [17456/20000], Training Loss: 0.0275\n",
            "Epoch [17457/20000], Training Loss: 0.0278\n",
            "Epoch [17458/20000], Training Loss: 0.0277\n",
            "Epoch [17459/20000], Training Loss: 0.0267\n",
            "Epoch [17460/20000], Training Loss: 0.0276\n",
            "Epoch [17461/20000], Training Loss: 0.0249\n",
            "Epoch [17462/20000], Training Loss: 0.0284\n",
            "Epoch [17463/20000], Training Loss: 0.0273\n",
            "Epoch [17464/20000], Training Loss: 0.0264\n",
            "Epoch [17465/20000], Training Loss: 0.0245\n",
            "Epoch [17466/20000], Training Loss: 0.0265\n",
            "Epoch [17467/20000], Training Loss: 0.0266\n",
            "Epoch [17468/20000], Training Loss: 0.0263\n",
            "Epoch [17469/20000], Training Loss: 0.0288\n",
            "Epoch [17470/20000], Training Loss: 0.0263\n",
            "Epoch [17471/20000], Training Loss: 0.0273\n",
            "Epoch [17472/20000], Training Loss: 0.0263\n",
            "Epoch [17473/20000], Training Loss: 0.0278\n",
            "Epoch [17474/20000], Training Loss: 0.0259\n",
            "Epoch [17475/20000], Training Loss: 0.0251\n",
            "Epoch [17476/20000], Training Loss: 0.0270\n",
            "Epoch [17477/20000], Training Loss: 0.0261\n",
            "Epoch [17478/20000], Training Loss: 0.0259\n",
            "Epoch [17479/20000], Training Loss: 0.0279\n",
            "Epoch [17480/20000], Training Loss: 0.0272\n",
            "Epoch [17481/20000], Training Loss: 0.0277\n",
            "Epoch [17482/20000], Training Loss: 0.0265\n",
            "Epoch [17483/20000], Training Loss: 0.0253\n",
            "Epoch [17484/20000], Training Loss: 0.0254\n",
            "Epoch [17485/20000], Training Loss: 0.0272\n",
            "Epoch [17486/20000], Training Loss: 0.0259\n",
            "Epoch [17487/20000], Training Loss: 0.0259\n",
            "Epoch [17488/20000], Training Loss: 0.0257\n",
            "Epoch [17489/20000], Training Loss: 0.0278\n",
            "Epoch [17490/20000], Training Loss: 0.0249\n",
            "Epoch [17491/20000], Training Loss: 0.0263\n",
            "Epoch [17492/20000], Training Loss: 0.0261\n",
            "Epoch [17493/20000], Training Loss: 0.0262\n",
            "Epoch [17494/20000], Training Loss: 0.0255\n",
            "Epoch [17495/20000], Training Loss: 0.0273\n",
            "Epoch [17496/20000], Training Loss: 0.0261\n",
            "Epoch [17497/20000], Training Loss: 0.0264\n",
            "Epoch [17498/20000], Training Loss: 0.0265\n",
            "Epoch [17499/20000], Training Loss: 0.0271\n",
            "Epoch [17500/20000], Training Loss: 0.0251\n",
            "Epoch [17501/20000], Training Loss: 0.0263\n",
            "Epoch [17502/20000], Training Loss: 0.0280\n",
            "Epoch [17503/20000], Training Loss: 0.0293\n",
            "Epoch [17504/20000], Training Loss: 0.0254\n",
            "Epoch [17505/20000], Training Loss: 0.0259\n",
            "Epoch [17506/20000], Training Loss: 0.0273\n",
            "Epoch [17507/20000], Training Loss: 0.0267\n",
            "Epoch [17508/20000], Training Loss: 0.0258\n",
            "Epoch [17509/20000], Training Loss: 0.0253\n",
            "Epoch [17510/20000], Training Loss: 0.0277\n",
            "Epoch [17511/20000], Training Loss: 0.0270\n",
            "Epoch [17512/20000], Training Loss: 0.0274\n",
            "Epoch [17513/20000], Training Loss: 0.0259\n",
            "Epoch [17514/20000], Training Loss: 0.0246\n",
            "Epoch [17515/20000], Training Loss: 0.0279\n",
            "Epoch [17516/20000], Training Loss: 0.0272\n",
            "Epoch [17517/20000], Training Loss: 0.0287\n",
            "Epoch [17518/20000], Training Loss: 0.0261\n",
            "Epoch [17519/20000], Training Loss: 0.0291\n",
            "Epoch [17520/20000], Training Loss: 0.0246\n",
            "Epoch [17521/20000], Training Loss: 0.0259\n",
            "Epoch [17522/20000], Training Loss: 0.0256\n",
            "Epoch [17523/20000], Training Loss: 0.0242\n",
            "Epoch [17524/20000], Training Loss: 0.0270\n",
            "Epoch [17525/20000], Training Loss: 0.0275\n",
            "Epoch [17526/20000], Training Loss: 0.0255\n",
            "Epoch [17527/20000], Training Loss: 0.0256\n",
            "Epoch [17528/20000], Training Loss: 0.0275\n",
            "Epoch [17529/20000], Training Loss: 0.0253\n",
            "Epoch [17530/20000], Training Loss: 0.0283\n",
            "Epoch [17531/20000], Training Loss: 0.0272\n",
            "Epoch [17532/20000], Training Loss: 0.0264\n",
            "Epoch [17533/20000], Training Loss: 0.0262\n",
            "Epoch [17534/20000], Training Loss: 0.0248\n",
            "Epoch [17535/20000], Training Loss: 0.0260\n",
            "Epoch [17536/20000], Training Loss: 0.0280\n",
            "Epoch [17537/20000], Training Loss: 0.0266\n",
            "Epoch [17538/20000], Training Loss: 0.0252\n",
            "Epoch [17539/20000], Training Loss: 0.0256\n",
            "Epoch [17540/20000], Training Loss: 0.0294\n",
            "Epoch [17541/20000], Training Loss: 0.0245\n",
            "Epoch [17542/20000], Training Loss: 0.0258\n",
            "Epoch [17543/20000], Training Loss: 0.0277\n",
            "Epoch [17544/20000], Training Loss: 0.0249\n",
            "Epoch [17545/20000], Training Loss: 0.0285\n",
            "Epoch [17546/20000], Training Loss: 0.0280\n",
            "Epoch [17547/20000], Training Loss: 0.0282\n",
            "Epoch [17548/20000], Training Loss: 0.0267\n",
            "Epoch [17549/20000], Training Loss: 0.0264\n",
            "Epoch [17550/20000], Training Loss: 0.0279\n",
            "Epoch [17551/20000], Training Loss: 0.0271\n",
            "Epoch [17552/20000], Training Loss: 0.0260\n",
            "Epoch [17553/20000], Training Loss: 0.0283\n",
            "Epoch [17554/20000], Training Loss: 0.0270\n",
            "Epoch [17555/20000], Training Loss: 0.0260\n",
            "Epoch [17556/20000], Training Loss: 0.0267\n",
            "Epoch [17557/20000], Training Loss: 0.0238\n",
            "Epoch [17558/20000], Training Loss: 0.0243\n",
            "Epoch [17559/20000], Training Loss: 0.0266\n",
            "Epoch [17560/20000], Training Loss: 0.0243\n",
            "Epoch [17561/20000], Training Loss: 0.0263\n",
            "Epoch [17562/20000], Training Loss: 0.0259\n",
            "Epoch [17563/20000], Training Loss: 0.0250\n",
            "Epoch [17564/20000], Training Loss: 0.0268\n",
            "Epoch [17565/20000], Training Loss: 0.0259\n",
            "Epoch [17566/20000], Training Loss: 0.0256\n",
            "Epoch [17567/20000], Training Loss: 0.0282\n",
            "Epoch [17568/20000], Training Loss: 0.0273\n",
            "Epoch [17569/20000], Training Loss: 0.0252\n",
            "Epoch [17570/20000], Training Loss: 0.0260\n",
            "Epoch [17571/20000], Training Loss: 0.0255\n",
            "Epoch [17572/20000], Training Loss: 0.0255\n",
            "Epoch [17573/20000], Training Loss: 0.0281\n",
            "Epoch [17574/20000], Training Loss: 0.0266\n",
            "Epoch [17575/20000], Training Loss: 0.0279\n",
            "Epoch [17576/20000], Training Loss: 0.0248\n",
            "Epoch [17577/20000], Training Loss: 0.0262\n",
            "Epoch [17578/20000], Training Loss: 0.0272\n",
            "Epoch [17579/20000], Training Loss: 0.0264\n",
            "Epoch [17580/20000], Training Loss: 0.0272\n",
            "Epoch [17581/20000], Training Loss: 0.0265\n",
            "Epoch [17582/20000], Training Loss: 0.0280\n",
            "Epoch [17583/20000], Training Loss: 0.0262\n",
            "Epoch [17584/20000], Training Loss: 0.0266\n",
            "Epoch [17585/20000], Training Loss: 0.0273\n",
            "Epoch [17586/20000], Training Loss: 0.0256\n",
            "Epoch [17587/20000], Training Loss: 0.0258\n",
            "Epoch [17588/20000], Training Loss: 0.0256\n",
            "Epoch [17589/20000], Training Loss: 0.0258\n",
            "Epoch [17590/20000], Training Loss: 0.0290\n",
            "Epoch [17591/20000], Training Loss: 0.0274\n",
            "Epoch [17592/20000], Training Loss: 0.0257\n",
            "Epoch [17593/20000], Training Loss: 0.0274\n",
            "Epoch [17594/20000], Training Loss: 0.0274\n",
            "Epoch [17595/20000], Training Loss: 0.0262\n",
            "Epoch [17596/20000], Training Loss: 0.0263\n",
            "Epoch [17597/20000], Training Loss: 0.0269\n",
            "Epoch [17598/20000], Training Loss: 0.0262\n",
            "Epoch [17599/20000], Training Loss: 0.0265\n",
            "Epoch [17600/20000], Training Loss: 0.0258\n",
            "Epoch [17601/20000], Training Loss: 0.0269\n",
            "Epoch [17602/20000], Training Loss: 0.0272\n",
            "Epoch [17603/20000], Training Loss: 0.0246\n",
            "Epoch [17604/20000], Training Loss: 0.0252\n",
            "Epoch [17605/20000], Training Loss: 0.0277\n",
            "Epoch [17606/20000], Training Loss: 0.0295\n",
            "Epoch [17607/20000], Training Loss: 0.0275\n",
            "Epoch [17608/20000], Training Loss: 0.0252\n",
            "Epoch [17609/20000], Training Loss: 0.0240\n",
            "Epoch [17610/20000], Training Loss: 0.0253\n",
            "Epoch [17611/20000], Training Loss: 0.0255\n",
            "Epoch [17612/20000], Training Loss: 0.0253\n",
            "Epoch [17613/20000], Training Loss: 0.0285\n",
            "Epoch [17614/20000], Training Loss: 0.0260\n",
            "Epoch [17615/20000], Training Loss: 0.0254\n",
            "Epoch [17616/20000], Training Loss: 0.0262\n",
            "Epoch [17617/20000], Training Loss: 0.0256\n",
            "Epoch [17618/20000], Training Loss: 0.0250\n",
            "Epoch [17619/20000], Training Loss: 0.0250\n",
            "Epoch [17620/20000], Training Loss: 0.0260\n",
            "Epoch [17621/20000], Training Loss: 0.0283\n",
            "Epoch [17622/20000], Training Loss: 0.0271\n",
            "Epoch [17623/20000], Training Loss: 0.0255\n",
            "Epoch [17624/20000], Training Loss: 0.0276\n",
            "Epoch [17625/20000], Training Loss: 0.0251\n",
            "Epoch [17626/20000], Training Loss: 0.0262\n",
            "Epoch [17627/20000], Training Loss: 0.0268\n",
            "Epoch [17628/20000], Training Loss: 0.0264\n",
            "Epoch [17629/20000], Training Loss: 0.0252\n",
            "Epoch [17630/20000], Training Loss: 0.0251\n",
            "Epoch [17631/20000], Training Loss: 0.0267\n",
            "Epoch [17632/20000], Training Loss: 0.0267\n",
            "Epoch [17633/20000], Training Loss: 0.0277\n",
            "Epoch [17634/20000], Training Loss: 0.0250\n",
            "Epoch [17635/20000], Training Loss: 0.0270\n",
            "Epoch [17636/20000], Training Loss: 0.0247\n",
            "Epoch [17637/20000], Training Loss: 0.0261\n",
            "Epoch [17638/20000], Training Loss: 0.0248\n",
            "Epoch [17639/20000], Training Loss: 0.0247\n",
            "Epoch [17640/20000], Training Loss: 0.0256\n",
            "Epoch [17641/20000], Training Loss: 0.0247\n",
            "Epoch [17642/20000], Training Loss: 0.0274\n",
            "Epoch [17643/20000], Training Loss: 0.0276\n",
            "Epoch [17644/20000], Training Loss: 0.0263\n",
            "Epoch [17645/20000], Training Loss: 0.0279\n",
            "Epoch [17646/20000], Training Loss: 0.0252\n",
            "Epoch [17647/20000], Training Loss: 0.0263\n",
            "Epoch [17648/20000], Training Loss: 0.0246\n",
            "Epoch [17649/20000], Training Loss: 0.0261\n",
            "Epoch [17650/20000], Training Loss: 0.0276\n",
            "Epoch [17651/20000], Training Loss: 0.0258\n",
            "Epoch [17652/20000], Training Loss: 0.0257\n",
            "Epoch [17653/20000], Training Loss: 0.0263\n",
            "Epoch [17654/20000], Training Loss: 0.0283\n",
            "Epoch [17655/20000], Training Loss: 0.0254\n",
            "Epoch [17656/20000], Training Loss: 0.0266\n",
            "Epoch [17657/20000], Training Loss: 0.0267\n",
            "Epoch [17658/20000], Training Loss: 0.0238\n",
            "Epoch [17659/20000], Training Loss: 0.0259\n",
            "Epoch [17660/20000], Training Loss: 0.0282\n",
            "Epoch [17661/20000], Training Loss: 0.0286\n",
            "Epoch [17662/20000], Training Loss: 0.0266\n",
            "Epoch [17663/20000], Training Loss: 0.0273\n",
            "Epoch [17664/20000], Training Loss: 0.0259\n",
            "Epoch [17665/20000], Training Loss: 0.0259\n",
            "Epoch [17666/20000], Training Loss: 0.0260\n",
            "Epoch [17667/20000], Training Loss: 0.0261\n",
            "Epoch [17668/20000], Training Loss: 0.0261\n",
            "Epoch [17669/20000], Training Loss: 0.0273\n",
            "Epoch [17670/20000], Training Loss: 0.0266\n",
            "Epoch [17671/20000], Training Loss: 0.0289\n",
            "Epoch [17672/20000], Training Loss: 0.0290\n",
            "Epoch [17673/20000], Training Loss: 0.0291\n",
            "Epoch [17674/20000], Training Loss: 0.0256\n",
            "Epoch [17675/20000], Training Loss: 0.0265\n",
            "Epoch [17676/20000], Training Loss: 0.0248\n",
            "Epoch [17677/20000], Training Loss: 0.0273\n",
            "Epoch [17678/20000], Training Loss: 0.0251\n",
            "Epoch [17679/20000], Training Loss: 0.0255\n",
            "Epoch [17680/20000], Training Loss: 0.0251\n",
            "Epoch [17681/20000], Training Loss: 0.0268\n",
            "Epoch [17682/20000], Training Loss: 0.0258\n",
            "Epoch [17683/20000], Training Loss: 0.0256\n",
            "Epoch [17684/20000], Training Loss: 0.0275\n",
            "Epoch [17685/20000], Training Loss: 0.0268\n",
            "Epoch [17686/20000], Training Loss: 0.0269\n",
            "Epoch [17687/20000], Training Loss: 0.0289\n",
            "Epoch [17688/20000], Training Loss: 0.0282\n",
            "Epoch [17689/20000], Training Loss: 0.0268\n",
            "Epoch [17690/20000], Training Loss: 0.0263\n",
            "Epoch [17691/20000], Training Loss: 0.0263\n",
            "Epoch [17692/20000], Training Loss: 0.0295\n",
            "Epoch [17693/20000], Training Loss: 0.0263\n",
            "Epoch [17694/20000], Training Loss: 0.0258\n",
            "Epoch [17695/20000], Training Loss: 0.0278\n",
            "Epoch [17696/20000], Training Loss: 0.0251\n",
            "Epoch [17697/20000], Training Loss: 0.0255\n",
            "Epoch [17698/20000], Training Loss: 0.0272\n",
            "Epoch [17699/20000], Training Loss: 0.0263\n",
            "Epoch [17700/20000], Training Loss: 0.0281\n",
            "Epoch [17701/20000], Training Loss: 0.0258\n",
            "Epoch [17702/20000], Training Loss: 0.0264\n",
            "Epoch [17703/20000], Training Loss: 0.0251\n",
            "Epoch [17704/20000], Training Loss: 0.0260\n",
            "Epoch [17705/20000], Training Loss: 0.0260\n",
            "Epoch [17706/20000], Training Loss: 0.0254\n",
            "Epoch [17707/20000], Training Loss: 0.0266\n",
            "Epoch [17708/20000], Training Loss: 0.0259\n",
            "Epoch [17709/20000], Training Loss: 0.0272\n",
            "Epoch [17710/20000], Training Loss: 0.0254\n",
            "Epoch [17711/20000], Training Loss: 0.0253\n",
            "Epoch [17712/20000], Training Loss: 0.0265\n",
            "Epoch [17713/20000], Training Loss: 0.0270\n",
            "Epoch [17714/20000], Training Loss: 0.0272\n",
            "Epoch [17715/20000], Training Loss: 0.0268\n",
            "Epoch [17716/20000], Training Loss: 0.0275\n",
            "Epoch [17717/20000], Training Loss: 0.0237\n",
            "Epoch [17718/20000], Training Loss: 0.0273\n",
            "Epoch [17719/20000], Training Loss: 0.0280\n",
            "Epoch [17720/20000], Training Loss: 0.0280\n",
            "Epoch [17721/20000], Training Loss: 0.0261\n",
            "Epoch [17722/20000], Training Loss: 0.0275\n",
            "Epoch [17723/20000], Training Loss: 0.0264\n",
            "Epoch [17724/20000], Training Loss: 0.0256\n",
            "Epoch [17725/20000], Training Loss: 0.0264\n",
            "Epoch [17726/20000], Training Loss: 0.0264\n",
            "Epoch [17727/20000], Training Loss: 0.0257\n",
            "Epoch [17728/20000], Training Loss: 0.0248\n",
            "Epoch [17729/20000], Training Loss: 0.0270\n",
            "Epoch [17730/20000], Training Loss: 0.0256\n",
            "Epoch [17731/20000], Training Loss: 0.0278\n",
            "Epoch [17732/20000], Training Loss: 0.0262\n",
            "Epoch [17733/20000], Training Loss: 0.0259\n",
            "Epoch [17734/20000], Training Loss: 0.0257\n",
            "Epoch [17735/20000], Training Loss: 0.0248\n",
            "Epoch [17736/20000], Training Loss: 0.0277\n",
            "Epoch [17737/20000], Training Loss: 0.0265\n",
            "Epoch [17738/20000], Training Loss: 0.0254\n",
            "Epoch [17739/20000], Training Loss: 0.0265\n",
            "Epoch [17740/20000], Training Loss: 0.0260\n",
            "Epoch [17741/20000], Training Loss: 0.0259\n",
            "Epoch [17742/20000], Training Loss: 0.0271\n",
            "Epoch [17743/20000], Training Loss: 0.0267\n",
            "Epoch [17744/20000], Training Loss: 0.0275\n",
            "Epoch [17745/20000], Training Loss: 0.0249\n",
            "Epoch [17746/20000], Training Loss: 0.0274\n",
            "Epoch [17747/20000], Training Loss: 0.0270\n",
            "Epoch [17748/20000], Training Loss: 0.0261\n",
            "Epoch [17749/20000], Training Loss: 0.0266\n",
            "Epoch [17750/20000], Training Loss: 0.0269\n",
            "Epoch [17751/20000], Training Loss: 0.0287\n",
            "Epoch [17752/20000], Training Loss: 0.0265\n",
            "Epoch [17753/20000], Training Loss: 0.0261\n",
            "Epoch [17754/20000], Training Loss: 0.0260\n",
            "Epoch [17755/20000], Training Loss: 0.0276\n",
            "Epoch [17756/20000], Training Loss: 0.0264\n",
            "Epoch [17757/20000], Training Loss: 0.0282\n",
            "Epoch [17758/20000], Training Loss: 0.0268\n",
            "Epoch [17759/20000], Training Loss: 0.0262\n",
            "Epoch [17760/20000], Training Loss: 0.0267\n",
            "Epoch [17761/20000], Training Loss: 0.0258\n",
            "Epoch [17762/20000], Training Loss: 0.0256\n",
            "Epoch [17763/20000], Training Loss: 0.0267\n",
            "Epoch [17764/20000], Training Loss: 0.0269\n",
            "Epoch [17765/20000], Training Loss: 0.0243\n",
            "Epoch [17766/20000], Training Loss: 0.0255\n",
            "Epoch [17767/20000], Training Loss: 0.0248\n",
            "Epoch [17768/20000], Training Loss: 0.0261\n",
            "Epoch [17769/20000], Training Loss: 0.0267\n",
            "Epoch [17770/20000], Training Loss: 0.0274\n",
            "Epoch [17771/20000], Training Loss: 0.0260\n",
            "Epoch [17772/20000], Training Loss: 0.0285\n",
            "Epoch [17773/20000], Training Loss: 0.0257\n",
            "Epoch [17774/20000], Training Loss: 0.0287\n",
            "Epoch [17775/20000], Training Loss: 0.0251\n",
            "Epoch [17776/20000], Training Loss: 0.0284\n",
            "Epoch [17777/20000], Training Loss: 0.0267\n",
            "Epoch [17778/20000], Training Loss: 0.0256\n",
            "Epoch [17779/20000], Training Loss: 0.0285\n",
            "Epoch [17780/20000], Training Loss: 0.0260\n",
            "Epoch [17781/20000], Training Loss: 0.0266\n",
            "Epoch [17782/20000], Training Loss: 0.0261\n",
            "Epoch [17783/20000], Training Loss: 0.0251\n",
            "Epoch [17784/20000], Training Loss: 0.0264\n",
            "Epoch [17785/20000], Training Loss: 0.0258\n",
            "Epoch [17786/20000], Training Loss: 0.0263\n",
            "Epoch [17787/20000], Training Loss: 0.0254\n",
            "Epoch [17788/20000], Training Loss: 0.0238\n",
            "Epoch [17789/20000], Training Loss: 0.0256\n",
            "Epoch [17790/20000], Training Loss: 0.0253\n",
            "Epoch [17791/20000], Training Loss: 0.0253\n",
            "Epoch [17792/20000], Training Loss: 0.0247\n",
            "Epoch [17793/20000], Training Loss: 0.0264\n",
            "Epoch [17794/20000], Training Loss: 0.0269\n",
            "Epoch [17795/20000], Training Loss: 0.0259\n",
            "Epoch [17796/20000], Training Loss: 0.0270\n",
            "Epoch [17797/20000], Training Loss: 0.0280\n",
            "Epoch [17798/20000], Training Loss: 0.0266\n",
            "Epoch [17799/20000], Training Loss: 0.0273\n",
            "Epoch [17800/20000], Training Loss: 0.0281\n",
            "Epoch [17801/20000], Training Loss: 0.0275\n",
            "Epoch [17802/20000], Training Loss: 0.0258\n",
            "Epoch [17803/20000], Training Loss: 0.0261\n",
            "Epoch [17804/20000], Training Loss: 0.0247\n",
            "Epoch [17805/20000], Training Loss: 0.0271\n",
            "Epoch [17806/20000], Training Loss: 0.0272\n",
            "Epoch [17807/20000], Training Loss: 0.0246\n",
            "Epoch [17808/20000], Training Loss: 0.0279\n",
            "Epoch [17809/20000], Training Loss: 0.0250\n",
            "Epoch [17810/20000], Training Loss: 0.0258\n",
            "Epoch [17811/20000], Training Loss: 0.0275\n",
            "Epoch [17812/20000], Training Loss: 0.0261\n",
            "Epoch [17813/20000], Training Loss: 0.0275\n",
            "Epoch [17814/20000], Training Loss: 0.0260\n",
            "Epoch [17815/20000], Training Loss: 0.0263\n",
            "Epoch [17816/20000], Training Loss: 0.0267\n",
            "Epoch [17817/20000], Training Loss: 0.0261\n",
            "Epoch [17818/20000], Training Loss: 0.0283\n",
            "Epoch [17819/20000], Training Loss: 0.0260\n",
            "Epoch [17820/20000], Training Loss: 0.0279\n",
            "Epoch [17821/20000], Training Loss: 0.0282\n",
            "Epoch [17822/20000], Training Loss: 0.0268\n",
            "Epoch [17823/20000], Training Loss: 0.0252\n",
            "Epoch [17824/20000], Training Loss: 0.0262\n",
            "Epoch [17825/20000], Training Loss: 0.0263\n",
            "Epoch [17826/20000], Training Loss: 0.0272\n",
            "Epoch [17827/20000], Training Loss: 0.0261\n",
            "Epoch [17828/20000], Training Loss: 0.0264\n",
            "Epoch [17829/20000], Training Loss: 0.0281\n",
            "Epoch [17830/20000], Training Loss: 0.0270\n",
            "Epoch [17831/20000], Training Loss: 0.0274\n",
            "Epoch [17832/20000], Training Loss: 0.0260\n",
            "Epoch [17833/20000], Training Loss: 0.0248\n",
            "Epoch [17834/20000], Training Loss: 0.0271\n",
            "Epoch [17835/20000], Training Loss: 0.0288\n",
            "Epoch [17836/20000], Training Loss: 0.0270\n",
            "Epoch [17837/20000], Training Loss: 0.0257\n",
            "Epoch [17838/20000], Training Loss: 0.0263\n",
            "Epoch [17839/20000], Training Loss: 0.0261\n",
            "Epoch [17840/20000], Training Loss: 0.0246\n",
            "Epoch [17841/20000], Training Loss: 0.0268\n",
            "Epoch [17842/20000], Training Loss: 0.0266\n",
            "Epoch [17843/20000], Training Loss: 0.0257\n",
            "Epoch [17844/20000], Training Loss: 0.0266\n",
            "Epoch [17845/20000], Training Loss: 0.0278\n",
            "Epoch [17846/20000], Training Loss: 0.0263\n",
            "Epoch [17847/20000], Training Loss: 0.0258\n",
            "Epoch [17848/20000], Training Loss: 0.0275\n",
            "Epoch [17849/20000], Training Loss: 0.0283\n",
            "Epoch [17850/20000], Training Loss: 0.0262\n",
            "Epoch [17851/20000], Training Loss: 0.0243\n",
            "Epoch [17852/20000], Training Loss: 0.0269\n",
            "Epoch [17853/20000], Training Loss: 0.0259\n",
            "Epoch [17854/20000], Training Loss: 0.0257\n",
            "Epoch [17855/20000], Training Loss: 0.0270\n",
            "Epoch [17856/20000], Training Loss: 0.0252\n",
            "Epoch [17857/20000], Training Loss: 0.0277\n",
            "Epoch [17858/20000], Training Loss: 0.0249\n",
            "Epoch [17859/20000], Training Loss: 0.0259\n",
            "Epoch [17860/20000], Training Loss: 0.0270\n",
            "Epoch [17861/20000], Training Loss: 0.0244\n",
            "Epoch [17862/20000], Training Loss: 0.0242\n",
            "Epoch [17863/20000], Training Loss: 0.0250\n",
            "Epoch [17864/20000], Training Loss: 0.0258\n",
            "Epoch [17865/20000], Training Loss: 0.0254\n",
            "Epoch [17866/20000], Training Loss: 0.0261\n",
            "Epoch [17867/20000], Training Loss: 0.0247\n",
            "Epoch [17868/20000], Training Loss: 0.0284\n",
            "Epoch [17869/20000], Training Loss: 0.0237\n",
            "Epoch [17870/20000], Training Loss: 0.0260\n",
            "Epoch [17871/20000], Training Loss: 0.0277\n",
            "Epoch [17872/20000], Training Loss: 0.0264\n",
            "Epoch [17873/20000], Training Loss: 0.0286\n",
            "Epoch [17874/20000], Training Loss: 0.0247\n",
            "Epoch [17875/20000], Training Loss: 0.0265\n",
            "Epoch [17876/20000], Training Loss: 0.0251\n",
            "Epoch [17877/20000], Training Loss: 0.0285\n",
            "Epoch [17878/20000], Training Loss: 0.0286\n",
            "Epoch [17879/20000], Training Loss: 0.0283\n",
            "Epoch [17880/20000], Training Loss: 0.0273\n",
            "Epoch [17881/20000], Training Loss: 0.0257\n",
            "Epoch [17882/20000], Training Loss: 0.0256\n",
            "Epoch [17883/20000], Training Loss: 0.0252\n",
            "Epoch [17884/20000], Training Loss: 0.0275\n",
            "Epoch [17885/20000], Training Loss: 0.0260\n",
            "Epoch [17886/20000], Training Loss: 0.0262\n",
            "Epoch [17887/20000], Training Loss: 0.0268\n",
            "Epoch [17888/20000], Training Loss: 0.0269\n",
            "Epoch [17889/20000], Training Loss: 0.0252\n",
            "Epoch [17890/20000], Training Loss: 0.0271\n",
            "Epoch [17891/20000], Training Loss: 0.0241\n",
            "Epoch [17892/20000], Training Loss: 0.0254\n",
            "Epoch [17893/20000], Training Loss: 0.0260\n",
            "Epoch [17894/20000], Training Loss: 0.0252\n",
            "Epoch [17895/20000], Training Loss: 0.0245\n",
            "Epoch [17896/20000], Training Loss: 0.0237\n",
            "Epoch [17897/20000], Training Loss: 0.0278\n",
            "Epoch [17898/20000], Training Loss: 0.0239\n",
            "Epoch [17899/20000], Training Loss: 0.0248\n",
            "Epoch [17900/20000], Training Loss: 0.0269\n",
            "Epoch [17901/20000], Training Loss: 0.0278\n",
            "Epoch [17902/20000], Training Loss: 0.0258\n",
            "Epoch [17903/20000], Training Loss: 0.0257\n",
            "Epoch [17904/20000], Training Loss: 0.0263\n",
            "Epoch [17905/20000], Training Loss: 0.0276\n",
            "Epoch [17906/20000], Training Loss: 0.0265\n",
            "Epoch [17907/20000], Training Loss: 0.0271\n",
            "Epoch [17908/20000], Training Loss: 0.0280\n",
            "Epoch [17909/20000], Training Loss: 0.0260\n",
            "Epoch [17910/20000], Training Loss: 0.0272\n",
            "Epoch [17911/20000], Training Loss: 0.0271\n",
            "Epoch [17912/20000], Training Loss: 0.0254\n",
            "Epoch [17913/20000], Training Loss: 0.0266\n",
            "Epoch [17914/20000], Training Loss: 0.0247\n",
            "Epoch [17915/20000], Training Loss: 0.0267\n",
            "Epoch [17916/20000], Training Loss: 0.0246\n",
            "Epoch [17917/20000], Training Loss: 0.0277\n",
            "Epoch [17918/20000], Training Loss: 0.0288\n",
            "Epoch [17919/20000], Training Loss: 0.0260\n",
            "Epoch [17920/20000], Training Loss: 0.0244\n",
            "Epoch [17921/20000], Training Loss: 0.0266\n",
            "Epoch [17922/20000], Training Loss: 0.0272\n",
            "Epoch [17923/20000], Training Loss: 0.0253\n",
            "Epoch [17924/20000], Training Loss: 0.0290\n",
            "Epoch [17925/20000], Training Loss: 0.0265\n",
            "Epoch [17926/20000], Training Loss: 0.0265\n",
            "Epoch [17927/20000], Training Loss: 0.0271\n",
            "Epoch [17928/20000], Training Loss: 0.0266\n",
            "Epoch [17929/20000], Training Loss: 0.0268\n",
            "Epoch [17930/20000], Training Loss: 0.0248\n",
            "Epoch [17931/20000], Training Loss: 0.0264\n",
            "Epoch [17932/20000], Training Loss: 0.0264\n",
            "Epoch [17933/20000], Training Loss: 0.0251\n",
            "Epoch [17934/20000], Training Loss: 0.0272\n",
            "Epoch [17935/20000], Training Loss: 0.0248\n",
            "Epoch [17936/20000], Training Loss: 0.0275\n",
            "Epoch [17937/20000], Training Loss: 0.0276\n",
            "Epoch [17938/20000], Training Loss: 0.0249\n",
            "Epoch [17939/20000], Training Loss: 0.0273\n",
            "Epoch [17940/20000], Training Loss: 0.0269\n",
            "Epoch [17941/20000], Training Loss: 0.0257\n",
            "Epoch [17942/20000], Training Loss: 0.0272\n",
            "Epoch [17943/20000], Training Loss: 0.0270\n",
            "Epoch [17944/20000], Training Loss: 0.0257\n",
            "Epoch [17945/20000], Training Loss: 0.0266\n",
            "Epoch [17946/20000], Training Loss: 0.0256\n",
            "Epoch [17947/20000], Training Loss: 0.0257\n",
            "Epoch [17948/20000], Training Loss: 0.0246\n",
            "Epoch [17949/20000], Training Loss: 0.0254\n",
            "Epoch [17950/20000], Training Loss: 0.0254\n",
            "Epoch [17951/20000], Training Loss: 0.0255\n",
            "Epoch [17952/20000], Training Loss: 0.0288\n",
            "Epoch [17953/20000], Training Loss: 0.0267\n",
            "Epoch [17954/20000], Training Loss: 0.0264\n",
            "Epoch [17955/20000], Training Loss: 0.0255\n",
            "Epoch [17956/20000], Training Loss: 0.0264\n",
            "Epoch [17957/20000], Training Loss: 0.0264\n",
            "Epoch [17958/20000], Training Loss: 0.0277\n",
            "Epoch [17959/20000], Training Loss: 0.0261\n",
            "Epoch [17960/20000], Training Loss: 0.0302\n",
            "Epoch [17961/20000], Training Loss: 0.0276\n",
            "Epoch [17962/20000], Training Loss: 0.0261\n",
            "Epoch [17963/20000], Training Loss: 0.0259\n",
            "Epoch [17964/20000], Training Loss: 0.0287\n",
            "Epoch [17965/20000], Training Loss: 0.0249\n",
            "Epoch [17966/20000], Training Loss: 0.0249\n",
            "Epoch [17967/20000], Training Loss: 0.0269\n",
            "Epoch [17968/20000], Training Loss: 0.0269\n",
            "Epoch [17969/20000], Training Loss: 0.0255\n",
            "Epoch [17970/20000], Training Loss: 0.0273\n",
            "Epoch [17971/20000], Training Loss: 0.0254\n",
            "Epoch [17972/20000], Training Loss: 0.0265\n",
            "Epoch [17973/20000], Training Loss: 0.0269\n",
            "Epoch [17974/20000], Training Loss: 0.0296\n",
            "Epoch [17975/20000], Training Loss: 0.0273\n",
            "Epoch [17976/20000], Training Loss: 0.0260\n",
            "Epoch [17977/20000], Training Loss: 0.0254\n",
            "Epoch [17978/20000], Training Loss: 0.0278\n",
            "Epoch [17979/20000], Training Loss: 0.0277\n",
            "Epoch [17980/20000], Training Loss: 0.0254\n",
            "Epoch [17981/20000], Training Loss: 0.0262\n",
            "Epoch [17982/20000], Training Loss: 0.0268\n",
            "Epoch [17983/20000], Training Loss: 0.0256\n",
            "Epoch [17984/20000], Training Loss: 0.0261\n",
            "Epoch [17985/20000], Training Loss: 0.0255\n",
            "Epoch [17986/20000], Training Loss: 0.0271\n",
            "Epoch [17987/20000], Training Loss: 0.0267\n",
            "Epoch [17988/20000], Training Loss: 0.0266\n",
            "Epoch [17989/20000], Training Loss: 0.0283\n",
            "Epoch [17990/20000], Training Loss: 0.0269\n",
            "Epoch [17991/20000], Training Loss: 0.0270\n",
            "Epoch [17992/20000], Training Loss: 0.0261\n",
            "Epoch [17993/20000], Training Loss: 0.0264\n",
            "Epoch [17994/20000], Training Loss: 0.0271\n",
            "Epoch [17995/20000], Training Loss: 0.0259\n",
            "Epoch [17996/20000], Training Loss: 0.0259\n",
            "Epoch [17997/20000], Training Loss: 0.0255\n",
            "Epoch [17998/20000], Training Loss: 0.0255\n",
            "Epoch [17999/20000], Training Loss: 0.0256\n",
            "Epoch [18000/20000], Training Loss: 0.0271\n",
            "Epoch [18001/20000], Training Loss: 0.0257\n",
            "Epoch [18002/20000], Training Loss: 0.0249\n",
            "Epoch [18003/20000], Training Loss: 0.0278\n",
            "Epoch [18004/20000], Training Loss: 0.0248\n",
            "Epoch [18005/20000], Training Loss: 0.0264\n",
            "Epoch [18006/20000], Training Loss: 0.0265\n",
            "Epoch [18007/20000], Training Loss: 0.0269\n",
            "Epoch [18008/20000], Training Loss: 0.0272\n",
            "Epoch [18009/20000], Training Loss: 0.0264\n",
            "Epoch [18010/20000], Training Loss: 0.0261\n",
            "Epoch [18011/20000], Training Loss: 0.0257\n",
            "Epoch [18012/20000], Training Loss: 0.0264\n",
            "Epoch [18013/20000], Training Loss: 0.0283\n",
            "Epoch [18014/20000], Training Loss: 0.0263\n",
            "Epoch [18015/20000], Training Loss: 0.0271\n",
            "Epoch [18016/20000], Training Loss: 0.0265\n",
            "Epoch [18017/20000], Training Loss: 0.0278\n",
            "Epoch [18018/20000], Training Loss: 0.0253\n",
            "Epoch [18019/20000], Training Loss: 0.0241\n",
            "Epoch [18020/20000], Training Loss: 0.0254\n",
            "Epoch [18021/20000], Training Loss: 0.0261\n",
            "Epoch [18022/20000], Training Loss: 0.0264\n",
            "Epoch [18023/20000], Training Loss: 0.0250\n",
            "Epoch [18024/20000], Training Loss: 0.0246\n",
            "Epoch [18025/20000], Training Loss: 0.0258\n",
            "Epoch [18026/20000], Training Loss: 0.0275\n",
            "Epoch [18027/20000], Training Loss: 0.0271\n",
            "Epoch [18028/20000], Training Loss: 0.0280\n",
            "Epoch [18029/20000], Training Loss: 0.0267\n",
            "Epoch [18030/20000], Training Loss: 0.0259\n",
            "Epoch [18031/20000], Training Loss: 0.0246\n",
            "Epoch [18032/20000], Training Loss: 0.0252\n",
            "Epoch [18033/20000], Training Loss: 0.0269\n",
            "Epoch [18034/20000], Training Loss: 0.0260\n",
            "Epoch [18035/20000], Training Loss: 0.0272\n",
            "Epoch [18036/20000], Training Loss: 0.0254\n",
            "Epoch [18037/20000], Training Loss: 0.0260\n",
            "Epoch [18038/20000], Training Loss: 0.0263\n",
            "Epoch [18039/20000], Training Loss: 0.0281\n",
            "Epoch [18040/20000], Training Loss: 0.0265\n",
            "Epoch [18041/20000], Training Loss: 0.0249\n",
            "Epoch [18042/20000], Training Loss: 0.0274\n",
            "Epoch [18043/20000], Training Loss: 0.0252\n",
            "Epoch [18044/20000], Training Loss: 0.0284\n",
            "Epoch [18045/20000], Training Loss: 0.0258\n",
            "Epoch [18046/20000], Training Loss: 0.0256\n",
            "Epoch [18047/20000], Training Loss: 0.0249\n",
            "Epoch [18048/20000], Training Loss: 0.0299\n",
            "Epoch [18049/20000], Training Loss: 0.0263\n",
            "Epoch [18050/20000], Training Loss: 0.0254\n",
            "Epoch [18051/20000], Training Loss: 0.0289\n",
            "Epoch [18052/20000], Training Loss: 0.0253\n",
            "Epoch [18053/20000], Training Loss: 0.0255\n",
            "Epoch [18054/20000], Training Loss: 0.0247\n",
            "Epoch [18055/20000], Training Loss: 0.0246\n",
            "Epoch [18056/20000], Training Loss: 0.0257\n",
            "Epoch [18057/20000], Training Loss: 0.0283\n",
            "Epoch [18058/20000], Training Loss: 0.0272\n",
            "Epoch [18059/20000], Training Loss: 0.0273\n",
            "Epoch [18060/20000], Training Loss: 0.0265\n",
            "Epoch [18061/20000], Training Loss: 0.0264\n",
            "Epoch [18062/20000], Training Loss: 0.0282\n",
            "Epoch [18063/20000], Training Loss: 0.0281\n",
            "Epoch [18064/20000], Training Loss: 0.0279\n",
            "Epoch [18065/20000], Training Loss: 0.0265\n",
            "Epoch [18066/20000], Training Loss: 0.0265\n",
            "Epoch [18067/20000], Training Loss: 0.0254\n",
            "Epoch [18068/20000], Training Loss: 0.0271\n",
            "Epoch [18069/20000], Training Loss: 0.0258\n",
            "Epoch [18070/20000], Training Loss: 0.0255\n",
            "Epoch [18071/20000], Training Loss: 0.0248\n",
            "Epoch [18072/20000], Training Loss: 0.0259\n",
            "Epoch [18073/20000], Training Loss: 0.0276\n",
            "Epoch [18074/20000], Training Loss: 0.0260\n",
            "Epoch [18075/20000], Training Loss: 0.0261\n",
            "Epoch [18076/20000], Training Loss: 0.0280\n",
            "Epoch [18077/20000], Training Loss: 0.0269\n",
            "Epoch [18078/20000], Training Loss: 0.0263\n",
            "Epoch [18079/20000], Training Loss: 0.0261\n",
            "Epoch [18080/20000], Training Loss: 0.0251\n",
            "Epoch [18081/20000], Training Loss: 0.0277\n",
            "Epoch [18082/20000], Training Loss: 0.0262\n",
            "Epoch [18083/20000], Training Loss: 0.0265\n",
            "Epoch [18084/20000], Training Loss: 0.0274\n",
            "Epoch [18085/20000], Training Loss: 0.0281\n",
            "Epoch [18086/20000], Training Loss: 0.0293\n",
            "Epoch [18087/20000], Training Loss: 0.0259\n",
            "Epoch [18088/20000], Training Loss: 0.0273\n",
            "Epoch [18089/20000], Training Loss: 0.0286\n",
            "Epoch [18090/20000], Training Loss: 0.0265\n",
            "Epoch [18091/20000], Training Loss: 0.0274\n",
            "Epoch [18092/20000], Training Loss: 0.0259\n",
            "Epoch [18093/20000], Training Loss: 0.0253\n",
            "Epoch [18094/20000], Training Loss: 0.0280\n",
            "Epoch [18095/20000], Training Loss: 0.0265\n",
            "Epoch [18096/20000], Training Loss: 0.0272\n",
            "Epoch [18097/20000], Training Loss: 0.0275\n",
            "Epoch [18098/20000], Training Loss: 0.0255\n",
            "Epoch [18099/20000], Training Loss: 0.0260\n",
            "Epoch [18100/20000], Training Loss: 0.0251\n",
            "Epoch [18101/20000], Training Loss: 0.0250\n",
            "Epoch [18102/20000], Training Loss: 0.0262\n",
            "Epoch [18103/20000], Training Loss: 0.0264\n",
            "Epoch [18104/20000], Training Loss: 0.0250\n",
            "Epoch [18105/20000], Training Loss: 0.0256\n",
            "Epoch [18106/20000], Training Loss: 0.0256\n",
            "Epoch [18107/20000], Training Loss: 0.0269\n",
            "Epoch [18108/20000], Training Loss: 0.0280\n",
            "Epoch [18109/20000], Training Loss: 0.0269\n",
            "Epoch [18110/20000], Training Loss: 0.0254\n",
            "Epoch [18111/20000], Training Loss: 0.0273\n",
            "Epoch [18112/20000], Training Loss: 0.0256\n",
            "Epoch [18113/20000], Training Loss: 0.0266\n",
            "Epoch [18114/20000], Training Loss: 0.0260\n",
            "Epoch [18115/20000], Training Loss: 0.0292\n",
            "Epoch [18116/20000], Training Loss: 0.0279\n",
            "Epoch [18117/20000], Training Loss: 0.0274\n",
            "Epoch [18118/20000], Training Loss: 0.0268\n",
            "Epoch [18119/20000], Training Loss: 0.0273\n",
            "Epoch [18120/20000], Training Loss: 0.0255\n",
            "Epoch [18121/20000], Training Loss: 0.0264\n",
            "Epoch [18122/20000], Training Loss: 0.0258\n",
            "Epoch [18123/20000], Training Loss: 0.0285\n",
            "Epoch [18124/20000], Training Loss: 0.0252\n",
            "Epoch [18125/20000], Training Loss: 0.0256\n",
            "Epoch [18126/20000], Training Loss: 0.0277\n",
            "Epoch [18127/20000], Training Loss: 0.0252\n",
            "Epoch [18128/20000], Training Loss: 0.0254\n",
            "Epoch [18129/20000], Training Loss: 0.0254\n",
            "Epoch [18130/20000], Training Loss: 0.0272\n",
            "Epoch [18131/20000], Training Loss: 0.0257\n",
            "Epoch [18132/20000], Training Loss: 0.0264\n",
            "Epoch [18133/20000], Training Loss: 0.0270\n",
            "Epoch [18134/20000], Training Loss: 0.0258\n",
            "Epoch [18135/20000], Training Loss: 0.0258\n",
            "Epoch [18136/20000], Training Loss: 0.0276\n",
            "Epoch [18137/20000], Training Loss: 0.0260\n",
            "Epoch [18138/20000], Training Loss: 0.0271\n",
            "Epoch [18139/20000], Training Loss: 0.0241\n",
            "Epoch [18140/20000], Training Loss: 0.0250\n",
            "Epoch [18141/20000], Training Loss: 0.0250\n",
            "Epoch [18142/20000], Training Loss: 0.0250\n",
            "Epoch [18143/20000], Training Loss: 0.0275\n",
            "Epoch [18144/20000], Training Loss: 0.0277\n",
            "Epoch [18145/20000], Training Loss: 0.0262\n",
            "Epoch [18146/20000], Training Loss: 0.0258\n",
            "Epoch [18147/20000], Training Loss: 0.0262\n",
            "Epoch [18148/20000], Training Loss: 0.0278\n",
            "Epoch [18149/20000], Training Loss: 0.0266\n",
            "Epoch [18150/20000], Training Loss: 0.0271\n",
            "Epoch [18151/20000], Training Loss: 0.0236\n",
            "Epoch [18152/20000], Training Loss: 0.0262\n",
            "Epoch [18153/20000], Training Loss: 0.0286\n",
            "Epoch [18154/20000], Training Loss: 0.0246\n",
            "Epoch [18155/20000], Training Loss: 0.0260\n",
            "Epoch [18156/20000], Training Loss: 0.0286\n",
            "Epoch [18157/20000], Training Loss: 0.0263\n",
            "Epoch [18158/20000], Training Loss: 0.0293\n",
            "Epoch [18159/20000], Training Loss: 0.0250\n",
            "Epoch [18160/20000], Training Loss: 0.0259\n",
            "Epoch [18161/20000], Training Loss: 0.0251\n",
            "Epoch [18162/20000], Training Loss: 0.0269\n",
            "Epoch [18163/20000], Training Loss: 0.0267\n",
            "Epoch [18164/20000], Training Loss: 0.0247\n",
            "Epoch [18165/20000], Training Loss: 0.0246\n",
            "Epoch [18166/20000], Training Loss: 0.0275\n",
            "Epoch [18167/20000], Training Loss: 0.0257\n",
            "Epoch [18168/20000], Training Loss: 0.0250\n",
            "Epoch [18169/20000], Training Loss: 0.0273\n",
            "Epoch [18170/20000], Training Loss: 0.0286\n",
            "Epoch [18171/20000], Training Loss: 0.0259\n",
            "Epoch [18172/20000], Training Loss: 0.0269\n",
            "Epoch [18173/20000], Training Loss: 0.0257\n",
            "Epoch [18174/20000], Training Loss: 0.0255\n",
            "Epoch [18175/20000], Training Loss: 0.0261\n",
            "Epoch [18176/20000], Training Loss: 0.0278\n",
            "Epoch [18177/20000], Training Loss: 0.0270\n",
            "Epoch [18178/20000], Training Loss: 0.0254\n",
            "Epoch [18179/20000], Training Loss: 0.0264\n",
            "Epoch [18180/20000], Training Loss: 0.0260\n",
            "Epoch [18181/20000], Training Loss: 0.0249\n",
            "Epoch [18182/20000], Training Loss: 0.0278\n",
            "Epoch [18183/20000], Training Loss: 0.0252\n",
            "Epoch [18184/20000], Training Loss: 0.0264\n",
            "Epoch [18185/20000], Training Loss: 0.0234\n",
            "Epoch [18186/20000], Training Loss: 0.0258\n",
            "Epoch [18187/20000], Training Loss: 0.0293\n",
            "Epoch [18188/20000], Training Loss: 0.0259\n",
            "Epoch [18189/20000], Training Loss: 0.0273\n",
            "Epoch [18190/20000], Training Loss: 0.0269\n",
            "Epoch [18191/20000], Training Loss: 0.0256\n",
            "Epoch [18192/20000], Training Loss: 0.0257\n",
            "Epoch [18193/20000], Training Loss: 0.0268\n",
            "Epoch [18194/20000], Training Loss: 0.0264\n",
            "Epoch [18195/20000], Training Loss: 0.0281\n",
            "Epoch [18196/20000], Training Loss: 0.0242\n",
            "Epoch [18197/20000], Training Loss: 0.0265\n",
            "Epoch [18198/20000], Training Loss: 0.0268\n",
            "Epoch [18199/20000], Training Loss: 0.0267\n",
            "Epoch [18200/20000], Training Loss: 0.0266\n",
            "Epoch [18201/20000], Training Loss: 0.0263\n",
            "Epoch [18202/20000], Training Loss: 0.0270\n",
            "Epoch [18203/20000], Training Loss: 0.0272\n",
            "Epoch [18204/20000], Training Loss: 0.0237\n",
            "Epoch [18205/20000], Training Loss: 0.0266\n",
            "Epoch [18206/20000], Training Loss: 0.0247\n",
            "Epoch [18207/20000], Training Loss: 0.0268\n",
            "Epoch [18208/20000], Training Loss: 0.0262\n",
            "Epoch [18209/20000], Training Loss: 0.0252\n",
            "Epoch [18210/20000], Training Loss: 0.0274\n",
            "Epoch [18211/20000], Training Loss: 0.0270\n",
            "Epoch [18212/20000], Training Loss: 0.0266\n",
            "Epoch [18213/20000], Training Loss: 0.0267\n",
            "Epoch [18214/20000], Training Loss: 0.0260\n",
            "Epoch [18215/20000], Training Loss: 0.0269\n",
            "Epoch [18216/20000], Training Loss: 0.0275\n",
            "Epoch [18217/20000], Training Loss: 0.0242\n",
            "Epoch [18218/20000], Training Loss: 0.0250\n",
            "Epoch [18219/20000], Training Loss: 0.0262\n",
            "Epoch [18220/20000], Training Loss: 0.0259\n",
            "Epoch [18221/20000], Training Loss: 0.0275\n",
            "Epoch [18222/20000], Training Loss: 0.0256\n",
            "Epoch [18223/20000], Training Loss: 0.0268\n",
            "Epoch [18224/20000], Training Loss: 0.0264\n",
            "Epoch [18225/20000], Training Loss: 0.0278\n",
            "Epoch [18226/20000], Training Loss: 0.0269\n",
            "Epoch [18227/20000], Training Loss: 0.0272\n",
            "Epoch [18228/20000], Training Loss: 0.0273\n",
            "Epoch [18229/20000], Training Loss: 0.0247\n",
            "Epoch [18230/20000], Training Loss: 0.0271\n",
            "Epoch [18231/20000], Training Loss: 0.0262\n",
            "Epoch [18232/20000], Training Loss: 0.0282\n",
            "Epoch [18233/20000], Training Loss: 0.0284\n",
            "Epoch [18234/20000], Training Loss: 0.0256\n",
            "Epoch [18235/20000], Training Loss: 0.0271\n",
            "Epoch [18236/20000], Training Loss: 0.0252\n",
            "Epoch [18237/20000], Training Loss: 0.0268\n",
            "Epoch [18238/20000], Training Loss: 0.0254\n",
            "Epoch [18239/20000], Training Loss: 0.0260\n",
            "Epoch [18240/20000], Training Loss: 0.0262\n",
            "Epoch [18241/20000], Training Loss: 0.0258\n",
            "Epoch [18242/20000], Training Loss: 0.0266\n",
            "Epoch [18243/20000], Training Loss: 0.0270\n",
            "Epoch [18244/20000], Training Loss: 0.0263\n",
            "Epoch [18245/20000], Training Loss: 0.0268\n",
            "Epoch [18246/20000], Training Loss: 0.0274\n",
            "Epoch [18247/20000], Training Loss: 0.0267\n",
            "Epoch [18248/20000], Training Loss: 0.0264\n",
            "Epoch [18249/20000], Training Loss: 0.0267\n",
            "Epoch [18250/20000], Training Loss: 0.0271\n",
            "Epoch [18251/20000], Training Loss: 0.0260\n",
            "Epoch [18252/20000], Training Loss: 0.0249\n",
            "Epoch [18253/20000], Training Loss: 0.0251\n",
            "Epoch [18254/20000], Training Loss: 0.0298\n",
            "Epoch [18255/20000], Training Loss: 0.0281\n",
            "Epoch [18256/20000], Training Loss: 0.0259\n",
            "Epoch [18257/20000], Training Loss: 0.0238\n",
            "Epoch [18258/20000], Training Loss: 0.0255\n",
            "Epoch [18259/20000], Training Loss: 0.0268\n",
            "Epoch [18260/20000], Training Loss: 0.0276\n",
            "Epoch [18261/20000], Training Loss: 0.0251\n",
            "Epoch [18262/20000], Training Loss: 0.0262\n",
            "Epoch [18263/20000], Training Loss: 0.0257\n",
            "Epoch [18264/20000], Training Loss: 0.0266\n",
            "Epoch [18265/20000], Training Loss: 0.0266\n",
            "Epoch [18266/20000], Training Loss: 0.0254\n",
            "Epoch [18267/20000], Training Loss: 0.0244\n",
            "Epoch [18268/20000], Training Loss: 0.0252\n",
            "Epoch [18269/20000], Training Loss: 0.0264\n",
            "Epoch [18270/20000], Training Loss: 0.0253\n",
            "Epoch [18271/20000], Training Loss: 0.0264\n",
            "Epoch [18272/20000], Training Loss: 0.0273\n",
            "Epoch [18273/20000], Training Loss: 0.0274\n",
            "Epoch [18274/20000], Training Loss: 0.0245\n",
            "Epoch [18275/20000], Training Loss: 0.0248\n",
            "Epoch [18276/20000], Training Loss: 0.0257\n",
            "Epoch [18277/20000], Training Loss: 0.0275\n",
            "Epoch [18278/20000], Training Loss: 0.0240\n",
            "Epoch [18279/20000], Training Loss: 0.0254\n",
            "Epoch [18280/20000], Training Loss: 0.0264\n",
            "Epoch [18281/20000], Training Loss: 0.0262\n",
            "Epoch [18282/20000], Training Loss: 0.0279\n",
            "Epoch [18283/20000], Training Loss: 0.0282\n",
            "Epoch [18284/20000], Training Loss: 0.0245\n",
            "Epoch [18285/20000], Training Loss: 0.0287\n",
            "Epoch [18286/20000], Training Loss: 0.0259\n",
            "Epoch [18287/20000], Training Loss: 0.0252\n",
            "Epoch [18288/20000], Training Loss: 0.0265\n",
            "Epoch [18289/20000], Training Loss: 0.0277\n",
            "Epoch [18290/20000], Training Loss: 0.0246\n",
            "Epoch [18291/20000], Training Loss: 0.0256\n",
            "Epoch [18292/20000], Training Loss: 0.0272\n",
            "Epoch [18293/20000], Training Loss: 0.0289\n",
            "Epoch [18294/20000], Training Loss: 0.0252\n",
            "Epoch [18295/20000], Training Loss: 0.0269\n",
            "Epoch [18296/20000], Training Loss: 0.0281\n",
            "Epoch [18297/20000], Training Loss: 0.0276\n",
            "Epoch [18298/20000], Training Loss: 0.0257\n",
            "Epoch [18299/20000], Training Loss: 0.0253\n",
            "Epoch [18300/20000], Training Loss: 0.0243\n",
            "Epoch [18301/20000], Training Loss: 0.0250\n",
            "Epoch [18302/20000], Training Loss: 0.0263\n",
            "Epoch [18303/20000], Training Loss: 0.0280\n",
            "Epoch [18304/20000], Training Loss: 0.0258\n",
            "Epoch [18305/20000], Training Loss: 0.0287\n",
            "Epoch [18306/20000], Training Loss: 0.0269\n",
            "Epoch [18307/20000], Training Loss: 0.0252\n",
            "Epoch [18308/20000], Training Loss: 0.0261\n",
            "Epoch [18309/20000], Training Loss: 0.0274\n",
            "Epoch [18310/20000], Training Loss: 0.0276\n",
            "Epoch [18311/20000], Training Loss: 0.0262\n",
            "Epoch [18312/20000], Training Loss: 0.0253\n",
            "Epoch [18313/20000], Training Loss: 0.0298\n",
            "Epoch [18314/20000], Training Loss: 0.0280\n",
            "Epoch [18315/20000], Training Loss: 0.0247\n",
            "Epoch [18316/20000], Training Loss: 0.0248\n",
            "Epoch [18317/20000], Training Loss: 0.0260\n",
            "Epoch [18318/20000], Training Loss: 0.0255\n",
            "Epoch [18319/20000], Training Loss: 0.0270\n",
            "Epoch [18320/20000], Training Loss: 0.0249\n",
            "Epoch [18321/20000], Training Loss: 0.0242\n",
            "Epoch [18322/20000], Training Loss: 0.0257\n",
            "Epoch [18323/20000], Training Loss: 0.0280\n",
            "Epoch [18324/20000], Training Loss: 0.0280\n",
            "Epoch [18325/20000], Training Loss: 0.0267\n",
            "Epoch [18326/20000], Training Loss: 0.0273\n",
            "Epoch [18327/20000], Training Loss: 0.0269\n",
            "Epoch [18328/20000], Training Loss: 0.0293\n",
            "Epoch [18329/20000], Training Loss: 0.0286\n",
            "Epoch [18330/20000], Training Loss: 0.0273\n",
            "Epoch [18331/20000], Training Loss: 0.0268\n",
            "Epoch [18332/20000], Training Loss: 0.0254\n",
            "Epoch [18333/20000], Training Loss: 0.0253\n",
            "Epoch [18334/20000], Training Loss: 0.0274\n",
            "Epoch [18335/20000], Training Loss: 0.0255\n",
            "Epoch [18336/20000], Training Loss: 0.0251\n",
            "Epoch [18337/20000], Training Loss: 0.0269\n",
            "Epoch [18338/20000], Training Loss: 0.0265\n",
            "Epoch [18339/20000], Training Loss: 0.0282\n",
            "Epoch [18340/20000], Training Loss: 0.0251\n",
            "Epoch [18341/20000], Training Loss: 0.0265\n",
            "Epoch [18342/20000], Training Loss: 0.0263\n",
            "Epoch [18343/20000], Training Loss: 0.0263\n",
            "Epoch [18344/20000], Training Loss: 0.0249\n",
            "Epoch [18345/20000], Training Loss: 0.0262\n",
            "Epoch [18346/20000], Training Loss: 0.0267\n",
            "Epoch [18347/20000], Training Loss: 0.0277\n",
            "Epoch [18348/20000], Training Loss: 0.0276\n",
            "Epoch [18349/20000], Training Loss: 0.0253\n",
            "Epoch [18350/20000], Training Loss: 0.0269\n",
            "Epoch [18351/20000], Training Loss: 0.0275\n",
            "Epoch [18352/20000], Training Loss: 0.0255\n",
            "Epoch [18353/20000], Training Loss: 0.0276\n",
            "Epoch [18354/20000], Training Loss: 0.0263\n",
            "Epoch [18355/20000], Training Loss: 0.0275\n",
            "Epoch [18356/20000], Training Loss: 0.0256\n",
            "Epoch [18357/20000], Training Loss: 0.0253\n",
            "Epoch [18358/20000], Training Loss: 0.0256\n",
            "Epoch [18359/20000], Training Loss: 0.0263\n",
            "Epoch [18360/20000], Training Loss: 0.0261\n",
            "Epoch [18361/20000], Training Loss: 0.0279\n",
            "Epoch [18362/20000], Training Loss: 0.0246\n",
            "Epoch [18363/20000], Training Loss: 0.0261\n",
            "Epoch [18364/20000], Training Loss: 0.0270\n",
            "Epoch [18365/20000], Training Loss: 0.0260\n",
            "Epoch [18366/20000], Training Loss: 0.0245\n",
            "Epoch [18367/20000], Training Loss: 0.0275\n",
            "Epoch [18368/20000], Training Loss: 0.0280\n",
            "Epoch [18369/20000], Training Loss: 0.0282\n",
            "Epoch [18370/20000], Training Loss: 0.0284\n",
            "Epoch [18371/20000], Training Loss: 0.0262\n",
            "Epoch [18372/20000], Training Loss: 0.0274\n",
            "Epoch [18373/20000], Training Loss: 0.0238\n",
            "Epoch [18374/20000], Training Loss: 0.0278\n",
            "Epoch [18375/20000], Training Loss: 0.0276\n",
            "Epoch [18376/20000], Training Loss: 0.0266\n",
            "Epoch [18377/20000], Training Loss: 0.0297\n",
            "Epoch [18378/20000], Training Loss: 0.0269\n",
            "Epoch [18379/20000], Training Loss: 0.0272\n",
            "Epoch [18380/20000], Training Loss: 0.0263\n",
            "Epoch [18381/20000], Training Loss: 0.0265\n",
            "Epoch [18382/20000], Training Loss: 0.0262\n",
            "Epoch [18383/20000], Training Loss: 0.0252\n",
            "Epoch [18384/20000], Training Loss: 0.0263\n",
            "Epoch [18385/20000], Training Loss: 0.0269\n",
            "Epoch [18386/20000], Training Loss: 0.0268\n",
            "Epoch [18387/20000], Training Loss: 0.0261\n",
            "Epoch [18388/20000], Training Loss: 0.0267\n",
            "Epoch [18389/20000], Training Loss: 0.0249\n",
            "Epoch [18390/20000], Training Loss: 0.0282\n",
            "Epoch [18391/20000], Training Loss: 0.0257\n",
            "Epoch [18392/20000], Training Loss: 0.0271\n",
            "Epoch [18393/20000], Training Loss: 0.0251\n",
            "Epoch [18394/20000], Training Loss: 0.0240\n",
            "Epoch [18395/20000], Training Loss: 0.0275\n",
            "Epoch [18396/20000], Training Loss: 0.0256\n",
            "Epoch [18397/20000], Training Loss: 0.0288\n",
            "Epoch [18398/20000], Training Loss: 0.0270\n",
            "Epoch [18399/20000], Training Loss: 0.0261\n",
            "Epoch [18400/20000], Training Loss: 0.0240\n",
            "Epoch [18401/20000], Training Loss: 0.0270\n",
            "Epoch [18402/20000], Training Loss: 0.0258\n",
            "Epoch [18403/20000], Training Loss: 0.0251\n",
            "Epoch [18404/20000], Training Loss: 0.0261\n",
            "Epoch [18405/20000], Training Loss: 0.0264\n",
            "Epoch [18406/20000], Training Loss: 0.0246\n",
            "Epoch [18407/20000], Training Loss: 0.0282\n",
            "Epoch [18408/20000], Training Loss: 0.0260\n",
            "Epoch [18409/20000], Training Loss: 0.0264\n",
            "Epoch [18410/20000], Training Loss: 0.0277\n",
            "Epoch [18411/20000], Training Loss: 0.0282\n",
            "Epoch [18412/20000], Training Loss: 0.0293\n",
            "Epoch [18413/20000], Training Loss: 0.0265\n",
            "Epoch [18414/20000], Training Loss: 0.0262\n",
            "Epoch [18415/20000], Training Loss: 0.0270\n",
            "Epoch [18416/20000], Training Loss: 0.0276\n",
            "Epoch [18417/20000], Training Loss: 0.0279\n",
            "Epoch [18418/20000], Training Loss: 0.0245\n",
            "Epoch [18419/20000], Training Loss: 0.0267\n",
            "Epoch [18420/20000], Training Loss: 0.0273\n",
            "Epoch [18421/20000], Training Loss: 0.0271\n",
            "Epoch [18422/20000], Training Loss: 0.0251\n",
            "Epoch [18423/20000], Training Loss: 0.0262\n",
            "Epoch [18424/20000], Training Loss: 0.0268\n",
            "Epoch [18425/20000], Training Loss: 0.0268\n",
            "Epoch [18426/20000], Training Loss: 0.0267\n",
            "Epoch [18427/20000], Training Loss: 0.0254\n",
            "Epoch [18428/20000], Training Loss: 0.0281\n",
            "Epoch [18429/20000], Training Loss: 0.0274\n",
            "Epoch [18430/20000], Training Loss: 0.0250\n",
            "Epoch [18431/20000], Training Loss: 0.0249\n",
            "Epoch [18432/20000], Training Loss: 0.0275\n",
            "Epoch [18433/20000], Training Loss: 0.0246\n",
            "Epoch [18434/20000], Training Loss: 0.0254\n",
            "Epoch [18435/20000], Training Loss: 0.0274\n",
            "Epoch [18436/20000], Training Loss: 0.0265\n",
            "Epoch [18437/20000], Training Loss: 0.0250\n",
            "Epoch [18438/20000], Training Loss: 0.0259\n",
            "Epoch [18439/20000], Training Loss: 0.0261\n",
            "Epoch [18440/20000], Training Loss: 0.0261\n",
            "Epoch [18441/20000], Training Loss: 0.0290\n",
            "Epoch [18442/20000], Training Loss: 0.0290\n",
            "Epoch [18443/20000], Training Loss: 0.0255\n",
            "Epoch [18444/20000], Training Loss: 0.0266\n",
            "Epoch [18445/20000], Training Loss: 0.0248\n",
            "Epoch [18446/20000], Training Loss: 0.0259\n",
            "Epoch [18447/20000], Training Loss: 0.0293\n",
            "Epoch [18448/20000], Training Loss: 0.0260\n",
            "Epoch [18449/20000], Training Loss: 0.0286\n",
            "Epoch [18450/20000], Training Loss: 0.0274\n",
            "Epoch [18451/20000], Training Loss: 0.0272\n",
            "Epoch [18452/20000], Training Loss: 0.0278\n",
            "Epoch [18453/20000], Training Loss: 0.0277\n",
            "Epoch [18454/20000], Training Loss: 0.0274\n",
            "Epoch [18455/20000], Training Loss: 0.0260\n",
            "Epoch [18456/20000], Training Loss: 0.0255\n",
            "Epoch [18457/20000], Training Loss: 0.0278\n",
            "Epoch [18458/20000], Training Loss: 0.0266\n",
            "Epoch [18459/20000], Training Loss: 0.0262\n",
            "Epoch [18460/20000], Training Loss: 0.0262\n",
            "Epoch [18461/20000], Training Loss: 0.0271\n",
            "Epoch [18462/20000], Training Loss: 0.0247\n",
            "Epoch [18463/20000], Training Loss: 0.0253\n",
            "Epoch [18464/20000], Training Loss: 0.0274\n",
            "Epoch [18465/20000], Training Loss: 0.0253\n",
            "Epoch [18466/20000], Training Loss: 0.0244\n",
            "Epoch [18467/20000], Training Loss: 0.0259\n",
            "Epoch [18468/20000], Training Loss: 0.0248\n",
            "Epoch [18469/20000], Training Loss: 0.0269\n",
            "Epoch [18470/20000], Training Loss: 0.0265\n",
            "Epoch [18471/20000], Training Loss: 0.0288\n",
            "Epoch [18472/20000], Training Loss: 0.0263\n",
            "Epoch [18473/20000], Training Loss: 0.0265\n",
            "Epoch [18474/20000], Training Loss: 0.0272\n",
            "Epoch [18475/20000], Training Loss: 0.0258\n",
            "Epoch [18476/20000], Training Loss: 0.0270\n",
            "Epoch [18477/20000], Training Loss: 0.0255\n",
            "Epoch [18478/20000], Training Loss: 0.0287\n",
            "Epoch [18479/20000], Training Loss: 0.0248\n",
            "Epoch [18480/20000], Training Loss: 0.0258\n",
            "Epoch [18481/20000], Training Loss: 0.0262\n",
            "Epoch [18482/20000], Training Loss: 0.0251\n",
            "Epoch [18483/20000], Training Loss: 0.0279\n",
            "Epoch [18484/20000], Training Loss: 0.0276\n",
            "Epoch [18485/20000], Training Loss: 0.0265\n",
            "Epoch [18486/20000], Training Loss: 0.0289\n",
            "Epoch [18487/20000], Training Loss: 0.0268\n",
            "Epoch [18488/20000], Training Loss: 0.0250\n",
            "Epoch [18489/20000], Training Loss: 0.0248\n",
            "Epoch [18490/20000], Training Loss: 0.0260\n",
            "Epoch [18491/20000], Training Loss: 0.0252\n",
            "Epoch [18492/20000], Training Loss: 0.0257\n",
            "Epoch [18493/20000], Training Loss: 0.0252\n",
            "Epoch [18494/20000], Training Loss: 0.0283\n",
            "Epoch [18495/20000], Training Loss: 0.0268\n",
            "Epoch [18496/20000], Training Loss: 0.0259\n",
            "Epoch [18497/20000], Training Loss: 0.0262\n",
            "Epoch [18498/20000], Training Loss: 0.0283\n",
            "Epoch [18499/20000], Training Loss: 0.0247\n",
            "Epoch [18500/20000], Training Loss: 0.0283\n",
            "Epoch [18501/20000], Training Loss: 0.0261\n",
            "Epoch [18502/20000], Training Loss: 0.0283\n",
            "Epoch [18503/20000], Training Loss: 0.0279\n",
            "Epoch [18504/20000], Training Loss: 0.0273\n",
            "Epoch [18505/20000], Training Loss: 0.0244\n",
            "Epoch [18506/20000], Training Loss: 0.0249\n",
            "Epoch [18507/20000], Training Loss: 0.0279\n",
            "Epoch [18508/20000], Training Loss: 0.0267\n",
            "Epoch [18509/20000], Training Loss: 0.0273\n",
            "Epoch [18510/20000], Training Loss: 0.0239\n",
            "Epoch [18511/20000], Training Loss: 0.0273\n",
            "Epoch [18512/20000], Training Loss: 0.0274\n",
            "Epoch [18513/20000], Training Loss: 0.0271\n",
            "Epoch [18514/20000], Training Loss: 0.0267\n",
            "Epoch [18515/20000], Training Loss: 0.0249\n",
            "Epoch [18516/20000], Training Loss: 0.0300\n",
            "Epoch [18517/20000], Training Loss: 0.0260\n",
            "Epoch [18518/20000], Training Loss: 0.0275\n",
            "Epoch [18519/20000], Training Loss: 0.0250\n",
            "Epoch [18520/20000], Training Loss: 0.0277\n",
            "Epoch [18521/20000], Training Loss: 0.0259\n",
            "Epoch [18522/20000], Training Loss: 0.0287\n",
            "Epoch [18523/20000], Training Loss: 0.0265\n",
            "Epoch [18524/20000], Training Loss: 0.0270\n",
            "Epoch [18525/20000], Training Loss: 0.0266\n",
            "Epoch [18526/20000], Training Loss: 0.0281\n",
            "Epoch [18527/20000], Training Loss: 0.0257\n",
            "Epoch [18528/20000], Training Loss: 0.0258\n",
            "Epoch [18529/20000], Training Loss: 0.0262\n",
            "Epoch [18530/20000], Training Loss: 0.0262\n",
            "Epoch [18531/20000], Training Loss: 0.0268\n",
            "Epoch [18532/20000], Training Loss: 0.0280\n",
            "Epoch [18533/20000], Training Loss: 0.0263\n",
            "Epoch [18534/20000], Training Loss: 0.0261\n",
            "Epoch [18535/20000], Training Loss: 0.0278\n",
            "Epoch [18536/20000], Training Loss: 0.0267\n",
            "Epoch [18537/20000], Training Loss: 0.0265\n",
            "Epoch [18538/20000], Training Loss: 0.0262\n",
            "Epoch [18539/20000], Training Loss: 0.0268\n",
            "Epoch [18540/20000], Training Loss: 0.0270\n",
            "Epoch [18541/20000], Training Loss: 0.0259\n",
            "Epoch [18542/20000], Training Loss: 0.0275\n",
            "Epoch [18543/20000], Training Loss: 0.0274\n",
            "Epoch [18544/20000], Training Loss: 0.0286\n",
            "Epoch [18545/20000], Training Loss: 0.0260\n",
            "Epoch [18546/20000], Training Loss: 0.0278\n",
            "Epoch [18547/20000], Training Loss: 0.0253\n",
            "Epoch [18548/20000], Training Loss: 0.0271\n",
            "Epoch [18549/20000], Training Loss: 0.0276\n",
            "Epoch [18550/20000], Training Loss: 0.0263\n",
            "Epoch [18551/20000], Training Loss: 0.0286\n",
            "Epoch [18552/20000], Training Loss: 0.0253\n",
            "Epoch [18553/20000], Training Loss: 0.0269\n",
            "Epoch [18554/20000], Training Loss: 0.0259\n",
            "Epoch [18555/20000], Training Loss: 0.0247\n",
            "Epoch [18556/20000], Training Loss: 0.0261\n",
            "Epoch [18557/20000], Training Loss: 0.0255\n",
            "Epoch [18558/20000], Training Loss: 0.0253\n",
            "Epoch [18559/20000], Training Loss: 0.0271\n",
            "Epoch [18560/20000], Training Loss: 0.0295\n",
            "Epoch [18561/20000], Training Loss: 0.0251\n",
            "Epoch [18562/20000], Training Loss: 0.0272\n",
            "Epoch [18563/20000], Training Loss: 0.0279\n",
            "Epoch [18564/20000], Training Loss: 0.0264\n",
            "Epoch [18565/20000], Training Loss: 0.0243\n",
            "Epoch [18566/20000], Training Loss: 0.0272\n",
            "Epoch [18567/20000], Training Loss: 0.0261\n",
            "Epoch [18568/20000], Training Loss: 0.0267\n",
            "Epoch [18569/20000], Training Loss: 0.0267\n",
            "Epoch [18570/20000], Training Loss: 0.0290\n",
            "Epoch [18571/20000], Training Loss: 0.0281\n",
            "Epoch [18572/20000], Training Loss: 0.0279\n",
            "Epoch [18573/20000], Training Loss: 0.0270\n",
            "Epoch [18574/20000], Training Loss: 0.0253\n",
            "Epoch [18575/20000], Training Loss: 0.0259\n",
            "Epoch [18576/20000], Training Loss: 0.0233\n",
            "Epoch [18577/20000], Training Loss: 0.0260\n",
            "Epoch [18578/20000], Training Loss: 0.0268\n",
            "Epoch [18579/20000], Training Loss: 0.0252\n",
            "Epoch [18580/20000], Training Loss: 0.0269\n",
            "Epoch [18581/20000], Training Loss: 0.0289\n",
            "Epoch [18582/20000], Training Loss: 0.0264\n",
            "Epoch [18583/20000], Training Loss: 0.0255\n",
            "Epoch [18584/20000], Training Loss: 0.0260\n",
            "Epoch [18585/20000], Training Loss: 0.0274\n",
            "Epoch [18586/20000], Training Loss: 0.0298\n",
            "Epoch [18587/20000], Training Loss: 0.0273\n",
            "Epoch [18588/20000], Training Loss: 0.0247\n",
            "Epoch [18589/20000], Training Loss: 0.0248\n",
            "Epoch [18590/20000], Training Loss: 0.0252\n",
            "Epoch [18591/20000], Training Loss: 0.0268\n",
            "Epoch [18592/20000], Training Loss: 0.0269\n",
            "Epoch [18593/20000], Training Loss: 0.0265\n",
            "Epoch [18594/20000], Training Loss: 0.0249\n",
            "Epoch [18595/20000], Training Loss: 0.0259\n",
            "Epoch [18596/20000], Training Loss: 0.0265\n",
            "Epoch [18597/20000], Training Loss: 0.0264\n",
            "Epoch [18598/20000], Training Loss: 0.0289\n",
            "Epoch [18599/20000], Training Loss: 0.0284\n",
            "Epoch [18600/20000], Training Loss: 0.0253\n",
            "Epoch [18601/20000], Training Loss: 0.0266\n",
            "Epoch [18602/20000], Training Loss: 0.0257\n",
            "Epoch [18603/20000], Training Loss: 0.0269\n",
            "Epoch [18604/20000], Training Loss: 0.0270\n",
            "Epoch [18605/20000], Training Loss: 0.0278\n",
            "Epoch [18606/20000], Training Loss: 0.0255\n",
            "Epoch [18607/20000], Training Loss: 0.0267\n",
            "Epoch [18608/20000], Training Loss: 0.0263\n",
            "Epoch [18609/20000], Training Loss: 0.0258\n",
            "Epoch [18610/20000], Training Loss: 0.0282\n",
            "Epoch [18611/20000], Training Loss: 0.0250\n",
            "Epoch [18612/20000], Training Loss: 0.0248\n",
            "Epoch [18613/20000], Training Loss: 0.0284\n",
            "Epoch [18614/20000], Training Loss: 0.0259\n",
            "Epoch [18615/20000], Training Loss: 0.0259\n",
            "Epoch [18616/20000], Training Loss: 0.0266\n",
            "Epoch [18617/20000], Training Loss: 0.0287\n",
            "Epoch [18618/20000], Training Loss: 0.0254\n",
            "Epoch [18619/20000], Training Loss: 0.0260\n",
            "Epoch [18620/20000], Training Loss: 0.0280\n",
            "Epoch [18621/20000], Training Loss: 0.0270\n",
            "Epoch [18622/20000], Training Loss: 0.0252\n",
            "Epoch [18623/20000], Training Loss: 0.0254\n",
            "Epoch [18624/20000], Training Loss: 0.0263\n",
            "Epoch [18625/20000], Training Loss: 0.0271\n",
            "Epoch [18626/20000], Training Loss: 0.0247\n",
            "Epoch [18627/20000], Training Loss: 0.0253\n",
            "Epoch [18628/20000], Training Loss: 0.0247\n",
            "Epoch [18629/20000], Training Loss: 0.0253\n",
            "Epoch [18630/20000], Training Loss: 0.0257\n",
            "Epoch [18631/20000], Training Loss: 0.0254\n",
            "Epoch [18632/20000], Training Loss: 0.0260\n",
            "Epoch [18633/20000], Training Loss: 0.0264\n",
            "Epoch [18634/20000], Training Loss: 0.0252\n",
            "Epoch [18635/20000], Training Loss: 0.0280\n",
            "Epoch [18636/20000], Training Loss: 0.0266\n",
            "Epoch [18637/20000], Training Loss: 0.0271\n",
            "Epoch [18638/20000], Training Loss: 0.0250\n",
            "Epoch [18639/20000], Training Loss: 0.0262\n",
            "Epoch [18640/20000], Training Loss: 0.0277\n",
            "Epoch [18641/20000], Training Loss: 0.0259\n",
            "Epoch [18642/20000], Training Loss: 0.0268\n",
            "Epoch [18643/20000], Training Loss: 0.0284\n",
            "Epoch [18644/20000], Training Loss: 0.0250\n",
            "Epoch [18645/20000], Training Loss: 0.0265\n",
            "Epoch [18646/20000], Training Loss: 0.0275\n",
            "Epoch [18647/20000], Training Loss: 0.0284\n",
            "Epoch [18648/20000], Training Loss: 0.0284\n",
            "Epoch [18649/20000], Training Loss: 0.0248\n",
            "Epoch [18650/20000], Training Loss: 0.0258\n",
            "Epoch [18651/20000], Training Loss: 0.0249\n",
            "Epoch [18652/20000], Training Loss: 0.0278\n",
            "Epoch [18653/20000], Training Loss: 0.0288\n",
            "Epoch [18654/20000], Training Loss: 0.0254\n",
            "Epoch [18655/20000], Training Loss: 0.0256\n",
            "Epoch [18656/20000], Training Loss: 0.0281\n",
            "Epoch [18657/20000], Training Loss: 0.0268\n",
            "Epoch [18658/20000], Training Loss: 0.0265\n",
            "Epoch [18659/20000], Training Loss: 0.0265\n",
            "Epoch [18660/20000], Training Loss: 0.0262\n",
            "Epoch [18661/20000], Training Loss: 0.0272\n",
            "Epoch [18662/20000], Training Loss: 0.0247\n",
            "Epoch [18663/20000], Training Loss: 0.0246\n",
            "Epoch [18664/20000], Training Loss: 0.0259\n",
            "Epoch [18665/20000], Training Loss: 0.0271\n",
            "Epoch [18666/20000], Training Loss: 0.0264\n",
            "Epoch [18667/20000], Training Loss: 0.0243\n",
            "Epoch [18668/20000], Training Loss: 0.0264\n",
            "Epoch [18669/20000], Training Loss: 0.0263\n",
            "Epoch [18670/20000], Training Loss: 0.0276\n",
            "Epoch [18671/20000], Training Loss: 0.0272\n",
            "Epoch [18672/20000], Training Loss: 0.0267\n",
            "Epoch [18673/20000], Training Loss: 0.0272\n",
            "Epoch [18674/20000], Training Loss: 0.0249\n",
            "Epoch [18675/20000], Training Loss: 0.0263\n",
            "Epoch [18676/20000], Training Loss: 0.0271\n",
            "Epoch [18677/20000], Training Loss: 0.0285\n",
            "Epoch [18678/20000], Training Loss: 0.0256\n",
            "Epoch [18679/20000], Training Loss: 0.0263\n",
            "Epoch [18680/20000], Training Loss: 0.0246\n",
            "Epoch [18681/20000], Training Loss: 0.0276\n",
            "Epoch [18682/20000], Training Loss: 0.0272\n",
            "Epoch [18683/20000], Training Loss: 0.0277\n",
            "Epoch [18684/20000], Training Loss: 0.0250\n",
            "Epoch [18685/20000], Training Loss: 0.0255\n",
            "Epoch [18686/20000], Training Loss: 0.0276\n",
            "Epoch [18687/20000], Training Loss: 0.0257\n",
            "Epoch [18688/20000], Training Loss: 0.0265\n",
            "Epoch [18689/20000], Training Loss: 0.0266\n",
            "Epoch [18690/20000], Training Loss: 0.0273\n",
            "Epoch [18691/20000], Training Loss: 0.0278\n",
            "Epoch [18692/20000], Training Loss: 0.0263\n",
            "Epoch [18693/20000], Training Loss: 0.0255\n",
            "Epoch [18694/20000], Training Loss: 0.0256\n",
            "Epoch [18695/20000], Training Loss: 0.0251\n",
            "Epoch [18696/20000], Training Loss: 0.0256\n",
            "Epoch [18697/20000], Training Loss: 0.0281\n",
            "Epoch [18698/20000], Training Loss: 0.0272\n",
            "Epoch [18699/20000], Training Loss: 0.0256\n",
            "Epoch [18700/20000], Training Loss: 0.0247\n",
            "Epoch [18701/20000], Training Loss: 0.0272\n",
            "Epoch [18702/20000], Training Loss: 0.0255\n",
            "Epoch [18703/20000], Training Loss: 0.0257\n",
            "Epoch [18704/20000], Training Loss: 0.0294\n",
            "Epoch [18705/20000], Training Loss: 0.0273\n",
            "Epoch [18706/20000], Training Loss: 0.0269\n",
            "Epoch [18707/20000], Training Loss: 0.0258\n",
            "Epoch [18708/20000], Training Loss: 0.0272\n",
            "Epoch [18709/20000], Training Loss: 0.0236\n",
            "Epoch [18710/20000], Training Loss: 0.0263\n",
            "Epoch [18711/20000], Training Loss: 0.0259\n",
            "Epoch [18712/20000], Training Loss: 0.0253\n",
            "Epoch [18713/20000], Training Loss: 0.0285\n",
            "Epoch [18714/20000], Training Loss: 0.0252\n",
            "Epoch [18715/20000], Training Loss: 0.0274\n",
            "Epoch [18716/20000], Training Loss: 0.0272\n",
            "Epoch [18717/20000], Training Loss: 0.0275\n",
            "Epoch [18718/20000], Training Loss: 0.0273\n",
            "Epoch [18719/20000], Training Loss: 0.0267\n",
            "Epoch [18720/20000], Training Loss: 0.0274\n",
            "Epoch [18721/20000], Training Loss: 0.0252\n",
            "Epoch [18722/20000], Training Loss: 0.0260\n",
            "Epoch [18723/20000], Training Loss: 0.0253\n",
            "Epoch [18724/20000], Training Loss: 0.0267\n",
            "Epoch [18725/20000], Training Loss: 0.0276\n",
            "Epoch [18726/20000], Training Loss: 0.0267\n",
            "Epoch [18727/20000], Training Loss: 0.0259\n",
            "Epoch [18728/20000], Training Loss: 0.0263\n",
            "Epoch [18729/20000], Training Loss: 0.0270\n",
            "Epoch [18730/20000], Training Loss: 0.0280\n",
            "Epoch [18731/20000], Training Loss: 0.0264\n",
            "Epoch [18732/20000], Training Loss: 0.0266\n",
            "Epoch [18733/20000], Training Loss: 0.0260\n",
            "Epoch [18734/20000], Training Loss: 0.0266\n",
            "Epoch [18735/20000], Training Loss: 0.0274\n",
            "Epoch [18736/20000], Training Loss: 0.0268\n",
            "Epoch [18737/20000], Training Loss: 0.0268\n",
            "Epoch [18738/20000], Training Loss: 0.0258\n",
            "Epoch [18739/20000], Training Loss: 0.0267\n",
            "Epoch [18740/20000], Training Loss: 0.0241\n",
            "Epoch [18741/20000], Training Loss: 0.0273\n",
            "Epoch [18742/20000], Training Loss: 0.0267\n",
            "Epoch [18743/20000], Training Loss: 0.0278\n",
            "Epoch [18744/20000], Training Loss: 0.0261\n",
            "Epoch [18745/20000], Training Loss: 0.0284\n",
            "Epoch [18746/20000], Training Loss: 0.0267\n",
            "Epoch [18747/20000], Training Loss: 0.0258\n",
            "Epoch [18748/20000], Training Loss: 0.0247\n",
            "Epoch [18749/20000], Training Loss: 0.0249\n",
            "Epoch [18750/20000], Training Loss: 0.0269\n",
            "Epoch [18751/20000], Training Loss: 0.0263\n",
            "Epoch [18752/20000], Training Loss: 0.0280\n",
            "Epoch [18753/20000], Training Loss: 0.0285\n",
            "Epoch [18754/20000], Training Loss: 0.0264\n",
            "Epoch [18755/20000], Training Loss: 0.0242\n",
            "Epoch [18756/20000], Training Loss: 0.0261\n",
            "Epoch [18757/20000], Training Loss: 0.0273\n",
            "Epoch [18758/20000], Training Loss: 0.0292\n",
            "Epoch [18759/20000], Training Loss: 0.0277\n",
            "Epoch [18760/20000], Training Loss: 0.0260\n",
            "Epoch [18761/20000], Training Loss: 0.0246\n",
            "Epoch [18762/20000], Training Loss: 0.0262\n",
            "Epoch [18763/20000], Training Loss: 0.0242\n",
            "Epoch [18764/20000], Training Loss: 0.0257\n",
            "Epoch [18765/20000], Training Loss: 0.0268\n",
            "Epoch [18766/20000], Training Loss: 0.0268\n",
            "Epoch [18767/20000], Training Loss: 0.0263\n",
            "Epoch [18768/20000], Training Loss: 0.0271\n",
            "Epoch [18769/20000], Training Loss: 0.0272\n",
            "Epoch [18770/20000], Training Loss: 0.0286\n",
            "Epoch [18771/20000], Training Loss: 0.0267\n",
            "Epoch [18772/20000], Training Loss: 0.0269\n",
            "Epoch [18773/20000], Training Loss: 0.0252\n",
            "Epoch [18774/20000], Training Loss: 0.0270\n",
            "Epoch [18775/20000], Training Loss: 0.0261\n",
            "Epoch [18776/20000], Training Loss: 0.0276\n",
            "Epoch [18777/20000], Training Loss: 0.0251\n",
            "Epoch [18778/20000], Training Loss: 0.0261\n",
            "Epoch [18779/20000], Training Loss: 0.0261\n",
            "Epoch [18780/20000], Training Loss: 0.0251\n",
            "Epoch [18781/20000], Training Loss: 0.0282\n",
            "Epoch [18782/20000], Training Loss: 0.0276\n",
            "Epoch [18783/20000], Training Loss: 0.0278\n",
            "Epoch [18784/20000], Training Loss: 0.0243\n",
            "Epoch [18785/20000], Training Loss: 0.0285\n",
            "Epoch [18786/20000], Training Loss: 0.0277\n",
            "Epoch [18787/20000], Training Loss: 0.0260\n",
            "Epoch [18788/20000], Training Loss: 0.0271\n",
            "Epoch [18789/20000], Training Loss: 0.0244\n",
            "Epoch [18790/20000], Training Loss: 0.0299\n",
            "Epoch [18791/20000], Training Loss: 0.0253\n",
            "Epoch [18792/20000], Training Loss: 0.0256\n",
            "Epoch [18793/20000], Training Loss: 0.0264\n",
            "Epoch [18794/20000], Training Loss: 0.0284\n",
            "Epoch [18795/20000], Training Loss: 0.0262\n",
            "Epoch [18796/20000], Training Loss: 0.0243\n",
            "Epoch [18797/20000], Training Loss: 0.0257\n",
            "Epoch [18798/20000], Training Loss: 0.0276\n",
            "Epoch [18799/20000], Training Loss: 0.0278\n",
            "Epoch [18800/20000], Training Loss: 0.0249\n",
            "Epoch [18801/20000], Training Loss: 0.0258\n",
            "Epoch [18802/20000], Training Loss: 0.0263\n",
            "Epoch [18803/20000], Training Loss: 0.0262\n",
            "Epoch [18804/20000], Training Loss: 0.0267\n",
            "Epoch [18805/20000], Training Loss: 0.0249\n",
            "Epoch [18806/20000], Training Loss: 0.0257\n",
            "Epoch [18807/20000], Training Loss: 0.0264\n",
            "Epoch [18808/20000], Training Loss: 0.0243\n",
            "Epoch [18809/20000], Training Loss: 0.0246\n",
            "Epoch [18810/20000], Training Loss: 0.0265\n",
            "Epoch [18811/20000], Training Loss: 0.0278\n",
            "Epoch [18812/20000], Training Loss: 0.0263\n",
            "Epoch [18813/20000], Training Loss: 0.0279\n",
            "Epoch [18814/20000], Training Loss: 0.0241\n",
            "Epoch [18815/20000], Training Loss: 0.0277\n",
            "Epoch [18816/20000], Training Loss: 0.0257\n",
            "Epoch [18817/20000], Training Loss: 0.0274\n",
            "Epoch [18818/20000], Training Loss: 0.0268\n",
            "Epoch [18819/20000], Training Loss: 0.0258\n",
            "Epoch [18820/20000], Training Loss: 0.0264\n",
            "Epoch [18821/20000], Training Loss: 0.0292\n",
            "Epoch [18822/20000], Training Loss: 0.0258\n",
            "Epoch [18823/20000], Training Loss: 0.0248\n",
            "Epoch [18824/20000], Training Loss: 0.0262\n",
            "Epoch [18825/20000], Training Loss: 0.0263\n",
            "Epoch [18826/20000], Training Loss: 0.0269\n",
            "Epoch [18827/20000], Training Loss: 0.0258\n",
            "Epoch [18828/20000], Training Loss: 0.0247\n",
            "Epoch [18829/20000], Training Loss: 0.0247\n",
            "Epoch [18830/20000], Training Loss: 0.0275\n",
            "Epoch [18831/20000], Training Loss: 0.0257\n",
            "Epoch [18832/20000], Training Loss: 0.0281\n",
            "Epoch [18833/20000], Training Loss: 0.0272\n",
            "Epoch [18834/20000], Training Loss: 0.0273\n",
            "Epoch [18835/20000], Training Loss: 0.0283\n",
            "Epoch [18836/20000], Training Loss: 0.0253\n",
            "Epoch [18837/20000], Training Loss: 0.0277\n",
            "Epoch [18838/20000], Training Loss: 0.0283\n",
            "Epoch [18839/20000], Training Loss: 0.0253\n",
            "Epoch [18840/20000], Training Loss: 0.0246\n",
            "Epoch [18841/20000], Training Loss: 0.0258\n",
            "Epoch [18842/20000], Training Loss: 0.0253\n",
            "Epoch [18843/20000], Training Loss: 0.0266\n",
            "Epoch [18844/20000], Training Loss: 0.0242\n",
            "Epoch [18845/20000], Training Loss: 0.0285\n",
            "Epoch [18846/20000], Training Loss: 0.0261\n",
            "Epoch [18847/20000], Training Loss: 0.0274\n",
            "Epoch [18848/20000], Training Loss: 0.0259\n",
            "Epoch [18849/20000], Training Loss: 0.0252\n",
            "Epoch [18850/20000], Training Loss: 0.0275\n",
            "Epoch [18851/20000], Training Loss: 0.0265\n",
            "Epoch [18852/20000], Training Loss: 0.0257\n",
            "Epoch [18853/20000], Training Loss: 0.0271\n",
            "Epoch [18854/20000], Training Loss: 0.0272\n",
            "Epoch [18855/20000], Training Loss: 0.0264\n",
            "Epoch [18856/20000], Training Loss: 0.0269\n",
            "Epoch [18857/20000], Training Loss: 0.0266\n",
            "Epoch [18858/20000], Training Loss: 0.0253\n",
            "Epoch [18859/20000], Training Loss: 0.0252\n",
            "Epoch [18860/20000], Training Loss: 0.0256\n",
            "Epoch [18861/20000], Training Loss: 0.0265\n",
            "Epoch [18862/20000], Training Loss: 0.0281\n",
            "Epoch [18863/20000], Training Loss: 0.0258\n",
            "Epoch [18864/20000], Training Loss: 0.0274\n",
            "Epoch [18865/20000], Training Loss: 0.0267\n",
            "Epoch [18866/20000], Training Loss: 0.0281\n",
            "Epoch [18867/20000], Training Loss: 0.0277\n",
            "Epoch [18868/20000], Training Loss: 0.0285\n",
            "Epoch [18869/20000], Training Loss: 0.0282\n",
            "Epoch [18870/20000], Training Loss: 0.0282\n",
            "Epoch [18871/20000], Training Loss: 0.0269\n",
            "Epoch [18872/20000], Training Loss: 0.0287\n",
            "Epoch [18873/20000], Training Loss: 0.0265\n",
            "Epoch [18874/20000], Training Loss: 0.0251\n",
            "Epoch [18875/20000], Training Loss: 0.0264\n",
            "Epoch [18876/20000], Training Loss: 0.0249\n",
            "Epoch [18877/20000], Training Loss: 0.0283\n",
            "Epoch [18878/20000], Training Loss: 0.0274\n",
            "Epoch [18879/20000], Training Loss: 0.0277\n",
            "Epoch [18880/20000], Training Loss: 0.0267\n",
            "Epoch [18881/20000], Training Loss: 0.0270\n",
            "Epoch [18882/20000], Training Loss: 0.0256\n",
            "Epoch [18883/20000], Training Loss: 0.0257\n",
            "Epoch [18884/20000], Training Loss: 0.0260\n",
            "Epoch [18885/20000], Training Loss: 0.0262\n",
            "Epoch [18886/20000], Training Loss: 0.0263\n",
            "Epoch [18887/20000], Training Loss: 0.0258\n",
            "Epoch [18888/20000], Training Loss: 0.0253\n",
            "Epoch [18889/20000], Training Loss: 0.0263\n",
            "Epoch [18890/20000], Training Loss: 0.0281\n",
            "Epoch [18891/20000], Training Loss: 0.0255\n",
            "Epoch [18892/20000], Training Loss: 0.0252\n",
            "Epoch [18893/20000], Training Loss: 0.0266\n",
            "Epoch [18894/20000], Training Loss: 0.0259\n",
            "Epoch [18895/20000], Training Loss: 0.0284\n",
            "Epoch [18896/20000], Training Loss: 0.0277\n",
            "Epoch [18897/20000], Training Loss: 0.0266\n",
            "Epoch [18898/20000], Training Loss: 0.0253\n",
            "Epoch [18899/20000], Training Loss: 0.0254\n",
            "Epoch [18900/20000], Training Loss: 0.0273\n",
            "Epoch [18901/20000], Training Loss: 0.0252\n",
            "Epoch [18902/20000], Training Loss: 0.0290\n",
            "Epoch [18903/20000], Training Loss: 0.0261\n",
            "Epoch [18904/20000], Training Loss: 0.0282\n",
            "Epoch [18905/20000], Training Loss: 0.0256\n",
            "Epoch [18906/20000], Training Loss: 0.0258\n",
            "Epoch [18907/20000], Training Loss: 0.0258\n",
            "Epoch [18908/20000], Training Loss: 0.0284\n",
            "Epoch [18909/20000], Training Loss: 0.0249\n",
            "Epoch [18910/20000], Training Loss: 0.0260\n",
            "Epoch [18911/20000], Training Loss: 0.0272\n",
            "Epoch [18912/20000], Training Loss: 0.0281\n",
            "Epoch [18913/20000], Training Loss: 0.0288\n",
            "Epoch [18914/20000], Training Loss: 0.0265\n",
            "Epoch [18915/20000], Training Loss: 0.0281\n",
            "Epoch [18916/20000], Training Loss: 0.0251\n",
            "Epoch [18917/20000], Training Loss: 0.0272\n",
            "Epoch [18918/20000], Training Loss: 0.0260\n",
            "Epoch [18919/20000], Training Loss: 0.0267\n",
            "Epoch [18920/20000], Training Loss: 0.0258\n",
            "Epoch [18921/20000], Training Loss: 0.0288\n",
            "Epoch [18922/20000], Training Loss: 0.0259\n",
            "Epoch [18923/20000], Training Loss: 0.0260\n",
            "Epoch [18924/20000], Training Loss: 0.0245\n",
            "Epoch [18925/20000], Training Loss: 0.0251\n",
            "Epoch [18926/20000], Training Loss: 0.0261\n",
            "Epoch [18927/20000], Training Loss: 0.0271\n",
            "Epoch [18928/20000], Training Loss: 0.0273\n",
            "Epoch [18929/20000], Training Loss: 0.0268\n",
            "Epoch [18930/20000], Training Loss: 0.0299\n",
            "Epoch [18931/20000], Training Loss: 0.0273\n",
            "Epoch [18932/20000], Training Loss: 0.0267\n",
            "Epoch [18933/20000], Training Loss: 0.0275\n",
            "Epoch [18934/20000], Training Loss: 0.0284\n",
            "Epoch [18935/20000], Training Loss: 0.0289\n",
            "Epoch [18936/20000], Training Loss: 0.0268\n",
            "Epoch [18937/20000], Training Loss: 0.0250\n",
            "Epoch [18938/20000], Training Loss: 0.0254\n",
            "Epoch [18939/20000], Training Loss: 0.0248\n",
            "Epoch [18940/20000], Training Loss: 0.0280\n",
            "Epoch [18941/20000], Training Loss: 0.0263\n",
            "Epoch [18942/20000], Training Loss: 0.0247\n",
            "Epoch [18943/20000], Training Loss: 0.0276\n",
            "Epoch [18944/20000], Training Loss: 0.0258\n",
            "Epoch [18945/20000], Training Loss: 0.0261\n",
            "Epoch [18946/20000], Training Loss: 0.0249\n",
            "Epoch [18947/20000], Training Loss: 0.0253\n",
            "Epoch [18948/20000], Training Loss: 0.0284\n",
            "Epoch [18949/20000], Training Loss: 0.0241\n",
            "Epoch [18950/20000], Training Loss: 0.0249\n",
            "Epoch [18951/20000], Training Loss: 0.0254\n",
            "Epoch [18952/20000], Training Loss: 0.0279\n",
            "Epoch [18953/20000], Training Loss: 0.0261\n",
            "Epoch [18954/20000], Training Loss: 0.0238\n",
            "Epoch [18955/20000], Training Loss: 0.0265\n",
            "Epoch [18956/20000], Training Loss: 0.0256\n",
            "Epoch [18957/20000], Training Loss: 0.0263\n",
            "Epoch [18958/20000], Training Loss: 0.0271\n",
            "Epoch [18959/20000], Training Loss: 0.0253\n",
            "Epoch [18960/20000], Training Loss: 0.0271\n",
            "Epoch [18961/20000], Training Loss: 0.0296\n",
            "Epoch [18962/20000], Training Loss: 0.0252\n",
            "Epoch [18963/20000], Training Loss: 0.0256\n",
            "Epoch [18964/20000], Training Loss: 0.0274\n",
            "Epoch [18965/20000], Training Loss: 0.0256\n",
            "Epoch [18966/20000], Training Loss: 0.0255\n",
            "Epoch [18967/20000], Training Loss: 0.0275\n",
            "Epoch [18968/20000], Training Loss: 0.0282\n",
            "Epoch [18969/20000], Training Loss: 0.0250\n",
            "Epoch [18970/20000], Training Loss: 0.0268\n",
            "Epoch [18971/20000], Training Loss: 0.0254\n",
            "Epoch [18972/20000], Training Loss: 0.0258\n",
            "Epoch [18973/20000], Training Loss: 0.0258\n",
            "Epoch [18974/20000], Training Loss: 0.0254\n",
            "Epoch [18975/20000], Training Loss: 0.0280\n",
            "Epoch [18976/20000], Training Loss: 0.0279\n",
            "Epoch [18977/20000], Training Loss: 0.0282\n",
            "Epoch [18978/20000], Training Loss: 0.0276\n",
            "Epoch [18979/20000], Training Loss: 0.0283\n",
            "Epoch [18980/20000], Training Loss: 0.0256\n",
            "Epoch [18981/20000], Training Loss: 0.0264\n",
            "Epoch [18982/20000], Training Loss: 0.0288\n",
            "Epoch [18983/20000], Training Loss: 0.0256\n",
            "Epoch [18984/20000], Training Loss: 0.0259\n",
            "Epoch [18985/20000], Training Loss: 0.0254\n",
            "Epoch [18986/20000], Training Loss: 0.0260\n",
            "Epoch [18987/20000], Training Loss: 0.0254\n",
            "Epoch [18988/20000], Training Loss: 0.0280\n",
            "Epoch [18989/20000], Training Loss: 0.0269\n",
            "Epoch [18990/20000], Training Loss: 0.0268\n",
            "Epoch [18991/20000], Training Loss: 0.0267\n",
            "Epoch [18992/20000], Training Loss: 0.0275\n",
            "Epoch [18993/20000], Training Loss: 0.0281\n",
            "Epoch [18994/20000], Training Loss: 0.0247\n",
            "Epoch [18995/20000], Training Loss: 0.0263\n",
            "Epoch [18996/20000], Training Loss: 0.0271\n",
            "Epoch [18997/20000], Training Loss: 0.0263\n",
            "Epoch [18998/20000], Training Loss: 0.0269\n",
            "Epoch [18999/20000], Training Loss: 0.0260\n",
            "Epoch [19000/20000], Training Loss: 0.0250\n",
            "Epoch [19001/20000], Training Loss: 0.0260\n",
            "Epoch [19002/20000], Training Loss: 0.0259\n",
            "Epoch [19003/20000], Training Loss: 0.0269\n",
            "Epoch [19004/20000], Training Loss: 0.0255\n",
            "Epoch [19005/20000], Training Loss: 0.0268\n",
            "Epoch [19006/20000], Training Loss: 0.0257\n",
            "Epoch [19007/20000], Training Loss: 0.0269\n",
            "Epoch [19008/20000], Training Loss: 0.0265\n",
            "Epoch [19009/20000], Training Loss: 0.0297\n",
            "Epoch [19010/20000], Training Loss: 0.0275\n",
            "Epoch [19011/20000], Training Loss: 0.0279\n",
            "Epoch [19012/20000], Training Loss: 0.0251\n",
            "Epoch [19013/20000], Training Loss: 0.0240\n",
            "Epoch [19014/20000], Training Loss: 0.0267\n",
            "Epoch [19015/20000], Training Loss: 0.0285\n",
            "Epoch [19016/20000], Training Loss: 0.0243\n",
            "Epoch [19017/20000], Training Loss: 0.0263\n",
            "Epoch [19018/20000], Training Loss: 0.0252\n",
            "Epoch [19019/20000], Training Loss: 0.0264\n",
            "Epoch [19020/20000], Training Loss: 0.0252\n",
            "Epoch [19021/20000], Training Loss: 0.0279\n",
            "Epoch [19022/20000], Training Loss: 0.0272\n",
            "Epoch [19023/20000], Training Loss: 0.0289\n",
            "Epoch [19024/20000], Training Loss: 0.0266\n",
            "Epoch [19025/20000], Training Loss: 0.0255\n",
            "Epoch [19026/20000], Training Loss: 0.0272\n",
            "Epoch [19027/20000], Training Loss: 0.0265\n",
            "Epoch [19028/20000], Training Loss: 0.0248\n",
            "Epoch [19029/20000], Training Loss: 0.0267\n",
            "Epoch [19030/20000], Training Loss: 0.0273\n",
            "Epoch [19031/20000], Training Loss: 0.0266\n",
            "Epoch [19032/20000], Training Loss: 0.0253\n",
            "Epoch [19033/20000], Training Loss: 0.0259\n",
            "Epoch [19034/20000], Training Loss: 0.0274\n",
            "Epoch [19035/20000], Training Loss: 0.0259\n",
            "Epoch [19036/20000], Training Loss: 0.0270\n",
            "Epoch [19037/20000], Training Loss: 0.0272\n",
            "Epoch [19038/20000], Training Loss: 0.0264\n",
            "Epoch [19039/20000], Training Loss: 0.0276\n",
            "Epoch [19040/20000], Training Loss: 0.0277\n",
            "Epoch [19041/20000], Training Loss: 0.0260\n",
            "Epoch [19042/20000], Training Loss: 0.0259\n",
            "Epoch [19043/20000], Training Loss: 0.0267\n",
            "Epoch [19044/20000], Training Loss: 0.0287\n",
            "Epoch [19045/20000], Training Loss: 0.0262\n",
            "Epoch [19046/20000], Training Loss: 0.0270\n",
            "Epoch [19047/20000], Training Loss: 0.0279\n",
            "Epoch [19048/20000], Training Loss: 0.0250\n",
            "Epoch [19049/20000], Training Loss: 0.0268\n",
            "Epoch [19050/20000], Training Loss: 0.0249\n",
            "Epoch [19051/20000], Training Loss: 0.0267\n",
            "Epoch [19052/20000], Training Loss: 0.0264\n",
            "Epoch [19053/20000], Training Loss: 0.0279\n",
            "Epoch [19054/20000], Training Loss: 0.0274\n",
            "Epoch [19055/20000], Training Loss: 0.0265\n",
            "Epoch [19056/20000], Training Loss: 0.0282\n",
            "Epoch [19057/20000], Training Loss: 0.0264\n",
            "Epoch [19058/20000], Training Loss: 0.0267\n",
            "Epoch [19059/20000], Training Loss: 0.0266\n",
            "Epoch [19060/20000], Training Loss: 0.0253\n",
            "Epoch [19061/20000], Training Loss: 0.0262\n",
            "Epoch [19062/20000], Training Loss: 0.0253\n",
            "Epoch [19063/20000], Training Loss: 0.0251\n",
            "Epoch [19064/20000], Training Loss: 0.0262\n",
            "Epoch [19065/20000], Training Loss: 0.0257\n",
            "Epoch [19066/20000], Training Loss: 0.0287\n",
            "Epoch [19067/20000], Training Loss: 0.0269\n",
            "Epoch [19068/20000], Training Loss: 0.0256\n",
            "Epoch [19069/20000], Training Loss: 0.0270\n",
            "Epoch [19070/20000], Training Loss: 0.0253\n",
            "Epoch [19071/20000], Training Loss: 0.0272\n",
            "Epoch [19072/20000], Training Loss: 0.0253\n",
            "Epoch [19073/20000], Training Loss: 0.0260\n",
            "Epoch [19074/20000], Training Loss: 0.0266\n",
            "Epoch [19075/20000], Training Loss: 0.0271\n",
            "Epoch [19076/20000], Training Loss: 0.0247\n",
            "Epoch [19077/20000], Training Loss: 0.0259\n",
            "Epoch [19078/20000], Training Loss: 0.0283\n",
            "Epoch [19079/20000], Training Loss: 0.0256\n",
            "Epoch [19080/20000], Training Loss: 0.0254\n",
            "Epoch [19081/20000], Training Loss: 0.0287\n",
            "Epoch [19082/20000], Training Loss: 0.0249\n",
            "Epoch [19083/20000], Training Loss: 0.0251\n",
            "Epoch [19084/20000], Training Loss: 0.0274\n",
            "Epoch [19085/20000], Training Loss: 0.0259\n",
            "Epoch [19086/20000], Training Loss: 0.0265\n",
            "Epoch [19087/20000], Training Loss: 0.0246\n",
            "Epoch [19088/20000], Training Loss: 0.0246\n",
            "Epoch [19089/20000], Training Loss: 0.0267\n",
            "Epoch [19090/20000], Training Loss: 0.0268\n",
            "Epoch [19091/20000], Training Loss: 0.0255\n",
            "Epoch [19092/20000], Training Loss: 0.0271\n",
            "Epoch [19093/20000], Training Loss: 0.0275\n",
            "Epoch [19094/20000], Training Loss: 0.0281\n",
            "Epoch [19095/20000], Training Loss: 0.0278\n",
            "Epoch [19096/20000], Training Loss: 0.0256\n",
            "Epoch [19097/20000], Training Loss: 0.0245\n",
            "Epoch [19098/20000], Training Loss: 0.0258\n",
            "Epoch [19099/20000], Training Loss: 0.0269\n",
            "Epoch [19100/20000], Training Loss: 0.0252\n",
            "Epoch [19101/20000], Training Loss: 0.0262\n",
            "Epoch [19102/20000], Training Loss: 0.0261\n",
            "Epoch [19103/20000], Training Loss: 0.0280\n",
            "Epoch [19104/20000], Training Loss: 0.0268\n",
            "Epoch [19105/20000], Training Loss: 0.0251\n",
            "Epoch [19106/20000], Training Loss: 0.0282\n",
            "Epoch [19107/20000], Training Loss: 0.0278\n",
            "Epoch [19108/20000], Training Loss: 0.0242\n",
            "Epoch [19109/20000], Training Loss: 0.0255\n",
            "Epoch [19110/20000], Training Loss: 0.0282\n",
            "Epoch [19111/20000], Training Loss: 0.0270\n",
            "Epoch [19112/20000], Training Loss: 0.0265\n",
            "Epoch [19113/20000], Training Loss: 0.0282\n",
            "Epoch [19114/20000], Training Loss: 0.0247\n",
            "Epoch [19115/20000], Training Loss: 0.0265\n",
            "Epoch [19116/20000], Training Loss: 0.0253\n",
            "Epoch [19117/20000], Training Loss: 0.0269\n",
            "Epoch [19118/20000], Training Loss: 0.0251\n",
            "Epoch [19119/20000], Training Loss: 0.0264\n",
            "Epoch [19120/20000], Training Loss: 0.0273\n",
            "Epoch [19121/20000], Training Loss: 0.0266\n",
            "Epoch [19122/20000], Training Loss: 0.0254\n",
            "Epoch [19123/20000], Training Loss: 0.0282\n",
            "Epoch [19124/20000], Training Loss: 0.0269\n",
            "Epoch [19125/20000], Training Loss: 0.0277\n",
            "Epoch [19126/20000], Training Loss: 0.0255\n",
            "Epoch [19127/20000], Training Loss: 0.0262\n",
            "Epoch [19128/20000], Training Loss: 0.0269\n",
            "Epoch [19129/20000], Training Loss: 0.0249\n",
            "Epoch [19130/20000], Training Loss: 0.0287\n",
            "Epoch [19131/20000], Training Loss: 0.0286\n",
            "Epoch [19132/20000], Training Loss: 0.0277\n",
            "Epoch [19133/20000], Training Loss: 0.0272\n",
            "Epoch [19134/20000], Training Loss: 0.0253\n",
            "Epoch [19135/20000], Training Loss: 0.0245\n",
            "Epoch [19136/20000], Training Loss: 0.0255\n",
            "Epoch [19137/20000], Training Loss: 0.0255\n",
            "Epoch [19138/20000], Training Loss: 0.0248\n",
            "Epoch [19139/20000], Training Loss: 0.0254\n",
            "Epoch [19140/20000], Training Loss: 0.0269\n",
            "Epoch [19141/20000], Training Loss: 0.0254\n",
            "Epoch [19142/20000], Training Loss: 0.0259\n",
            "Epoch [19143/20000], Training Loss: 0.0259\n",
            "Epoch [19144/20000], Training Loss: 0.0271\n",
            "Epoch [19145/20000], Training Loss: 0.0252\n",
            "Epoch [19146/20000], Training Loss: 0.0266\n",
            "Epoch [19147/20000], Training Loss: 0.0237\n",
            "Epoch [19148/20000], Training Loss: 0.0273\n",
            "Epoch [19149/20000], Training Loss: 0.0264\n",
            "Epoch [19150/20000], Training Loss: 0.0253\n",
            "Epoch [19151/20000], Training Loss: 0.0254\n",
            "Epoch [19152/20000], Training Loss: 0.0261\n",
            "Epoch [19153/20000], Training Loss: 0.0262\n",
            "Epoch [19154/20000], Training Loss: 0.0256\n",
            "Epoch [19155/20000], Training Loss: 0.0256\n",
            "Epoch [19156/20000], Training Loss: 0.0273\n",
            "Epoch [19157/20000], Training Loss: 0.0279\n",
            "Epoch [19158/20000], Training Loss: 0.0244\n",
            "Epoch [19159/20000], Training Loss: 0.0266\n",
            "Epoch [19160/20000], Training Loss: 0.0282\n",
            "Epoch [19161/20000], Training Loss: 0.0286\n",
            "Epoch [19162/20000], Training Loss: 0.0250\n",
            "Epoch [19163/20000], Training Loss: 0.0267\n",
            "Epoch [19164/20000], Training Loss: 0.0248\n",
            "Epoch [19165/20000], Training Loss: 0.0241\n",
            "Epoch [19166/20000], Training Loss: 0.0275\n",
            "Epoch [19167/20000], Training Loss: 0.0250\n",
            "Epoch [19168/20000], Training Loss: 0.0282\n",
            "Epoch [19169/20000], Training Loss: 0.0262\n",
            "Epoch [19170/20000], Training Loss: 0.0269\n",
            "Epoch [19171/20000], Training Loss: 0.0260\n",
            "Epoch [19172/20000], Training Loss: 0.0262\n",
            "Epoch [19173/20000], Training Loss: 0.0268\n",
            "Epoch [19174/20000], Training Loss: 0.0255\n",
            "Epoch [19175/20000], Training Loss: 0.0263\n",
            "Epoch [19176/20000], Training Loss: 0.0265\n",
            "Epoch [19177/20000], Training Loss: 0.0259\n",
            "Epoch [19178/20000], Training Loss: 0.0281\n",
            "Epoch [19179/20000], Training Loss: 0.0269\n",
            "Epoch [19180/20000], Training Loss: 0.0264\n",
            "Epoch [19181/20000], Training Loss: 0.0250\n",
            "Epoch [19182/20000], Training Loss: 0.0285\n",
            "Epoch [19183/20000], Training Loss: 0.0241\n",
            "Epoch [19184/20000], Training Loss: 0.0275\n",
            "Epoch [19185/20000], Training Loss: 0.0276\n",
            "Epoch [19186/20000], Training Loss: 0.0262\n",
            "Epoch [19187/20000], Training Loss: 0.0257\n",
            "Epoch [19188/20000], Training Loss: 0.0265\n",
            "Epoch [19189/20000], Training Loss: 0.0256\n",
            "Epoch [19190/20000], Training Loss: 0.0259\n",
            "Epoch [19191/20000], Training Loss: 0.0269\n",
            "Epoch [19192/20000], Training Loss: 0.0252\n",
            "Epoch [19193/20000], Training Loss: 0.0246\n",
            "Epoch [19194/20000], Training Loss: 0.0258\n",
            "Epoch [19195/20000], Training Loss: 0.0265\n",
            "Epoch [19196/20000], Training Loss: 0.0258\n",
            "Epoch [19197/20000], Training Loss: 0.0278\n",
            "Epoch [19198/20000], Training Loss: 0.0254\n",
            "Epoch [19199/20000], Training Loss: 0.0265\n",
            "Epoch [19200/20000], Training Loss: 0.0256\n",
            "Epoch [19201/20000], Training Loss: 0.0253\n",
            "Epoch [19202/20000], Training Loss: 0.0249\n",
            "Epoch [19203/20000], Training Loss: 0.0282\n",
            "Epoch [19204/20000], Training Loss: 0.0254\n",
            "Epoch [19205/20000], Training Loss: 0.0258\n",
            "Epoch [19206/20000], Training Loss: 0.0259\n",
            "Epoch [19207/20000], Training Loss: 0.0261\n",
            "Epoch [19208/20000], Training Loss: 0.0281\n",
            "Epoch [19209/20000], Training Loss: 0.0255\n",
            "Epoch [19210/20000], Training Loss: 0.0266\n",
            "Epoch [19211/20000], Training Loss: 0.0259\n",
            "Epoch [19212/20000], Training Loss: 0.0262\n",
            "Epoch [19213/20000], Training Loss: 0.0255\n",
            "Epoch [19214/20000], Training Loss: 0.0299\n",
            "Epoch [19215/20000], Training Loss: 0.0275\n",
            "Epoch [19216/20000], Training Loss: 0.0277\n",
            "Epoch [19217/20000], Training Loss: 0.0268\n",
            "Epoch [19218/20000], Training Loss: 0.0253\n",
            "Epoch [19219/20000], Training Loss: 0.0252\n",
            "Epoch [19220/20000], Training Loss: 0.0265\n",
            "Epoch [19221/20000], Training Loss: 0.0260\n",
            "Epoch [19222/20000], Training Loss: 0.0251\n",
            "Epoch [19223/20000], Training Loss: 0.0274\n",
            "Epoch [19224/20000], Training Loss: 0.0256\n",
            "Epoch [19225/20000], Training Loss: 0.0256\n",
            "Epoch [19226/20000], Training Loss: 0.0250\n",
            "Epoch [19227/20000], Training Loss: 0.0269\n",
            "Epoch [19228/20000], Training Loss: 0.0256\n",
            "Epoch [19229/20000], Training Loss: 0.0256\n",
            "Epoch [19230/20000], Training Loss: 0.0290\n",
            "Epoch [19231/20000], Training Loss: 0.0280\n",
            "Epoch [19232/20000], Training Loss: 0.0267\n",
            "Epoch [19233/20000], Training Loss: 0.0260\n",
            "Epoch [19234/20000], Training Loss: 0.0258\n",
            "Epoch [19235/20000], Training Loss: 0.0273\n",
            "Epoch [19236/20000], Training Loss: 0.0284\n",
            "Epoch [19237/20000], Training Loss: 0.0242\n",
            "Epoch [19238/20000], Training Loss: 0.0275\n",
            "Epoch [19239/20000], Training Loss: 0.0275\n",
            "Epoch [19240/20000], Training Loss: 0.0268\n",
            "Epoch [19241/20000], Training Loss: 0.0284\n",
            "Epoch [19242/20000], Training Loss: 0.0249\n",
            "Epoch [19243/20000], Training Loss: 0.0259\n",
            "Epoch [19244/20000], Training Loss: 0.0262\n",
            "Epoch [19245/20000], Training Loss: 0.0274\n",
            "Epoch [19246/20000], Training Loss: 0.0271\n",
            "Epoch [19247/20000], Training Loss: 0.0253\n",
            "Epoch [19248/20000], Training Loss: 0.0262\n",
            "Epoch [19249/20000], Training Loss: 0.0251\n",
            "Epoch [19250/20000], Training Loss: 0.0271\n",
            "Epoch [19251/20000], Training Loss: 0.0264\n",
            "Epoch [19252/20000], Training Loss: 0.0279\n",
            "Epoch [19253/20000], Training Loss: 0.0278\n",
            "Epoch [19254/20000], Training Loss: 0.0276\n",
            "Epoch [19255/20000], Training Loss: 0.0254\n",
            "Epoch [19256/20000], Training Loss: 0.0267\n",
            "Epoch [19257/20000], Training Loss: 0.0248\n",
            "Epoch [19258/20000], Training Loss: 0.0282\n",
            "Epoch [19259/20000], Training Loss: 0.0261\n",
            "Epoch [19260/20000], Training Loss: 0.0243\n",
            "Epoch [19261/20000], Training Loss: 0.0256\n",
            "Epoch [19262/20000], Training Loss: 0.0274\n",
            "Epoch [19263/20000], Training Loss: 0.0272\n",
            "Epoch [19264/20000], Training Loss: 0.0261\n",
            "Epoch [19265/20000], Training Loss: 0.0285\n",
            "Epoch [19266/20000], Training Loss: 0.0259\n",
            "Epoch [19267/20000], Training Loss: 0.0263\n",
            "Epoch [19268/20000], Training Loss: 0.0262\n",
            "Epoch [19269/20000], Training Loss: 0.0252\n",
            "Epoch [19270/20000], Training Loss: 0.0277\n",
            "Epoch [19271/20000], Training Loss: 0.0271\n",
            "Epoch [19272/20000], Training Loss: 0.0279\n",
            "Epoch [19273/20000], Training Loss: 0.0255\n",
            "Epoch [19274/20000], Training Loss: 0.0275\n",
            "Epoch [19275/20000], Training Loss: 0.0267\n",
            "Epoch [19276/20000], Training Loss: 0.0274\n",
            "Epoch [19277/20000], Training Loss: 0.0246\n",
            "Epoch [19278/20000], Training Loss: 0.0278\n",
            "Epoch [19279/20000], Training Loss: 0.0265\n",
            "Epoch [19280/20000], Training Loss: 0.0268\n",
            "Epoch [19281/20000], Training Loss: 0.0266\n",
            "Epoch [19282/20000], Training Loss: 0.0252\n",
            "Epoch [19283/20000], Training Loss: 0.0261\n",
            "Epoch [19284/20000], Training Loss: 0.0262\n",
            "Epoch [19285/20000], Training Loss: 0.0273\n",
            "Epoch [19286/20000], Training Loss: 0.0282\n",
            "Epoch [19287/20000], Training Loss: 0.0272\n",
            "Epoch [19288/20000], Training Loss: 0.0273\n",
            "Epoch [19289/20000], Training Loss: 0.0252\n",
            "Epoch [19290/20000], Training Loss: 0.0262\n",
            "Epoch [19291/20000], Training Loss: 0.0260\n",
            "Epoch [19292/20000], Training Loss: 0.0284\n",
            "Epoch [19293/20000], Training Loss: 0.0274\n",
            "Epoch [19294/20000], Training Loss: 0.0266\n",
            "Epoch [19295/20000], Training Loss: 0.0257\n",
            "Epoch [19296/20000], Training Loss: 0.0270\n",
            "Epoch [19297/20000], Training Loss: 0.0270\n",
            "Epoch [19298/20000], Training Loss: 0.0246\n",
            "Epoch [19299/20000], Training Loss: 0.0246\n",
            "Epoch [19300/20000], Training Loss: 0.0279\n",
            "Epoch [19301/20000], Training Loss: 0.0277\n",
            "Epoch [19302/20000], Training Loss: 0.0260\n",
            "Epoch [19303/20000], Training Loss: 0.0247\n",
            "Epoch [19304/20000], Training Loss: 0.0264\n",
            "Epoch [19305/20000], Training Loss: 0.0273\n",
            "Epoch [19306/20000], Training Loss: 0.0269\n",
            "Epoch [19307/20000], Training Loss: 0.0265\n",
            "Epoch [19308/20000], Training Loss: 0.0268\n",
            "Epoch [19309/20000], Training Loss: 0.0271\n",
            "Epoch [19310/20000], Training Loss: 0.0273\n",
            "Epoch [19311/20000], Training Loss: 0.0250\n",
            "Epoch [19312/20000], Training Loss: 0.0251\n",
            "Epoch [19313/20000], Training Loss: 0.0267\n",
            "Epoch [19314/20000], Training Loss: 0.0254\n",
            "Epoch [19315/20000], Training Loss: 0.0250\n",
            "Epoch [19316/20000], Training Loss: 0.0271\n",
            "Epoch [19317/20000], Training Loss: 0.0261\n",
            "Epoch [19318/20000], Training Loss: 0.0249\n",
            "Epoch [19319/20000], Training Loss: 0.0280\n",
            "Epoch [19320/20000], Training Loss: 0.0276\n",
            "Epoch [19321/20000], Training Loss: 0.0252\n",
            "Epoch [19322/20000], Training Loss: 0.0264\n",
            "Epoch [19323/20000], Training Loss: 0.0276\n",
            "Epoch [19324/20000], Training Loss: 0.0279\n",
            "Epoch [19325/20000], Training Loss: 0.0275\n",
            "Epoch [19326/20000], Training Loss: 0.0255\n",
            "Epoch [19327/20000], Training Loss: 0.0270\n",
            "Epoch [19328/20000], Training Loss: 0.0264\n",
            "Epoch [19329/20000], Training Loss: 0.0257\n",
            "Epoch [19330/20000], Training Loss: 0.0268\n",
            "Epoch [19331/20000], Training Loss: 0.0284\n",
            "Epoch [19332/20000], Training Loss: 0.0283\n",
            "Epoch [19333/20000], Training Loss: 0.0264\n",
            "Epoch [19334/20000], Training Loss: 0.0247\n",
            "Epoch [19335/20000], Training Loss: 0.0257\n",
            "Epoch [19336/20000], Training Loss: 0.0262\n",
            "Epoch [19337/20000], Training Loss: 0.0284\n",
            "Epoch [19338/20000], Training Loss: 0.0273\n",
            "Epoch [19339/20000], Training Loss: 0.0251\n",
            "Epoch [19340/20000], Training Loss: 0.0289\n",
            "Epoch [19341/20000], Training Loss: 0.0275\n",
            "Epoch [19342/20000], Training Loss: 0.0264\n",
            "Epoch [19343/20000], Training Loss: 0.0256\n",
            "Epoch [19344/20000], Training Loss: 0.0265\n",
            "Epoch [19345/20000], Training Loss: 0.0270\n",
            "Epoch [19346/20000], Training Loss: 0.0270\n",
            "Epoch [19347/20000], Training Loss: 0.0265\n",
            "Epoch [19348/20000], Training Loss: 0.0264\n",
            "Epoch [19349/20000], Training Loss: 0.0255\n",
            "Epoch [19350/20000], Training Loss: 0.0274\n",
            "Epoch [19351/20000], Training Loss: 0.0259\n",
            "Epoch [19352/20000], Training Loss: 0.0249\n",
            "Epoch [19353/20000], Training Loss: 0.0260\n",
            "Epoch [19354/20000], Training Loss: 0.0262\n",
            "Epoch [19355/20000], Training Loss: 0.0268\n",
            "Epoch [19356/20000], Training Loss: 0.0272\n",
            "Epoch [19357/20000], Training Loss: 0.0267\n",
            "Epoch [19358/20000], Training Loss: 0.0237\n",
            "Epoch [19359/20000], Training Loss: 0.0285\n",
            "Epoch [19360/20000], Training Loss: 0.0251\n",
            "Epoch [19361/20000], Training Loss: 0.0273\n",
            "Epoch [19362/20000], Training Loss: 0.0255\n",
            "Epoch [19363/20000], Training Loss: 0.0263\n",
            "Epoch [19364/20000], Training Loss: 0.0261\n",
            "Epoch [19365/20000], Training Loss: 0.0269\n",
            "Epoch [19366/20000], Training Loss: 0.0269\n",
            "Epoch [19367/20000], Training Loss: 0.0279\n",
            "Epoch [19368/20000], Training Loss: 0.0256\n",
            "Epoch [19369/20000], Training Loss: 0.0267\n",
            "Epoch [19370/20000], Training Loss: 0.0239\n",
            "Epoch [19371/20000], Training Loss: 0.0276\n",
            "Epoch [19372/20000], Training Loss: 0.0266\n",
            "Epoch [19373/20000], Training Loss: 0.0268\n",
            "Epoch [19374/20000], Training Loss: 0.0254\n",
            "Epoch [19375/20000], Training Loss: 0.0251\n",
            "Epoch [19376/20000], Training Loss: 0.0268\n",
            "Epoch [19377/20000], Training Loss: 0.0273\n",
            "Epoch [19378/20000], Training Loss: 0.0286\n",
            "Epoch [19379/20000], Training Loss: 0.0263\n",
            "Epoch [19380/20000], Training Loss: 0.0270\n",
            "Epoch [19381/20000], Training Loss: 0.0269\n",
            "Epoch [19382/20000], Training Loss: 0.0257\n",
            "Epoch [19383/20000], Training Loss: 0.0263\n",
            "Epoch [19384/20000], Training Loss: 0.0265\n",
            "Epoch [19385/20000], Training Loss: 0.0276\n",
            "Epoch [19386/20000], Training Loss: 0.0275\n",
            "Epoch [19387/20000], Training Loss: 0.0260\n",
            "Epoch [19388/20000], Training Loss: 0.0245\n",
            "Epoch [19389/20000], Training Loss: 0.0273\n",
            "Epoch [19390/20000], Training Loss: 0.0262\n",
            "Epoch [19391/20000], Training Loss: 0.0279\n",
            "Epoch [19392/20000], Training Loss: 0.0274\n",
            "Epoch [19393/20000], Training Loss: 0.0266\n",
            "Epoch [19394/20000], Training Loss: 0.0259\n",
            "Epoch [19395/20000], Training Loss: 0.0243\n",
            "Epoch [19396/20000], Training Loss: 0.0253\n",
            "Epoch [19397/20000], Training Loss: 0.0265\n",
            "Epoch [19398/20000], Training Loss: 0.0240\n",
            "Epoch [19399/20000], Training Loss: 0.0247\n",
            "Epoch [19400/20000], Training Loss: 0.0250\n",
            "Epoch [19401/20000], Training Loss: 0.0274\n",
            "Epoch [19402/20000], Training Loss: 0.0261\n",
            "Epoch [19403/20000], Training Loss: 0.0284\n",
            "Epoch [19404/20000], Training Loss: 0.0274\n",
            "Epoch [19405/20000], Training Loss: 0.0251\n",
            "Epoch [19406/20000], Training Loss: 0.0263\n",
            "Epoch [19407/20000], Training Loss: 0.0260\n",
            "Epoch [19408/20000], Training Loss: 0.0255\n",
            "Epoch [19409/20000], Training Loss: 0.0274\n",
            "Epoch [19410/20000], Training Loss: 0.0271\n",
            "Epoch [19411/20000], Training Loss: 0.0282\n",
            "Epoch [19412/20000], Training Loss: 0.0264\n",
            "Epoch [19413/20000], Training Loss: 0.0255\n",
            "Epoch [19414/20000], Training Loss: 0.0260\n",
            "Epoch [19415/20000], Training Loss: 0.0279\n",
            "Epoch [19416/20000], Training Loss: 0.0263\n",
            "Epoch [19417/20000], Training Loss: 0.0269\n",
            "Epoch [19418/20000], Training Loss: 0.0256\n",
            "Epoch [19419/20000], Training Loss: 0.0278\n",
            "Epoch [19420/20000], Training Loss: 0.0257\n",
            "Epoch [19421/20000], Training Loss: 0.0264\n",
            "Epoch [19422/20000], Training Loss: 0.0252\n",
            "Epoch [19423/20000], Training Loss: 0.0252\n",
            "Epoch [19424/20000], Training Loss: 0.0269\n",
            "Epoch [19425/20000], Training Loss: 0.0279\n",
            "Epoch [19426/20000], Training Loss: 0.0250\n",
            "Epoch [19427/20000], Training Loss: 0.0277\n",
            "Epoch [19428/20000], Training Loss: 0.0282\n",
            "Epoch [19429/20000], Training Loss: 0.0267\n",
            "Epoch [19430/20000], Training Loss: 0.0258\n",
            "Epoch [19431/20000], Training Loss: 0.0267\n",
            "Epoch [19432/20000], Training Loss: 0.0289\n",
            "Epoch [19433/20000], Training Loss: 0.0258\n",
            "Epoch [19434/20000], Training Loss: 0.0251\n",
            "Epoch [19435/20000], Training Loss: 0.0265\n",
            "Epoch [19436/20000], Training Loss: 0.0271\n",
            "Epoch [19437/20000], Training Loss: 0.0273\n",
            "Epoch [19438/20000], Training Loss: 0.0251\n",
            "Epoch [19439/20000], Training Loss: 0.0271\n",
            "Epoch [19440/20000], Training Loss: 0.0260\n",
            "Epoch [19441/20000], Training Loss: 0.0270\n",
            "Epoch [19442/20000], Training Loss: 0.0256\n",
            "Epoch [19443/20000], Training Loss: 0.0262\n",
            "Epoch [19444/20000], Training Loss: 0.0268\n",
            "Epoch [19445/20000], Training Loss: 0.0252\n",
            "Epoch [19446/20000], Training Loss: 0.0263\n",
            "Epoch [19447/20000], Training Loss: 0.0269\n",
            "Epoch [19448/20000], Training Loss: 0.0251\n",
            "Epoch [19449/20000], Training Loss: 0.0253\n",
            "Epoch [19450/20000], Training Loss: 0.0270\n",
            "Epoch [19451/20000], Training Loss: 0.0263\n",
            "Epoch [19452/20000], Training Loss: 0.0244\n",
            "Epoch [19453/20000], Training Loss: 0.0267\n",
            "Epoch [19454/20000], Training Loss: 0.0253\n",
            "Epoch [19455/20000], Training Loss: 0.0261\n",
            "Epoch [19456/20000], Training Loss: 0.0260\n",
            "Epoch [19457/20000], Training Loss: 0.0264\n",
            "Epoch [19458/20000], Training Loss: 0.0262\n",
            "Epoch [19459/20000], Training Loss: 0.0258\n",
            "Epoch [19460/20000], Training Loss: 0.0257\n",
            "Epoch [19461/20000], Training Loss: 0.0247\n",
            "Epoch [19462/20000], Training Loss: 0.0267\n",
            "Epoch [19463/20000], Training Loss: 0.0262\n",
            "Epoch [19464/20000], Training Loss: 0.0278\n",
            "Epoch [19465/20000], Training Loss: 0.0249\n",
            "Epoch [19466/20000], Training Loss: 0.0292\n",
            "Epoch [19467/20000], Training Loss: 0.0270\n",
            "Epoch [19468/20000], Training Loss: 0.0266\n",
            "Epoch [19469/20000], Training Loss: 0.0270\n",
            "Epoch [19470/20000], Training Loss: 0.0248\n",
            "Epoch [19471/20000], Training Loss: 0.0246\n",
            "Epoch [19472/20000], Training Loss: 0.0272\n",
            "Epoch [19473/20000], Training Loss: 0.0251\n",
            "Epoch [19474/20000], Training Loss: 0.0286\n",
            "Epoch [19475/20000], Training Loss: 0.0254\n",
            "Epoch [19476/20000], Training Loss: 0.0260\n",
            "Epoch [19477/20000], Training Loss: 0.0281\n",
            "Epoch [19478/20000], Training Loss: 0.0255\n",
            "Epoch [19479/20000], Training Loss: 0.0277\n",
            "Epoch [19480/20000], Training Loss: 0.0269\n",
            "Epoch [19481/20000], Training Loss: 0.0266\n",
            "Epoch [19482/20000], Training Loss: 0.0251\n",
            "Epoch [19483/20000], Training Loss: 0.0264\n",
            "Epoch [19484/20000], Training Loss: 0.0243\n",
            "Epoch [19485/20000], Training Loss: 0.0266\n",
            "Epoch [19486/20000], Training Loss: 0.0299\n",
            "Epoch [19487/20000], Training Loss: 0.0263\n",
            "Epoch [19488/20000], Training Loss: 0.0269\n",
            "Epoch [19489/20000], Training Loss: 0.0261\n",
            "Epoch [19490/20000], Training Loss: 0.0252\n",
            "Epoch [19491/20000], Training Loss: 0.0270\n",
            "Epoch [19492/20000], Training Loss: 0.0259\n",
            "Epoch [19493/20000], Training Loss: 0.0273\n",
            "Epoch [19494/20000], Training Loss: 0.0269\n",
            "Epoch [19495/20000], Training Loss: 0.0267\n",
            "Epoch [19496/20000], Training Loss: 0.0261\n",
            "Epoch [19497/20000], Training Loss: 0.0253\n",
            "Epoch [19498/20000], Training Loss: 0.0273\n",
            "Epoch [19499/20000], Training Loss: 0.0289\n",
            "Epoch [19500/20000], Training Loss: 0.0283\n",
            "Epoch [19501/20000], Training Loss: 0.0265\n",
            "Epoch [19502/20000], Training Loss: 0.0279\n",
            "Epoch [19503/20000], Training Loss: 0.0265\n",
            "Epoch [19504/20000], Training Loss: 0.0251\n",
            "Epoch [19505/20000], Training Loss: 0.0273\n",
            "Epoch [19506/20000], Training Loss: 0.0239\n",
            "Epoch [19507/20000], Training Loss: 0.0252\n",
            "Epoch [19508/20000], Training Loss: 0.0254\n",
            "Epoch [19509/20000], Training Loss: 0.0257\n",
            "Epoch [19510/20000], Training Loss: 0.0244\n",
            "Epoch [19511/20000], Training Loss: 0.0245\n",
            "Epoch [19512/20000], Training Loss: 0.0261\n",
            "Epoch [19513/20000], Training Loss: 0.0274\n",
            "Epoch [19514/20000], Training Loss: 0.0260\n",
            "Epoch [19515/20000], Training Loss: 0.0269\n",
            "Epoch [19516/20000], Training Loss: 0.0249\n",
            "Epoch [19517/20000], Training Loss: 0.0258\n",
            "Epoch [19518/20000], Training Loss: 0.0270\n",
            "Epoch [19519/20000], Training Loss: 0.0272\n",
            "Epoch [19520/20000], Training Loss: 0.0275\n",
            "Epoch [19521/20000], Training Loss: 0.0263\n",
            "Epoch [19522/20000], Training Loss: 0.0267\n",
            "Epoch [19523/20000], Training Loss: 0.0269\n",
            "Epoch [19524/20000], Training Loss: 0.0285\n",
            "Epoch [19525/20000], Training Loss: 0.0276\n",
            "Epoch [19526/20000], Training Loss: 0.0243\n",
            "Epoch [19527/20000], Training Loss: 0.0251\n",
            "Epoch [19528/20000], Training Loss: 0.0273\n",
            "Epoch [19529/20000], Training Loss: 0.0253\n",
            "Epoch [19530/20000], Training Loss: 0.0255\n",
            "Epoch [19531/20000], Training Loss: 0.0262\n",
            "Epoch [19532/20000], Training Loss: 0.0264\n",
            "Epoch [19533/20000], Training Loss: 0.0271\n",
            "Epoch [19534/20000], Training Loss: 0.0252\n",
            "Epoch [19535/20000], Training Loss: 0.0270\n",
            "Epoch [19536/20000], Training Loss: 0.0249\n",
            "Epoch [19537/20000], Training Loss: 0.0256\n",
            "Epoch [19538/20000], Training Loss: 0.0265\n",
            "Epoch [19539/20000], Training Loss: 0.0251\n",
            "Epoch [19540/20000], Training Loss: 0.0279\n",
            "Epoch [19541/20000], Training Loss: 0.0257\n",
            "Epoch [19542/20000], Training Loss: 0.0260\n",
            "Epoch [19543/20000], Training Loss: 0.0274\n",
            "Epoch [19544/20000], Training Loss: 0.0263\n",
            "Epoch [19545/20000], Training Loss: 0.0278\n",
            "Epoch [19546/20000], Training Loss: 0.0267\n",
            "Epoch [19547/20000], Training Loss: 0.0269\n",
            "Epoch [19548/20000], Training Loss: 0.0245\n",
            "Epoch [19549/20000], Training Loss: 0.0282\n",
            "Epoch [19550/20000], Training Loss: 0.0255\n",
            "Epoch [19551/20000], Training Loss: 0.0287\n",
            "Epoch [19552/20000], Training Loss: 0.0275\n",
            "Epoch [19553/20000], Training Loss: 0.0251\n",
            "Epoch [19554/20000], Training Loss: 0.0274\n",
            "Epoch [19555/20000], Training Loss: 0.0254\n",
            "Epoch [19556/20000], Training Loss: 0.0256\n",
            "Epoch [19557/20000], Training Loss: 0.0276\n",
            "Epoch [19558/20000], Training Loss: 0.0280\n",
            "Epoch [19559/20000], Training Loss: 0.0255\n",
            "Epoch [19560/20000], Training Loss: 0.0285\n",
            "Epoch [19561/20000], Training Loss: 0.0273\n",
            "Epoch [19562/20000], Training Loss: 0.0266\n",
            "Epoch [19563/20000], Training Loss: 0.0271\n",
            "Epoch [19564/20000], Training Loss: 0.0265\n",
            "Epoch [19565/20000], Training Loss: 0.0253\n",
            "Epoch [19566/20000], Training Loss: 0.0271\n",
            "Epoch [19567/20000], Training Loss: 0.0264\n",
            "Epoch [19568/20000], Training Loss: 0.0254\n",
            "Epoch [19569/20000], Training Loss: 0.0244\n",
            "Epoch [19570/20000], Training Loss: 0.0264\n",
            "Epoch [19571/20000], Training Loss: 0.0265\n",
            "Epoch [19572/20000], Training Loss: 0.0246\n",
            "Epoch [19573/20000], Training Loss: 0.0263\n",
            "Epoch [19574/20000], Training Loss: 0.0245\n",
            "Epoch [19575/20000], Training Loss: 0.0278\n",
            "Epoch [19576/20000], Training Loss: 0.0262\n",
            "Epoch [19577/20000], Training Loss: 0.0245\n",
            "Epoch [19578/20000], Training Loss: 0.0273\n",
            "Epoch [19579/20000], Training Loss: 0.0264\n",
            "Epoch [19580/20000], Training Loss: 0.0258\n",
            "Epoch [19581/20000], Training Loss: 0.0282\n",
            "Epoch [19582/20000], Training Loss: 0.0270\n",
            "Epoch [19583/20000], Training Loss: 0.0270\n",
            "Epoch [19584/20000], Training Loss: 0.0269\n",
            "Epoch [19585/20000], Training Loss: 0.0273\n",
            "Epoch [19586/20000], Training Loss: 0.0272\n",
            "Epoch [19587/20000], Training Loss: 0.0276\n",
            "Epoch [19588/20000], Training Loss: 0.0250\n",
            "Epoch [19589/20000], Training Loss: 0.0261\n",
            "Epoch [19590/20000], Training Loss: 0.0267\n",
            "Epoch [19591/20000], Training Loss: 0.0242\n",
            "Epoch [19592/20000], Training Loss: 0.0275\n",
            "Epoch [19593/20000], Training Loss: 0.0277\n",
            "Epoch [19594/20000], Training Loss: 0.0280\n",
            "Epoch [19595/20000], Training Loss: 0.0267\n",
            "Epoch [19596/20000], Training Loss: 0.0266\n",
            "Epoch [19597/20000], Training Loss: 0.0269\n",
            "Epoch [19598/20000], Training Loss: 0.0263\n",
            "Epoch [19599/20000], Training Loss: 0.0288\n",
            "Epoch [19600/20000], Training Loss: 0.0244\n",
            "Epoch [19601/20000], Training Loss: 0.0254\n",
            "Epoch [19602/20000], Training Loss: 0.0272\n",
            "Epoch [19603/20000], Training Loss: 0.0279\n",
            "Epoch [19604/20000], Training Loss: 0.0262\n",
            "Epoch [19605/20000], Training Loss: 0.0244\n",
            "Epoch [19606/20000], Training Loss: 0.0243\n",
            "Epoch [19607/20000], Training Loss: 0.0251\n",
            "Epoch [19608/20000], Training Loss: 0.0266\n",
            "Epoch [19609/20000], Training Loss: 0.0260\n",
            "Epoch [19610/20000], Training Loss: 0.0259\n",
            "Epoch [19611/20000], Training Loss: 0.0263\n",
            "Epoch [19612/20000], Training Loss: 0.0275\n",
            "Epoch [19613/20000], Training Loss: 0.0271\n",
            "Epoch [19614/20000], Training Loss: 0.0281\n",
            "Epoch [19615/20000], Training Loss: 0.0274\n",
            "Epoch [19616/20000], Training Loss: 0.0269\n",
            "Epoch [19617/20000], Training Loss: 0.0258\n",
            "Epoch [19618/20000], Training Loss: 0.0273\n",
            "Epoch [19619/20000], Training Loss: 0.0260\n",
            "Epoch [19620/20000], Training Loss: 0.0249\n",
            "Epoch [19621/20000], Training Loss: 0.0274\n",
            "Epoch [19622/20000], Training Loss: 0.0255\n",
            "Epoch [19623/20000], Training Loss: 0.0274\n",
            "Epoch [19624/20000], Training Loss: 0.0279\n",
            "Epoch [19625/20000], Training Loss: 0.0271\n",
            "Epoch [19626/20000], Training Loss: 0.0272\n",
            "Epoch [19627/20000], Training Loss: 0.0267\n",
            "Epoch [19628/20000], Training Loss: 0.0277\n",
            "Epoch [19629/20000], Training Loss: 0.0279\n",
            "Epoch [19630/20000], Training Loss: 0.0261\n",
            "Epoch [19631/20000], Training Loss: 0.0299\n",
            "Epoch [19632/20000], Training Loss: 0.0258\n",
            "Epoch [19633/20000], Training Loss: 0.0249\n",
            "Epoch [19634/20000], Training Loss: 0.0246\n",
            "Epoch [19635/20000], Training Loss: 0.0286\n",
            "Epoch [19636/20000], Training Loss: 0.0251\n",
            "Epoch [19637/20000], Training Loss: 0.0271\n",
            "Epoch [19638/20000], Training Loss: 0.0267\n",
            "Epoch [19639/20000], Training Loss: 0.0271\n",
            "Epoch [19640/20000], Training Loss: 0.0262\n",
            "Epoch [19641/20000], Training Loss: 0.0245\n",
            "Epoch [19642/20000], Training Loss: 0.0248\n",
            "Epoch [19643/20000], Training Loss: 0.0279\n",
            "Epoch [19644/20000], Training Loss: 0.0247\n",
            "Epoch [19645/20000], Training Loss: 0.0247\n",
            "Epoch [19646/20000], Training Loss: 0.0288\n",
            "Epoch [19647/20000], Training Loss: 0.0277\n",
            "Epoch [19648/20000], Training Loss: 0.0251\n",
            "Epoch [19649/20000], Training Loss: 0.0262\n",
            "Epoch [19650/20000], Training Loss: 0.0259\n",
            "Epoch [19651/20000], Training Loss: 0.0259\n",
            "Epoch [19652/20000], Training Loss: 0.0262\n",
            "Epoch [19653/20000], Training Loss: 0.0272\n",
            "Epoch [19654/20000], Training Loss: 0.0260\n",
            "Epoch [19655/20000], Training Loss: 0.0268\n",
            "Epoch [19656/20000], Training Loss: 0.0292\n",
            "Epoch [19657/20000], Training Loss: 0.0255\n",
            "Epoch [19658/20000], Training Loss: 0.0259\n",
            "Epoch [19659/20000], Training Loss: 0.0259\n",
            "Epoch [19660/20000], Training Loss: 0.0263\n",
            "Epoch [19661/20000], Training Loss: 0.0297\n",
            "Epoch [19662/20000], Training Loss: 0.0260\n",
            "Epoch [19663/20000], Training Loss: 0.0262\n",
            "Epoch [19664/20000], Training Loss: 0.0272\n",
            "Epoch [19665/20000], Training Loss: 0.0275\n",
            "Epoch [19666/20000], Training Loss: 0.0260\n",
            "Epoch [19667/20000], Training Loss: 0.0239\n",
            "Epoch [19668/20000], Training Loss: 0.0265\n",
            "Epoch [19669/20000], Training Loss: 0.0259\n",
            "Epoch [19670/20000], Training Loss: 0.0268\n",
            "Epoch [19671/20000], Training Loss: 0.0257\n",
            "Epoch [19672/20000], Training Loss: 0.0256\n",
            "Epoch [19673/20000], Training Loss: 0.0267\n",
            "Epoch [19674/20000], Training Loss: 0.0278\n",
            "Epoch [19675/20000], Training Loss: 0.0287\n",
            "Epoch [19676/20000], Training Loss: 0.0239\n",
            "Epoch [19677/20000], Training Loss: 0.0239\n",
            "Epoch [19678/20000], Training Loss: 0.0261\n",
            "Epoch [19679/20000], Training Loss: 0.0256\n",
            "Epoch [19680/20000], Training Loss: 0.0279\n",
            "Epoch [19681/20000], Training Loss: 0.0259\n",
            "Epoch [19682/20000], Training Loss: 0.0254\n",
            "Epoch [19683/20000], Training Loss: 0.0273\n",
            "Epoch [19684/20000], Training Loss: 0.0258\n",
            "Epoch [19685/20000], Training Loss: 0.0283\n",
            "Epoch [19686/20000], Training Loss: 0.0245\n",
            "Epoch [19687/20000], Training Loss: 0.0270\n",
            "Epoch [19688/20000], Training Loss: 0.0256\n",
            "Epoch [19689/20000], Training Loss: 0.0264\n",
            "Epoch [19690/20000], Training Loss: 0.0275\n",
            "Epoch [19691/20000], Training Loss: 0.0263\n",
            "Epoch [19692/20000], Training Loss: 0.0259\n",
            "Epoch [19693/20000], Training Loss: 0.0274\n",
            "Epoch [19694/20000], Training Loss: 0.0263\n",
            "Epoch [19695/20000], Training Loss: 0.0266\n",
            "Epoch [19696/20000], Training Loss: 0.0236\n",
            "Epoch [19697/20000], Training Loss: 0.0250\n",
            "Epoch [19698/20000], Training Loss: 0.0278\n",
            "Epoch [19699/20000], Training Loss: 0.0269\n",
            "Epoch [19700/20000], Training Loss: 0.0242\n",
            "Epoch [19701/20000], Training Loss: 0.0261\n",
            "Epoch [19702/20000], Training Loss: 0.0284\n",
            "Epoch [19703/20000], Training Loss: 0.0275\n",
            "Epoch [19704/20000], Training Loss: 0.0281\n",
            "Epoch [19705/20000], Training Loss: 0.0243\n",
            "Epoch [19706/20000], Training Loss: 0.0284\n",
            "Epoch [19707/20000], Training Loss: 0.0256\n",
            "Epoch [19708/20000], Training Loss: 0.0263\n",
            "Epoch [19709/20000], Training Loss: 0.0280\n",
            "Epoch [19710/20000], Training Loss: 0.0268\n",
            "Epoch [19711/20000], Training Loss: 0.0257\n",
            "Epoch [19712/20000], Training Loss: 0.0248\n",
            "Epoch [19713/20000], Training Loss: 0.0258\n",
            "Epoch [19714/20000], Training Loss: 0.0261\n",
            "Epoch [19715/20000], Training Loss: 0.0267\n",
            "Epoch [19716/20000], Training Loss: 0.0276\n",
            "Epoch [19717/20000], Training Loss: 0.0273\n",
            "Epoch [19718/20000], Training Loss: 0.0261\n",
            "Epoch [19719/20000], Training Loss: 0.0286\n",
            "Epoch [19720/20000], Training Loss: 0.0249\n",
            "Epoch [19721/20000], Training Loss: 0.0275\n",
            "Epoch [19722/20000], Training Loss: 0.0285\n",
            "Epoch [19723/20000], Training Loss: 0.0252\n",
            "Epoch [19724/20000], Training Loss: 0.0241\n",
            "Epoch [19725/20000], Training Loss: 0.0259\n",
            "Epoch [19726/20000], Training Loss: 0.0263\n",
            "Epoch [19727/20000], Training Loss: 0.0260\n",
            "Epoch [19728/20000], Training Loss: 0.0242\n",
            "Epoch [19729/20000], Training Loss: 0.0257\n",
            "Epoch [19730/20000], Training Loss: 0.0275\n",
            "Epoch [19731/20000], Training Loss: 0.0240\n",
            "Epoch [19732/20000], Training Loss: 0.0282\n",
            "Epoch [19733/20000], Training Loss: 0.0261\n",
            "Epoch [19734/20000], Training Loss: 0.0258\n",
            "Epoch [19735/20000], Training Loss: 0.0273\n",
            "Epoch [19736/20000], Training Loss: 0.0257\n",
            "Epoch [19737/20000], Training Loss: 0.0263\n",
            "Epoch [19738/20000], Training Loss: 0.0276\n",
            "Epoch [19739/20000], Training Loss: 0.0275\n",
            "Epoch [19740/20000], Training Loss: 0.0268\n",
            "Epoch [19741/20000], Training Loss: 0.0261\n",
            "Epoch [19742/20000], Training Loss: 0.0263\n",
            "Epoch [19743/20000], Training Loss: 0.0255\n",
            "Epoch [19744/20000], Training Loss: 0.0287\n",
            "Epoch [19745/20000], Training Loss: 0.0249\n",
            "Epoch [19746/20000], Training Loss: 0.0264\n",
            "Epoch [19747/20000], Training Loss: 0.0256\n",
            "Epoch [19748/20000], Training Loss: 0.0264\n",
            "Epoch [19749/20000], Training Loss: 0.0272\n",
            "Epoch [19750/20000], Training Loss: 0.0249\n",
            "Epoch [19751/20000], Training Loss: 0.0292\n",
            "Epoch [19752/20000], Training Loss: 0.0261\n",
            "Epoch [19753/20000], Training Loss: 0.0261\n",
            "Epoch [19754/20000], Training Loss: 0.0251\n",
            "Epoch [19755/20000], Training Loss: 0.0250\n",
            "Epoch [19756/20000], Training Loss: 0.0280\n",
            "Epoch [19757/20000], Training Loss: 0.0244\n",
            "Epoch [19758/20000], Training Loss: 0.0272\n",
            "Epoch [19759/20000], Training Loss: 0.0263\n",
            "Epoch [19760/20000], Training Loss: 0.0263\n",
            "Epoch [19761/20000], Training Loss: 0.0253\n",
            "Epoch [19762/20000], Training Loss: 0.0265\n",
            "Epoch [19763/20000], Training Loss: 0.0263\n",
            "Epoch [19764/20000], Training Loss: 0.0281\n",
            "Epoch [19765/20000], Training Loss: 0.0246\n",
            "Epoch [19766/20000], Training Loss: 0.0276\n",
            "Epoch [19767/20000], Training Loss: 0.0267\n",
            "Epoch [19768/20000], Training Loss: 0.0268\n",
            "Epoch [19769/20000], Training Loss: 0.0273\n",
            "Epoch [19770/20000], Training Loss: 0.0278\n",
            "Epoch [19771/20000], Training Loss: 0.0284\n",
            "Epoch [19772/20000], Training Loss: 0.0257\n",
            "Epoch [19773/20000], Training Loss: 0.0273\n",
            "Epoch [19774/20000], Training Loss: 0.0278\n",
            "Epoch [19775/20000], Training Loss: 0.0249\n",
            "Epoch [19776/20000], Training Loss: 0.0258\n",
            "Epoch [19777/20000], Training Loss: 0.0260\n",
            "Epoch [19778/20000], Training Loss: 0.0253\n",
            "Epoch [19779/20000], Training Loss: 0.0248\n",
            "Epoch [19780/20000], Training Loss: 0.0254\n",
            "Epoch [19781/20000], Training Loss: 0.0268\n",
            "Epoch [19782/20000], Training Loss: 0.0267\n",
            "Epoch [19783/20000], Training Loss: 0.0263\n",
            "Epoch [19784/20000], Training Loss: 0.0256\n",
            "Epoch [19785/20000], Training Loss: 0.0257\n",
            "Epoch [19786/20000], Training Loss: 0.0268\n",
            "Epoch [19787/20000], Training Loss: 0.0274\n",
            "Epoch [19788/20000], Training Loss: 0.0252\n",
            "Epoch [19789/20000], Training Loss: 0.0255\n",
            "Epoch [19790/20000], Training Loss: 0.0281\n",
            "Epoch [19791/20000], Training Loss: 0.0265\n",
            "Epoch [19792/20000], Training Loss: 0.0263\n",
            "Epoch [19793/20000], Training Loss: 0.0257\n",
            "Epoch [19794/20000], Training Loss: 0.0266\n",
            "Epoch [19795/20000], Training Loss: 0.0277\n",
            "Epoch [19796/20000], Training Loss: 0.0275\n",
            "Epoch [19797/20000], Training Loss: 0.0260\n",
            "Epoch [19798/20000], Training Loss: 0.0284\n",
            "Epoch [19799/20000], Training Loss: 0.0264\n",
            "Epoch [19800/20000], Training Loss: 0.0263\n",
            "Epoch [19801/20000], Training Loss: 0.0246\n",
            "Epoch [19802/20000], Training Loss: 0.0281\n",
            "Epoch [19803/20000], Training Loss: 0.0254\n",
            "Epoch [19804/20000], Training Loss: 0.0270\n",
            "Epoch [19805/20000], Training Loss: 0.0255\n",
            "Epoch [19806/20000], Training Loss: 0.0246\n",
            "Epoch [19807/20000], Training Loss: 0.0255\n",
            "Epoch [19808/20000], Training Loss: 0.0261\n",
            "Epoch [19809/20000], Training Loss: 0.0272\n",
            "Epoch [19810/20000], Training Loss: 0.0271\n",
            "Epoch [19811/20000], Training Loss: 0.0248\n",
            "Epoch [19812/20000], Training Loss: 0.0254\n",
            "Epoch [19813/20000], Training Loss: 0.0258\n",
            "Epoch [19814/20000], Training Loss: 0.0264\n",
            "Epoch [19815/20000], Training Loss: 0.0257\n",
            "Epoch [19816/20000], Training Loss: 0.0262\n",
            "Epoch [19817/20000], Training Loss: 0.0263\n",
            "Epoch [19818/20000], Training Loss: 0.0264\n",
            "Epoch [19819/20000], Training Loss: 0.0281\n",
            "Epoch [19820/20000], Training Loss: 0.0256\n",
            "Epoch [19821/20000], Training Loss: 0.0265\n",
            "Epoch [19822/20000], Training Loss: 0.0258\n",
            "Epoch [19823/20000], Training Loss: 0.0289\n",
            "Epoch [19824/20000], Training Loss: 0.0263\n",
            "Epoch [19825/20000], Training Loss: 0.0275\n",
            "Epoch [19826/20000], Training Loss: 0.0252\n",
            "Epoch [19827/20000], Training Loss: 0.0255\n",
            "Epoch [19828/20000], Training Loss: 0.0241\n",
            "Epoch [19829/20000], Training Loss: 0.0272\n",
            "Epoch [19830/20000], Training Loss: 0.0270\n",
            "Epoch [19831/20000], Training Loss: 0.0265\n",
            "Epoch [19832/20000], Training Loss: 0.0257\n",
            "Epoch [19833/20000], Training Loss: 0.0270\n",
            "Epoch [19834/20000], Training Loss: 0.0270\n",
            "Epoch [19835/20000], Training Loss: 0.0243\n",
            "Epoch [19836/20000], Training Loss: 0.0267\n",
            "Epoch [19837/20000], Training Loss: 0.0259\n",
            "Epoch [19838/20000], Training Loss: 0.0286\n",
            "Epoch [19839/20000], Training Loss: 0.0258\n",
            "Epoch [19840/20000], Training Loss: 0.0253\n",
            "Epoch [19841/20000], Training Loss: 0.0254\n",
            "Epoch [19842/20000], Training Loss: 0.0268\n",
            "Epoch [19843/20000], Training Loss: 0.0255\n",
            "Epoch [19844/20000], Training Loss: 0.0270\n",
            "Epoch [19845/20000], Training Loss: 0.0282\n",
            "Epoch [19846/20000], Training Loss: 0.0277\n",
            "Epoch [19847/20000], Training Loss: 0.0263\n",
            "Epoch [19848/20000], Training Loss: 0.0275\n",
            "Epoch [19849/20000], Training Loss: 0.0279\n",
            "Epoch [19850/20000], Training Loss: 0.0281\n",
            "Epoch [19851/20000], Training Loss: 0.0273\n",
            "Epoch [19852/20000], Training Loss: 0.0270\n",
            "Epoch [19853/20000], Training Loss: 0.0244\n",
            "Epoch [19854/20000], Training Loss: 0.0288\n",
            "Epoch [19855/20000], Training Loss: 0.0276\n",
            "Epoch [19856/20000], Training Loss: 0.0266\n",
            "Epoch [19857/20000], Training Loss: 0.0261\n",
            "Epoch [19858/20000], Training Loss: 0.0247\n",
            "Epoch [19859/20000], Training Loss: 0.0255\n",
            "Epoch [19860/20000], Training Loss: 0.0261\n",
            "Epoch [19861/20000], Training Loss: 0.0249\n",
            "Epoch [19862/20000], Training Loss: 0.0259\n",
            "Epoch [19863/20000], Training Loss: 0.0284\n",
            "Epoch [19864/20000], Training Loss: 0.0276\n",
            "Epoch [19865/20000], Training Loss: 0.0286\n",
            "Epoch [19866/20000], Training Loss: 0.0269\n",
            "Epoch [19867/20000], Training Loss: 0.0263\n",
            "Epoch [19868/20000], Training Loss: 0.0256\n",
            "Epoch [19869/20000], Training Loss: 0.0258\n",
            "Epoch [19870/20000], Training Loss: 0.0271\n",
            "Epoch [19871/20000], Training Loss: 0.0251\n",
            "Epoch [19872/20000], Training Loss: 0.0259\n",
            "Epoch [19873/20000], Training Loss: 0.0257\n",
            "Epoch [19874/20000], Training Loss: 0.0262\n",
            "Epoch [19875/20000], Training Loss: 0.0265\n",
            "Epoch [19876/20000], Training Loss: 0.0282\n",
            "Epoch [19877/20000], Training Loss: 0.0258\n",
            "Epoch [19878/20000], Training Loss: 0.0257\n",
            "Epoch [19879/20000], Training Loss: 0.0278\n",
            "Epoch [19880/20000], Training Loss: 0.0266\n",
            "Epoch [19881/20000], Training Loss: 0.0262\n",
            "Epoch [19882/20000], Training Loss: 0.0296\n",
            "Epoch [19883/20000], Training Loss: 0.0262\n",
            "Epoch [19884/20000], Training Loss: 0.0280\n",
            "Epoch [19885/20000], Training Loss: 0.0255\n",
            "Epoch [19886/20000], Training Loss: 0.0277\n",
            "Epoch [19887/20000], Training Loss: 0.0269\n",
            "Epoch [19888/20000], Training Loss: 0.0278\n",
            "Epoch [19889/20000], Training Loss: 0.0254\n",
            "Epoch [19890/20000], Training Loss: 0.0274\n",
            "Epoch [19891/20000], Training Loss: 0.0241\n",
            "Epoch [19892/20000], Training Loss: 0.0258\n",
            "Epoch [19893/20000], Training Loss: 0.0242\n",
            "Epoch [19894/20000], Training Loss: 0.0265\n",
            "Epoch [19895/20000], Training Loss: 0.0261\n",
            "Epoch [19896/20000], Training Loss: 0.0264\n",
            "Epoch [19897/20000], Training Loss: 0.0264\n",
            "Epoch [19898/20000], Training Loss: 0.0246\n",
            "Epoch [19899/20000], Training Loss: 0.0259\n",
            "Epoch [19900/20000], Training Loss: 0.0261\n",
            "Epoch [19901/20000], Training Loss: 0.0262\n",
            "Epoch [19902/20000], Training Loss: 0.0259\n",
            "Epoch [19903/20000], Training Loss: 0.0277\n",
            "Epoch [19904/20000], Training Loss: 0.0246\n",
            "Epoch [19905/20000], Training Loss: 0.0253\n",
            "Epoch [19906/20000], Training Loss: 0.0264\n",
            "Epoch [19907/20000], Training Loss: 0.0261\n",
            "Epoch [19908/20000], Training Loss: 0.0280\n",
            "Epoch [19909/20000], Training Loss: 0.0273\n",
            "Epoch [19910/20000], Training Loss: 0.0274\n",
            "Epoch [19911/20000], Training Loss: 0.0257\n",
            "Epoch [19912/20000], Training Loss: 0.0260\n",
            "Epoch [19913/20000], Training Loss: 0.0271\n",
            "Epoch [19914/20000], Training Loss: 0.0260\n",
            "Epoch [19915/20000], Training Loss: 0.0274\n",
            "Epoch [19916/20000], Training Loss: 0.0275\n",
            "Epoch [19917/20000], Training Loss: 0.0271\n",
            "Epoch [19918/20000], Training Loss: 0.0267\n",
            "Epoch [19919/20000], Training Loss: 0.0263\n",
            "Epoch [19920/20000], Training Loss: 0.0244\n",
            "Epoch [19921/20000], Training Loss: 0.0247\n",
            "Epoch [19922/20000], Training Loss: 0.0281\n",
            "Epoch [19923/20000], Training Loss: 0.0260\n",
            "Epoch [19924/20000], Training Loss: 0.0252\n",
            "Epoch [19925/20000], Training Loss: 0.0263\n",
            "Epoch [19926/20000], Training Loss: 0.0259\n",
            "Epoch [19927/20000], Training Loss: 0.0273\n",
            "Epoch [19928/20000], Training Loss: 0.0267\n",
            "Epoch [19929/20000], Training Loss: 0.0259\n",
            "Epoch [19930/20000], Training Loss: 0.0280\n",
            "Epoch [19931/20000], Training Loss: 0.0250\n",
            "Epoch [19932/20000], Training Loss: 0.0258\n",
            "Epoch [19933/20000], Training Loss: 0.0240\n",
            "Epoch [19934/20000], Training Loss: 0.0283\n",
            "Epoch [19935/20000], Training Loss: 0.0259\n",
            "Epoch [19936/20000], Training Loss: 0.0255\n",
            "Epoch [19937/20000], Training Loss: 0.0258\n",
            "Epoch [19938/20000], Training Loss: 0.0276\n",
            "Epoch [19939/20000], Training Loss: 0.0260\n",
            "Epoch [19940/20000], Training Loss: 0.0244\n",
            "Epoch [19941/20000], Training Loss: 0.0243\n",
            "Epoch [19942/20000], Training Loss: 0.0262\n",
            "Epoch [19943/20000], Training Loss: 0.0247\n",
            "Epoch [19944/20000], Training Loss: 0.0297\n",
            "Epoch [19945/20000], Training Loss: 0.0261\n",
            "Epoch [19946/20000], Training Loss: 0.0277\n",
            "Epoch [19947/20000], Training Loss: 0.0263\n",
            "Epoch [19948/20000], Training Loss: 0.0264\n",
            "Epoch [19949/20000], Training Loss: 0.0276\n",
            "Epoch [19950/20000], Training Loss: 0.0261\n",
            "Epoch [19951/20000], Training Loss: 0.0273\n",
            "Epoch [19952/20000], Training Loss: 0.0291\n",
            "Epoch [19953/20000], Training Loss: 0.0253\n",
            "Epoch [19954/20000], Training Loss: 0.0250\n",
            "Epoch [19955/20000], Training Loss: 0.0271\n",
            "Epoch [19956/20000], Training Loss: 0.0267\n",
            "Epoch [19957/20000], Training Loss: 0.0272\n",
            "Epoch [19958/20000], Training Loss: 0.0269\n",
            "Epoch [19959/20000], Training Loss: 0.0261\n",
            "Epoch [19960/20000], Training Loss: 0.0281\n",
            "Epoch [19961/20000], Training Loss: 0.0288\n",
            "Epoch [19962/20000], Training Loss: 0.0249\n",
            "Epoch [19963/20000], Training Loss: 0.0258\n",
            "Epoch [19964/20000], Training Loss: 0.0274\n",
            "Epoch [19965/20000], Training Loss: 0.0259\n",
            "Epoch [19966/20000], Training Loss: 0.0253\n",
            "Epoch [19967/20000], Training Loss: 0.0261\n",
            "Epoch [19968/20000], Training Loss: 0.0275\n",
            "Epoch [19969/20000], Training Loss: 0.0268\n",
            "Epoch [19970/20000], Training Loss: 0.0255\n",
            "Epoch [19971/20000], Training Loss: 0.0276\n",
            "Epoch [19972/20000], Training Loss: 0.0256\n",
            "Epoch [19973/20000], Training Loss: 0.0290\n",
            "Epoch [19974/20000], Training Loss: 0.0289\n",
            "Epoch [19975/20000], Training Loss: 0.0272\n",
            "Epoch [19976/20000], Training Loss: 0.0257\n",
            "Epoch [19977/20000], Training Loss: 0.0236\n",
            "Epoch [19978/20000], Training Loss: 0.0280\n",
            "Epoch [19979/20000], Training Loss: 0.0279\n",
            "Epoch [19980/20000], Training Loss: 0.0265\n",
            "Epoch [19981/20000], Training Loss: 0.0248\n",
            "Epoch [19982/20000], Training Loss: 0.0264\n",
            "Epoch [19983/20000], Training Loss: 0.0255\n",
            "Epoch [19984/20000], Training Loss: 0.0271\n",
            "Epoch [19985/20000], Training Loss: 0.0259\n",
            "Epoch [19986/20000], Training Loss: 0.0284\n",
            "Epoch [19987/20000], Training Loss: 0.0254\n",
            "Epoch [19988/20000], Training Loss: 0.0264\n",
            "Epoch [19989/20000], Training Loss: 0.0266\n",
            "Epoch [19990/20000], Training Loss: 0.0256\n",
            "Epoch [19991/20000], Training Loss: 0.0275\n",
            "Epoch [19992/20000], Training Loss: 0.0267\n",
            "Epoch [19993/20000], Training Loss: 0.0272\n",
            "Epoch [19994/20000], Training Loss: 0.0276\n",
            "Epoch [19995/20000], Training Loss: 0.0266\n",
            "Epoch [19996/20000], Training Loss: 0.0283\n",
            "Epoch [19997/20000], Training Loss: 0.0252\n",
            "Epoch [19998/20000], Training Loss: 0.0263\n",
            "Epoch [19999/20000], Training Loss: 0.0256\n",
            "Epoch [20000/20000], Training Loss: 0.0269\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Normalize the data\n",
        "train_mean = train_data.mean()\n",
        "train_std = train_data.std()\n",
        "train_data = (train_data - train_mean) / train_std\n",
        "test_data = (test_data - train_mean) / train_std\n",
        "\n",
        "# Define the model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Define the hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20000\n",
        "batch_size = 16\n",
        "\n",
        "# Initialize the model\n",
        "model = LinearModel(len(input_cols), len(output_cols))\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the training loop\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    batch_losses = []\n",
        "    # Shuffle the data\n",
        "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "    # Split the data into batches\n",
        "    for i in range(0, len(train_data), batch_size):\n",
        "        batch_data = train_data[i:i+batch_size]\n",
        "        # Extract the input and output data\n",
        "        x_batch = torch.tensor(batch_data[input_cols].values, dtype=torch.float32)\n",
        "        y_batch = torch.tensor(batch_data[output_cols].values, dtype=torch.float32)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch.view(-1, len(output_cols)))\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_losses.append(loss.item())\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}')\n",
        "\n",
        "# Prepare the input data\n",
        "input_data = np.arange(2015, 2026).reshape(-1, 1)\n",
        "input_data = (input_data - train_mean['Year']) / train_std['Year']\n",
        "input_data = torch.tensor(input_data, dtype=torch.float32)\n",
        "\n",
        "# Predict the output\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output_data = model(input_data)\n",
        "output_data = output_data.detach().numpy()\n",
        "\n",
        "# Convert the output data back to original scale\n",
        "output_data = (output_data * train_std[output_cols].values) + train_mean[output_cols].values\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Plot the predicted output\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(output_cols):\n",
        "    plt.plot(data['Year'].values, data[col].values, label=col)\n",
        "    plt.plot(np.arange(2015, 2026), output_data[:, i], label=col + ' (predicted)', linestyle='--')\n",
        "plt.legend()\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mortality Rates')\n",
        "plt.title('Mortality Rateson Mental Health and substance use disorders in chosen countries (linear)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ilc4xIgXFuSq"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss over Time Linear')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.5210\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_inputs = torch.tensor(test_data[input_cols].values, dtype=torch.float32)\n",
        "    test_outputs = torch.tensor(test_data[output_cols].values, dtype=torch.float32)\n",
        "    test_predictions = model(test_inputs)\n",
        "    test_predictions = test_predictions.squeeze(1)  # remove the extra dimension\n",
        "    test_loss = criterion(test_predictions, test_outputs)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
